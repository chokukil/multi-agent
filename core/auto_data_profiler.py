"""
Automatic Data Profiling System

ìë™ ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ì‹œìŠ¤í…œ
- ì—…ë¡œë“œëœ ë°ì´í„°ì˜ ìë™ ë¶„ì„
- ë°ì´í„° í’ˆì§ˆ í‰ê°€
- êµ¬ì¡° ë° íŒ¨í„´ íƒì§€
- ë°ì´í„° íƒ€ì… ì¶”ë¡ 
- ì´ìƒì¹˜ ë° ëˆ„ë½ê°’ íƒì§€
- í†µê³„ì  íŠ¹ì„± ë¶„ì„
- ì•¡ì…˜ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ

Author: CherryAI Team
Date: 2024-12-30
"""

import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

# Optional imports for enhanced profiling
try:
    from scipy import stats
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False

# Our imports
try:
    from core.enhanced_langfuse_tracer import get_enhanced_tracer
    from core.user_file_tracker import get_user_file_tracker
    CORE_SYSTEMS_AVAILABLE = True
except ImportError:
    CORE_SYSTEMS_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataQuality(Enum):
    """ë°ì´í„° í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"      # 90-100%
    GOOD = "good"               # 80-89%
    FAIR = "fair"               # 60-79%
    POOR = "poor"               # 40-59%
    CRITICAL = "critical"       # 0-39%


class ColumnType(Enum):
    """ì»¬ëŸ¼ íƒ€ì… ë¶„ë¥˜"""
    NUMERIC_CONTINUOUS = "numeric_continuous"
    NUMERIC_DISCRETE = "numeric_discrete"
    CATEGORICAL = "categorical"
    DATETIME = "datetime"
    TEXT = "text"
    BOOLEAN = "boolean"
    MIXED = "mixed"
    UNKNOWN = "unknown"


class DataPattern(Enum):
    """ë°ì´í„° íŒ¨í„´ ìœ í˜•"""
    TIME_SERIES = "time_series"
    HIERARCHICAL = "hierarchical"
    RELATIONAL = "relational"
    TRANSACTIONAL = "transactional"
    EXPERIMENTAL = "experimental"
    SURVEY = "survey"
    LOG_DATA = "log_data"
    MIXED = "mixed"


@dataclass
class ColumnProfile:
    """ì»¬ëŸ¼ í”„ë¡œíŒŒì¼ ì •ë³´"""
    name: str
    dtype: str
    inferred_type: ColumnType
    null_count: int
    null_percentage: float
    unique_count: int
    unique_percentage: float
    
    # í†µê³„ ì •ë³´
    mean: Optional[float] = None
    median: Optional[float] = None
    std: Optional[float] = None
    min_value: Optional[Any] = None
    max_value: Optional[Any] = None
    
    # ë¶„í¬ ì •ë³´
    skewness: Optional[float] = None
    kurtosis: Optional[float] = None
    
    # ë²”ì£¼í˜• ë°ì´í„° ì •ë³´
    top_values: Optional[List[Tuple[Any, int]]] = None
    
    # í’ˆì§ˆ ì§€í‘œ
    quality_score: float = 0.0
    quality_issues: List[str] = None
    
    # ì¶”ì²œì‚¬í•­
    recommendations: List[str] = None


@dataclass 
class DataProfile:
    """ì „ì²´ ë°ì´í„° í”„ë¡œíŒŒì¼"""
    # ê¸°ë³¸ ì •ë³´
    dataset_name: str
    shape: Tuple[int, int]
    memory_usage: float  # MB
    dtypes_summary: Dict[str, int]
    
    # í’ˆì§ˆ ì •ë³´
    overall_quality: DataQuality
    quality_score: float
    
    # ì»¬ëŸ¼ í”„ë¡œíŒŒì¼
    columns: List[ColumnProfile]
    
    # ë°ì´í„° íŒ¨í„´
    detected_patterns: List[DataPattern]
    
    # ìƒê´€ê´€ê³„ ì •ë³´
    correlations: Optional[Dict[str, Any]] = None
    
    # ì¤‘ë³µ ë° ëˆ„ë½ê°’ ì •ë³´
    duplicate_rows: int = 0
    duplicate_percentage: float = 0.0
    total_missing: int = 0
    missing_percentage: float = 0.0
    
    # ì´ìƒì¹˜ ì •ë³´
    outliers_detected: Dict[str, int] = None
    
    # ì¸ì‚¬ì´íŠ¸ ë° ì¶”ì²œì‚¬í•­
    key_insights: List[str] = None
    data_quality_issues: List[str] = None
    recommendations: List[str] = None
    
    # ë©”íƒ€ë°ì´í„°
    profiling_timestamp: str = None
    profiling_duration: float = 0.0


class AutoDataProfiler:
    """
    ìë™ ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ì‹œìŠ¤í…œ
    
    ì—…ë¡œë“œëœ ë°ì´í„°ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ êµ¬ì¡°, í’ˆì§ˆ, íŒ¨í„´ì„ íŒŒì•…
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.enhanced_tracer = None
        self.user_file_tracker = None
        
        # í”„ë¡œíŒŒì¼ë§ ì„¤ì •
        self.max_categorical_values = self.config.get('max_categorical_values', 50)
        self.outlier_threshold = self.config.get('outlier_threshold', 3.0)  # Z-score
        self.correlation_threshold = self.config.get('correlation_threshold', 0.7)
        
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self._initialize_systems()
        
        logger.info("ğŸ” Auto Data Profiler ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_systems(self):
        """í•µì‹¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        if not CORE_SYSTEMS_AVAILABLE:
            logger.warning("âš ï¸ Core systems not available")
            return
        
        try:
            self.enhanced_tracer = get_enhanced_tracer()
            self.user_file_tracker = get_user_file_tracker()
            logger.info("âœ… Enhanced tracking systems activated")
        except Exception as e:
            logger.warning(f"âš ï¸ System initialization failed: {e}")
    
    def profile_data(
        self, 
        data: Union[pd.DataFrame, str, Dict], 
        dataset_name: Optional[str] = None,
        session_id: Optional[str] = None
    ) -> DataProfile:
        """
        ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ìˆ˜í–‰
        
        Args:
            data: ë¶„ì„í•  ë°ì´í„° (DataFrame, íŒŒì¼ ê²½ë¡œ, ë˜ëŠ” dict)
            dataset_name: ë°ì´í„°ì…‹ ì´ë¦„
            session_id: ì„¸ì…˜ ID
            
        Returns:
            DataProfile: ì¢…í•© ë°ì´í„° í”„ë¡œíŒŒì¼
        """
        start_time = datetime.now()
        
        try:
            logger.info(f"ğŸ”„ ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ì‹œì‘: {dataset_name or 'Unknown'}")
            
            # Enhanced Tracking
            if self.enhanced_tracer:
                self.enhanced_tracer.log_data_operation(
                    "data_profiling_start",
                    {"dataset_name": dataset_name, "session_id": session_id},
                    "Starting automatic data profiling"
                )
            
            # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
            df = self._prepare_data(data)
            if df is None or df.empty:
                raise ValueError("Invalid or empty dataset")
            
            # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
            basic_info = self._analyze_basic_info(df, dataset_name)
            
            # ì»¬ëŸ¼ë³„ ìƒì„¸ ë¶„ì„
            column_profiles = self._analyze_columns(df)
            
            # ë°ì´í„° í’ˆì§ˆ í‰ê°€
            quality_info = self._assess_data_quality(df, column_profiles)
            
            # íŒ¨í„´ íƒì§€
            patterns = self._detect_data_patterns(df, column_profiles)
            
            # ìƒê´€ê´€ê³„ ë¶„ì„
            correlations = self._analyze_correlations(df)
            
            # ì´ìƒì¹˜ íƒì§€
            outliers = self._detect_outliers(df)
            
            # ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = self._generate_insights(df, column_profiles, quality_info, patterns)
            
            # ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = self._generate_recommendations(df, column_profiles, quality_info)
            
            # í”„ë¡œíŒŒì¼ ìƒì„±
            profile = DataProfile(
                dataset_name=dataset_name or f"Dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                shape=df.shape,
                memory_usage=df.memory_usage(deep=True).sum() / 1024 / 1024,  # MB
                dtypes_summary=df.dtypes.value_counts().to_dict(),
                overall_quality=quality_info['overall_quality'],
                quality_score=quality_info['quality_score'],
                columns=column_profiles,
                detected_patterns=patterns,
                correlations=correlations,
                duplicate_rows=df.duplicated().sum(),
                duplicate_percentage=(df.duplicated().sum() / len(df)) * 100,
                total_missing=df.isnull().sum().sum(),
                missing_percentage=(df.isnull().sum().sum() / df.size) * 100,
                outliers_detected=outliers,
                key_insights=insights,
                data_quality_issues=quality_info['issues'],
                recommendations=recommendations,
                profiling_timestamp=datetime.now().isoformat(),
                profiling_duration=(datetime.now() - start_time).total_seconds()
            )
            
            # Enhanced Tracking
            if self.enhanced_tracer:
                self.enhanced_tracer.log_data_operation(
                    "data_profiling_complete",
                    {
                        "dataset_name": dataset_name,
                        "quality_score": profile.quality_score,
                        "shape": profile.shape,
                        "duration": profile.profiling_duration
                    },
                    "Data profiling completed successfully"
                )
            
            logger.info(f"âœ… ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {profile.profiling_duration:.2f}ì´ˆ)")
            return profile
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ì‹¤íŒ¨: {e}")
            
            if self.enhanced_tracer:
                self.enhanced_tracer.log_data_operation(
                    "data_profiling_error",
                    {"error": str(e), "dataset_name": dataset_name},
                    "Data profiling failed"
                )
            
            # ê¸°ë³¸ í”„ë¡œíŒŒì¼ ë°˜í™˜
            return self._create_error_profile(str(e), dataset_name)
    
    def _prepare_data(self, data: Union[pd.DataFrame, str, Dict]) -> pd.DataFrame:
        """ë°ì´í„° ì¤€ë¹„ ë° ë¡œë“œ"""
        if isinstance(data, pd.DataFrame):
            return data.copy()
        
        elif isinstance(data, str):
            # íŒŒì¼ ê²½ë¡œì¸ ê²½ìš°
            try:
                if data.endswith('.csv'):
                    return pd.read_csv(data, nrows=10000)  # ìµœëŒ€ 10k í–‰
                elif data.endswith(('.xlsx', '.xls')):
                    return pd.read_excel(data, nrows=10000)
                elif data.endswith('.json'):
                    return pd.read_json(data, lines=True, nrows=10000)
                elif data.endswith('.parquet'):
                    return pd.read_parquet(data).head(10000)
                else:
                    # ê¸°ë³¸ì ìœ¼ë¡œ CSVë¡œ ì‹œë„
                    return pd.read_csv(data, nrows=10000)
            except Exception as e:
                logger.warning(f"âš ï¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
                return None
        
        elif isinstance(data, dict):
            # Dictionaryë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            try:
                return pd.DataFrame(data)
            except Exception as e:
                logger.warning(f"âš ï¸ Dict to DataFrame ë³€í™˜ ì‹¤íŒ¨: {e}")
                return None
        
        elif isinstance(data, list):
            # Listë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            try:
                return pd.DataFrame(data)
            except Exception as e:
                logger.warning(f"âš ï¸ List to DataFrame ë³€í™˜ ì‹¤íŒ¨: {e}")
                return None
        
        else:
            logger.warning(f"âš ï¸ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(data)}")
            return None
    
    def _analyze_basic_info(self, df: pd.DataFrame, dataset_name: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ì •ë³´ ë¶„ì„"""
        return {
            'name': dataset_name,
            'shape': df.shape,
            'columns': list(df.columns),
            'dtypes': df.dtypes.to_dict(),
            'memory_usage': df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
        }
    
    def _analyze_columns(self, df: pd.DataFrame) -> List[ColumnProfile]:
        """ì»¬ëŸ¼ë³„ ìƒì„¸ ë¶„ì„"""
        profiles = []
        
        for col in df.columns:
            try:
                profile = self._analyze_single_column(df[col], col)
                profiles.append(profile)
            except Exception as e:
                logger.warning(f"âš ï¸ ì»¬ëŸ¼ {col} ë¶„ì„ ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ í”„ë¡œíŒŒì¼ ìƒì„±
                profile = ColumnProfile(
                    name=col,
                    dtype=str(df[col].dtype),
                    inferred_type=ColumnType.UNKNOWN,
                    null_count=df[col].isnull().sum(),
                    null_percentage=(df[col].isnull().sum() / len(df)) * 100,
                    unique_count=df[col].nunique(),
                    unique_percentage=(df[col].nunique() / len(df)) * 100,
                    quality_score=0.5,
                    quality_issues=[f"ë¶„ì„ ì˜¤ë¥˜: {str(e)}"],
                    recommendations=["ì»¬ëŸ¼ ë°ì´í„°ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”"]
                )
                profiles.append(profile)
        
        return profiles
    
    def _analyze_single_column(self, series: pd.Series, col_name: str) -> ColumnProfile:
        """ë‹¨ì¼ ì»¬ëŸ¼ ë¶„ì„"""
        # ê¸°ë³¸ ì •ë³´
        null_count = series.isnull().sum()
        null_percentage = (null_count / len(series)) * 100
        unique_count = series.nunique()
        unique_percentage = (unique_count / len(series)) * 100
        
        # íƒ€ì… ì¶”ë¡ 
        inferred_type = self._infer_column_type(series)
        
        # í†µê³„ ì •ë³´ (ìˆ˜ì¹˜í˜•ì¸ ê²½ìš°)
        mean = median = std = skewness = kurtosis = None
        min_value = max_value = None
        
        if inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]:
            try:
                numeric_series = pd.to_numeric(series, errors='coerce')
                mean = numeric_series.mean()
                median = numeric_series.median()
                std = numeric_series.std()
                min_value = numeric_series.min()
                max_value = numeric_series.max()
                
                if SCIPY_AVAILABLE:
                    skewness = stats.skew(numeric_series.dropna())
                    kurtosis = stats.kurtosis(numeric_series.dropna())
            except Exception as e:
                logger.warning(f"âš ï¸ ìˆ˜ì¹˜ í†µê³„ ê³„ì‚° ì‹¤íŒ¨ {col_name}: {e}")
        
        # Top values (ë²”ì£¼í˜• ë˜ëŠ” ì ì€ unique ê°’ì¸ ê²½ìš°)
        top_values = None
        if unique_count <= self.max_categorical_values:
            try:
                value_counts = series.value_counts().head(10)
                top_values = [(val, count) for val, count in value_counts.items()]
            except Exception as e:
                logger.warning(f"âš ï¸ Top values ê³„ì‚° ì‹¤íŒ¨ {col_name}: {e}")
        
        # í’ˆì§ˆ í‰ê°€
        quality_score, quality_issues = self._assess_column_quality(series, inferred_type)
        
        # ì¶”ì²œì‚¬í•­
        recommendations = self._generate_column_recommendations(series, inferred_type, quality_issues)
        
        return ColumnProfile(
            name=col_name,
            dtype=str(series.dtype),
            inferred_type=inferred_type,
            null_count=null_count,
            null_percentage=null_percentage,
            unique_count=unique_count,
            unique_percentage=unique_percentage,
            mean=mean,
            median=median,
            std=std,
            min_value=min_value,
            max_value=max_value,
            skewness=skewness,
            kurtosis=kurtosis,
            top_values=top_values,
            quality_score=quality_score,
            quality_issues=quality_issues,
            recommendations=recommendations
        )
    
    def _infer_column_type(self, series: pd.Series) -> ColumnType:
        """ì»¬ëŸ¼ íƒ€ì… ì¶”ë¡ """
        # ë„ˆë¬´ ë§ì€ ëˆ„ë½ê°’ì´ ìˆëŠ” ê²½ìš°
        if series.isnull().sum() / len(series) > 0.9:
            return ColumnType.UNKNOWN
        
        # Boolean íƒ€ì… ì²´í¬
        if series.dtype == 'bool' or set(series.dropna().unique()) <= {True, False, 0, 1, 'True', 'False', 'true', 'false'}:
            return ColumnType.BOOLEAN
        
        # ìˆ˜ì¹˜í˜• íƒ€ì… ì²´í¬
        if pd.api.types.is_numeric_dtype(series):
            # ì •ìˆ˜í˜•ì´ê³  unique ê°’ì´ ì ìœ¼ë©´ discrete
            if pd.api.types.is_integer_dtype(series) and series.nunique() <= 20:
                return ColumnType.NUMERIC_DISCRETE
            else:
                return ColumnType.NUMERIC_CONTINUOUS
        
        # ë‚ ì§œ/ì‹œê°„ íƒ€ì… ì²´í¬
        if pd.api.types.is_datetime64_any_dtype(series):
            return ColumnType.DATETIME
        
        # ë¬¸ìì—´ íƒ€ì…ì—ì„œ ë‚ ì§œ ì¶”ë¡  ì‹œë„
        if series.dtype == 'object':
            try:
                # ìƒ˜í”Œ ë°ì´í„°ë¡œ ë‚ ì§œ íŒŒì‹± ì‹œë„
                sample = series.dropna().head(100)
                parsed = pd.to_datetime(sample, errors='coerce')
                if parsed.notna().sum() / len(sample) > 0.8:
                    return ColumnType.DATETIME
            except:
                pass
        
        # ë²”ì£¼í˜• vs í…ìŠ¤íŠ¸ êµ¬ë¶„
        if series.dtype == 'object' or series.dtype.name == 'category':
            unique_ratio = series.nunique() / len(series)
            
            # Unique ë¹„ìœ¨ì´ ë‚®ìœ¼ë©´ ë²”ì£¼í˜•
            if unique_ratio < 0.5 and series.nunique() <= self.max_categorical_values:
                return ColumnType.CATEGORICAL
            else:
                # í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸
                try:
                    avg_length = series.astype(str).str.len().mean()
                    if avg_length > 50:  # ê¸´ í…ìŠ¤íŠ¸
                        return ColumnType.TEXT
                    else:
                        return ColumnType.CATEGORICAL
                except:
                    return ColumnType.TEXT
        
        return ColumnType.UNKNOWN
    
    def _assess_column_quality(self, series: pd.Series, col_type: ColumnType) -> Tuple[float, List[str]]:
        """ì»¬ëŸ¼ í’ˆì§ˆ í‰ê°€"""
        issues = []
        score = 100.0
        
        # ëˆ„ë½ê°’ ê²€ì‚¬
        null_percentage = (series.isnull().sum() / len(series)) * 100
        if null_percentage > 50:
            issues.append(f"ë†’ì€ ëˆ„ë½ê°’ ë¹„ìœ¨: {null_percentage:.1f}%")
            score -= 30
        elif null_percentage > 20:
            issues.append(f"ìƒë‹¹í•œ ëˆ„ë½ê°’: {null_percentage:.1f}%")
            score -= 15
        elif null_percentage > 5:
            score -= 5
        
        # ì¤‘ë³µê°’ ê²€ì‚¬ (ë²”ì£¼í˜•ì´ ì•„ë‹Œ ê²½ìš°)
        if col_type not in [ColumnType.CATEGORICAL, ColumnType.BOOLEAN]:
            duplicate_percentage = (series.duplicated().sum() / len(series)) * 100
            if duplicate_percentage > 80:
                issues.append(f"ë§¤ìš° ë†’ì€ ì¤‘ë³µê°’ ë¹„ìœ¨: {duplicate_percentage:.1f}%")
                score -= 20
            elif duplicate_percentage > 50:
                issues.append(f"ë†’ì€ ì¤‘ë³µê°’ ë¹„ìœ¨: {duplicate_percentage:.1f}%")
                score -= 10
        
        # ê³ ìœ ê°’ ë¹„ìœ¨ ê²€ì‚¬
        unique_ratio = series.nunique() / len(series)
        if col_type == ColumnType.CATEGORICAL and unique_ratio > 0.8:
            issues.append("ë²”ì£¼í˜• ë°ì´í„°ì— ë„ˆë¬´ ë§ì€ ê³ ìœ ê°’")
            score -= 15
        
        # ìˆ˜ì¹˜í˜• ë°ì´í„° íŠ¹ë³„ ê²€ì‚¬
        if col_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]:
            try:
                numeric_series = pd.to_numeric(series, errors='coerce')
                
                # ì´ìƒì¹˜ ê²€ì‚¬ (Z-score ë°©ë²•)
                if len(numeric_series.dropna()) > 0:
                    z_scores = np.abs(stats.zscore(numeric_series.dropna())) if SCIPY_AVAILABLE else []
                    if len(z_scores) > 0:
                        outlier_percentage = (np.sum(z_scores > self.outlier_threshold) / len(z_scores)) * 100
                        if outlier_percentage > 10:
                            issues.append(f"ë†’ì€ ì´ìƒì¹˜ ë¹„ìœ¨: {outlier_percentage:.1f}%")
                            score -= 10
                
                # ì˜ ë¶„ì‚° ê²€ì‚¬
                if numeric_series.std() == 0:
                    issues.append("ëª¨ë“  ê°’ì´ ë™ì¼ (ì˜ ë¶„ì‚°)")
                    score -= 25
            except Exception as e:
                logger.warning(f"âš ï¸ ìˆ˜ì¹˜í˜• í’ˆì§ˆ ê²€ì‚¬ ì‹¤íŒ¨: {e}")
        
        # ìµœì¢… ì ìˆ˜ ì¡°ì •
        score = max(0, min(100, score))
        
        return score / 100.0, issues
    
    def _generate_column_recommendations(
        self, 
        series: pd.Series, 
        col_type: ColumnType, 
        issues: List[str]
    ) -> List[str]:
        """ì»¬ëŸ¼ë³„ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ëˆ„ë½ê°’ ì²˜ë¦¬
        null_percentage = (series.isnull().sum() / len(series)) * 100
        if null_percentage > 20:
            if col_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]:
                recommendations.append("ìˆ˜ì¹˜í˜• ëˆ„ë½ê°’: í‰ê· /ì¤‘ì•™ê°’ ëŒ€ì²´ ë˜ëŠ” ë³´ê°„ ê³ ë ¤")
            elif col_type == ColumnType.CATEGORICAL:
                recommendations.append("ë²”ì£¼í˜• ëˆ„ë½ê°’: ìµœë¹ˆê°’ ëŒ€ì²´ ë˜ëŠ” 'Unknown' ì¹´í…Œê³ ë¦¬ ìƒì„±")
            else:
                recommendations.append("ëˆ„ë½ê°’ ì²˜ë¦¬ ì „ëµ ìˆ˜ë¦½ í•„ìš”")
        
        # íƒ€ì…ë³„ ì¶”ì²œì‚¬í•­
        if col_type == ColumnType.NUMERIC_CONTINUOUS:
            if series.skew() > 2 if hasattr(series, 'skew') else False:
                recommendations.append("ë†’ì€ ì™œë„: ë¡œê·¸ ë³€í™˜ ë˜ëŠ” Box-Cox ë³€í™˜ ê³ ë ¤")
        
        elif col_type == ColumnType.CATEGORICAL:
            if series.nunique() > 20:
                recommendations.append("ë²”ì£¼ê°€ ë§ìŒ: ë²”ì£¼ í†µí•© ë˜ëŠ” ì›-í•« ì¸ì½”ë”© ê³ ë ¤")
        
        elif col_type == ColumnType.TEXT:
            recommendations.append("í…ìŠ¤íŠ¸ ë°ì´í„°: NLP ì „ì²˜ë¦¬ (í† í°í™”, ì •ê·œí™”) í•„ìš”")
        
        elif col_type == ColumnType.DATETIME:
            recommendations.append("ë‚ ì§œ ë°ì´í„°: ì‹œê³„ì—´ ë¶„ì„ ë˜ëŠ” ì‹œê°„ í”¼ì²˜ ì¶”ì¶œ ê³ ë ¤")
        
        elif col_type == ColumnType.UNKNOWN:
            recommendations.append("ë°ì´í„° íƒ€ì… ëª…í™•í™” ë° ì „ì²˜ë¦¬ í•„ìš”")
        
        # ì´ìƒì¹˜ ê´€ë ¨
        if "ì´ìƒì¹˜" in " ".join(issues):
            recommendations.append("ì´ìƒì¹˜ ì²˜ë¦¬: ì œê±°, ë³€í™˜, ë˜ëŠ” ë³„ë„ ë¶„ì„ ê³ ë ¤")
        
        return recommendations
    
    def _assess_data_quality(self, df: pd.DataFrame, column_profiles: List[ColumnProfile]) -> Dict[str, Any]:
        """ì „ì²´ ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        # ì»¬ëŸ¼ë³„ í’ˆì§ˆ ì ìˆ˜ í‰ê· 
        column_scores = [col.quality_score for col in column_profiles]
        avg_column_quality = np.mean(column_scores) if column_scores else 0.0
        
        # ì „ì²´ ë°ì´í„° ì´ìŠˆë“¤
        issues = []
        quality_penalty = 0
        
        # ì¤‘ë³µ í–‰ ê²€ì‚¬
        duplicate_percentage = (df.duplicated().sum() / len(df)) * 100
        if duplicate_percentage > 10:
            issues.append(f"ë†’ì€ ì¤‘ë³µ í–‰ ë¹„ìœ¨: {duplicate_percentage:.1f}%")
            quality_penalty += 0.1
        
        # ì „ì²´ ëˆ„ë½ê°’ ë¹„ìœ¨
        missing_percentage = (df.isnull().sum().sum() / df.size) * 100
        if missing_percentage > 30:
            issues.append(f"ë†’ì€ ì „ì²´ ëˆ„ë½ê°’ ë¹„ìœ¨: {missing_percentage:.1f}%")
            quality_penalty += 0.15
        elif missing_percentage > 10:
            issues.append(f"ìƒë‹¹í•œ ëˆ„ë½ê°’: {missing_percentage:.1f}%")
            quality_penalty += 0.05
        
        # ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬
        type_consistency = self._check_type_consistency(df)
        if not type_consistency:
            issues.append("ë°ì´í„° íƒ€ì… ì¼ê´€ì„± ë¬¸ì œ ë°œê²¬")
            quality_penalty += 0.1
        
        # ìµœì¢… í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        final_score = max(0, avg_column_quality - quality_penalty)
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if final_score >= 0.9:
            quality_grade = DataQuality.EXCELLENT
        elif final_score >= 0.8:
            quality_grade = DataQuality.GOOD
        elif final_score >= 0.6:
            quality_grade = DataQuality.FAIR
        elif final_score >= 0.4:
            quality_grade = DataQuality.POOR
        else:
            quality_grade = DataQuality.CRITICAL
        
        return {
            'overall_quality': quality_grade,
            'quality_score': final_score,
            'issues': issues,
            'column_scores': column_scores
        }
    
    def _check_type_consistency(self, df: pd.DataFrame) -> bool:
        """ë°ì´í„° íƒ€ì… ì¼ê´€ì„± ê²€ì‚¬"""
        try:
            # ê° ì»¬ëŸ¼ì˜ ì‹¤ì œ ê°’ë“¤ì´ ì˜ˆìƒ íƒ€ì…ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
            for col in df.columns:
                if df[col].dtype == 'object':
                    # ë¬¸ìì—´ ì»¬ëŸ¼ì—ì„œ ìˆ˜ì¹˜ê°’ í˜¼ì¬ í™•ì¸
                    sample = df[col].dropna().astype(str).head(100)
                    numeric_count = 0
                    for val in sample:
                        try:
                            float(val)
                            numeric_count += 1
                        except ValueError:
                            pass
                    
                    # ì ˆë°˜ ì´ìƒì´ ìˆ«ìë©´ íƒ€ì… ë¶ˆì¼ì¹˜
                    if numeric_count / len(sample) > 0.5:
                        return False
            
            return True
        except Exception:
            return True  # ê²€ì‚¬ ì‹¤íŒ¨ ì‹œ ì¼ê´€ì„± ìˆë‹¤ê³  ê°€ì •
    
    def _detect_data_patterns(self, df: pd.DataFrame, column_profiles: List[ColumnProfile]) -> List[DataPattern]:
        """ë°ì´í„° íŒ¨í„´ íƒì§€"""
        patterns = []
        
        # ì‹œê³„ì—´ íŒ¨í„´ íƒì§€
        datetime_columns = [col.name for col in column_profiles if col.inferred_type == ColumnType.DATETIME]
        if datetime_columns:
            patterns.append(DataPattern.TIME_SERIES)
        
        # ê³„ì¸µì  íŒ¨í„´ íƒì§€ (ID ê´€ë ¨ ì»¬ëŸ¼ë“¤)
        hierarchical_keywords = ['id', 'code', 'key', 'parent', 'child', 'level']
        hierarchical_cols = [col for col in df.columns if any(kw in col.lower() for kw in hierarchical_keywords)]
        if len(hierarchical_cols) >= 2:
            patterns.append(DataPattern.HIERARCHICAL)
        
        # ê´€ê³„í˜• íŒ¨í„´ íƒì§€ (ì™¸ë˜í‚¤ ê°™ì€ ì°¸ì¡° ê´€ê³„)
        if len([col for col in column_profiles if col.inferred_type in [ColumnType.NUMERIC_DISCRETE, ColumnType.CATEGORICAL]]) >= 3:
            patterns.append(DataPattern.RELATIONAL)
        
        # ê±°ë˜ ë°ì´í„° íŒ¨í„´ íƒì§€
        transaction_keywords = ['amount', 'price', 'cost', 'fee', 'payment', 'transaction', 'order']
        transaction_cols = [col for col in df.columns if any(kw in col.lower() for kw in transaction_keywords)]
        if transaction_cols and datetime_columns:
            patterns.append(DataPattern.TRANSACTIONAL)
        
        # ì‹¤í—˜ ë°ì´í„° íŒ¨í„´ íƒì§€
        experiment_keywords = ['experiment', 'test', 'trial', 'group', 'treatment', 'control']
        experiment_cols = [col for col in df.columns if any(kw in col.lower() for kw in experiment_keywords)]
        if experiment_cols:
            patterns.append(DataPattern.EXPERIMENTAL)
        
        # ì„¤ë¬¸ì¡°ì‚¬ íŒ¨í„´ íƒì§€
        survey_keywords = ['score', 'rating', 'satisfaction', 'response', 'answer', 'question']
        survey_cols = [col for col in df.columns if any(kw in col.lower() for kw in survey_keywords)]
        if len(survey_cols) >= 3:
            patterns.append(DataPattern.SURVEY)
        
        # ë¡œê·¸ ë°ì´í„° íŒ¨í„´ íƒì§€
        log_keywords = ['log', 'event', 'timestamp', 'level', 'message', 'error']
        log_cols = [col for col in df.columns if any(kw in col.lower() for kw in log_keywords)]
        if log_cols and datetime_columns:
            patterns.append(DataPattern.LOG_DATA)
        
        # íŒ¨í„´ì´ ì—¬ëŸ¬ ê°œë©´ MIXED
        if len(patterns) > 2:
            patterns = [DataPattern.MIXED]
        elif not patterns:
            patterns = [DataPattern.MIXED]
        
        return patterns
    
    def _analyze_correlations(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        try:
            # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) < 2:
                return None
            
            corr_matrix = df[numeric_cols].corr()
            
            # ë†’ì€ ìƒê´€ê´€ê³„ ìŒ ì°¾ê¸°
            high_correlations = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i + 1, len(corr_matrix.columns)):
                    corr_value = corr_matrix.iloc[i, j]
                    if abs(corr_value) >= self.correlation_threshold:
                        high_correlations.append({
                            'var1': corr_matrix.columns[i],
                            'var2': corr_matrix.columns[j],
                            'correlation': corr_value
                        })
            
            return {
                'matrix': corr_matrix.to_dict(),
                'high_correlations': high_correlations,
                'summary': {
                    'total_pairs': len(numeric_cols) * (len(numeric_cols) - 1) // 2,
                    'high_correlation_pairs': len(high_correlations)
                }
            }
        
        except Exception as e:
            logger.warning(f"âš ï¸ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _detect_outliers(self, df: pd.DataFrame) -> Dict[str, int]:
        """ì´ìƒì¹˜ íƒì§€"""
        outliers = {}
        
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            try:
                if SCIPY_AVAILABLE:
                    z_scores = np.abs(stats.zscore(df[col].dropna()))
                    outlier_count = np.sum(z_scores > self.outlier_threshold)
                else:
                    # IQR ë°©ë²• ì‚¬ìš©
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    outlier_count = len(df[(df[col] < lower_bound) | (df[col] > upper_bound)])
                
                if outlier_count > 0:
                    outliers[col] = outlier_count
            
            except Exception as e:
                logger.warning(f"âš ï¸ {col} ì´ìƒì¹˜ íƒì§€ ì‹¤íŒ¨: {e}")
        
        return outliers
    
    def _generate_insights(
        self, 
        df: pd.DataFrame, 
        column_profiles: List[ColumnProfile], 
        quality_info: Dict[str, Any], 
        patterns: List[DataPattern]
    ) -> List[str]:
        """ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ë°ì´í„° í¬ê¸° ê´€ë ¨
        rows, cols = df.shape
        insights.append(f"ë°ì´í„°ì…‹ í¬ê¸°: {rows:,}í–‰ Ã— {cols}ì—´")
        
        # í’ˆì§ˆ ê´€ë ¨
        quality_grade = quality_info['overall_quality'].value
        insights.append(f"ì „ì²´ ë°ì´í„° í’ˆì§ˆ: {quality_grade.upper()} ({quality_info['quality_score']:.1%})")
        
        # ì»¬ëŸ¼ íƒ€ì… ë¶„í¬
        type_counts = {}
        for profile in column_profiles:
            type_name = profile.inferred_type.value
            type_counts[type_name] = type_counts.get(type_name, 0) + 1
        
        top_type = max(type_counts.items(), key=lambda x: x[1])
        insights.append(f"ì£¼ìš” ë°ì´í„° íƒ€ì…: {top_type[0]} ({top_type[1]}ê°œ ì»¬ëŸ¼)")
        
        # ëˆ„ë½ê°’ ê´€ë ¨
        missing_percentage = (df.isnull().sum().sum() / df.size) * 100
        if missing_percentage > 5:
            insights.append(f"ì „ì²´ ëˆ„ë½ê°’ ë¹„ìœ¨: {missing_percentage:.1f}%")
        
        # ì¤‘ë³µ í–‰ ê´€ë ¨
        duplicate_percentage = (df.duplicated().sum() / len(df)) * 100
        if duplicate_percentage > 1:
            insights.append(f"ì¤‘ë³µ í–‰ ë¹„ìœ¨: {duplicate_percentage:.1f}%")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        insights.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_mb:.1f}MB")
        
        # íŒ¨í„´ ê´€ë ¨
        if patterns:
            pattern_names = [p.value for p in patterns]
            insights.append(f"íƒì§€ëœ ë°ì´í„° íŒ¨í„´: {', '.join(pattern_names)}")
        
        # í’ˆì§ˆ ì´ìŠˆê°€ ìˆëŠ” ì»¬ëŸ¼
        poor_quality_cols = [col.name for col in column_profiles if col.quality_score < 0.7]
        if poor_quality_cols:
            insights.append(f"í’ˆì§ˆ ì£¼ì˜ ì»¬ëŸ¼: {', '.join(poor_quality_cols[:3])}{'...' if len(poor_quality_cols) > 3 else ''}")
        
        return insights
    
    def _generate_recommendations(
        self, 
        df: pd.DataFrame, 
        column_profiles: List[ColumnProfile], 
        quality_info: Dict[str, Any]
    ) -> List[str]:
        """ì „ì²´ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œì‚¬í•­
        if quality_info['overall_quality'] in [DataQuality.POOR, DataQuality.CRITICAL]:
            recommendations.append("ğŸš¨ ë°ì´í„° í’ˆì§ˆì´ ë‚®ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ ë° ì •ì œ ì‘ì—…ì„ ìš°ì„  ìˆ˜í–‰í•˜ì„¸ìš”.")
        
        # ëˆ„ë½ê°’ ì²˜ë¦¬
        missing_percentage = (df.isnull().sum().sum() / df.size) * 100
        if missing_percentage > 20:
            recommendations.append("ğŸ“‹ ë†’ì€ ëˆ„ë½ê°’ ë¹„ìœ¨: ëˆ„ë½ê°’ ì²˜ë¦¬ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”.")
        
        # ì¤‘ë³µ ì²˜ë¦¬
        duplicate_percentage = (df.duplicated().sum() / len(df)) * 100
        if duplicate_percentage > 5:
            recommendations.append("ğŸ”„ ì¤‘ë³µ í–‰ í™•ì¸ ë° ì œê±°ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_mb > 500:  # 500MB ì´ìƒ
            recommendations.append("ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŠµë‹ˆë‹¤. ë°ì´í„° íƒ€ì… ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì»¬ëŸ¼ë³„ ì¶”ì²œì‚¬í•­ ì§‘ê³„
        column_issues = {}
        for profile in column_profiles:
            for rec in profile.recommendations or []:
                issue_type = rec.split(':')[0] if ':' in rec else rec
                column_issues[issue_type] = column_issues.get(issue_type, 0) + 1
        
        # ê³µí†µ ì´ìŠˆë“¤ì„ ì „ì²´ ì¶”ì²œì‚¬í•­ìœ¼ë¡œ
        if column_issues:
            top_issue = max(column_issues.items(), key=lambda x: x[1])
            if top_issue[1] >= 3:  # 3ê°œ ì´ìƒ ì»¬ëŸ¼ì—ì„œ ë°œìƒ
                recommendations.append(f"âš ï¸ ê³µí†µ ì´ìŠˆ ë°œê²¬: {top_issue[0]} ({top_issue[1]}ê°œ ì»¬ëŸ¼)")
        
        # ë¶„ì„ ë°©í–¥ ì¶”ì²œ
        numeric_cols = len([col for col in column_profiles if col.inferred_type in [ColumnType.NUMERIC_CONTINUOUS, ColumnType.NUMERIC_DISCRETE]])
        categorical_cols = len([col for col in column_profiles if col.inferred_type == ColumnType.CATEGORICAL])
        datetime_cols = len([col for col in column_profiles if col.inferred_type == ColumnType.DATETIME])
        
        if numeric_cols >= 3:
            recommendations.append("ğŸ“Š ìˆ˜ì¹˜í˜• ë°ì´í„°ê°€ í’ë¶€í•©ë‹ˆë‹¤. í†µê³„ ë¶„ì„ ë° ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.")
        
        if categorical_cols >= 3:
            recommendations.append("ğŸ·ï¸ ë²”ì£¼í˜• ë°ì´í„°ê°€ ë§ìŠµë‹ˆë‹¤. êµì°¨ ë¶„ì„ ë° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        if datetime_cols >= 1:
            recommendations.append("ğŸ“… ì‹œê°„ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ì‹œê³„ì—´ ë¶„ì„ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ê¸°ë³¸ ì¶”ì²œì‚¬í•­
        if not recommendations:
            recommendations.append("âœ… ë°ì´í„° í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤. ë¶„ì„ì„ ì§„í–‰í•˜ì„¸ìš”.")
        
        return recommendations
    
    def _create_error_profile(self, error_message: str, dataset_name: Optional[str]) -> DataProfile:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í”„ë¡œíŒŒì¼ ìƒì„±"""
        return DataProfile(
            dataset_name=dataset_name or "Error Dataset",
            shape=(0, 0),
            memory_usage=0.0,
            dtypes_summary={},
            overall_quality=DataQuality.CRITICAL,
            quality_score=0.0,
            columns=[],
            detected_patterns=[],
            key_insights=[f"í”„ë¡œíŒŒì¼ë§ ì˜¤ë¥˜ ë°œìƒ: {error_message}"],
            data_quality_issues=[error_message],
            recommendations=["ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."],
            profiling_timestamp=datetime.now().isoformat(),
            profiling_duration=0.0
        )
    
    def export_profile_report(self, profile: DataProfile, format: str = 'json') -> str:
        """í”„ë¡œíŒŒì¼ ë³´ê³ ì„œ ë‚´ë³´ë‚´ê¸°"""
        try:
            if format.lower() == 'json':
                return json.dumps(asdict(profile), indent=2, default=str)
            
            elif format.lower() == 'html':
                return self._generate_html_report(profile)
            
            elif format.lower() == 'markdown':
                return self._generate_markdown_report(profile)
            
            else:
                raise ValueError(f"Unsupported format: {format}")
        
        except Exception as e:
            logger.error(f"âŒ í”„ë¡œíŒŒì¼ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    def _generate_html_report(self, profile: DataProfile) -> str:
        """HTML ë³´ê³ ì„œ ìƒì„±"""
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Data Profile Report - {profile.dataset_name}</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; }}
                .quality-{profile.overall_quality.value} {{ 
                    background-color: {'#d4edda' if profile.overall_quality == DataQuality.EXCELLENT else 
                                     '#fff3cd' if profile.overall_quality == DataQuality.GOOD else
                                     '#f8d7da' if profile.overall_quality in [DataQuality.POOR, DataQuality.CRITICAL] else '#e2e3e5'};
                    padding: 10px; border-radius: 3px; 
                }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ë°ì´í„° í”„ë¡œíŒŒì¼ ë³´ê³ ì„œ</h1>
                <h2>{profile.dataset_name}</h2>
                <p>ìƒì„±ì¼ì‹œ: {profile.profiling_timestamp}</p>
            </div>
            
            <div class="section quality-{profile.overall_quality.value}">
                <h3>ì „ì²´ í’ˆì§ˆ í‰ê°€</h3>
                <p><strong>í’ˆì§ˆ ë“±ê¸‰:</strong> {profile.overall_quality.value.upper()}</p>
                <p><strong>í’ˆì§ˆ ì ìˆ˜:</strong> {profile.quality_score:.1%}</p>
            </div>
            
            <div class="section">
                <h3>ê¸°ë³¸ ì •ë³´</h3>
                <ul>
                    <li>í¬ê¸°: {profile.shape[0]:,}í–‰ Ã— {profile.shape[1]}ì—´</li>
                    <li>ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {profile.memory_usage:.1f}MB</li>
                    <li>ì¤‘ë³µ í–‰: {profile.duplicate_percentage:.1f}%</li>
                    <li>ëˆ„ë½ê°’: {profile.missing_percentage:.1f}%</li>
                </ul>
            </div>
            
            <div class="section">
                <h3>ì£¼ìš” ì¸ì‚¬ì´íŠ¸</h3>
                <ul>
                    {''.join(f'<li>{insight}</li>' for insight in profile.key_insights or [])}
                </ul>
            </div>
            
            <div class="section">
                <h3>ì¶”ì²œì‚¬í•­</h3>
                <ul>
                    {''.join(f'<li>{rec}</li>' for rec in profile.recommendations or [])}
                </ul>
            </div>
        </body>
        </html>
        """
        return html
    
    def _generate_markdown_report(self, profile: DataProfile) -> str:
        """Markdown ë³´ê³ ì„œ ìƒì„±"""
        md = f"""# ë°ì´í„° í”„ë¡œíŒŒì¼ ë³´ê³ ì„œ: {profile.dataset_name}

**ìƒì„±ì¼ì‹œ:** {profile.profiling_timestamp}  
**í”„ë¡œíŒŒì¼ë§ ì†Œìš”ì‹œê°„:** {profile.profiling_duration:.2f}ì´ˆ

## ì „ì²´ í’ˆì§ˆ í‰ê°€
- **í’ˆì§ˆ ë“±ê¸‰:** {profile.overall_quality.value.upper()}
- **í’ˆì§ˆ ì ìˆ˜:** {profile.quality_score:.1%}

## ê¸°ë³¸ ì •ë³´
- **ë°ì´í„° í¬ê¸°:** {profile.shape[0]:,}í–‰ Ã— {profile.shape[1]}ì—´
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:** {profile.memory_usage:.1f}MB
- **ì¤‘ë³µ í–‰ ë¹„ìœ¨:** {profile.duplicate_percentage:.1f}%
- **ì „ì²´ ëˆ„ë½ê°’ ë¹„ìœ¨:** {profile.missing_percentage:.1f}%

## ì£¼ìš” ì¸ì‚¬ì´íŠ¸
{chr(10).join(f'- {insight}' for insight in profile.key_insights or [])}

## ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ
{chr(10).join(f'- {issue}' for issue in profile.data_quality_issues or [])}

## ì¶”ì²œì‚¬í•­
{chr(10).join(f'- {rec}' for rec in profile.recommendations or [])}

## ì»¬ëŸ¼ ì •ë³´
| ì»¬ëŸ¼ëª… | íƒ€ì… | ëˆ„ë½ê°’(%) | ê³ ìœ ê°’(%) | í’ˆì§ˆì ìˆ˜ |
|--------|------|-----------|-----------|----------|
{chr(10).join(f'| {col.name} | {col.inferred_type.value} | {col.null_percentage:.1f}% | {col.unique_percentage:.1f}% | {col.quality_score:.1%} |' for col in profile.columns)}
"""
        return md


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_profiler_instance = None


def get_auto_data_profiler(config: Optional[Dict] = None) -> AutoDataProfiler:
    """Auto Data Profiler ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _profiler_instance
    if _profiler_instance is None:
        _profiler_instance = AutoDataProfiler(config)
    return _profiler_instance


# í¸ì˜ í•¨ìˆ˜ë“¤
def profile_dataset(
    data: Union[pd.DataFrame, str, Dict], 
    dataset_name: Optional[str] = None,
    session_id: Optional[str] = None,
    config: Optional[Dict] = None
) -> DataProfile:
    """ë°ì´í„°ì…‹ í”„ë¡œíŒŒì¼ë§ í¸ì˜ í•¨ìˆ˜"""
    profiler = get_auto_data_profiler(config)
    return profiler.profile_data(data, dataset_name, session_id)


def quick_profile(data: Union[pd.DataFrame, str, Dict]) -> Dict[str, Any]:
    """ë¹ ë¥¸ í”„ë¡œíŒŒì¼ë§ (ì£¼ìš” ì •ë³´ë§Œ)"""
    try:
        profile = profile_dataset(data, "Quick Profile")
        return {
            'shape': profile.shape,
            'quality': profile.overall_quality.value,
            'quality_score': profile.quality_score,
            'missing_percentage': profile.missing_percentage,
            'insights': profile.key_insights[:3],  # ìƒìœ„ 3ê°œë§Œ
            'recommendations': profile.recommendations[:3]  # ìƒìœ„ 3ê°œë§Œ
        }
    except Exception as e:
        return {'error': str(e)}


# CLI í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_auto_data_profiler():
    """Auto Data Profiler í…ŒìŠ¤íŠ¸"""
    print("ğŸ” Auto Data Profiler í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    
    # ë‹¤ì–‘í•œ íƒ€ì…ì˜ ë°ì´í„°
    sample_data = pd.DataFrame({
        'id': range(1, 1001),
        'name': [f'User_{i}' for i in range(1, 1001)],
        'age': np.random.randint(18, 80, 1000),
        'salary': np.random.normal(50000, 15000, 1000),
        'department': np.random.choice(['HR', 'IT', 'Finance', 'Marketing'], 1000),
        'join_date': pd.date_range('2020-01-01', periods=1000, freq='D'),
        'is_active': np.random.choice([True, False], 1000, p=[0.8, 0.2]),
        'performance_score': np.random.uniform(1, 5, 1000),
        'comments': [f'This is comment {i}' if i % 10 != 0 else None for i in range(1000)]
    })
    
    # ì˜ë„ì ìœ¼ë¡œ í’ˆì§ˆ ì´ìŠˆ ì¶”ê°€
    sample_data.loc[50:100, 'salary'] = None  # ëˆ„ë½ê°’
    sample_data.loc[200:250, :] = sample_data.loc[200:250, :].copy()  # ì¤‘ë³µê°’
    
    profiler = get_auto_data_profiler()
    
    print("ğŸ“Š ìƒ˜í”Œ ë°ì´í„° í”„ë¡œíŒŒì¼ë§...")
    profile = profiler.profile_data(sample_data, "Sample Employee Data", "test_session")
    
    print(f"\nâœ… í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ!")
    print(f"ğŸ“ˆ ë°ì´í„° í¬ê¸°: {profile.shape[0]:,}í–‰ Ã— {profile.shape[1]}ì—´")
    print(f"ğŸ† í’ˆì§ˆ ë“±ê¸‰: {profile.overall_quality.value.upper()} ({profile.quality_score:.1%})")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {profile.memory_usage:.1f}MB")
    print(f"â±ï¸ í”„ë¡œíŒŒì¼ë§ ì‹œê°„: {profile.profiling_duration:.2f}ì´ˆ")
    
    print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    for insight in profile.key_insights[:5]:
        print(f"  â€¢ {insight}")
    
    print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
    for rec in profile.recommendations[:3]:
        print(f"  â€¢ {rec}")
    
    print(f"\nğŸ“‹ ì»¬ëŸ¼ í’ˆì§ˆ ìš”ì•½:")
    for col in profile.columns[:5]:  # ìƒìœ„ 5ê°œ ì»¬ëŸ¼ë§Œ
        print(f"  â€¢ {col.name}: {col.inferred_type.value} (í’ˆì§ˆ: {col.quality_score:.1%})")
    
    # JSON ë‚´ë³´ë‚´ê¸° í…ŒìŠ¤íŠ¸
    print(f"\nğŸ“„ JSON ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸...")
    json_report = profiler.export_profile_report(profile, 'json')
    print(f"JSON ë³´ê³ ì„œ í¬ê¸°: {len(json_report):,} ë¬¸ì")
    
    # Quick profile í…ŒìŠ¤íŠ¸
    print(f"\nâš¡ ë¹ ë¥¸ í”„ë¡œíŒŒì¼ë§ í…ŒìŠ¤íŠ¸...")
    quick_result = quick_profile(sample_data.head(100))
    print(f"ë¹ ë¥¸ í”„ë¡œíŒŒì¼ë§ ê²°ê³¼: {quick_result}")
    
    print(f"\nâœ… Auto Data Profiler í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    test_auto_data_profiler() 