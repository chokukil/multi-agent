"""
Intelligent Planner - LLM ê¸°ë°˜ ë²”ìš© ì§€ëŠ¥í˜• ê³„íš ìƒì„±

Phase 5 í•µì‹¬ ì›ì¹™:
- Rule ê¸°ë°˜ í•˜ë“œì½”ë”© ì™„ì „ ì œê±°
- LLMì˜ ìì—°ì–´ ì´í•´ ëŠ¥ë ¥ ìµœëŒ€ í™œìš©
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í†µí•œ ë²”ìš©ì„± í™•ë³´
- ì»¨í…ìŠ¤íŠ¸ í•™ìŠµì„ í†µí•œ ì ì‘ì  ê³„íš ìƒì„±
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
import time

logger = logging.getLogger(__name__)

@dataclass
class PlanningContext:
    """ê³„íš ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸"""
    user_query: str
    data_context: Optional[Dict] = None
    available_agents: Optional[Dict] = None
    execution_history: Optional[List[Dict]] = None
    performance_insights: Optional[Dict] = None

class IntelligentPlanner:
    """LLM ê¸°ë°˜ ë²”ìš© ì§€ëŠ¥í˜• ê³„íš ìƒì„±ê¸°"""
    
    def __init__(self):
        self.orchestrator_url = "http://localhost:8100"
        self.execution_memory: List[Dict] = []
        self.success_patterns: List[str] = []
        
    async def generate_context_aware_plan(
        self,
        user_query: str,
        data_context: Optional[Dict] = None,
        available_agents: Dict = None,
        execution_history: Optional[List[Dict]] = None
    ) -> Dict[str, Any]:
        """
        LLM ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê³„íš ìƒì„±
        
        Args:
            user_query: ì‚¬ìš©ì ìš”ì²­
            data_context: ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
            available_agents: ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë“¤
            execution_history: ê³¼ê±° ì‹¤í–‰ ì´ë ¥
            
        Returns:
            LLMì´ ìƒì„±í•œ ìµœì í™”ëœ ì‹¤í–‰ ê³„íš
        """
        logger.info(f"ğŸ§  LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ê³„íš ìƒì„± ì‹œì‘: {user_query}")
        
        try:
            # 1ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° êµ¬ì„±
            planning_context = self._build_planning_context(
                user_query, data_context, available_agents or {}, execution_history
            )
            
            # 2ë‹¨ê³„: LLMì„ ìœ„í•œ ì§€ëŠ¥í˜• í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            intelligent_prompt = self._create_intelligent_prompt(planning_context)
            
            # 3ë‹¨ê³„: LLMì„ í†µí•œ ê³„íš ìƒì„±
            llm_response = await self._query_llm_for_plan(intelligent_prompt)
            
            # 4ë‹¨ê³„: LLM ì‘ë‹µ íŒŒì‹± ë° ê²€ì¦
            parsed_plan = self._parse_and_validate_llm_response(llm_response, available_agents or {})
            
            # 5ë‹¨ê³„: ì„±ê³µ íŒ¨í„´ í•™ìŠµ (LLM ì‘ë‹µ ê¸°ë°˜)
            self._learn_from_llm_response(user_query, parsed_plan)
            
            logger.info(f"âœ… LLM ì§€ëŠ¥í˜• ê³„íš ìƒì„± ì™„ë£Œ: {len(parsed_plan.get('steps', []))}ë‹¨ê³„")
            return parsed_plan
            
        except Exception as e:
            logger.error(f"âŒ LLM ì§€ëŠ¥í˜• ê³„íš ìƒì„± ì‹¤íŒ¨: {e}")
            # ìµœì†Œí•œì˜ í´ë°± (LLM ì—†ì´ëŠ” ë¶ˆê°€ëŠ¥)
            return {
                "error": f"ì§€ëŠ¥í˜• ê³„íš ìƒì„± ì‹¤íŒ¨: {str(e)}",
                "fallback_reason": "LLM í†µì‹  ì˜¤ë¥˜"
            }
    
    def _build_planning_context(
        self, 
        user_query: str, 
        data_context: Optional[Dict], 
        available_agents: Dict,
        execution_history: Optional[List[Dict]]
    ) -> PlanningContext:
        """ê³„íš ìƒì„±ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        
        # ì„±ê³µ íŒ¨í„´ ì¶”ì¶œ (LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìì—°ì–´ í˜•íƒœë¡œ)
        success_insights = self._extract_success_insights_for_llm(execution_history)
        
        # ì—ì´ì „íŠ¸ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ (LLMì´ íŒë‹¨í•  ìˆ˜ ìˆëŠ” ì •ë³´ ì œê³µ)
        performance_insights = self._generate_performance_insights_for_llm(available_agents)
        
        return PlanningContext(
            user_query=user_query,
            data_context=data_context,
            available_agents=available_agents,
            execution_history=execution_history,
            performance_insights={
                "success_insights": success_insights,
                "agent_insights": performance_insights
            }
        )
    
    def _extract_success_insights_for_llm(self, execution_history: Optional[List[Dict]]) -> List[str]:
        """ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ë¥¼ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¡œ ë³€í™˜"""
        if not execution_history:
            return []
        
        insights = []
        successful_executions = [
            exec_data for exec_data in execution_history[-10:]  # ìµœê·¼ 10ê°œë§Œ
            if exec_data.get('execution_result', {}).get('status') == 'completed'
        ]
        
        for execution in successful_executions:
            prompt = execution.get('prompt', '')
            steps = execution.get('plan', {}).get('steps', [])
            execution_time = execution.get('execution_result', {}).get('execution_time', 0)
            
            if steps:
                agent_sequence = [step.get('agent_name', '') for step in steps]
                insight = f"'{prompt[:50]}...' ìš”ì²­ì— ëŒ€í•´ {' -> '.join(agent_sequence)} ìˆœì„œë¡œ ì‹¤í–‰í•˜ì—¬ {execution_time:.1f}ì´ˆ ë§Œì— ì„±ê³µ"
                insights.append(insight)
        
        return insights
    
    def _generate_performance_insights_for_llm(self, available_agents: Dict) -> List[str]:
        """ì—ì´ì „íŠ¸ ì„±ëŠ¥ì„ LLMì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¡œ ë³€í™˜"""
        insights = []
        
        for agent_name, agent_info in available_agents.items():
            if agent_info.get('status') == 'available':
                description = agent_info.get('description', '')
                # LLMì´ íŒë‹¨í•  ìˆ˜ ìˆë„ë¡ ì—ì´ì „íŠ¸ ì •ë³´ë¥¼ ìì—°ì–´ë¡œ ì œê³µ
                insight = f"{agent_name}: {description} (í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥)"
                insights.append(insight)
        
        return insights
    
    def _create_intelligent_prompt(self, context: PlanningContext) -> str:
        """LLMì„ ìœ„í•œ ì§€ëŠ¥í˜• í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°
        prompt_parts = []
        
        # 1. ì—­í•  ì •ì˜ ë° ëª©í‘œ
        prompt_parts.append("""
ë‹¹ì‹ ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì „ë¬¸ê°€ì´ì ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ AI ì—ì´ì „íŠ¸ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.
""")
        
        # 2. ì‚¬ìš©ì ìš”ì²­
        prompt_parts.append(f"""
=== ì‚¬ìš©ì ìš”ì²­ ===
{context.user_query}
""")
        
        # 3. ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ (ìˆëŠ” ê²½ìš°)
        if context.data_context:
            data_summary = self._summarize_data_context_for_llm(context.data_context)
            prompt_parts.append(f"""
=== ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ===
{data_summary}
""")
        
        # 4. ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ì •ë³´
        if context.available_agents:
            agent_info = "\n".join([
                f"- {name}: {info.get('description', 'AI ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì—ì´ì „íŠ¸')}"
                for name, info in context.available_agents.items()
                if info.get('status') == 'available'
            ])
            prompt_parts.append(f"""
=== ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—ì´ì „íŠ¸ë“¤ ===
{agent_info}
""")
        
        # 5. ì„±ê³µ íŒ¨í„´ ì¸ì‚¬ì´íŠ¸ (ìˆëŠ” ê²½ìš°)
        if context.performance_insights and context.performance_insights.get('success_insights'):
            insights = "\n".join(context.performance_insights['success_insights'])
            prompt_parts.append(f"""
=== ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ ì¸ì‚¬ì´íŠ¸ ===
{insights}
""")
        
        # 6. ê³„íš ìƒì„± ê°€ì´ë“œë¼ì¸
        prompt_parts.append("""
=== ê³„íš ìƒì„± ê°€ì´ë“œë¼ì¸ ===
1. ì‚¬ìš©ì ìš”ì²­ì˜ ì˜ë„ì™€ ëª©ì ì„ ê¹Šì´ ë¶„ì„í•˜ì„¸ìš”
2. ë°ì´í„° íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ë¶„ì„ ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”
3. ì—ì´ì „íŠ¸ë“¤ì˜ ì „ë¬¸ì„±ì„ ê³ ë ¤í•˜ì—¬ ìµœì ì˜ ìˆœì„œë¥¼ ê²°ì •í•˜ì„¸ìš”
4. ê³¼ê±° ì„±ê³µ ì‚¬ë¡€ë¥¼ ì°¸ê³ í•˜ë˜, í˜„ì¬ ìš”ì²­ì— ë§ê²Œ ì ì‘í•˜ì„¸ìš”
5. ë¶ˆí•„ìš”í•œ ë‹¨ê³„ëŠ” ì œê±°í•˜ê³  í•µì‹¬ì ì¸ ë‹¨ê³„ë§Œ í¬í•¨í•˜ì„¸ìš”
6. ê° ë‹¨ê³„ì˜ ëª©ì ê³¼ ê¸°ëŒ€ ê²°ê³¼ë¥¼ ëª…í™•íˆ í•˜ì„¸ìš”
""")
        
        # 7. ì‘ë‹µ í˜•ì‹ ì§€ì •
        prompt_parts.append("""
=== ì‘ë‹µ í˜•ì‹ (JSON) ===
ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{
    "objective": "ì´ ê³„íšì´ ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” êµ¬ì²´ì ì¸ ëª©í‘œ",
    "reasoning": "ì´ ê³„íšì„ ì„ íƒí•œ ì´ìœ ì™€ ê° ë‹¨ê³„ì˜ ë…¼ë¦¬ì  ê·¼ê±°",
    "steps": [
        {
            "step_number": 1,
            "agent_name": "ì •í™•í•œ ì—ì´ì „íŠ¸ ì´ë¦„",
            "task_description": "ì´ ë‹¨ê³„ì—ì„œ ìˆ˜í–‰í•  êµ¬ì²´ì ì¸ ì‘ì—… ë‚´ìš©"
        }
    ],
    "selected_agents": ["ì‚¬ìš©ë  ì—ì´ì „íŠ¸ ì´ë¦„ë“¤ì˜ ë°°ì—´"],
    "confidence_score": 0.95,
    "adaptation_notes": "ê³¼ê±° íŒ¨í„´ ëŒ€ë¹„ ì´ë²ˆ ê³„íšì˜ íŠ¹ë³„í•œ ì ì‘ ì‚¬í•­"
}

ì¤‘ìš”: ë°˜ë“œì‹œ ìœ„ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ reasoning í•„ë“œì— í¬í•¨í•˜ì„¸ìš”.
""")
        
        return "\n".join(prompt_parts)
    
    def _summarize_data_context_for_llm(self, data_context: Dict) -> str:
        """ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ë¥¼ LLMì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ìš”ì•½"""
        summary_parts = []
        
        # ë°ì´í„° í¬ê¸°
        if 'dataset_info' in data_context:
            summary_parts.append(f"ë°ì´í„° í¬ê¸°: {data_context['dataset_info']}")
        
        # ì»¬ëŸ¼ ì •ë³´
        if 'columns' in data_context:
            columns = data_context['columns']
            if len(columns) <= 10:
                summary_parts.append(f"ì»¬ëŸ¼ë“¤: {', '.join(columns)}")
            else:
                summary_parts.append(f"ì´ {len(columns)}ê°œ ì»¬ëŸ¼ (ì˜ˆì‹œ: {', '.join(columns[:5])}...)")
        
        # ë°ì´í„° íƒ€ì… ì •ë³´
        if 'dtypes' in data_context:
            dtypes = data_context['dtypes']
            type_summary = {}
            for col, dtype in dtypes.items():
                category = self._categorize_dtype_for_llm(dtype)
                type_summary[category] = type_summary.get(category, 0) + 1
            
            type_desc = ", ".join([f"{cat}: {count}ê°œ" for cat, count in type_summary.items()])
            summary_parts.append(f"ë°ì´í„° íƒ€ì… ë¶„í¬: {type_desc}")
        
        # ìƒ˜í”Œ ë°ì´í„° (ìˆëŠ” ê²½ìš°)
        if 'sample_data' in data_context and data_context['sample_data']:
            summary_parts.append("ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš© ê°€ëŠ¥")
        
        return "\n".join(summary_parts)
    
    def _categorize_dtype_for_llm(self, dtype: str) -> str:
        """ë°ì´í„° íƒ€ì…ì„ LLMì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜"""
        dtype_lower = dtype.lower()
        if 'int' in dtype_lower or 'float' in dtype_lower:
            return "ìˆ˜ì¹˜í˜•"
        elif 'datetime' in dtype_lower or 'date' in dtype_lower:
            return "ë‚ ì§œì‹œê°„"
        elif 'bool' in dtype_lower:
            return "ë¶ˆë¦°"
        else:
            return "í…ìŠ¤íŠ¸/ë²”ì£¼í˜•"
    
    async def _query_llm_for_plan(self, prompt: str) -> str:
        """LLMì—ê²Œ ê³„íš ìƒì„±ì„ ìš”ì²­"""
        try:
            import httpx
            
            message_id = f"intelligent_plan_{int(time.time())}"
            payload = {
                "jsonrpc": "2.0",
                "method": "message/send",
                "params": {
                    "message": {
                        "messageId": message_id,
                        "role": "user",
                        "parts": [
                            {
                                "type": "text",
                                "text": prompt
                            }
                        ]
                    }
                },
                "id": 1
            }
            
            async with httpx.AsyncClient(timeout=httpx.Timeout(60.0)) as client:
                response = await client.post(
                    self.orchestrator_url,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if "result" in result and "parts" in result["result"]:
                        for part in result["result"]["parts"]:
                            if part.get("type") == "text":
                                return part.get("text", "")
                
                raise Exception(f"LLM ì‘ë‹µ ì˜¤ë¥˜: HTTP {response.status_code}")
                
        except Exception as e:
            logger.error(f"âŒ LLM ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            raise e
    
    def _parse_and_validate_llm_response(self, llm_response: str, available_agents: Dict) -> Dict[str, Any]:
        """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ê³  ê²€ì¦"""
        try:
            # JSON ë¶€ë¶„ ì¶”ì¶œ
            json_start = llm_response.find('{')
            json_end = llm_response.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = llm_response[json_start:json_end]
                plan_data = json.loads(json_str)
                
                # ê¸°ë³¸ ê²€ì¦: í•„ìˆ˜ í•„ë“œ í™•ì¸
                required_fields = ['objective', 'reasoning', 'steps', 'selected_agents']
                for field in required_fields:
                    if field not in plan_data:
                        raise ValueError(f"í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {field}")
                
                # ì—ì´ì „íŠ¸ ì¡´ì¬ ê²€ì¦ (LLMì´ ì˜ëª»ëœ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí–ˆì„ ê²½ìš°ë§Œ)
                validated_steps = []
                for step in plan_data.get('steps', []):
                    agent_name = step.get('agent_name')
                    if agent_name in available_agents and available_agents[agent_name].get('status') == 'available':
                        validated_steps.append(step)
                    else:
                        # LLMì´ ì„ íƒí•œ ì—ì´ì „íŠ¸ê°€ ì—†ì„ ê²½ìš°, ê°€ì¥ ìœ ì‚¬í•œ ì—ì´ì „íŠ¸ë¡œ ëŒ€ì²´
                        alternative = self._find_best_alternative_for_llm_choice(agent_name, available_agents)
                        if alternative:
                            step_copy = step.copy()
                            step_copy['agent_name'] = alternative
                            step_copy['task_description'] += f" (LLM ì„ íƒ: {agent_name} -> ëŒ€ì²´: {alternative})"
                            validated_steps.append(step_copy)
                
                plan_data['steps'] = validated_steps
                plan_data['selected_agents'] = [step['agent_name'] for step in validated_steps]
                
                return plan_data
            
            raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
        except Exception as e:
            logger.error(f"âŒ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise Exception(f"LLM ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
    
    def _find_best_alternative_for_llm_choice(self, llm_chosen_agent: str, available_agents: Dict) -> Optional[str]:
        """LLMì´ ì„ íƒí•œ ì—ì´ì „íŠ¸ê°€ ì—†ì„ ê²½ìš° ê°€ì¥ ìœ ì‚¬í•œ ëŒ€ì•ˆ ì°¾ê¸°"""
        available_names = [
            name for name, info in available_agents.items()
            if info.get('status') == 'available'
        ]
        
        if not available_names:
            return None
        
        # ë‹¨ìˆœ ë¬¸ìì—´ ìœ ì‚¬ë„ ê¸°ë°˜ (LLMì˜ ì˜ë„ë¥¼ ìµœëŒ€í•œ ì¡´ì¤‘)
        llm_choice_lower = llm_chosen_agent.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
        for available_name in available_names:
            available_lower = available_name.lower()
            
            # ê³µí†µ í‚¤ì›Œë“œ ì°¾ê¸°
            llm_keywords = set(llm_choice_lower.split())
            available_keywords = set(available_lower.split())
            
            if llm_keywords & available_keywords:  # êµì§‘í•©ì´ ìˆìœ¼ë©´
                return available_name
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ë°˜í™˜
        return available_names[0]
    
    def _learn_from_llm_response(self, user_query: str, plan: Dict[str, Any]):
        """LLM ì‘ë‹µìœ¼ë¡œë¶€í„° í•™ìŠµ (íŒ¨í„´ ì €ì¥)"""
        if plan.get('steps'):
            success_pattern = {
                "query_type": user_query[:100],  # ì¿¼ë¦¬ íƒ€ì… í•™ìŠµ
                "agent_sequence": [step.get('agent_name') for step in plan['steps']],
                "reasoning": plan.get('reasoning', ''),
                "confidence": plan.get('confidence_score', 0.5),
                "timestamp": datetime.now().isoformat()
            }
            
            self.execution_memory.append(success_pattern)
            # ìµœê·¼ 20ê°œë§Œ ìœ ì§€
            self.execution_memory = self.execution_memory[-20:]
    
    def _update_execution_history(self, execution_history: List[Dict]):
        """ì‹¤í–‰ ì´ë ¥ ì—…ë°ì´íŠ¸ (ì™¸ë¶€ì—ì„œ í˜¸ì¶œ)"""
        # ì‹¤ì œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë©”ëª¨ë¦¬ì— ì¶”ê°€
        for execution in execution_history:
            if execution.get('execution_result', {}).get('status') == 'completed':
                self.execution_memory.append({
                    "actual_execution": True,
                    "query": execution.get('prompt', ''),
                    "plan": execution.get('plan', {}),
                    "result": execution.get('execution_result', {}),
                    "timestamp": datetime.now().isoformat()
                })
        
        # ë©”ëª¨ë¦¬ í¬ê¸° ì œí•œ
        self.execution_memory = self.execution_memory[-30:]

# ê¸€ë¡œë²Œ ì§€ëŠ¥í˜• ê³„íš ìƒì„±ê¸° ì¸ìŠ¤í„´ìŠ¤
intelligent_planner = IntelligentPlanner()
