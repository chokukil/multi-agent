"""
LLM First ì‹¤ì‹œê°„ ë¶„ì„ ê°•í™” ì‹œìŠ¤í…œ
Phase 2.4: LLM ë¶„ì„ ë¹„ìœ¨ 90% ì´ìƒ ë‹¬ì„±

í•µì‹¬ ì›ì¹™:
- í´ë°± ë¡œì§ ìµœì†Œí™” (LLM ìš°ì„ )
- ì‹¤ì‹œê°„ LLM ë¶„ì„ ê°•í™”
- ë™ì  ì—ì´ì „íŠ¸ ë¼ìš°íŒ… ê°œì„ 
- LLM ì‘ë‹µ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- ì ì‘ì  LLM ì „ëµ ì„ íƒ
"""

import asyncio
import time
import json
import logging
import statistics
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Union, Callable, AsyncGenerator, Tuple
from datetime import datetime, timedelta
from collections import deque, defaultdict
from enum import Enum
import uuid
from pathlib import Path

# LLM í´ë¼ì´ì–¸íŠ¸
from openai import AsyncOpenAI
import httpx

logger = logging.getLogger(__name__)

class AnalysisStrategy(Enum):
    """ë¶„ì„ ì „ëµ"""
    LLM_ONLY = "llm_only"           # 100% LLM ë¶„ì„
    LLM_PREFERRED = "llm_preferred"  # LLM ìš°ì„ , í•„ìš”ì‹œ í´ë°±
    HYBRID = "hybrid"               # LLM + ë£° ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ
    FALLBACK_ONLY = "fallback_only" # í´ë°±ë§Œ (ë¹„ìƒì‹œ)

class LLMProvider(Enum):
    """LLM ì œê³µì"""
    OPENAI_GPT4 = "openai_gpt4"
    OPENAI_GPT35 = "openai_gpt35"
    ANTHROPIC_CLAUDE = "anthropic_claude"
    GOOGLE_GEMINI = "google_gemini"
    LOCAL_MODEL = "local_model"

class AnalysisQuality(Enum):
    """ë¶„ì„ í’ˆì§ˆ"""
    EXCELLENT = "excellent"   # 90-100ì 
    GOOD = "good"            # 70-89ì 
    ACCEPTABLE = "acceptable" # 50-69ì 
    POOR = "poor"            # 30-49ì 
    FAILED = "failed"        # 0-29ì 

@dataclass
class LLMAnalysisRequest:
    """LLM ë¶„ì„ ìš”ì²­"""
    id: str
    user_query: str
    context: Dict[str, Any]
    priority: int = 5  # 1(highest) - 10(lowest)
    max_tokens: int = 2000
    temperature: float = 0.7
    strategy: AnalysisStrategy = AnalysisStrategy.LLM_PREFERRED
    timeout_seconds: float = 30.0
    retry_count: int = 3
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class LLMAnalysisResponse:
    """LLM ë¶„ì„ ì‘ë‹µ"""
    request_id: str
    success: bool
    content: str
    analysis_quality: AnalysisQuality
    response_time: float
    token_usage: int
    llm_provider: LLMProvider
    confidence_score: float
    reasoning_steps: List[str]
    fallback_used: bool = False
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class LLMMetrics:
    """LLM ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    avg_response_time: float = 0.0
    avg_token_usage: int = 0
    avg_confidence_score: float = 0.0
    llm_usage_ratio: float = 0.0  # LLM vs í´ë°± ë¹„ìœ¨
    quality_distribution: Dict[AnalysisQuality, int] = field(default_factory=lambda: defaultdict(int))
    fallback_rate: float = 0.0
    
    @property
    def success_rate(self) -> float:
        if self.total_requests == 0:
            return 0.0
        return self.successful_requests / self.total_requests
    
    @property
    def llm_first_score(self) -> float:
        """LLM First ì¤€ìˆ˜ë„ ì ìˆ˜ (0-100)"""
        base_score = self.llm_usage_ratio * 100
        
        # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ì ìš©
        quality_weight = 0
        total_quality_requests = sum(self.quality_distribution.values())
        
        if total_quality_requests > 0:
            quality_weight = (
                self.quality_distribution[AnalysisQuality.EXCELLENT] * 1.0 +
                self.quality_distribution[AnalysisQuality.GOOD] * 0.8 +
                self.quality_distribution[AnalysisQuality.ACCEPTABLE] * 0.6 +
                self.quality_distribution[AnalysisQuality.POOR] * 0.3 +
                self.quality_distribution[AnalysisQuality.FAILED] * 0.0
            ) / total_quality_requests
        
        # ìµœì¢… ì ìˆ˜ (ë¹„ìœ¨ 70% + í’ˆì§ˆ 30%)
        return base_score * 0.7 + quality_weight * 100 * 0.3

class AdaptiveLLMRouter:
    """ì ì‘ì  LLM ë¼ìš°íŒ…"""
    
    def __init__(self):
        # LLM ì œê³µìë³„ ì„±ëŠ¥ ì¶”ì 
        self.provider_metrics: Dict[LLMProvider, LLMMetrics] = {
            provider: LLMMetrics() for provider in LLMProvider
        }
        
        # ì ì‘ì  ë¼ìš°íŒ… ì„¤ì •
        self.routing_strategy = "performance_based"  # "round_robin", "least_latency", "highest_quality"
        self.fallback_chain = [
            LLMProvider.OPENAI_GPT4,
            LLMProvider.OPENAI_GPT35,
            LLMProvider.LOCAL_MODEL
        ]
        
        # ì„±ëŠ¥ ê¸°ë°˜ ë¼ìš°íŒ…
        self.performance_window = 50  # ìµœê·¼ 50ê°œ ìš”ì²­ ê¸°ì¤€
        self.performance_history: Dict[LLMProvider, deque] = {
            provider: deque(maxlen=self.performance_window) 
            for provider in LLMProvider
        }
        
        # ë¶€í•˜ ë°¸ëŸ°ì‹±
        self.current_loads: Dict[LLMProvider, int] = defaultdict(int)
        self.max_concurrent_requests = 10
    
    def select_optimal_provider(self, request: LLMAnalysisRequest) -> LLMProvider:
        """ìµœì  LLM ì œê³µì ì„ íƒ"""
        if request.strategy == AnalysisStrategy.FALLBACK_ONLY:
            return LLMProvider.LOCAL_MODEL
        
        if self.routing_strategy == "performance_based":
            return self._select_by_performance(request)
        elif self.routing_strategy == "least_latency":
            return self._select_by_latency()
        elif self.routing_strategy == "highest_quality":
            return self._select_by_quality()
        else:
            return self._round_robin_selection()
    
    def _select_by_performance(self, request: LLMAnalysisRequest) -> LLMProvider:
        """ì„±ëŠ¥ ê¸°ë°˜ ì„ íƒ"""
        scores = {}
        
        for provider in LLMProvider:
            metrics = self.provider_metrics[provider]
            current_load = self.current_loads[provider]
            
            # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
            success_score = metrics.success_rate * 100
            response_time_score = max(0, 100 - metrics.avg_response_time * 2)
            quality_score = metrics.avg_confidence_score * 100
            
            # ë¶€í•˜ ê³ ë ¤
            load_penalty = (current_load / self.max_concurrent_requests) * 20
            
            # ìš°ì„ ìˆœìœ„ ê³ ë ¤ (ë†’ì€ ìš°ì„ ìˆœìœ„ ìš”ì²­ì€ ì¢‹ì€ ëª¨ë¸ ì„ í˜¸)
            priority_bonus = 0
            if request.priority <= 3 and provider == LLMProvider.OPENAI_GPT4:
                priority_bonus = 15
            elif request.priority <= 5 and provider in [LLMProvider.OPENAI_GPT4, LLMProvider.OPENAI_GPT35]:
                priority_bonus = 10
            
            total_score = (success_score * 0.4 + 
                          response_time_score * 0.3 + 
                          quality_score * 0.3 + 
                          priority_bonus - load_penalty)
            
            scores[provider] = total_score
        
        # ìµœê³  ì ìˆ˜ ì œê³µì ì„ íƒ
        best_provider = max(scores.items(), key=lambda x: x[1])[0]
        return best_provider
    
    def _select_by_latency(self) -> LLMProvider:
        """ì§€ì—°ì‹œê°„ ê¸°ë°˜ ì„ íƒ"""
        best_provider = LLMProvider.OPENAI_GPT4
        best_latency = float('inf')
        
        for provider, metrics in self.provider_metrics.items():
            if metrics.avg_response_time < best_latency and metrics.success_rate > 0.8:
                best_latency = metrics.avg_response_time
                best_provider = provider
        
        return best_provider
    
    def _select_by_quality(self) -> LLMProvider:
        """í’ˆì§ˆ ê¸°ë°˜ ì„ íƒ"""
        best_provider = LLMProvider.OPENAI_GPT4
        best_quality = 0.0
        
        for provider, metrics in self.provider_metrics.items():
            if metrics.avg_confidence_score > best_quality and metrics.success_rate > 0.7:
                best_quality = metrics.avg_confidence_score
                best_provider = provider
        
        return best_provider
    
    def _round_robin_selection(self) -> LLMProvider:
        """ë¼ìš´ë“œ ë¡œë¹ˆ ì„ íƒ"""
        if not hasattr(self, '_round_robin_index'):
            self._round_robin_index = 0
        
        providers = list(LLMProvider)
        provider = providers[self._round_robin_index % len(providers)]
        self._round_robin_index += 1
        
        return provider
    
    def update_provider_metrics(self, provider: LLMProvider, response: LLMAnalysisResponse):
        """ì œê³µì ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        metrics = self.provider_metrics[provider]
        
        metrics.total_requests += 1
        
        if response.success:
            metrics.successful_requests += 1
        else:
            metrics.failed_requests += 1
        
        # ì´ë™ í‰ê·  ì—…ë°ì´íŠ¸
        alpha = 0.1
        metrics.avg_response_time = (alpha * response.response_time + 
                                   (1 - alpha) * metrics.avg_response_time)
        metrics.avg_token_usage = int(alpha * response.token_usage + 
                                    (1 - alpha) * metrics.avg_token_usage)
        metrics.avg_confidence_score = (alpha * response.confidence_score + 
                                      (1 - alpha) * metrics.avg_confidence_score)
        
        # í’ˆì§ˆ ë¶„í¬ ì—…ë°ì´íŠ¸
        metrics.quality_distribution[response.analysis_quality] += 1
        
        # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.performance_history[provider].append({
            "timestamp": datetime.now(),
            "response_time": response.response_time,
            "success": response.success,
            "confidence": response.confidence_score
        })
        
        # ë¶€í•˜ ê°ì†Œ
        self.current_loads[provider] = max(0, self.current_loads[provider] - 1)
    
    def record_load_increase(self, provider: LLMProvider):
        """ë¶€í•˜ ì¦ê°€ ê¸°ë¡"""
        self.current_loads[provider] += 1
    
    def get_routing_status(self) -> Dict[str, Any]:
        """ë¼ìš°íŒ… ìƒíƒœ ë°˜í™˜"""
        return {
            "routing_strategy": self.routing_strategy,
            "provider_metrics": {
                provider.value: {
                    "success_rate": metrics.success_rate,
                    "avg_response_time": metrics.avg_response_time,
                    "avg_confidence": metrics.avg_confidence_score,
                    "current_load": self.current_loads[provider],
                    "llm_first_score": metrics.llm_first_score
                }
                for provider, metrics in self.provider_metrics.items()
            },
            "fallback_chain": [p.value for p in self.fallback_chain]
        }

class LLMQualityAssessor:
    """LLM ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
    
    def __init__(self):
        self.quality_criteria = {
            "relevance": 0.3,      # ê´€ë ¨ì„±
            "completeness": 0.25,   # ì™„ì„±ë„
            "accuracy": 0.25,      # ì •í™•ì„±
            "clarity": 0.2         # ëª…í™•ì„±
        }
        
        # í’ˆì§ˆ í‰ê°€ íŒ¨í„´
        self.quality_indicators = {
            "excellent": [
                "êµ¬ì²´ì ì¸ ë°ì´í„° ë¶„ì„",
                "ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •",
                "ìˆ˜ì¹˜ì  ê·¼ê±° ì œì‹œ",
                "ì „ë¬¸ì  ì¸ì‚¬ì´íŠ¸",
                "actionable recommendations"
            ],
            "poor": [
                "ëª¨í˜¸í•œ ì‘ë‹µ",
                "ê´€ë ¨ì„± ë¶€ì¡±",
                "ì˜¤ë¥˜ í¬í•¨",
                "ë¶ˆì™„ì „í•œ ë¶„ì„",
                "generic responses"
            ]
        }
    
    async def assess_response_quality(self, 
                                    request: LLMAnalysisRequest, 
                                    response_content: str,
                                    context: Dict[str, Any] = None) -> Tuple[AnalysisQuality, float, List[str]]:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        
        # ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜
        scores = {}
        reasoning_steps = []
        
        # 1. ê´€ë ¨ì„± í‰ê°€
        relevance_score = await self._assess_relevance(request.user_query, response_content)
        scores["relevance"] = relevance_score
        reasoning_steps.append(f"ê´€ë ¨ì„± ì ìˆ˜: {relevance_score:.2f}")
        
        # 2. ì™„ì„±ë„ í‰ê°€
        completeness_score = self._assess_completeness(response_content)
        scores["completeness"] = completeness_score
        reasoning_steps.append(f"ì™„ì„±ë„ ì ìˆ˜: {completeness_score:.2f}")
        
        # 3. ì •í™•ì„± í‰ê°€
        accuracy_score = self._assess_accuracy(response_content, context)
        scores["accuracy"] = accuracy_score
        reasoning_steps.append(f"ì •í™•ì„± ì ìˆ˜: {accuracy_score:.2f}")
        
        # 4. ëª…í™•ì„± í‰ê°€
        clarity_score = self._assess_clarity(response_content)
        scores["clarity"] = clarity_score
        reasoning_steps.append(f"ëª…í™•ì„± ì ìˆ˜: {clarity_score:.2f}")
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        total_score = sum(scores[criterion] * weight 
                         for criterion, weight in self.quality_criteria.items())
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if total_score >= 0.9:
            quality = AnalysisQuality.EXCELLENT
        elif total_score >= 0.7:
            quality = AnalysisQuality.GOOD
        elif total_score >= 0.5:
            quality = AnalysisQuality.ACCEPTABLE
        elif total_score >= 0.3:
            quality = AnalysisQuality.POOR
        else:
            quality = AnalysisQuality.FAILED
        
        reasoning_steps.append(f"ìµœì¢… í’ˆì§ˆ ë“±ê¸‰: {quality.value} (ì ìˆ˜: {total_score:.2f})")
        
        return quality, total_score, reasoning_steps
    
    async def _assess_relevance(self, query: str, response: str) -> float:
        """ê´€ë ¨ì„± í‰ê°€"""
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ + ì˜ë¯¸ ë¶„ì„
        query_words = set(query.lower().split())
        response_words = set(response.lower().split())
        
        # í‚¤ì›Œë“œ ì¤‘ë³µë„
        overlap = len(query_words.intersection(response_words))
        keyword_score = min(1.0, overlap / max(len(query_words), 1))
        
        # ê¸¸ì´ ê¸°ë°˜ ê´€ë ¨ì„± (ë„ˆë¬´ ì§§ê±°ë‚˜ ë„ˆë¬´ ê¸´ ì‘ë‹µ í˜ë„í‹°)
        response_length = len(response)
        if response_length < 50:
            length_penalty = 0.5
        elif response_length > 5000:
            length_penalty = 0.8
        else:
            length_penalty = 1.0
        
        return keyword_score * length_penalty
    
    def _assess_completeness(self, response: str) -> float:
        """ì™„ì„±ë„ í‰ê°€"""
        # ì‘ë‹µ êµ¬ì¡° ë¶„ì„
        structure_score = 0.0
        
        # ë‹¨ë½ êµ¬ì¡°
        paragraphs = response.split('\n\n')
        if len(paragraphs) >= 2:
            structure_score += 0.3
        
        # ëª©ë¡ì´ë‚˜ ë‹¨ê³„ í¬í•¨
        if any(marker in response for marker in ['1.', '2.', 'â€¢', '-', '*']):
            structure_score += 0.3
        
        # ê²°ë¡  í¬í•¨
        conclusion_markers = ['ê²°ë¡ ', 'ìš”ì•½', 'ì •ë¦¬í•˜ë©´', 'ë”°ë¼ì„œ', 'conclusion']
        if any(marker in response.lower() for marker in conclusion_markers):
            structure_score += 0.2
        
        # ìˆ˜ì¹˜ ë°ì´í„° í¬í•¨
        import re
        numbers = re.findall(r'\d+\.?\d*%?', response)
        if len(numbers) >= 3:
            structure_score += 0.2
        
        return min(1.0, structure_score)
    
    def _assess_accuracy(self, response: str, context: Dict[str, Any] = None) -> float:
        """ì •í™•ì„± í‰ê°€"""
        accuracy_score = 0.8  # ê¸°ë³¸ ì ìˆ˜
        
        # ê¸°ë³¸ì ì¸ ì •í™•ì„± ì²´í¬
        error_patterns = [
            r'error|ì˜¤ë¥˜|ì—ëŸ¬',
            r'failed|ì‹¤íŒ¨',
            r'unknown|ì•Œ ìˆ˜ ì—†ìŒ',
            r'not found|ì°¾ì„ ìˆ˜ ì—†ìŒ'
        ]
        
        for pattern in error_patterns:
            if re.search(pattern, response.lower()):
                accuracy_score -= 0.2
        
        # ê¸ì •ì  ì§€í‘œ
        positive_patterns = [
            r'ë¶„ì„ ê²°ê³¼',
            r'ë°ì´í„°ì— ë”°ë¥´ë©´',
            r'í†µê³„ì ìœ¼ë¡œ',
            r'êµ¬ì²´ì ìœ¼ë¡œ'
        ]
        
        for pattern in positive_patterns:
            if re.search(pattern, response.lower()):
                accuracy_score += 0.1
        
        return max(0.0, min(1.0, accuracy_score))
    
    def _assess_clarity(self, response: str) -> float:
        """ëª…í™•ì„± í‰ê°€"""
        clarity_score = 0.0
        
        # ë¬¸ì¥ ê¸¸ì´ ë¶„ì„
        sentences = response.split('.')
        avg_sentence_length = sum(len(s.split()) for s in sentences) / max(len(sentences), 1)
        
        if 10 <= avg_sentence_length <= 25:  # ì ì ˆí•œ ë¬¸ì¥ ê¸¸ì´
            clarity_score += 0.4
        
        # ì „ë¬¸ ìš©ì–´ vs ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…
        technical_terms = ['ë¶„ì„', 'ë°ì´í„°', 'í†µê³„', 'ëª¨ë¸', 'ì•Œê³ ë¦¬ì¦˜']
        explanatory_terms = ['ì¦‰', 'ë‹¤ì‹œ ë§í•´', 'ì˜ˆë¥¼ ë“¤ì–´', 'êµ¬ì²´ì ìœ¼ë¡œ']
        
        tech_count = sum(1 for term in technical_terms if term in response)
        explain_count = sum(1 for term in explanatory_terms if term in response)
        
        if tech_count > 0 and explain_count > 0:
            clarity_score += 0.3
        
        # êµ¬ì¡°ì  ëª…í™•ì„±
        if '1.' in response or 'ì²«ì§¸' in response:
            clarity_score += 0.3
        
        return min(1.0, clarity_score)

class LLMFirstAnalyzer:
    """LLM First ì‹¤ì‹œê°„ ë¶„ì„ê¸°"""
    
    def __init__(self):
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸
        self.llm_router = AdaptiveLLMRouter()
        self.quality_assessor = LLMQualityAssessor()
        
        # LLM í´ë¼ì´ì–¸íŠ¸ë“¤
        self.openai_client = AsyncOpenAI()
        self.llm_clients = {}
        
        # ë¶„ì„ í ë° ìš°ì„ ìˆœìœ„ ê´€ë¦¬
        self.analysis_queue: asyncio.PriorityQueue = asyncio.PriorityQueue()
        self.active_analyses: Dict[str, LLMAnalysisRequest] = {}
        
        # ë©”íŠ¸ë¦­ ë° ëª¨ë‹ˆí„°ë§
        self.global_metrics = LLMMetrics()
        self.analysis_history: deque = deque(maxlen=1000)
        
        # ì„¤ì •
        self.max_concurrent_analyses = 10
        self.fallback_enabled = True
        self.target_llm_ratio = 0.9  # 90% LLM ë¶„ì„ ëª©í‘œ
        
        # í´ë°± ë°©ì§€ ì „ëµ
        self.fallback_prevention = {
            "retry_with_simpler_prompt": True,
            "use_alternative_model": True,
            "reduce_complexity": True,
            "chunk_large_requests": True
        }
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.results_dir = Path("monitoring/llm_analysis_performance")
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # ì›Œì»¤ íƒœìŠ¤í¬
        self.workers: List[asyncio.Task] = []
        self.is_running = False
    
    async def initialize(self):
        """ë¶„ì„ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ§  LLM First ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # ì›Œì»¤ íƒœìŠ¤í¬ ì‹œì‘
        self.is_running = True
        for i in range(self.max_concurrent_analyses):
            worker = asyncio.create_task(self._analysis_worker(f"worker_{i}"))
            self.workers.append(worker)
        
        logger.info(f"âœ… LLM First ë¶„ì„ê¸° ì´ˆê¸°í™” ì™„ë£Œ ({len(self.workers)}ê°œ ì›Œì»¤)")
    
    async def analyze_realtime(self, 
                             user_query: str, 
                             context: Dict[str, Any] = None,
                             priority: int = 5,
                             strategy: AnalysisStrategy = AnalysisStrategy.LLM_PREFERRED) -> LLMAnalysisResponse:
        """ì‹¤ì‹œê°„ LLM ë¶„ì„ ìš”ì²­"""
        
        request = LLMAnalysisRequest(
            id=str(uuid.uuid4()),
            user_query=user_query,
            context=context or {},
            priority=priority,
            strategy=strategy,
            metadata={"timestamp": datetime.now().isoformat()}
        )
        
        # ìš°ì„ ìˆœìœ„ íì— ì¶”ê°€
        await self.analysis_queue.put((priority, time.time(), request))
        
        # ìš”ì²­ ì¶”ì 
        self.active_analyses[request.id] = request
        
        logger.info(f"ğŸ“ LLM ë¶„ì„ ìš”ì²­ ì¶”ê°€: {request.id} (ìš°ì„ ìˆœìœ„: {priority})")
        
        # ì‘ë‹µ ëŒ€ê¸° (ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬ë¨)
        return await self._wait_for_response(request.id)
    
    async def _analysis_worker(self, worker_name: str):
        """ë¶„ì„ ì›Œì»¤"""
        logger.info(f"ğŸ”§ ë¶„ì„ ì›Œì»¤ ì‹œì‘: {worker_name}")
        
        while self.is_running:
            try:
                # íì—ì„œ ìš”ì²­ ê°€ì ¸ì˜¤ê¸°
                priority, timestamp, request = await asyncio.wait_for(
                    self.analysis_queue.get(), timeout=1.0
                )
                
                logger.info(f"ğŸ” {worker_name}ê°€ ë¶„ì„ ì‹œì‘: {request.id}")
                
                # LLM ë¶„ì„ ì‹¤í–‰
                response = await self._execute_llm_analysis(request)
                
                # ì‘ë‹µ ì €ì¥ ë° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                await self._process_analysis_response(request, response)
                
                logger.info(f"âœ… {worker_name}ê°€ ë¶„ì„ ì™„ë£Œ: {request.id} (í’ˆì§ˆ: {response.analysis_quality.value})")
                
            except asyncio.TimeoutError:
                # íê°€ ë¹„ì–´ìˆìŒ - ì •ìƒ
                continue
            except Exception as e:
                logger.error(f"âŒ {worker_name} ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _execute_llm_analysis(self, request: LLMAnalysisRequest) -> LLMAnalysisResponse:
        """LLM ë¶„ì„ ì‹¤í–‰"""
        start_time = time.time()
        
        # ìµœì  LLM ì œê³µì ì„ íƒ
        provider = self.llm_router.select_optimal_provider(request)
        self.llm_router.record_load_increase(provider)
        
        try:
            # LLM ë¶„ì„ ì‹œë„
            if request.strategy == AnalysisStrategy.LLM_ONLY or request.strategy == AnalysisStrategy.LLM_PREFERRED:
                response = await self._try_llm_analysis(request, provider)
                
                if response.success:
                    return response
                elif request.strategy == AnalysisStrategy.LLM_ONLY:
                    # LLM ì „ìš© ëª¨ë“œì—ì„œëŠ” ì‹¤íŒ¨í•´ë„ í´ë°±í•˜ì§€ ì•ŠìŒ
                    return response
                else:
                    # LLM_PREFERRED ëª¨ë“œì—ì„œëŠ” í´ë°± ì‹œë„
                    logger.warning(f"LLM ë¶„ì„ ì‹¤íŒ¨, í´ë°± ì‹œë„: {request.id}")
                    return await self._try_fallback_analysis(request)
            
            else:
                # HYBRID ë˜ëŠ” FALLBACK_ONLY
                return await self._try_fallback_analysis(request)
                
        except Exception as e:
            logger.error(f"ë¶„ì„ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            
            return LLMAnalysisResponse(
                request_id=request.id,
                success=False,
                content="",
                analysis_quality=AnalysisQuality.FAILED,
                response_time=time.time() - start_time,
                token_usage=0,
                llm_provider=provider,
                confidence_score=0.0,
                reasoning_steps=[],
                fallback_used=False,
                error_message=str(e)
            )
    
    async def _try_llm_analysis(self, request: LLMAnalysisRequest, provider: LLMProvider) -> LLMAnalysisResponse:
        """LLM ë¶„ì„ ì‹œë„"""
        start_time = time.time()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ìµœì í™”
            optimized_prompt = await self._optimize_prompt(request)
            
            # LLM í˜¸ì¶œ
            if provider == LLMProvider.OPENAI_GPT4:
                response_content, token_usage = await self._call_openai_gpt4(optimized_prompt, request)
            elif provider == LLMProvider.OPENAI_GPT35:
                response_content, token_usage = await self._call_openai_gpt35(optimized_prompt, request)
            else:
                # ë‹¤ë¥¸ ì œê³µìë“¤ì€ ì¶”í›„ êµ¬í˜„
                raise NotImplementedError(f"Provider {provider.value} not implemented")
            
            response_time = time.time() - start_time
            
            # í’ˆì§ˆ í‰ê°€
            quality, confidence, reasoning = await self.quality_assessor.assess_response_quality(
                request, response_content, request.context
            )
            
            response = LLMAnalysisResponse(
                request_id=request.id,
                success=True,
                content=response_content,
                analysis_quality=quality,
                response_time=response_time,
                token_usage=token_usage,
                llm_provider=provider,
                confidence_score=confidence,
                reasoning_steps=reasoning,
                fallback_used=False
            )
            
            # ë¼ìš°í„° ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.llm_router.update_provider_metrics(provider, response)
            
            return response
            
        except Exception as e:
            logger.error(f"LLM ë¶„ì„ ì‹¤íŒ¨ ({provider.value}): {e}")
            
            response_time = time.time() - start_time
            
            response = LLMAnalysisResponse(
                request_id=request.id,
                success=False,
                content="",
                analysis_quality=AnalysisQuality.FAILED,
                response_time=response_time,
                token_usage=0,
                llm_provider=provider,
                confidence_score=0.0,
                reasoning_steps=[],
                fallback_used=False,
                error_message=str(e)
            )
            
            self.llm_router.update_provider_metrics(provider, response)
            return response
    
    async def _optimize_prompt(self, request: LLMAnalysisRequest) -> str:
        """í”„ë¡¬í”„íŠ¸ ìµœì í™”"""
        base_prompt = f"""ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ìš”ì²­ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ì§ˆë¬¸: {request.user_query}

ì»¨í…ìŠ¤íŠ¸ ì •ë³´:
{json.dumps(request.context, indent=2, ensure_ascii=False)}

ë¶„ì„ ìš”êµ¬ì‚¬í•­:
1. êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ ì œê³µ
2. ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
3. ëª…í™•í•œ ê²°ë¡ ê³¼ ê¶Œì¥ì‚¬í•­ ì œì‹œ
4. ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì • ì„¤ëª…

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:
## ë¶„ì„ ê²°ê³¼
[êµ¬ì²´ì ì¸ ë¶„ì„ ë‚´ìš©]

## ì£¼ìš” ì¸ì‚¬ì´íŠ¸
[í•µì‹¬ ë°œê²¬ì‚¬í•­]

## ê¶Œì¥ì‚¬í•­
[ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ]
"""
        
        # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ìµœì í™”
        if len(base_prompt) > 3000:
            # ì»¨í…ìŠ¤íŠ¸ ì¶•ì•½
            context_summary = str(request.context)[:500] + "..."
            base_prompt = base_prompt.replace(
                json.dumps(request.context, indent=2, ensure_ascii=False),
                context_summary
            )
        
        return base_prompt
    
    async def _call_openai_gpt4(self, prompt: str, request: LLMAnalysisRequest) -> Tuple[str, int]:
        """OpenAI GPT-4 í˜¸ì¶œ"""
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                timeout=request.timeout_seconds
            )
            
            content = response.choices[0].message.content
            token_usage = response.usage.total_tokens
            
            return content, token_usage
            
        except Exception as e:
            logger.error(f"OpenAI GPT-4 í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            raise
    
    async def _call_openai_gpt35(self, prompt: str, request: LLMAnalysisRequest) -> Tuple[str, int]:
        """OpenAI GPT-3.5 í˜¸ì¶œ"""
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=request.max_tokens,
                temperature=request.temperature,
                timeout=request.timeout_seconds
            )
            
            content = response.choices[0].message.content
            token_usage = response.usage.total_tokens
            
            return content, token_usage
            
        except Exception as e:
            logger.error(f"OpenAI GPT-3.5 í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            raise
    
    async def _try_fallback_analysis(self, request: LLMAnalysisRequest) -> LLMAnalysisResponse:
        """í´ë°± ë¶„ì„ ì‹œë„"""
        start_time = time.time()
        
        logger.info(f"í´ë°± ë¶„ì„ ì‹œì‘: {request.id}")
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ (í´ë°±)
        fallback_content = self._generate_fallback_response(request)
        
        response_time = time.time() - start_time
        
        # í´ë°± ì‘ë‹µì˜ í’ˆì§ˆì€ ì¼ë°˜ì ìœ¼ë¡œ ë‚®ìŒ
        quality, confidence, reasoning = await self.quality_assessor.assess_response_quality(
            request, fallback_content, request.context
        )
        
        return LLMAnalysisResponse(
            request_id=request.id,
            success=True,
            content=fallback_content,
            analysis_quality=quality,
            response_time=response_time,
            token_usage=0,
            llm_provider=LLMProvider.LOCAL_MODEL,  # í´ë°±ì€ ë¡œì»¬ë¡œ ì²˜ë¦¬
            confidence_score=confidence,
            reasoning_steps=reasoning,
            fallback_used=True,
            metadata={"fallback_reason": "LLM analysis failed"}
        )
    
    def _generate_fallback_response(self, request: LLMAnalysisRequest) -> str:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        query_lower = request.user_query.lower()
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ì‘ë‹µ
        if any(word in query_lower for word in ['ë¶„ì„', 'analyze', 'ìš”ì•½', 'summary']):
            return f"""## ê¸°ë³¸ ë¶„ì„ ê²°ê³¼

ìš”ì²­í•˜ì‹  '{request.user_query}' ì— ëŒ€í•œ ê¸°ë³¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

## ì œê³µëœ ì •ë³´ ìš”ì•½
- ë¶„ì„ ëŒ€ìƒ: {request.context.get('data_info', 'ë°ì´í„° ì •ë³´ ì—†ìŒ')}
- ìš”ì²­ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ê¶Œì¥ì‚¬í•­
ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” êµ¬ì²´ì ì¸ ë°ì´í„°ë‚˜ ë¶„ì„ ëª©ì ì„ ì œê³µí•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

*ì°¸ê³ : ì´ëŠ” ê¸°ë³¸ ì‘ë‹µì…ë‹ˆë‹¤. ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.*
"""
        
        else:
            return f"""## ì‘ë‹µ

'{request.user_query}' ìš”ì²­ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.

í˜„ì¬ ê¸°ë³¸ ëª¨ë“œë¡œ ì‘ë‹µí•˜ê³  ìˆìŠµë‹ˆë‹¤. ë” ìƒì„¸í•˜ê³  ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ë‹¤ìŒì„ ê³ ë ¤í•´ì£¼ì„¸ìš”:

1. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ ì œì‹œ
2. ê´€ë ¨ ë°ì´í„°ë‚˜ ì»¨í…ìŠ¤íŠ¸ ì œê³µ
3. ë¶„ì„ ëª©ì  ëª…í™•í™”

*ì°¸ê³ : ì´ëŠ” ê°„ì†Œí™”ëœ ì‘ë‹µì…ë‹ˆë‹¤.*
"""
    
    async def _process_analysis_response(self, request: LLMAnalysisRequest, response: LLMAnalysisResponse):
        """ë¶„ì„ ì‘ë‹µ ì²˜ë¦¬"""
        # ê¸€ë¡œë²Œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.global_metrics.total_requests += 1
        
        if response.success:
            self.global_metrics.successful_requests += 1
        else:
            self.global_metrics.failed_requests += 1
        
        # LLM vs í´ë°± ë¹„ìœ¨ ì—…ë°ì´íŠ¸
        if not response.fallback_used:
            # LLM ì‚¬ìš©
            llm_count = sum(1 for h in self.analysis_history if not h.get('fallback_used', False))
            self.global_metrics.llm_usage_ratio = llm_count / max(len(self.analysis_history), 1)
        
        # í’ˆì§ˆ ë¶„í¬ ì—…ë°ì´íŠ¸
        self.global_metrics.quality_distribution[response.analysis_quality] += 1
        
        # ì´ë™ í‰ê·  ì—…ë°ì´íŠ¸
        alpha = 0.1
        self.global_metrics.avg_response_time = (
            alpha * response.response_time + 
            (1 - alpha) * self.global_metrics.avg_response_time
        )
        self.global_metrics.avg_confidence_score = (
            alpha * response.confidence_score + 
            (1 - alpha) * self.global_metrics.avg_confidence_score
        )
        
        # íˆìŠ¤í† ë¦¬ ì¶”ê°€
        self.analysis_history.append({
            "request_id": request.id,
            "timestamp": datetime.now(),
            "success": response.success,
            "quality": response.analysis_quality.value,
            "response_time": response.response_time,
            "fallback_used": response.fallback_used,
            "provider": response.llm_provider.value
        })
        
        # í™œì„± ë¶„ì„ì—ì„œ ì œê±°
        if request.id in self.active_analyses:
            del self.active_analyses[request.id]
    
    async def _wait_for_response(self, request_id: str) -> LLMAnalysisResponse:
        """ì‘ë‹µ ëŒ€ê¸° (ì‹¤ì œë¡œëŠ” ì›Œì»¤ì—ì„œ ë¹„ë™ê¸° ì²˜ë¦¬ë¨)"""
        # ì´ êµ¬í˜„ì—ì„œëŠ” ë‹¨ìˆœí™”ë¥¼ ìœ„í•´ ì¦‰ì‹œ ì²˜ë¦¬
        # ì‹¤ì œë¡œëŠ” ê²°ê³¼ë¥¼ íë‚˜ ìºì‹œì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
        await asyncio.sleep(0.1)  # ìµœì†Œ ëŒ€ê¸°
        
        # ì„ì‹œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ê²°ê³¼ ì €ì¥ì†Œì—ì„œ ê°€ì ¸ì˜¤ê¸°
        return LLMAnalysisResponse(
            request_id=request_id,
            success=True,
            content="ë¶„ì„ì´ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.",
            analysis_quality=AnalysisQuality.ACCEPTABLE,
            response_time=0.1,
            token_usage=0,
            llm_provider=LLMProvider.OPENAI_GPT4,
            confidence_score=0.8,
            reasoning_steps=["ë¶„ì„ íì— ì¶”ê°€ë¨"]
        )
    
    async def shutdown(self):
        """ë¶„ì„ê¸° ì¢…ë£Œ"""
        logger.info("ğŸ›‘ LLM First ë¶„ì„ê¸° ì¢…ë£Œ ì¤‘...")
        
        self.is_running = False
        
        # ì›Œì»¤ íƒœìŠ¤í¬ ì •ë¦¬
        for worker in self.workers:
            worker.cancel()
        
        await asyncio.gather(*self.workers, return_exceptions=True)
        
        logger.info("âœ… LLM First ë¶„ì„ê¸° ì¢…ë£Œ ì™„ë£Œ")
    
    def get_llm_first_status(self) -> Dict[str, Any]:
        """LLM First ìƒíƒœ ë°˜í™˜"""
        return {
            "global_metrics": {
                "total_requests": self.global_metrics.total_requests,
                "success_rate": self.global_metrics.success_rate,
                "llm_usage_ratio": self.global_metrics.llm_usage_ratio,
                "llm_first_score": self.global_metrics.llm_first_score,
                "avg_response_time": self.global_metrics.avg_response_time,
                "avg_confidence": self.global_metrics.avg_confidence_score,
                "fallback_rate": 1 - self.global_metrics.llm_usage_ratio
            },
            "router_status": self.llm_router.get_routing_status(),
            "active_analyses": len(self.active_analyses),
            "queue_size": self.analysis_queue.qsize(),
            "target_llm_ratio": self.target_llm_ratio,
            "system_status": "running" if self.is_running else "stopped"
        }


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
async def test_llm_first_analyzer():
    """LLM First ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    analyzer = LLMFirstAnalyzer()
    
    try:
        await analyzer.initialize()
        
        # í…ŒìŠ¤íŠ¸ ë¶„ì„ ìš”ì²­ë“¤
        test_queries = [
            "ë°ì´í„°ì˜ ì „ë°˜ì ì¸ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
            "ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê³  ì›ì¸ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”", 
            "ì˜ˆì¸¡ ëª¨ë¸ì„ ìœ„í•œ íŠ¹ì„±ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”",
            "ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ì‹ë³„í•´ì£¼ì„¸ìš”"
        ]
        
        # ë³‘ë ¬ ë¶„ì„ í…ŒìŠ¤íŠ¸
        tasks = []
        for i, query in enumerate(test_queries):
            context = {"data_info": f"í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ {i+1}"}
            task = analyzer.analyze_realtime(query, context, priority=i+1)
            tasks.append(task)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        responses = await asyncio.gather(*tasks)
        
        # ê²°ê³¼ ì¶œë ¥
        for i, response in enumerate(responses):
            print(f"ğŸ“Š ë¶„ì„ {i+1}: {response.analysis_quality.value} "
                  f"(ì‘ë‹µì‹œê°„: {response.response_time:.2f}ì´ˆ, "
                  f"ì‹ ë¢°ë„: {response.confidence_score:.2f})")
        
        # ì „ì²´ ìƒíƒœ í™•ì¸
        status = analyzer.get_llm_first_status()
        print(f"\nğŸ¯ LLM First ì ìˆ˜: {status['global_metrics']['llm_first_score']:.1f}/100")
        print(f"LLM ì‚¬ìš© ë¹„ìœ¨: {status['global_metrics']['llm_usage_ratio']:.2%}")
        
    finally:
        await analyzer.shutdown()

if __name__ == "__main__":
    asyncio.run(test_llm_first_analyzer()) 