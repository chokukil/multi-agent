"""
Enhanced File Upload - í–¥ìƒëœ íŒŒì¼ ì—…ë¡œë“œ ì¸í„°í˜ì´ìŠ¤

ìš”êµ¬ì‚¬í•­ 3.2ì— ë”°ë¥¸ êµ¬í˜„:
- Universal Engine ê¸°ë°˜ ìë™ ë„ë©”ì¸ ê°ì§€ ê¸°ëŠ¥
- ë°ì´í„° í’ˆì§ˆ í‰ê°€ ë° ì‹œê°í™”
- ì¶”ì²œ ë¶„ì„ ë²„íŠ¼ ë° ìë™ ì§ˆë¬¸ ìƒì„±
- ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ê¸°ë³¸ í†µê³„ í‘œì‹œ
"""

import streamlit as st
import pandas as pd
import numpy as np
import asyncio
import logging
from typing import Dict, Any, Optional, List, Union
from datetime import datetime
import json
import io

from ..dynamic_context_discovery import DynamicContextDiscovery
from ...llm_factory import LLMFactory

logger = logging.getLogger(__name__)


class EnhancedFileUpload:
    """
    í–¥ìƒëœ íŒŒì¼ ì—…ë¡œë“œ ì»´í¬ë„ŒíŠ¸
    - Universal Engine ê¸°ë°˜ ìë™ ë„ë©”ì¸ ê°ì§€
    - ì§€ëŠ¥ì  ë°ì´í„° ë¶„ì„ ë° ì¶”ì²œ
    - ì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤
    """
    
    def __init__(self):
        """EnhancedFileUpload ì´ˆê¸°í™”"""
        self.context_discovery = DynamicContextDiscovery()
        self.llm_client = LLMFactory.create_llm()
        self.supported_formats = ['csv', 'xlsx', 'xls', 'json', 'txt', 'parquet']
        logger.info("EnhancedFileUpload initialized")
    
    def render_file_upload_interface(self):
        """ğŸ“ ì§ê´€ì  íŒŒì¼ ì—…ë¡œë“œ ì¸í„°í˜ì´ìŠ¤ + Universal Engine ë°ì´í„° ë¶„ì„"""
        
        st.markdown("### ğŸ“ ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ")
        st.caption("Universal Engineì´ ìë™ìœ¼ë¡œ ë°ì´í„° ìœ í˜•ê³¼ ë„ë©”ì¸ì„ ê°ì§€í•©ë‹ˆë‹¤")
        
        # íŒŒì¼ ì—…ë¡œë“œ ìœ„ì ¯
        uploaded_file = st.file_uploader(
            "ë°ì´í„° íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”",
            type=self.supported_formats,
            help="ì§€ì› í˜•ì‹: CSV, Excel, JSON, TXT, Parquet"
        )
        
        if uploaded_file is not None:
            # ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬
            asyncio.run(self._process_uploaded_file(uploaded_file))
    
    async def _process_uploaded_file(self, uploaded_file):
        """ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬"""
        try:
            with st.spinner("ğŸ“Š ë°ì´í„° ë¶„ì„ ì¤‘..."):
                # 1. íŒŒì¼ ì •ë³´ í‘œì‹œ
                self._display_file_info(uploaded_file)
                
                # 2. ë°ì´í„° ë¡œë“œ
                data = self._load_data(uploaded_file)
                
                if data is not None:
                    # ì„¸ì…˜ ìƒíƒœì— ë°ì´í„° ì €ì¥
                    st.session_state.current_data = data
                    
                    # 3. ê¸°ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                    self._display_data_preview(data)
                    
                    # 4. Universal Engineìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
                    context_analysis = await self._analyze_data_context(data)
                    
                    # 5. ê°ì§€ëœ ë„ë©”ì¸ê³¼ ì¶”ì²œ ë¶„ì„ í‘œì‹œ
                    if context_analysis:
                        self._display_context_analysis(context_analysis)
                        
                        # 6. ë°ì´í„° í’ˆì§ˆ í‰ê°€
                        quality_assessment = await self._assess_data_quality(data, context_analysis)
                        self._display_quality_assessment(quality_assessment)
                        
                        # 7. ì¶”ì²œ ë¶„ì„ ë° ìë™ ì§ˆë¬¸ ìƒì„±
                        recommendations = await self._generate_analysis_recommendations(
                            data, context_analysis, quality_assessment
                        )
                        self._display_analysis_recommendations(recommendations)
                    
                else:
                    st.error("ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
        except Exception as e:
            logger.error(f"Error processing uploaded file: {e}")
            st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    def _display_file_info(self, uploaded_file):
        """íŒŒì¼ ì •ë³´ í‘œì‹œ"""
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("íŒŒì¼ëª…", uploaded_file.name)
        
        with col2:
            file_size = len(uploaded_file.getvalue())
            if file_size < 1024:
                size_str = f"{file_size} bytes"
            elif file_size < 1024 * 1024:
                size_str = f"{file_size / 1024:.1f} KB"
            else:
                size_str = f"{file_size / (1024 * 1024):.1f} MB"
            st.metric("íŒŒì¼ í¬ê¸°", size_str)
        
        with col3:
            file_type = uploaded_file.name.split('.')[-1].upper()
            st.metric("íŒŒì¼ í˜•ì‹", file_type)
    
    def _load_data(self, uploaded_file) -> Optional[Union[pd.DataFrame, Dict, List]]:
        """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë°ì´í„° ë¡œë“œ"""
        file_extension = uploaded_file.name.split('.')[-1].lower()
        
        try:
            if file_extension == 'csv':
                return pd.read_csv(uploaded_file)
            
            elif file_extension in ['xlsx', 'xls']:
                return pd.read_excel(uploaded_file)
            
            elif file_extension == 'json':
                content = uploaded_file.getvalue().decode('utf-8')
                return json.loads(content)
            
            elif file_extension == 'txt':
                content = uploaded_file.getvalue().decode('utf-8')
                # êµ¬ë¶„ì ìë™ ê°ì§€ ì‹œë„
                if '\t' in content:
                    return pd.read_csv(io.StringIO(content), sep='\t')
                elif ',' in content:
                    return pd.read_csv(io.StringIO(content))
                else:
                    return content
            
            elif file_extension == 'parquet':
                return pd.read_parquet(uploaded_file)
            
            else:
                st.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
                return None
                
        except Exception as e:
            logger.error(f"Error loading data: {e}")
            st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def _display_data_preview(self, data):
        """ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ê¸°ë³¸ í†µê³„ í‘œì‹œ"""
        st.markdown("### ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        
        if isinstance(data, pd.DataFrame):
            # DataFrame ì²˜ë¦¬
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**ê¸°ë³¸ ì •ë³´:**")
                st.write(f"â€¢ í–‰ ìˆ˜: {len(data):,}")
                st.write(f"â€¢ ì—´ ìˆ˜: {len(data.columns):,}")
                st.write(f"â€¢ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {data.memory_usage(deep=True).sum() / 1024:.1f} KB")
            
            with col2:
                st.write("**ë°ì´í„° íƒ€ì…:**")
                dtype_counts = data.dtypes.value_counts()
                for dtype, count in dtype_counts.items():
                    st.write(f"â€¢ {dtype}: {count}ê°œ ì—´")
            
            # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
            st.write("**ìƒ˜í”Œ ë°ì´í„°:**")
            st.dataframe(data.head(10), use_container_width=True)
            
            # ê¸°ë³¸ í†µê³„
            if len(data.select_dtypes(include=[np.number]).columns) > 0:
                with st.expander("ğŸ“ˆ ê¸°ë³¸ í†µê³„", expanded=False):
                    st.dataframe(data.describe(), use_container_width=True)
            
            # ê²°ì¸¡ê°’ ì •ë³´
            missing_data = data.isnull().sum()
            if missing_data.any():
                with st.expander("âš ï¸ ê²°ì¸¡ê°’ ì •ë³´", expanded=False):
                    missing_df = pd.DataFrame({
                        'ê²°ì¸¡ê°’ ìˆ˜': missing_data,
                        'ê²°ì¸¡ë¥ (%)': (missing_data / len(data) * 100).round(2)
                    })
                    missing_df = missing_df[missing_df['ê²°ì¸¡ê°’ ìˆ˜'] > 0]
                    st.dataframe(missing_df, use_container_width=True)
        
        elif isinstance(data, dict):
            # JSON ë°ì´í„° ì²˜ë¦¬
            st.write("**JSON êµ¬ì¡°:**")
            st.json(data)
            
        elif isinstance(data, list):
            # ë¦¬ìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬
            st.write(f"**ë¦¬ìŠ¤íŠ¸ ì •ë³´:** {len(data)}ê°œ í•­ëª©")
            if data:
                st.write("**ìƒ˜í”Œ í•­ëª©:**")
                for i, item in enumerate(data[:5]):
                    st.write(f"{i+1}. {item}")
        
        else:
            # í…ìŠ¤íŠ¸ ë˜ëŠ” ê¸°íƒ€ ë°ì´í„°
            st.write("**ë°ì´í„° ë‚´ìš©:**")
            if isinstance(data, str) and len(data) > 1000:
                st.text_area("í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°", data[:1000] + "...", height=200)
            else:
                st.text(str(data)[:1000])
    
    async def _analyze_data_context(self, data) -> Optional[Dict]:
        """Universal Engineìœ¼ë¡œ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ë¶„ì„"""
        try:
            context_analysis = await self.context_discovery.discover_context(data)
            return context_analysis
            
        except Exception as e:
            logger.error(f"Error in context analysis: {e}")
            st.warning("ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return None
    
    def _display_context_analysis(self, context_analysis: Dict):
        """ê°ì§€ëœ ë„ë©”ì¸ê³¼ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ"""
        st.markdown("### ğŸ§  ìë™ ë„ë©”ì¸ ê°ì§€ ê²°ê³¼")
        
        domain_context = context_analysis.get('domain_context', {})
        
        if domain_context:
            col1, col2 = st.columns(2)
            
            with col1:
                # ê°ì§€ëœ ë„ë©”ì¸
                domain = domain_context.get('domain', 'ì•Œ ìˆ˜ ì—†ìŒ')
                confidence = domain_context.get('confidence_level', 0.0)
                
                if confidence > 0.7:
                    st.success(f"âœ… **{domain}** ë„ë©”ì¸ìœ¼ë¡œ ê°ì§€ë¨ (ì‹ ë¢°ë„: {confidence:.1%})")
                elif confidence > 0.5:
                    st.info(f"â„¹ï¸ **{domain}** ë„ë©”ì¸ì¼ ê°€ëŠ¥ì„± ë†’ìŒ (ì‹ ë¢°ë„: {confidence:.1%})")
                else:
                    st.warning(f"âš ï¸ ë„ë©”ì¸ ë¶ˆëª…í™• (ì‹ ë¢°ë„: {confidence:.1%})")
                
                # ë„ë©”ì¸ íŠ¹ì„±
                domain_chars = domain_context.get('domain_characteristics', {})
                if domain_chars:
                    st.write("**ë„ë©”ì¸ íŠ¹ì„±:**")
                    for key, value in domain_chars.items():
                        if isinstance(value, list):
                            st.write(f"â€¢ {key}: {', '.join(value[:3])}")
                        else:
                            st.write(f"â€¢ {key}: {value}")
            
            with col2:
                # ê°ì§€ ê·¼ê±°
                evidence = domain_context.get('evidence', [])
                if evidence:
                    st.write("**ê°ì§€ ê·¼ê±°:**")
                    for item in evidence[:5]:
                        st.write(f"â€¢ {item}")
        
        # ë¶ˆí™•ì‹¤ì„± í‰ê°€
        uncertainty = context_analysis.get('uncertainty_assessment', {})
        if uncertainty and uncertainty.get('clarification_needed'):
            with st.expander("â“ ëª…í™•í™”ê°€ í•„ìš”í•œ ë¶€ë¶„", expanded=False):
                for question in uncertainty['clarification_needed'][:3]:
                    st.write(f"â€¢ {question.get('question', '')}")
    
    async def _assess_data_quality(self, data, context_analysis: Dict) -> Dict:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        quality_scores = {}
        
        if isinstance(data, pd.DataFrame):
            # ì™„ì „ì„± í‰ê°€ (ê²°ì¸¡ê°’ ê¸°ì¤€)
            completeness = 1.0 - (data.isnull().sum().sum() / (len(data) * len(data.columns)))
            quality_scores['completeness'] = completeness
            
            # ì¼ê´€ì„± í‰ê°€ (ë°ì´í„° íƒ€ì… ì¼ê´€ì„±)
            consistency = 0.8  # ê¸°ë³¸ê°’, í–¥í›„ ë” ì •êµí•œ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
            quality_scores['consistency'] = consistency
            
            # ìœ íš¨ì„± í‰ê°€ (ì´ìƒê°’ ë“±)
            validity = 0.9  # ê¸°ë³¸ê°’
            quality_scores['validity'] = validity
            
        else:
            # ë¹„ì •í˜• ë°ì´í„°ì˜ ê²½ìš° ê¸°ë³¸ ì ìˆ˜
            quality_scores = {
                'completeness': 0.8,
                'consistency': 0.7,
                'validity': 0.8
            }
        
        # LLM ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
        llm_assessment = await self._llm_assess_data_quality(data, context_analysis)
        quality_scores.update(llm_assessment)
        
        return quality_scores
    
    async def _llm_assess_data_quality(self, data, context_analysis: Dict) -> Dict:
        """LLM ê¸°ë°˜ ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        # ë°ì´í„° ìƒ˜í”Œ ì¤€ë¹„
        if isinstance(data, pd.DataFrame):
            data_sample = {
                'columns': list(data.columns),
                'dtypes': data.dtypes.to_dict(),
                'sample_rows': data.head(5).to_dict(),
                'missing_values': data.isnull().sum().to_dict()
            }
        else:
            data_sample = str(data)[:500]
        
        prompt = f"""
        ë‹¤ìŒ ë°ì´í„°ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì„¸ìš”.
        
        ë°ì´í„° ìƒ˜í”Œ: {json.dumps(data_sample, ensure_ascii=False, default=str)}
        ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸: {context_analysis.get('domain_context', {})}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 0.0-1.0 ì ìˆ˜ë¥¼ ë§¤ê¸°ì„¸ìš”:
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "data_richness": 0.0-1.0,
            "structure_quality": 0.0-1.0,
            "domain_relevance": 0.0-1.0,
            "analysis_readiness": 0.0-1.0,
            "quality_issues": ["ì´ìŠˆ1", "ì´ìŠˆ2"],
            "improvement_suggestions": ["ì œì•ˆ1", "ì œì•ˆ2"]
        }}
        """
        
        try:
            response = await self.llm_client.agenerate(prompt)
            return self._parse_json_response(response)
        except Exception as e:
            logger.error(f"LLM quality assessment failed: {e}")
            return {}
    
    def _display_quality_assessment(self, quality_assessment: Dict):
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€ ê²°ê³¼ í‘œì‹œ"""
        st.markdown("### ğŸ“Š ë°ì´í„° í’ˆì§ˆ í‰ê°€")
        
        if quality_assessment:
            # í’ˆì§ˆ ì ìˆ˜ë“¤
            col1, col2, col3, col4 = st.columns(4)
            
            quality_metrics = [
                ('completeness', 'ì™„ì „ì„±', col1),
                ('consistency', 'ì¼ê´€ì„±', col2),
                ('validity', 'ìœ íš¨ì„±', col3),
                ('analysis_readiness', 'ë¶„ì„ ì¤€ë¹„ë„', col4)
            ]
            
            for key, label, col in quality_metrics:
                if key in quality_assessment:
                    score = quality_assessment[key]
                    with col:
                        if score >= 0.8:
                            st.metric(label, f"{score:.1%}", delta="ìš°ìˆ˜")
                        elif score >= 0.6:
                            st.metric(label, f"{score:.1%}", delta="ë³´í†µ")
                        else:
                            st.metric(label, f"{score:.1%}", delta="ê°œì„  í•„ìš”")
            
            # í’ˆì§ˆ ì´ìŠˆ ë° ê°œì„  ì œì•ˆ
            if quality_assessment.get('quality_issues'):
                with st.expander("âš ï¸ ë°œê²¬ëœ í’ˆì§ˆ ì´ìŠˆ", expanded=False):
                    for issue in quality_assessment['quality_issues']:
                        st.write(f"â€¢ {issue}")
            
            if quality_assessment.get('improvement_suggestions'):
                with st.expander("ğŸ’¡ ê°œì„  ì œì•ˆ", expanded=False):
                    for suggestion in quality_assessment['improvement_suggestions']:
                        st.write(f"â€¢ {suggestion}")
    
    async def _generate_analysis_recommendations(
        self, 
        data, 
        context_analysis: Dict, 
        quality_assessment: Dict
    ) -> Dict:
        """ì¶”ì²œ ë¶„ì„ ë° ìë™ ì§ˆë¬¸ ìƒì„±"""
        prompt = f"""
        ë°ì´í„°ì™€ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì¶”ì²œí•  ë¶„ì„ê³¼ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.
        
        ë„ë©”ì¸: {context_analysis.get('domain_context', {}).get('domain', 'ì•Œ ìˆ˜ ì—†ìŒ')}
        ë°ì´í„° í’ˆì§ˆ: {quality_assessment}
        ë°ì´í„° íŠ¹ì„±: {context_analysis.get('data_characteristics', {})}
        
        ë‹¤ìŒì„ ìƒì„±í•˜ì„¸ìš”:
        1. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ê¸°ë³¸ ë¶„ì„ë“¤
        2. ì‹¬í™” ë¶„ì„ ì˜µì…˜ë“¤  
        3. ì‚¬ìš©ìê°€ ë¬¼ì–´ë³¼ ë§Œí•œ ìë™ ì§ˆë¬¸ë“¤
        4. ë„ë©”ì¸ë³„ íŠ¹í™” ë¶„ì„ë“¤
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "basic_analyses": [
                {{
                    "title": "ë¶„ì„ ì œëª©",
                    "description": "ë¶„ì„ ì„¤ëª…",
                    "estimated_time": "ì˜ˆìƒ ì‹œê°„",
                    "complexity": "low|medium|high"
                }}
            ],
            "advanced_analyses": [
                {{
                    "title": "ê³ ê¸‰ ë¶„ì„ ì œëª©",
                    "description": "ë¶„ì„ ì„¤ëª…",
                    "prerequisites": ["ì „ì œì¡°ê±´1", "ì „ì œì¡°ê±´2"],
                    "complexity": "medium|high|expert"
                }}
            ],
            "suggested_questions": [
                "ì´ ë°ì´í„°ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ íŒ¨í„´ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                "ì–´ë–¤ ì´ìƒê°’ì´ë‚˜ íŠ¹ì´ì ì´ ìˆë‚˜ìš”?",
                "ë°ì´í„° í’ˆì§ˆì— ë¬¸ì œê°€ ìˆë‚˜ìš”?"
            ],
            "domain_specific": [
                {{
                    "category": "ì¹´í…Œê³ ë¦¬ëª…",
                    "analyses": ["ë¶„ì„1", "ë¶„ì„2"]
                }}
            ]
        }}
        """
        
        try:
            response = await self.llm_client.agenerate(prompt)
            return self._parse_json_response(response)
        except Exception as e:
            logger.error(f"Failed to generate recommendations: {e}")
            return {}
    
    def _display_analysis_recommendations(self, recommendations: Dict):
        """ì¶”ì²œ ë¶„ì„ ë° ìë™ ì§ˆë¬¸ í‘œì‹œ"""
        if not recommendations:
            return
        
        st.markdown("### ğŸ¯ ì¶”ì²œ ë¶„ì„")
        
        # íƒ­ìœ¼ë¡œ êµ¬ë¶„
        tab1, tab2, tab3 = st.tabs(["ğŸš€ ì¦‰ì‹œ ì‹¤í–‰", "ğŸ”¬ ì‹¬í™” ë¶„ì„", "â“ ì¶”ì²œ ì§ˆë¬¸"])
        
        with tab1:
            basic_analyses = recommendations.get('basic_analyses', [])
            if basic_analyses:
                for analysis in basic_analyses:
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        
                        with col1:
                            st.write(f"**{analysis.get('title', '')}**")
                            st.caption(analysis.get('description', ''))
                            
                            complexity_icons = {'low': 'ğŸŸ¢', 'medium': 'ğŸŸ¡', 'high': 'ğŸ”´'}
                            complexity = analysis.get('complexity', 'medium')
                            st.caption(f"{complexity_icons.get(complexity, 'âšª')} ë³µì¡ë„: {complexity}")
                        
                        with col2:
                            if st.button(f"ì‹¤í–‰", key=f"basic_{analysis.get('title', '')}"):
                                # ë¶„ì„ ì‹¤í–‰ ë¡œì§ (í–¥í›„ êµ¬í˜„)
                                st.info(f"{analysis.get('title', '')} ë¶„ì„ì´ ì‹œì‘ë©ë‹ˆë‹¤.")
        
        with tab2:
            advanced_analyses = recommendations.get('advanced_analyses', [])
            if advanced_analyses:
                for analysis in advanced_analyses:
                    with st.expander(analysis.get('title', ''), expanded=False):
                        st.write(analysis.get('description', ''))
                        
                        if analysis.get('prerequisites'):
                            st.write("**ì „ì œì¡°ê±´:**")
                            for prereq in analysis['prerequisites']:
                                st.write(f"â€¢ {prereq}")
                        
                        if st.button(f"ì‹œì‘", key=f"advanced_{analysis.get('title', '')}"):
                            st.info(f"{analysis.get('title', '')} ë¶„ì„ ì¤€ë¹„ ì¤‘...")
        
        with tab3:
            suggested_questions = recommendations.get('suggested_questions', [])
            if suggested_questions:
                st.write("**í´ë¦­í•˜ë©´ ìë™ìœ¼ë¡œ ì§ˆë¬¸ì´ ì…ë ¥ë©ë‹ˆë‹¤:**")
                
                for i, question in enumerate(suggested_questions):
                    if st.button(question, key=f"question_{i}"):
                        # ì±„íŒ… ì…ë ¥ì— ì§ˆë¬¸ ì¶”ê°€
                        if 'messages' not in st.session_state:
                            st.session_state.messages = []
                        
                        # ìƒˆë¡œìš´ ë©”ì‹œì§€ë¡œ ì¶”ê°€í•˜ê³  í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                        st.session_state.pending_question = question
                        st.rerun()
    
    def _parse_json_response(self, response: str) -> Dict:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        try:
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response.strip()
            
            return json.loads(json_str)
        except Exception as e:
            logger.warning(f"Failed to parse JSON response: {e}")
            return {}