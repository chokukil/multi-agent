"""
Cherry AI Universal A2A Integration - ì™„ì „íˆ ìƒˆë¡œìš´ LLM First ë¶„ì„ ì‹œìŠ¤í…œ

ê¸°ì¡´ cherry_ai.pyì˜ ëª¨ë“  í•˜ë“œì½”ë”©ì„ ì œê±°í•˜ê³  Universal Engineìœ¼ë¡œ ì™„ì „ ëŒ€ì²´:
- í•˜ë“œì½”ë”©ëœ ë„ë©”ì¸ë³„ ì—”ì§„ ì„ íƒ â†’ Universal Engine ë™ì  ë„ë©”ì¸ ê°ì§€
- í•˜ë“œì½”ë”©ëœ ì¶”ì²œ í…œí”Œë¦¿ â†’ LLM ê¸°ë°˜ ë™ì  ì¶”ì²œ ìƒì„±
- í•˜ë“œì½”ë”©ëœ ì—ì´ì „íŠ¸ ì„ íƒ â†’ LLM ê¸°ë°˜ ë™ì  ì—ì´ì „íŠ¸ ì„ íƒ
- í•˜ë“œì½”ë”©ëœ íŒŒì¼ ì²˜ë¦¬ â†’ Universal Engine ê¸°ë°˜ ìë™ ì²˜ë¦¬
- í•˜ë“œì½”ë”©ëœ ì„ê³„ê°’ â†’ LLM ê¸°ë°˜ ë™ì  ì‹ ë¢°ë„ í‰ê°€
"""

import streamlit as st
import asyncio
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
import json
import pandas as pd
from pathlib import Path

from ..universal_query_processor import UniversalQueryProcessor
from ..a2a_integration.a2a_agent_discovery import A2AAgentDiscoverySystem
from ..a2a_integration.llm_based_agent_selector import LLMBasedAgentSelector
from ..a2a_integration.a2a_workflow_orchestrator import A2AWorkflowOrchestrator
from ..a2a_integration.a2a_communication_protocol import A2ACommunicationProtocol
from ..a2a_integration.a2a_result_integrator import A2AResultIntegrator
from .cherry_ai_universal_engine_ui import CherryAIUniversalEngineUI
from .enhanced_file_upload import EnhancedFileUpload
from .enhanced_chat_interface import EnhancedChatInterface
from .realtime_analysis_progress import RealtimeAnalysisProgress
from .progressive_disclosure_interface import ProgressiveDisclosureInterface

logger = logging.getLogger(__name__)


class CherryAIUniversalA2AIntegration:
    """
    Cherry AI Universal A2A Integration
    - ê¸°ì¡´ í•˜ë“œì½”ë”© ì™„ì „ ì œê±°
    - Universal Engine + A2A ì—ì´ì „íŠ¸ ì™„ì „ í†µí•©
    - LLM First ì›ì¹™ ê¸°ë°˜ ë™ì  ì˜ì‚¬ê²°ì •
    - ê¸°ì¡´ Cherry AI í˜¸í™˜ì„± ìœ ì§€
    """
    
    def __init__(self):
        """CherryAIUniversalA2AIntegration ì´ˆê¸°í™”"""
        # Universal Engine ì»´í¬ë„ŒíŠ¸ë“¤
        self.universal_engine = UniversalQueryProcessor()
        self.a2a_discovery = A2AAgentDiscoverySystem()
        self.agent_selector = LLMBasedAgentSelector(self.a2a_discovery)
        self.communication_protocol = A2ACommunicationProtocol()
        self.workflow_orchestrator = A2AWorkflowOrchestrator(self.communication_protocol)
        self.result_integrator = A2AResultIntegrator()
        
        # UI ì»´í¬ë„ŒíŠ¸ë“¤
        self.ui_controller = CherryAIUniversalEngineUI()
        self.file_upload = EnhancedFileUpload()
        self.chat_interface = EnhancedChatInterface(self.universal_engine)
        self.progress_monitor = RealtimeAnalysisProgress()
        self.disclosure_interface = ProgressiveDisclosureInterface()
        
        # ì‹œìŠ¤í…œ ìƒíƒœ
        self.is_initialized = False
        self.available_agents = {}
        self.session_id = None
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        self._initialize_session_state()
        
        logger.info("CherryAIUniversalA2AIntegration initialized")
    
    def _initialize_session_state(self):
        """Streamlit ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        if 'cherry_ai_universal_initialized' not in st.session_state:
            st.session_state.cherry_ai_universal_initialized = False
            st.session_state.messages = []
            st.session_state.current_data = None
            st.session_state.analysis_history = []
            st.session_state.dynamic_recommendations = []
            st.session_state.show_agent_details = True
            st.session_state.show_reasoning = True
            st.session_state.reasoning_depth = "ê¸°ë³¸"
            st.session_state.expertise_level = "ìë™ ê°ì§€"
            st.session_state.current_execution = None
            st.session_state.user_expertise_level = "intermediate"
    
    async def initialize_system(self) -> bool:
        """
        ì™„ì „íˆ ìƒˆë¡œìš´ LLM First ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        - ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ ì´ˆê¸°í™” ë¡œì§ ì™„ì „ ëŒ€ì²´
        
        Returns:
            ì´ˆê¸°í™” ì„±ê³µ ì—¬ë¶€
        """
        try:
            with st.spinner("ğŸ§  Universal Engine + A2A ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘..."):
                # 1. Universal Engine ì´ˆê¸°í™”
                logger.info("Initializing Universal Engine...")
                # Universal Engineì€ ì´ë¯¸ ì´ˆê¸°í™”ë¨
                
                # 2. A2A ì—ì´ì „íŠ¸ ë°œê²¬ ë° ì´ˆê¸°í™”
                logger.info("Starting A2A agent discovery...")
                await self.a2a_discovery.start_discovery()
                self.available_agents = self.a2a_discovery.get_available_agents()
                
                # 3. ì—ì´ì „íŠ¸ ì„ íƒê¸° ì´ˆê¸°í™”
                # LLM ê¸°ë°˜ ë™ì  ì„ íƒ - í•˜ë“œì½”ë”© ì—†ìŒ
                
                # 4. ì„¸ì…˜ ID ìƒì„± - Universal Engine ë°©ì‹
                self.session_id = f"universal_cherry_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                
                # 5. UI ì»¨íŠ¸ë¡¤ëŸ¬ ì´ˆê¸°í™”
                success = await self.ui_controller.initialize_system()
                
                if success:
                    self.is_initialized = True
                    st.session_state.cherry_ai_universal_initialized = True
                    
                    # ì´ˆê¸°í™” ì™„ë£Œ ì•Œë¦¼
                    agent_count = len(self.available_agents)
                    st.success(f"âœ… Universal Engine ì´ˆê¸°í™” ì™„ë£Œ! {agent_count}ê°œ A2A ì—ì´ì „íŠ¸ ë°œê²¬ë¨")
                    
                    logger.info(f"System initialized successfully with {agent_count} agents")
                    return True
                else:
                    raise Exception("UI Controller initialization failed")
                    
        except Exception as e:
            logger.error(f"System initialization failed: {e}")
            st.error(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            st.session_state.cherry_ai_universal_initialized = False
            return False
    
    def render_application(self):
        """
        ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë Œë”ë§
        - ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ UI ë¡œì§ì„ Universal Engine ê¸°ë°˜ìœ¼ë¡œ ì™„ì „ ëŒ€ì²´
        """
        # 1. í˜ì´ì§€ ì„¤ì • - ê¸°ì¡´ Cherry AI ìŠ¤íƒ€ì¼ ìœ ì§€
        st.set_page_config(
            page_title="ğŸ’ Cherry AI - Universal Engine",
            page_icon="ğŸ’",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # 2. Universal Engine í—¤ë” ë Œë”ë§
        self.ui_controller.render_header()
        
        # 3. ì´ˆê¸°í™” í™•ì¸
        if not st.session_state.cherry_ai_universal_initialized:
            with st.container():
                st.info("ğŸ”„ ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                success = asyncio.run(self.initialize_system())
                if not success:
                    st.stop()
        
        # 4. ì‚¬ì´ë“œë°” - Universal Engine ì œì–´
        self.ui_controller.render_sidebar()
        
        # 5. ë©”ì¸ ì½˜í…ì¸  ì˜ì—­
        self._render_main_content()
    
    def _render_main_content(self):
        """ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ ë Œë”ë§"""
        # íƒ­ êµ¬ì„± - Universal Engine ê¸°ëŠ¥ ì¤‘ì‹¬
        tab1, tab2, tab3, tab4 = st.tabs([
            "ğŸ’¬ Universal ë¶„ì„", 
            "ğŸ“ ìŠ¤ë§ˆíŠ¸ íŒŒì¼ ì—…ë¡œë“œ",
            "ğŸ“Š ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™©",
            "ğŸ¤– A2A ì—ì´ì „íŠ¸ ìƒíƒœ"
        ])
        
        with tab1:
            self._render_universal_analysis_tab()
        
        with tab2:
            self._render_smart_file_upload_tab()
        
        with tab3:
            self._render_realtime_progress_tab()
        
        with tab4:
            self._render_agent_status_tab()
    
    def _render_universal_analysis_tab(self):
        """Universal ë¶„ì„ íƒ­ - ì™„ì „íˆ ìƒˆë¡œìš´ LLM First ë¶„ì„"""
        
        # í˜„ì¬ ë°ì´í„° ìƒíƒœ í™•ì¸
        if st.session_state.current_data is None:
            st.info("ğŸ“ ë¨¼ì € 'ìŠ¤ë§ˆíŠ¸ íŒŒì¼ ì—…ë¡œë“œ' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            
            # ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸ ì˜µì…˜
            if st.button("ğŸ§ª ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸"):
                sample_data = pd.DataFrame({
                    'A': [1, 2, 3, 4, 5],
                    'B': [10, 20, 30, 40, 50],
                    'C': ['X', 'Y', 'Z', 'X', 'Y']
                })
                st.session_state.current_data = sample_data
                st.success("âœ… ìƒ˜í”Œ ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()
            
            return
        
        # í˜„ì¬ ë°ì´í„° ì •ë³´ í‘œì‹œ
        with st.expander("ğŸ“Š í˜„ì¬ ë°ì´í„° ì •ë³´", expanded=False):
            data = st.session_state.current_data
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("í–‰ ìˆ˜", f"{len(data):,}")
            with col2:
                st.metric("ì—´ ìˆ˜", f"{len(data.columns):,}")
            with col3:
                memory_usage = data.memory_usage(deep=True).sum() / 1024
                st.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", f"{memory_usage:.1f} KB")
            
            st.dataframe(data.head(5), use_container_width=True)
            
            if st.button("ğŸ—‘ï¸ ë°ì´í„° ì œê±°"):
                st.session_state.current_data = None
                st.session_state.dynamic_recommendations = []
                st.rerun()
        
        # Universal Engine ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        st.markdown("### ğŸ’¬ Universal Engine ë¶„ì„ ëŒ€í™”")
        
        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ë Œë”ë§
        self.chat_interface.render_chat_messages()
        
        # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
        user_input = self.chat_interface.render_chat_input()
        
        if user_input:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({
                'role': 'user',
                'content': user_input,
                'timestamp': datetime.now().isoformat()
            })
            
            # Universal Engineìœ¼ë¡œ ë¶„ì„ ì‹¤í–‰
            with st.chat_message("assistant"):
                asyncio.run(self._execute_universal_analysis(user_input))
    
    async def _execute_universal_analysis(self, user_query: str):
        """
        ì™„ì „íˆ ìƒˆë¡œìš´ Universal Engine ë¶„ì„ ì‹¤í–‰
        - ê¸°ì¡´ í•˜ë“œì½”ë”©ëœ ë¶„ì„ ë¡œì§ ì™„ì „ ëŒ€ì²´
        """
        try:
            # 1. ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì‹œì‘
            self.progress_monitor.start_monitoring(total_components=6)
            
            # 2. Universal Engine ë©”íƒ€ ì¶”ë¡  ìˆ˜í–‰
            st.info("ğŸ§  Universal Engine ë©”íƒ€ ì¶”ë¡  ì¤‘...")
            
            meta_analysis = await self.universal_engine.meta_reasoning_engine.analyze_request(
                query=user_query,
                data=st.session_state.current_data,
                context=self._get_session_context()
            )
            
            # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            from .realtime_analysis_progress import ProgressUpdate, ComponentType, TaskStatus
            self.progress_monitor.update_progress(ProgressUpdate(
                component=ComponentType.META_REASONING,
                stage="ë©”íƒ€ ì¶”ë¡  ì™„ë£Œ",
                status=TaskStatus.COMPLETED,
                progress_percent=100.0,
                message="DeepSeek-R1 ê¸°ë°˜ 4ë‹¨ê³„ ì¶”ë¡  ì™„ë£Œ"
            ))
            
            # ë©”íƒ€ ì¶”ë¡  ê²°ê³¼ í‘œì‹œ (ì˜µì…˜)
            if st.session_state.get('show_reasoning', True):
                with st.expander("ğŸ§  ë©”íƒ€ ì¶”ë¡  ê³¼ì •", expanded=False):
                    self.chat_interface._render_meta_reasoning_visualization(meta_analysis)
            
            # 3. A2A ì—ì´ì „íŠ¸ ë™ì  ì„ íƒ
            st.info("ğŸ¤– A2A ì—ì´ì „íŠ¸ ë™ì  ì„ íƒ ì¤‘...")
            
            if self.available_agents:
                selection_result = await self.agent_selector.select_agents_for_query(
                    meta_analysis=meta_analysis,
                    query=user_query,
                    data_info=self._get_data_info(),
                    user_preferences=self._get_user_preferences()
                )
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                self.progress_monitor.update_progress(ProgressUpdate(
                    component=ComponentType.AGENT_SELECTION,
                    stage="ì—ì´ì „íŠ¸ ì„ íƒ ì™„ë£Œ",
                    status=TaskStatus.COMPLETED,
                    progress_percent=100.0,
                    message=f"{len(selection_result.selected_agents)}ê°œ ì—ì´ì „íŠ¸ ì„ íƒë¨"
                ))
                
                # ì„ íƒëœ ì—ì´ì „íŠ¸ í‘œì‹œ
                if selection_result.selected_agents:
                    st.success(f"âœ… {len(selection_result.selected_agents)}ê°œ ì—ì´ì „íŠ¸ ì„ íƒë¨")
                    
                    cols = st.columns(min(len(selection_result.selected_agents), 4))
                    for i, agent in enumerate(selection_result.selected_agents):
                        with cols[i % 4]:
                            st.write(f"ğŸ¤– **{agent.name}**")
                            st.caption(f"í¬íŠ¸: {agent.port}")
                    
                    # 4. A2A ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                    st.info("âš¡ A2A ì—ì´ì „íŠ¸ í˜‘ì—… ì‹¤í–‰ ì¤‘...")
                    
                    workflow_result = None
                    progress_container = st.container()
                    
                    async for progress_update in self.workflow_orchestrator.execute_workflow_with_streaming(
                        selection_result, user_query, st.session_state.current_data
                    ):
                        self._update_progress_display(progress_container, progress_update)
                        
                        if progress_update.get('type') == 'workflow_completed':
                            workflow_result = progress_update.get('results')
                    
                    if workflow_result:
                        # 5. ê²°ê³¼ í†µí•©
                        st.info("ğŸ”„ ê²°ê³¼ í†µí•© ë° ì‘ë‹µ ìƒì„± ì¤‘...")
                        
                        # A2A ì‘ë‹µë“¤ì„ A2AResponse í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        a2a_responses = self._convert_to_a2a_responses(workflow_result)
                        
                        # ê²°ê³¼ í†µí•©
                        integrated_result = await self.result_integrator.integrate_results(
                            responses=a2a_responses,
                            agents=selection_result.selected_agents,
                            original_query=user_query,
                            meta_analysis=meta_analysis
                        )
                        
                        # 6. Universal Engineìœ¼ë¡œ ìµœì¢… ì ì‘í˜• ì‘ë‹µ ìƒì„±
                        final_response = await self.universal_engine.response_generator.generate_adaptive_response(
                            knowledge_result={'refined_result': integrated_result.consolidated_data},
                            user_profile=meta_analysis.get('user_profile', {}),
                            interaction_context=self._get_session_context()
                        )
                        
                        # 7. Progressive Disclosure ê¸°ë°˜ ê²°ê³¼ í‘œì‹œ
                        await self._display_adaptive_results(
                            final_response, integrated_result, meta_analysis, user_query
                        )
                        
                        # 8. ë™ì  ì¶”ì²œ ìƒì„± ë° í‘œì‹œ
                        await self._generate_and_display_dynamic_recommendations(
                            final_response, integrated_result, user_query
                        )
                        
                        # 9. ë©”ì‹œì§€ ì´ë ¥ì— ì¶”ê°€
                        assistant_message = {
                            'role': 'assistant',
                            'content': final_response.get('core_response', {}).get('summary', 'ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'),
                            'meta_reasoning': meta_analysis,
                            'agent_contributions': integrated_result.agent_contributions,
                            'final_response': final_response,
                            'timestamp': datetime.now().isoformat()
                        }
                        st.session_state.messages.append(assistant_message)
                        
                        # 10. ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì™„ë£Œ
                        self.progress_monitor.stop_monitoring()
                        
                    else:
                        st.error("ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                
                else:
                    st.warning("ì„ íƒëœ ì—ì´ì „íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            else:
                # A2A ì—ì´ì „íŠ¸ê°€ ì—†ëŠ” ê²½ìš° Universal Engine ë‹¨ë… ë¶„ì„
                st.info("ğŸ§  Universal Engine ë‹¨ë… ë¶„ì„ ì¤‘...")
                
                result = await self.universal_engine.process_query(
                    query=user_query,
                    data=st.session_state.current_data,
                    context=self._get_session_context()
                )
                
                if result.get('success'):
                    st.success("âœ… Universal Engine ë¶„ì„ ì™„ë£Œ")
                    st.write(result.get('response', {}).get('core_response', {}).get('summary', 'ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'))
                    
                    # ë©”ì‹œì§€ ì´ë ¥ì— ì¶”ê°€
                    assistant_message = {
                        'role': 'assistant',
                        'content': result.get('response', {}).get('core_response', {}).get('summary', 'ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'),
                        'meta_reasoning': result.get('meta_analysis'),
                        'timestamp': datetime.now().isoformat()
                    }
                    st.session_state.messages.append(assistant_message)
                else:
                    st.error(f"ë¶„ì„ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        
        except Exception as e:
            logger.error(f"Error in universal analysis execution: {e}")
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            
            # ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
            self.progress_monitor.stop_monitoring()
    
    async def _display_adaptive_results(
        self, 
        final_response: Dict, 
        integrated_result, 
        meta_analysis: Dict,
        user_query: str
    ):
        """Progressive Disclosure ê¸°ë°˜ ì ì‘ì  ê²°ê³¼ í‘œì‹œ"""
        
        # ì‚¬ìš©ì ì „ë¬¸ì„± ìˆ˜ì¤€ ê°ì§€
        user_expertise = meta_analysis.get('user_profile', {}).get('expertise_level', 'intermediate')
        
        # ì ì‘ì  ì½˜í…ì¸  ìƒì„±
        content_hierarchy = await self.disclosure_interface.generate_adaptive_content(
            analysis_result=integrated_result.consolidated_data,
            user_query=user_query,
            expertise_level=self.disclosure_interface.ExpertiseLevel(user_expertise)
        )
        
        # Progressive Disclosure ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§
        self.disclosure_interface.render_progressive_interface(content_hierarchy)
    
    async def _generate_and_display_dynamic_recommendations(
        self,
        final_response: Dict,
        integrated_result,
        user_query: str
    ):
        """ì™„ì „íˆ ë™ì ì¸ LLM ê¸°ë°˜ ì¶”ì²œ ìƒì„± ë° í‘œì‹œ"""
        
        from ...llm_factory import LLMFactory
        llm_client = LLMFactory.create_llm()
        
        # í•˜ë“œì½”ë”© ì—†ëŠ” ë™ì  ì¶”ì²œ ìƒì„±
        prompt = f"""
        ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì™„ì „íˆ ë™ì ì¸ í›„ì† ë¶„ì„ ì¶”ì²œì„ ìƒì„±í•˜ì„¸ìš”.
        
        ì‚¬ìš©ì ì¿¼ë¦¬: {user_query}
        ë¶„ì„ ê²°ê³¼: {json.dumps(integrated_result.consolidated_data, ensure_ascii=False)[:1000]}
        ì£¼ìš” ì¸ì‚¬ì´íŠ¸: {integrated_result.insights}
        
        í•˜ë“œì½”ë”©ëœ í…œí”Œë¦¿ ì‚¬ìš© ê¸ˆì§€. ìˆœìˆ˜ LLM ì¶”ë¡ ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”.
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "immediate_actions": [
                {{
                    "title": "ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì œëª©",
                    "description": "êµ¬ì²´ì ì¸ ì„¤ëª…",
                    "query": "ì‹¤í–‰í•  ì¿¼ë¦¬",
                    "complexity": "low|medium|high",
                    "estimated_time": "ì˜ˆìƒ ì‹œê°„"
                }}
            ],
            "deep_dive_options": [
                {{
                    "title": "ì‹¬í™” ë¶„ì„ ì˜µì…˜",
                    "description": "ìƒì„¸í•œ ë¶„ì„ ì„¤ëª…",
                    "query": "ì‹¬í™” ë¶„ì„ ì¿¼ë¦¬",
                    "prerequisites": ["ì „ì œì¡°ê±´1", "ì „ì œì¡°ê±´2"]
                }}
            ],
            "related_explorations": [
                {{
                    "title": "ê´€ë ¨ íƒìƒ‰ ì£¼ì œ",
                    "description": "íƒìƒ‰ ê°€ì¹˜ ì„¤ëª…",
                    "query": "íƒìƒ‰ ì¿¼ë¦¬"
                }}
            ]
        }}
        """
        
        try:
            response = await llm_client.agenerate(prompt)
            recommendations = self._parse_json_response(response)
            
            if recommendations:
                st.markdown("### ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ ì¶”ì²œ")
                
                # íƒ­ìœ¼ë¡œ êµ¬ë¶„ëœ ì¶”ì²œ í‘œì‹œ
                tab1, tab2, tab3 = st.tabs(["ğŸš€ ì¦‰ì‹œ ì‹¤í–‰", "ğŸ”¬ ì‹¬í™” ë¶„ì„", "ğŸ” ê´€ë ¨ íƒìƒ‰"])
                
                with tab1:
                    immediate_actions = recommendations.get('immediate_actions', [])
                    for action in immediate_actions:
                        with st.container():
                            col1, col2 = st.columns([3, 1])
                            
                            with col1:
                                st.write(f"**{action.get('title', '')}**")
                                st.caption(action.get('description', ''))
                                
                                complexity_icons = {'low': 'ğŸŸ¢', 'medium': 'ğŸŸ¡', 'high': 'ğŸ”´'}
                                complexity = action.get('complexity', 'medium')
                                st.caption(f"{complexity_icons.get(complexity, 'âšª')} ë³µì¡ë„: {complexity} | â±ï¸ {action.get('estimated_time', 'ì•Œ ìˆ˜ ì—†ìŒ')}")
                            
                            with col2:
                                if st.button("ì‹¤í–‰", key=f"immediate_{action.get('title', '')}"):
                                    # ì¶”ì²œ í´ë¦­ ì‹œ ìë™ ì‹¤í–‰
                                    st.session_state.messages.append({
                                        'role': 'user',
                                        'content': action.get('query', ''),
                                        'timestamp': datetime.now().isoformat()
                                    })
                                    st.rerun()
                
                with tab2:
                    deep_dive_options = recommendations.get('deep_dive_options', [])
                    for option in deep_dive_options:
                        with st.expander(option.get('title', ''), expanded=False):
                            st.write(option.get('description', ''))
                            
                            if option.get('prerequisites'):
                                st.write("**ì „ì œì¡°ê±´:**")
                                for prereq in option['prerequisites']:
                                    st.write(f"â€¢ {prereq}")
                            
                            if st.button("ì‹œì‘", key=f"deep_dive_{option.get('title', '')}"):
                                st.session_state.messages.append({
                                    'role': 'user',
                                    'content': option.get('query', ''),
                                    'timestamp': datetime.now().isoformat()
                                })
                                st.rerun()
                
                with tab3:
                    related_explorations = recommendations.get('related_explorations', [])
                    for exploration in related_explorations:
                        with st.container():
                            st.write(f"**{exploration.get('title', '')}**")
                            st.caption(exploration.get('description', ''))
                            
                            if st.button("íƒìƒ‰", key=f"explore_{exploration.get('title', '')}"):
                                st.session_state.messages.append({
                                    'role': 'user',
                                    'content': exploration.get('query', ''),
                                    'timestamp': datetime.now().isoformat()
                                })
                                st.rerun()
                
                # ë™ì  ì¶”ì²œì„ ì„¸ì…˜ì— ì €ì¥
                st.session_state.dynamic_recommendations = recommendations
        
        except Exception as e:
            logger.error(f"Error generating dynamic recommendations: {e}")
            st.warning("ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
    def _render_smart_file_upload_tab(self):
        """ìŠ¤ë§ˆíŠ¸ íŒŒì¼ ì—…ë¡œë“œ íƒ­ - Universal Engine ê¸°ë°˜"""
        st.markdown("### ğŸ“ Universal Engine ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ íŒŒì¼ ì—…ë¡œë“œ")
        
        # Enhanced File Upload ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
        self.file_upload.render_file_upload_interface()
    
    def _render_realtime_progress_tab(self):
        """ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© íƒ­"""
        st.markdown("### ğŸ“Š ì‹¤ì‹œê°„ ë¶„ì„ ì§„í–‰ ìƒí™©")
        
        # Realtime Analysis Progress ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
        self.progress_monitor.render_progress_dashboard()
        
        # ì§„í–‰ ìƒí™© íƒ€ì„ë¼ì¸
        self.progress_monitor.render_progress_timeline()
    
    def _render_agent_status_tab(self):
        """A2A ì—ì´ì „íŠ¸ ìƒíƒœ íƒ­"""
        st.markdown("### ğŸ¤– A2A ì—ì´ì „íŠ¸ ìƒíƒœ ë° ì„±ëŠ¥")
        
        if self.available_agents:
            # ì—ì´ì „íŠ¸ ìƒíƒœ ê·¸ë¦¬ë“œ
            cols = st.columns(4)
            for i, (agent_id, agent_info) in enumerate(self.available_agents.items()):
                with cols[i % 4]:
                    status_icon = "ğŸŸ¢" if agent_info.status == "active" else "ğŸ”´"
                    st.metric(
                        label=f"{status_icon} {agent_info.name}",
                        value=agent_info.status.upper(),
                        help=f"í¬íŠ¸: {agent_info.port}"
                    )
            
            # ì—ì´ì „íŠ¸ ìƒì„¸ ì •ë³´
            with st.expander("ğŸ”§ ì—ì´ì „íŠ¸ ìƒì„¸ ì •ë³´", expanded=False):
                for agent_id, agent_info in self.available_agents.items():
                    st.write(f"**{agent_info.name}** ({agent_id})")
                    st.write(f"â€¢ í¬íŠ¸: {agent_info.port}")
                    st.write(f"â€¢ ìƒíƒœ: {agent_info.status}")
                    st.write(f"â€¢ ëŠ¥ë ¥: {', '.join(agent_info.capabilities)}")
                    st.divider()
            
            # ì—ì´ì „íŠ¸ ì¬ë°œê²¬
            if st.button("ğŸ”„ ì—ì´ì „íŠ¸ ì¬ë°œê²¬"):
                with st.spinner("ì—ì´ì „íŠ¸ ì¬ë°œê²¬ ì¤‘..."):
                    asyncio.run(self.a2a_discovery.rediscover_agents())
                    self.available_agents = self.a2a_discovery.get_available_agents()
                    st.success(f"âœ… {len(self.available_agents)}ê°œ ì—ì´ì „íŠ¸ ë°œê²¬ë¨")
                    st.rerun()
        
        else:
            st.info("ì‚¬ìš© ê°€ëŠ¥í•œ A2A ì—ì´ì „íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            if st.button("ğŸ” ì—ì´ì „íŠ¸ ë°œê²¬ ì‹œë„"):
                with st.spinner("A2A ì—ì´ì „íŠ¸ ë°œê²¬ ì¤‘..."):
                    asyncio.run(self.a2a_discovery.start_discovery())
                    self.available_agents = self.a2a_discovery.get_available_agents()
                    
                    if self.available_agents:
                        st.success(f"âœ… {len(self.available_agents)}ê°œ ì—ì´ì „íŠ¸ ë°œê²¬ë¨!")
                        st.rerun()
                    else:
                        st.warning("A2A ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def _get_session_context(self) -> Dict:
        """ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        return {
            'session_id': self.session_id or 'default',
            'user_profile': st.session_state.get('user_profile', {}),
            'conversation_history': st.session_state.get('messages', [])[-5:],
            'settings': {
                'reasoning_depth': st.session_state.get('reasoning_depth', 'ê¸°ë³¸'),
                'expertise_level': st.session_state.get('expertise_level', 'ìë™ ê°ì§€'),
                'show_reasoning': st.session_state.get('show_reasoning', True)
            },
            'current_data_info': self._get_data_info()
        }
    
    def _get_data_info(self) -> Dict:
        """í˜„ì¬ ë°ì´í„° ì •ë³´ ì¶”ì¶œ"""
        current_data = st.session_state.get('current_data')
        if current_data is None:
            return {'type': 'none', 'description': 'No data uploaded'}
        
        return {
            'type': type(current_data).__name__,
            'shape': getattr(current_data, 'shape', 'unknown'),
            'size': len(current_data) if hasattr(current_data, '__len__') else 'unknown',
            'columns': list(current_data.columns) if hasattr(current_data, 'columns') else [],
            'description': 'User uploaded data'
        }
    
    def _get_user_preferences(self) -> Dict:
        """ì‚¬ìš©ì ì„ í˜¸ì‚¬í•­ ì¶”ì¶œ"""
        return {
            'expertise_level': st.session_state.get('expertise_level', 'ìë™ ê°ì§€'),
            'reasoning_depth': st.session_state.get('reasoning_depth', 'ê¸°ë³¸'),
            'show_agent_details': st.session_state.get('show_agent_details', True),
            'preferred_response_style': 'adaptive'
        }
    
    def _update_progress_display(self, container, progress_update: Dict):
        """ì§„í–‰ ìƒí™© í‘œì‹œ ì—…ë°ì´íŠ¸"""
        update_type = progress_update.get('type')
        
        with container:
            if update_type == 'group_started':
                st.info(f"ê·¸ë£¹ {progress_update.get('group_index')}/{progress_update.get('total_groups')} ì‹œì‘")
                st.write(f"ì‹¤í–‰ ì—ì´ì „íŠ¸: {', '.join(progress_update.get('agents', []))}")
            
            elif update_type == 'group_progress':
                progress = progress_update.get('completed', 0) / max(progress_update.get('total', 1), 1)
                st.progress(progress)
                st.caption(f"ê·¸ë£¹ ì§„í–‰ë¥ : {progress_update.get('completed')}/{progress_update.get('total')}")
            
            elif update_type == 'overall_progress':
                st.progress(progress_update.get('progress', 0) / 100)
                st.caption(f"ì „ì²´ ì§„í–‰ë¥ : {progress_update.get('progress', 0):.1f}%")
    
    def _convert_to_a2a_responses(self, workflow_result: Dict) -> List:
        """ì›Œí¬í”Œë¡œìš° ê²°ê³¼ë¥¼ A2AResponse í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        from ..a2a_integration.a2a_communication_protocol import A2AResponse
        
        responses = []
        agent_results = workflow_result.get('agent_results', {})
        
        for agent_name, result_data in agent_results.items():
            response = A2AResponse(
                request_id=f"workflow_{agent_name}",
                agent_id=result_data.get('agent_id', agent_name),
                status="success",
                data=result_data.get('result', {}),
                metadata={},
                timestamp=datetime.now().isoformat(),
                execution_time=result_data.get('execution_time', 0.0)
            )
            responses.append(response)
        
        return responses
    
    def _parse_json_response(self, response: str) -> Dict:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        try:
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response.strip()
            
            return json.loads(json_str)
        except Exception as e:
            logger.warning(f"Failed to parse JSON response: {e}")
            return {}
    
    def run(self):
        """Cherry AI Universal Integration ì‹¤í–‰"""
        try:
            # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ë Œë”ë§
            self.render_application()
            
        except Exception as e:
            logger.error(f"Error running Cherry AI Universal Integration: {e}")
            st.error(f"âŒ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜ - ê¸°ì¡´ cherry_ai.py ì™„ì „ ëŒ€ì²´"""
    try:
        # ì™„ì „íˆ ìƒˆë¡œìš´ Universal Engine ê¸°ë°˜ Cherry AI
        universal_cherry_ai = CherryAIUniversalA2AIntegration()
        universal_cherry_ai.run()
        
    except Exception as e:
        logger.error(f"Main execution error: {e}")
        st.error(f"âŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        
        # ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ
        if st.checkbox("ğŸ› ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ"):
            import traceback
            st.code(traceback.format_exc())


if __name__ == "__main__":
    main()