#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ ì‹œìŠ¤í…œ
LLM First ì›ì¹™ì„ ìœ ì§€í•˜ë©´ì„œ ì‹¤ìš©ì  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ì§€ëŠ¥í˜• ì‘ë‹µ ì‹œìŠ¤í…œ

í•µì‹¬ ì „ëµ:
1. ê³„ì¸µí™”ëœ ì‘ë‹µ (ì¦‰ì‹œ â†’ ë¹ ë¥¸ LLM â†’ ìƒì„¸ LLM)
2. ì§€ëŠ¥í˜• í”„ë¦¬ì»´í“¨íŒ… (ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ ë¯¸ë¦¬ ì²˜ë¦¬)
3. ì ì‘ì  í’ˆì§ˆ ì¡°ì • (ì‚¬ìš©ì ìš”êµ¬ì— ë”°ë¥¸ ë™ì  ì¡°ì •)
4. ë°±ê·¸ë¼ìš´ë“œ ê°œì„  (ì ì§„ì  í’ˆì§ˆ í–¥ìƒ)
"""

import asyncio
import time
import logging
from typing import Dict, List, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from datetime import datetime
import json
import hashlib
import re
from pathlib import Path

from .llm_factory import LLMFactory

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ResponseMetadata:
    """ì‘ë‹µ ë©”íƒ€ë°ì´í„°"""
    response_type: str  # immediate, quick_llm, detailed_llm, fallback
    generation_time: float
    quality_estimate: float
    source: str
    cached: bool = False
    background_improvement: bool = False

class PatternMatcher:
    """íŒ¨í„´ ê¸°ë°˜ ì¦‰ì‹œ ì‘ë‹µ ë§¤ì²˜"""
    
    def __init__(self):
        """íŒ¨í„´ ë§¤ì²˜ ì´ˆê¸°í™”"""
        self.patterns = {
            # ê¸°ë³¸ ì •ì˜ ì§ˆë¬¸
            r"what\s+is\s+(ai|artificial\s+intelligence)": {
                "template": "AI (Artificial Intelligence) refers to computer systems that can perform tasks typically requiring human intelligence, such as learning, reasoning, and problem-solving.",
                "quality": 0.75,
                "expand_prompt": "Provide a comprehensive explanation of artificial intelligence, including its applications and importance"
            },
            r"what\s+is\s+(ml|machine\s+learning)": {
                "template": "Machine Learning is a subset of AI that enables computers to learn and improve from data without being explicitly programmed for each task.",
                "quality": 0.75,
                "expand_prompt": "Explain machine learning in detail, including types, algorithms, and real-world applications"
            },
            r"what\s+is\s+(data\s+science|data\s+analysis)": {
                "template": "Data science combines statistics, programming, and domain expertise to extract insights from data for decision-making.",
                "quality": 0.7,
                "expand_prompt": "Provide a detailed overview of data science, methodologies, and career aspects"
            },
            
            # ë¹„êµ ì§ˆë¬¸
            r"(difference|compare)\s+between\s+(ai|artificial\s+intelligence)\s+and\s+(ml|machine\s+learning)": {
                "template": "AI is the broader concept of intelligent machines, while ML is a specific approach to achieve AI through data-driven learning algorithms.",
                "quality": 0.8,
                "expand_prompt": "Compare and contrast AI and ML with examples, applications, and technical differences"
            },
            
            # ë°©ë²• ì§ˆë¬¸
            r"how\s+to\s+(optimize|improve)\s+(performance|speed)": {
                "template": "Performance optimization typically involves: 1) Identifying bottlenecks, 2) Optimizing algorithms and data structures, 3) Using caching and parallel processing, 4) Hardware optimization.",
                "quality": 0.7,
                "expand_prompt": "Provide comprehensive performance optimization strategies with practical examples and implementation details"
            },
            
            # ê¸°ìˆ ì  ì§ˆë¬¸
            r"(explain|what\s+is)\s+(algorithm|neural\s+network|deep\s+learning)": {
                "template": "This is a complex technical topic that requires detailed explanation. Let me provide a comprehensive response.",
                "quality": 0.5,
                "expand_prompt": "Provide detailed technical explanation with examples, mathematical foundations, and practical applications"
            }
        }
        
        logger.info(f"PatternMatcher initialized with {len(self.patterns)} patterns")
    
    def match_pattern(self, query: str) -> Optional[Dict[str, Any]]:
        """ì¿¼ë¦¬ì— ëŒ€í•œ íŒ¨í„´ ë§¤ì¹­"""
        query_lower = query.lower().strip()
        
        for pattern, response_data in self.patterns.items():
            if re.search(pattern, query_lower):
                return {
                    "template_response": response_data["template"],
                    "quality_estimate": response_data["quality"],
                    "expand_prompt": response_data["expand_prompt"],
                    "pattern_matched": pattern
                }
        
        return None

class PrecomputedResponseManager:
    """í”„ë¦¬ì»´í“¨íŒ…ëœ ì‘ë‹µ ê´€ë¦¬ì"""
    
    def __init__(self):
        """í”„ë¦¬ì»´í“¨íŒ… ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        self.precomputed_responses = {}
        self.common_queries = [
            "What is artificial intelligence?",
            "Explain machine learning",
            "What is data science?",
            "How to optimize performance?",
            "What is the difference between AI and ML?",
            "Explain neural networks",
            "What is deep learning?",
            "How to implement machine learning?",
            "What are the applications of AI?",
            "Explain data analysis techniques"
        ]
        self.generation_in_progress = set()
        
        logger.info(f"PrecomputedResponseManager initialized with {len(self.common_queries)} common queries")
    
    async def initialize_precomputed_responses(self):
        """ë°±ê·¸ë¼ìš´ë“œì—ì„œ í”„ë¦¬ì»´í“¨íŒ…ëœ ì‘ë‹µ ìƒì„±"""
        logger.info("Starting background precomputation of common responses...")
        
        llm_client = LLMFactory.create_llm()
        
        for query in self.common_queries:
            if query not in self.precomputed_responses and query not in self.generation_in_progress:
                self.generation_in_progress.add(query)
                
                try:
                    # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë¡œ ì‹¤í–‰
                    asyncio.create_task(self._generate_precomputed_response(llm_client, query))
                    
                    # CPU ê³¼ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ì‘ì€ ì§€ì—°
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.warning(f"Failed to start precomputation for '{query}': {e}")
                    self.generation_in_progress.discard(query)
    
    async def _generate_precomputed_response(self, llm_client, query: str):
        """ë‹¨ì¼ ì¿¼ë¦¬ì— ëŒ€í•œ í”„ë¦¬ì»´í“¨íŒ…ëœ ì‘ë‹µ ìƒì„±"""
        try:
            start_time = time.time()
            
            # íƒ€ì„ì•„ì›ƒì„ ê¸¸ê²Œ ì„¤ì • (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…ì´ë¯€ë¡œ)
            response = await asyncio.wait_for(
                llm_client.ainvoke(query),
                timeout=30.0
            )
            
            generation_time = time.time() - start_time
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            # í’ˆì§ˆ ì¶”ì • (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            quality_estimate = min(len(response_content) / 200, 1.0)  # ê¸¸ì´ ê¸°ë°˜ í’ˆì§ˆ ì¶”ì •
            
            self.precomputed_responses[query] = {
                'response': response_content,
                'generated_at': time.time(),
                'generation_time': generation_time,
                'quality_estimate': quality_estimate,
                'usage_count': 0
            }
            
            logger.info(f"Precomputed response for '{query[:50]}...' in {generation_time:.2f}s")
            
        except Exception as e:
            logger.warning(f"Failed to precompute response for '{query}': {e}")
        
        finally:
            self.generation_in_progress.discard(query)
    
    def get_precomputed_response(self, query: str) -> Optional[Dict[str, Any]]:
        """í”„ë¦¬ì»´í“¨íŒ…ëœ ì‘ë‹µ ê²€ìƒ‰"""
        # ì •í™•í•œ ë§¤ì¹­
        if query in self.precomputed_responses:
            response_data = self.precomputed_responses[query]
            response_data['usage_count'] += 1
            return response_data
        
        # ìœ ì‚¬í•œ ì¿¼ë¦¬ ê²€ìƒ‰ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­)
        query_lower = query.lower()
        for precomputed_query, response_data in self.precomputed_responses.items():
            precomputed_lower = precomputed_query.lower()
            
            # í‚¤ì›Œë“œ ê²¹ì¹˜ëŠ” ì •ë„ í™•ì¸
            query_words = set(query_lower.split())
            precomputed_words = set(precomputed_lower.split())
            
            overlap = len(query_words.intersection(precomputed_words))
            if overlap >= 2 and overlap / len(query_words) >= 0.5:
                response_data['usage_count'] += 1
                return response_data
        
        return None

class HybridResponseSystem:
    """í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.pattern_matcher = PatternMatcher()
        self.precomputed_manager = PrecomputedResponseManager()
        self.llm_client = LLMFactory.create_llm()
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = {
            "total_requests": 0,
            "immediate_responses": 0,
            "quick_llm_responses": 0,
            "detailed_llm_responses": 0,
            "fallback_responses": 0,
            "background_improvements": 0
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ê°œì„  í
        self.improvement_queue = asyncio.Queue()
        
        logger.info("HybridResponseSystem initialized")
    
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("Initializing HybridResponseSystem...")
        
        # í”„ë¦¬ì»´í“¨íŒ…ëœ ì‘ë‹µ ì´ˆê¸°í™” (ë°±ê·¸ë¼ìš´ë“œ)
        asyncio.create_task(self.precomputed_manager.initialize_precomputed_responses())
        
        # ë°±ê·¸ë¼ìš´ë“œ ê°œì„  ì›Œì»¤ ì‹œì‘
        asyncio.create_task(self._background_improvement_worker())
        
        logger.info("HybridResponseSystem initialization completed")
    
    async def get_response(
        self, 
        query: str, 
        max_time: float = 8.0,
        quality_preference: str = "balanced"  # fast, balanced, detailed
    ) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ ìƒì„±"""
        start_time = time.time()
        self.metrics["total_requests"] += 1
        
        logger.info(f"Processing query: '{query[:50]}...' with {quality_preference} quality preference")
        
        try:
            # 1ë‹¨ê³„: ì¦‰ì‹œ ì‘ë‹µ ì‹œë„ (íŒ¨í„´ ë§¤ì¹­)
            immediate_response = await self._try_immediate_response(query)
            if immediate_response and quality_preference == "fast":
                return self._create_response_result(
                    immediate_response["response"],
                    time.time() - start_time,
                    "immediate",
                    immediate_response["metadata"]
                )
            
            # 2ë‹¨ê³„: í”„ë¦¬ì»´í“¨íŒ…ëœ ì‘ë‹µ í™•ì¸
            precomputed_response = await self._try_precomputed_response(query)
            if precomputed_response:
                # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê°œì„ ëœ ì‘ë‹µ ìƒì„± ì‹œì‘
                if quality_preference in ["balanced", "detailed"]:
                    asyncio.create_task(self._queue_background_improvement(query, precomputed_response))
                
                return self._create_response_result(
                    precomputed_response["response"],
                    time.time() - start_time,
                    "precomputed",
                    ResponseMetadata(
                        response_type="precomputed",
                        generation_time=precomputed_response["generation_time"],
                        quality_estimate=precomputed_response["quality_estimate"],
                        source="precomputed_cache",
                        cached=True
                    )
                )
            
            # 3ë‹¨ê³„: LLM ì‘ë‹µ (í’ˆì§ˆ ì„ í˜¸ë„ì— ë”°ë¼)
            if quality_preference == "fast":
                # ë¹ ë¥¸ LLM ì‘ë‹µ
                llm_response = await self._try_quick_llm_response(query, max_time / 2)
            elif quality_preference == "balanced":
                # ê· í˜• ì¡íŒ LLM ì‘ë‹µ
                llm_response = await self._try_balanced_llm_response(query, max_time)
            else:  # detailed
                # ìƒì„¸í•œ LLM ì‘ë‹µ
                llm_response = await self._try_detailed_llm_response(query, max_time)
            
            if llm_response:
                return llm_response
            
            # 4ë‹¨ê³„: í´ë°± ì‘ë‹µ
            return await self._get_fallback_response(query, time.time() - start_time)
            
        except Exception as e:
            logger.error(f"Error in hybrid response generation: {e}")
            return await self._get_fallback_response(query, time.time() - start_time, str(e))
    
    async def _try_immediate_response(self, query: str) -> Optional[Dict[str, Any]]:
        """ì¦‰ì‹œ ì‘ë‹µ ì‹œë„"""
        pattern_match = self.pattern_matcher.match_pattern(query)
        if pattern_match:
            self.metrics["immediate_responses"] += 1
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë” ìƒì„¸í•œ ì‘ë‹µ ì¤€ë¹„
            asyncio.create_task(self._queue_background_improvement(
                query, 
                {"response": pattern_match["template_response"], "expand_prompt": pattern_match["expand_prompt"]}
            ))
            
            return {
                "response": pattern_match["template_response"],
                "metadata": ResponseMetadata(
                    response_type="immediate",
                    generation_time=0.001,  # ì¦‰ì‹œ ì‘ë‹µ
                    quality_estimate=pattern_match["quality_estimate"],
                    source="pattern_matching",
                    background_improvement=True
                )
            }
        
        return None
    
    async def _try_precomputed_response(self, query: str) -> Optional[Dict[str, Any]]:
        """í”„ë¦¬ì»´í“¨íŒ…ëœ ì‘ë‹µ ì‹œë„"""
        precomputed = self.precomputed_manager.get_precomputed_response(query)
        if precomputed:
            self.metrics["quick_llm_responses"] += 1
            return precomputed
        
        return None
    
    async def _try_quick_llm_response(self, query: str, max_time: float) -> Optional[Dict[str, Any]]:
        """ë¹ ë¥¸ LLM ì‘ë‹µ ì‹œë„"""
        try:
            # ê°„ì†Œí™”ëœ í”„ë¡¬í”„íŠ¸
            quick_prompt = f"Briefly explain: {query}"
            
            start_time = time.time()
            response = await asyncio.wait_for(
                self.llm_client.ainvoke(quick_prompt),
                timeout=max_time
            )
            
            generation_time = time.time() - start_time
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            self.metrics["quick_llm_responses"] += 1
            
            return self._create_response_result(
                response_content,
                generation_time,
                "quick_llm",
                ResponseMetadata(
                    response_type="quick_llm",
                    generation_time=generation_time,
                    quality_estimate=0.7,  # ì¶”ì • í’ˆì§ˆ
                    source="quick_llm_call"
                )
            )
            
        except asyncio.TimeoutError:
            logger.warning(f"Quick LLM response timeout after {max_time}s")
            return None
        except Exception as e:
            logger.error(f"Quick LLM response failed: {e}")
            return None
    
    async def _try_balanced_llm_response(self, query: str, max_time: float) -> Optional[Dict[str, Any]]:
        """ê· í˜• ì¡íŒ LLM ì‘ë‹µ ì‹œë„"""
        try:
            start_time = time.time()
            response = await asyncio.wait_for(
                self.llm_client.ainvoke(query),
                timeout=max_time
            )
            
            generation_time = time.time() - start_time
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            self.metrics["detailed_llm_responses"] += 1
            
            return self._create_response_result(
                response_content,
                generation_time,
                "balanced_llm",
                ResponseMetadata(
                    response_type="balanced_llm",
                    generation_time=generation_time,
                    quality_estimate=0.85,
                    source="balanced_llm_call"
                )
            )
            
        except asyncio.TimeoutError:
            logger.warning(f"Balanced LLM response timeout after {max_time}s")
            return None
        except Exception as e:
            logger.error(f"Balanced LLM response failed: {e}")
            return None
    
    async def _try_detailed_llm_response(self, query: str, max_time: float) -> Optional[Dict[str, Any]]:
        """ìƒì„¸í•œ LLM ì‘ë‹µ ì‹œë„"""
        try:
            # ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸
            detailed_prompt = f"Provide a comprehensive and detailed explanation for: {query}. Include examples, technical details, and practical applications where relevant."
            
            start_time = time.time()
            response = await asyncio.wait_for(
                self.llm_client.ainvoke(detailed_prompt),
                timeout=max_time
            )
            
            generation_time = time.time() - start_time
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            self.metrics["detailed_llm_responses"] += 1
            
            return self._create_response_result(
                response_content,
                generation_time,
                "detailed_llm",
                ResponseMetadata(
                    response_type="detailed_llm",
                    generation_time=generation_time,
                    quality_estimate=0.95,
                    source="detailed_llm_call"
                )
            )
            
        except asyncio.TimeoutError:
            logger.warning(f"Detailed LLM response timeout after {max_time}s")
            return None
        except Exception as e:
            logger.error(f"Detailed LLM response failed: {e}")
            return None
    
    async def _get_fallback_response(self, query: str, elapsed_time: float, error: Optional[str] = None) -> Dict[str, Any]:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        self.metrics["fallback_responses"] += 1
        
        fallback_message = f"I understand you're asking about '{query}'. While I'm processing a detailed response, here's what I can tell you immediately: This appears to be a complex topic that requires careful analysis. I'm working on providing you with a comprehensive answer."
        
        if error:
            fallback_message += f" (Technical note: {error})"
        
        # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‹¤ì œ ì‘ë‹µ ìƒì„± ì‹œì‘
        asyncio.create_task(self._queue_background_improvement(query, {"response": fallback_message}))
        
        return self._create_response_result(
            fallback_message,
            elapsed_time,
            "fallback",
            ResponseMetadata(
                response_type="fallback",
                generation_time=elapsed_time,
                quality_estimate=0.3,
                source="fallback_system",
                background_improvement=True
            )
        )
    
    async def _queue_background_improvement(self, query: str, current_response: Dict[str, Any]):
        """ë°±ê·¸ë¼ìš´ë“œ ê°œì„  íì— ì¶”ê°€"""
        try:
            await self.improvement_queue.put({
                "query": query,
                "current_response": current_response,
                "queued_at": time.time()
            })
        except Exception as e:
            logger.warning(f"Failed to queue background improvement: {e}")
    
    async def _background_improvement_worker(self):
        """ë°±ê·¸ë¼ìš´ë“œ ê°œì„  ì›Œì»¤"""
        logger.info("Background improvement worker started")
        
        while True:
            try:
                # ê°œì„  ì‘ì—… ëŒ€ê¸°
                improvement_task = await self.improvement_queue.get()
                
                query = improvement_task["query"]
                current_response = improvement_task["current_response"]
                
                logger.info(f"Processing background improvement for: '{query[:50]}...'")
                
                # ë” ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¡œ ê°œì„ ëœ ì‘ë‹µ ìƒì„±
                improved_prompt = current_response.get("expand_prompt", 
                    f"Provide a comprehensive, detailed, and high-quality response to: {query}")
                
                try:
                    start_time = time.time()
                    response = await asyncio.wait_for(
                        self.llm_client.ainvoke(improved_prompt),
                        timeout=30.0  # ë°±ê·¸ë¼ìš´ë“œì´ë¯€ë¡œ ë” ê¸´ íƒ€ì„ì•„ì›ƒ
                    )
                    
                    generation_time = time.time() - start_time
                    response_content = response.content if hasattr(response, 'content') else str(response)
                    
                    # ê°œì„ ëœ ì‘ë‹µì„ í”„ë¦¬ì»´í“¨íŒ… ìºì‹œì— ì €ì¥
                    self.precomputed_manager.precomputed_responses[query] = {
                        'response': response_content,
                        'generated_at': time.time(),
                        'generation_time': generation_time,
                        'quality_estimate': 0.9,  # ê°œì„ ëœ ì‘ë‹µì˜ ë†’ì€ í’ˆì§ˆ
                        'usage_count': 0,
                        'background_improved': True
                    }
                    
                    self.metrics["background_improvements"] += 1
                    logger.info(f"Background improvement completed for '{query[:50]}...' in {generation_time:.2f}s")
                    
                except Exception as e:
                    logger.warning(f"Background improvement failed for '{query}': {e}")
                
                # ë‹¤ìŒ ì‘ì—… ì „ ì ì‹œ ëŒ€ê¸° (ì‹œìŠ¤í…œ ë¶€í•˜ ë°©ì§€)
                await asyncio.sleep(1.0)
                
            except Exception as e:
                logger.error(f"Background improvement worker error: {e}")
                await asyncio.sleep(5.0)  # ì—ëŸ¬ ë°œìƒì‹œ ë” ê¸´ ëŒ€ê¸°
    
    def _create_response_result(
        self, 
        response: str, 
        execution_time: float, 
        response_type: str,
        metadata: ResponseMetadata
    ) -> Dict[str, Any]:
        """ì‘ë‹µ ê²°ê³¼ ìƒì„±"""
        return {
            "response": response,
            "execution_time": execution_time,
            "response_type": response_type,
            "metadata": metadata.__dict__,
            "success": True,
            "hybrid_system": True,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_system_metrics(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        total_requests = self.metrics["total_requests"]
        if total_requests == 0:
            return self.metrics
        
        return {
            **self.metrics,
            "immediate_response_rate": self.metrics["immediate_responses"] / total_requests,
            "quick_llm_rate": self.metrics["quick_llm_responses"] / total_requests,
            "detailed_llm_rate": self.metrics["detailed_llm_responses"] / total_requests,
            "fallback_rate": self.metrics["fallback_responses"] / total_requests,
            "precomputed_cache_size": len(self.precomputed_manager.precomputed_responses)
        }

# ì „ì—­ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
_global_hybrid_system = None

async def get_hybrid_system() -> HybridResponseSystem:
    """ì „ì—­ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ë°˜í™˜"""
    global _global_hybrid_system
    if _global_hybrid_system is None:
        _global_hybrid_system = HybridResponseSystem()
        await _global_hybrid_system.initialize()
    return _global_hybrid_system

async def get_hybrid_response(
    query: str, 
    max_time: float = 8.0,
    quality_preference: str = "balanced"
) -> Dict[str, Any]:
    """í¸ì˜ í•¨ìˆ˜: í•˜ì´ë¸Œë¦¬ë“œ ì‘ë‹µ ìƒì„±"""
    hybrid_system = await get_hybrid_system()
    return await hybrid_system.get_response(query, max_time, quality_preference)