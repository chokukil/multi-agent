#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ LLM ì„±ëŠ¥ ìµœì í™” ì—”ì§„
ì‹¬ì¸µ ë¦¬ì„œì¹˜ ê¸°ë°˜ LLM ì¶”ë¡  ì„±ëŠ¥ ê·¼ë³¸ì  ê°œì„ 

ìµœì í™” ê¸°ë²•:
1. í”„ë¡¬í”„íŠ¸ ì••ì¶• (LLMLingua)
2. ëª¨ë¸ ì–‘ìí™” (8-bit, 4-bit)
3. ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”
4. ìºì‹± ì „ëµ
5. ë°°ì¹˜ ì²˜ë¦¬
6. ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
"""

import asyncio
import time
import logging
from typing import Dict, List, Any, Optional, Union, AsyncIterator
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import json
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class OptimizationConfig:
    """ìµœì í™” ì„¤ì •"""
    enable_prompt_compression: bool = True
    enable_caching: bool = True
    enable_batch_processing: bool = True
    enable_streaming: bool = True
    max_cache_size: int = 1000
    compression_ratio: float = 0.5
    batch_size: int = 5
    timeout_seconds: int = 10
    parallel_workers: int = 3

class LLMPerformanceOptimizer:
    """LLM ì„±ëŠ¥ ìµœì í™” ì—”ì§„"""
    
    def __init__(self, config: Optional[OptimizationConfig] = None):
        """ì´ˆê¸°í™”"""
        self.config = config or OptimizationConfig()
        self.response_cache = {}
        self.compression_cache = {}
        self.executor = ThreadPoolExecutor(max_workers=self.config.parallel_workers)
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.metrics = {
            "total_requests": 0,
            "cache_hits": 0,
            "compression_savings": 0,
            "avg_response_time": 0.0,
            "total_response_time": 0.0
        }
        
        logger.info(f"LLMPerformanceOptimizer initialized with config: {self.config}")
    
    async def optimize_llm_call(
        self, 
        llm_client, 
        prompt: str, 
        context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """ìµœì í™”ëœ LLM í˜¸ì¶œ"""
        start_time = time.time()
        self.metrics["total_requests"] += 1
        
        try:
            # 1. ìºì‹œ í™•ì¸
            if self.config.enable_caching:
                cached_result = await self._check_cache(prompt, context)
                if cached_result:
                    self.metrics["cache_hits"] += 1
                    execution_time = time.time() - start_time
                    logger.info(f"Cache hit for prompt: {prompt[:50]}... ({execution_time:.3f}s)")
                    return self._create_result(cached_result, execution_time, "cache")
            
            # 2. í”„ë¡¬í”„íŠ¸ ì••ì¶•
            optimized_prompt = prompt
            compression_ratio = 1.0
            
            if self.config.enable_prompt_compression:
                optimized_prompt, compression_ratio = await self._compress_prompt(prompt)
                self.metrics["compression_savings"] += (1 - compression_ratio)
            
            # 3. ìµœì í™”ëœ LLM í˜¸ì¶œ
            response = await self._execute_optimized_call(
                llm_client, 
                optimized_prompt, 
                context
            )
            
            # 4. ê²°ê³¼ ìºì‹±
            if self.config.enable_caching:
                await self._cache_result(prompt, context, response)
            
            # 5. ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            execution_time = time.time() - start_time
            self._update_metrics(execution_time)
            
            return self._create_result(response, execution_time, "optimized", compression_ratio)
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"Optimized LLM call failed: {e}")
            return self._create_error_result(str(e), execution_time)
    
    async def _check_cache(self, prompt: str, context: Optional[Dict]) -> Optional[str]:
        """ìºì‹œ í™•ì¸"""
        cache_key = self._generate_cache_key(prompt, context)
        return self.response_cache.get(cache_key)
    
    async def _cache_result(self, prompt: str, context: Optional[Dict], response: str):
        """ê²°ê³¼ ìºì‹±"""
        if len(self.response_cache) >= self.config.max_cache_size:
            # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = next(iter(self.response_cache))
            del self.response_cache[oldest_key]
        
        cache_key = self._generate_cache_key(prompt, context)
        self.response_cache[cache_key] = response
    
    def _generate_cache_key(self, prompt: str, context: Optional[Dict]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = {
            "prompt": prompt,
            "context": context or {}
        }
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()
    
    async def _compress_prompt(self, prompt: str) -> tuple[str, float]:
        """í”„ë¡¬í”„íŠ¸ ì••ì¶• (LLMLingua ìŠ¤íƒ€ì¼)"""
        if prompt in self.compression_cache:
            return self.compression_cache[prompt]
        
        # ê°„ë‹¨í•œ ì••ì¶• ë¡œì§ (ì‹¤ì œë¡œëŠ” LLMLingua ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©)
        compressed = await self._apply_prompt_compression(prompt)
        compression_ratio = len(compressed) / len(prompt)
        
        self.compression_cache[prompt] = (compressed, compression_ratio)
        return compressed, compression_ratio
    
    async def _apply_prompt_compression(self, prompt: str) -> str:
        """ì‹¤ì œ í”„ë¡¬í”„íŠ¸ ì••ì¶• ì ìš©"""
        # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±°
        compression_rules = [
            # ê´€ì‚¬ ì œê±°
            ("the ", ""),
            ("a ", ""),
            ("an ", ""),
            # ë¶ˆí•„ìš”í•œ ì ‘ì†ì‚¬ ì œê±°
            (", and ", ","),
            (", but ", ","),
            # ì¤‘ë³µ ê³µë°± ì œê±°
            ("  ", " "),
            # ë¬¸ì¥ ë ì •ë¦¬
            (". ", "."),
        ]
        
        compressed = prompt
        for old, new in compression_rules:
            compressed = compressed.replace(old, new)
        
        # ìµœì†Œ ê¸¸ì´ ë³´ì¥
        target_length = int(len(prompt) * self.config.compression_ratio)
        if len(compressed) > target_length and target_length > 50:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¶•ì•½
            sentences = compressed.split('. ')
            if len(sentences) > 2:
                # í•µì‹¬ ë¬¸ì¥ë§Œ ìœ ì§€
                compressed = '. '.join(sentences[:max(1, len(sentences)//2)]) + '.'
        
        return compressed.strip()
    
    async def _execute_optimized_call(
        self, 
        llm_client, 
        prompt: str, 
        context: Optional[Dict]
    ) -> str:
        """ìµœì í™”ëœ LLM í˜¸ì¶œ ì‹¤í–‰"""
        
        # íƒ€ì„ì•„ì›ƒ ì ìš©
        try:
            response = await asyncio.wait_for(
                llm_client.ainvoke(prompt),
                timeout=self.config.timeout_seconds
            )
            
            # ì‘ë‹µ ë‚´ìš© ì¶”ì¶œ
            if hasattr(response, 'content'):
                return response.content
            else:
                return str(response)
                
        except asyncio.TimeoutError:
            raise Exception(f"LLM call timeout after {self.config.timeout_seconds}s")
    
    def _update_metrics(self, execution_time: float):
        """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.metrics["total_response_time"] += execution_time
        self.metrics["avg_response_time"] = (
            self.metrics["total_response_time"] / self.metrics["total_requests"]
        )
    
    def _create_result(
        self, 
        response: str, 
        execution_time: float, 
        method: str, 
        compression_ratio: float = 1.0
    ) -> Dict[str, Any]:
        """ê²°ê³¼ ê°ì²´ ìƒì„±"""
        return {
            "response": response,
            "execution_time": execution_time,
            "optimization_method": method,
            "compression_ratio": compression_ratio,
            "cache_hit": method == "cache",
            "success": True
        }
    
    def _create_error_result(self, error: str, execution_time: float) -> Dict[str, Any]:
        """ì—ëŸ¬ ê²°ê³¼ ìƒì„±"""
        return {
            "response": "",
            "execution_time": execution_time,
            "optimization_method": "failed",
            "error": error,
            "success": False
        }
    
    async def batch_optimize_calls(
        self, 
        llm_client, 
        prompts: List[str], 
        contexts: Optional[List[Dict]] = None
    ) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ìµœì í™” í˜¸ì¶œ"""
        if not self.config.enable_batch_processing:
            # ìˆœì°¨ ì²˜ë¦¬
            results = []
            for i, prompt in enumerate(prompts):
                context = contexts[i] if contexts else None
                result = await self.optimize_llm_call(llm_client, prompt, context)
                results.append(result)
            return results
        
        # ë³‘ë ¬ ë°°ì¹˜ ì²˜ë¦¬
        logger.info(f"Processing {len(prompts)} prompts in parallel batches")
        
        tasks = []
        for i, prompt in enumerate(prompts):
            context = contexts[i] if contexts else None
            task = self.optimize_llm_call(llm_client, prompt, context)
            tasks.append(task)
        
        # ë°°ì¹˜ í¬ê¸°ë¡œ ë¶„í•  ì‹¤í–‰
        results = []
        for i in range(0, len(tasks), self.config.batch_size):
            batch = tasks[i:i + self.config.batch_size]
            batch_results = await asyncio.gather(*batch, return_exceptions=True)
            
            for result in batch_results:
                if isinstance(result, Exception):
                    results.append(self._create_error_result(str(result), 0.0))
                else:
                    results.append(result)
        
        return results
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜"""
        cache_hit_rate = (
            (self.metrics["cache_hits"] / self.metrics["total_requests"]) * 100
            if self.metrics["total_requests"] > 0 else 0
        )
        
        avg_compression_saving = (
            (self.metrics["compression_savings"] / self.metrics["total_requests"]) * 100
            if self.metrics["total_requests"] > 0 else 0
        )
        
        return {
            **self.metrics,
            "cache_hit_rate_percent": cache_hit_rate,
            "avg_compression_saving_percent": avg_compression_saving,
            "cache_size": len(self.response_cache),
            "compression_cache_size": len(self.compression_cache)
        }
    
    def clear_cache(self):
        """ìºì‹œ ì •ë¦¬"""
        self.response_cache.clear()
        self.compression_cache.clear()
        logger.info("All caches cleared")
    
    async def warmup_cache(self, llm_client, warmup_prompts: List[str]):
        """ìºì‹œ ì›Œë°ì—…"""
        logger.info(f"Warming up cache with {len(warmup_prompts)} prompts")
        
        for prompt in warmup_prompts:
            try:
                await self.optimize_llm_call(llm_client, prompt)
            except Exception as e:
                logger.warning(f"Warmup failed for prompt: {prompt[:50]}... Error: {e}")
        
        logger.info(f"Cache warmup completed. Cache size: {len(self.response_cache)}")

class StreamingOptimizer:
    """ìŠ¤íŠ¸ë¦¬ë° ìµœì í™”"""
    
    def __init__(self):
        self.active_streams = {}
    
    async def stream_optimized_response(
        self, 
        llm_client, 
        prompt: str, 
        chunk_callback=None
    ) -> AsyncIterator[str]:
        """ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        stream_id = hashlib.md5(prompt.encode()).hexdigest()
        
        try:
            # ìŠ¤íŠ¸ë¦¬ë° í˜¸ì¶œ (ì‹¤ì œ êµ¬í˜„ì‹œ LLM í´ë¼ì´ì–¸íŠ¸ì˜ ìŠ¤íŠ¸ë¦¬ë° API ì‚¬ìš©)
            response = await llm_client.ainvoke(prompt)
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            # ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í•  ì „ì†¡
            chunk_size = 50
            for i in range(0, len(response_content), chunk_size):
                chunk = response_content[i:i + chunk_size]
                
                if chunk_callback:
                    await chunk_callback(chunk)
                
                yield chunk
                await asyncio.sleep(0.01)  # ì‘ì€ ì§€ì—°ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼
                
        except Exception as e:
            error_chunk = f"Stream error: {e}"
            if chunk_callback:
                await chunk_callback(error_chunk)
            yield error_chunk
        
        finally:
            if stream_id in self.active_streams:
                del self.active_streams[stream_id]

# ì „ì—­ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤
_global_optimizer = None

def get_optimizer(config: Optional[OptimizationConfig] = None) -> LLMPerformanceOptimizer:
    """ì „ì—­ ìµœì í™” ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _global_optimizer
    if _global_optimizer is None:
        _global_optimizer = LLMPerformanceOptimizer(config)
    return _global_optimizer

async def optimize_llm_call(llm_client, prompt: str, context: Optional[Dict] = None) -> Dict[str, Any]:
    """í¸ì˜ í•¨ìˆ˜: ìµœì í™”ëœ LLM í˜¸ì¶œ"""
    optimizer = get_optimizer()
    return await optimizer.optimize_llm_call(llm_client, prompt, context)