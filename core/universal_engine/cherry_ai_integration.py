#!/usr/bin/env python3
"""
ğŸ’ CherryAI Universal Engine Integration
Connects SmartQueryRouter and LLMFirstOptimizedOrchestrator with CherryAI main system
"""

import logging
import asyncio
from typing import Dict, Any, Optional, AsyncGenerator
import json
from datetime import datetime

# Import our universal engine components
from core.universal_engine.smart_query_router import SmartQueryRouter
from core.universal_engine.llm_first_optimized_orchestrator import LLMFirstOptimizedOrchestrator
from core.universal_engine.langfuse_integration import global_tracer
from core.universal_engine.a2a_integration.agent_pool import AgentPool
from core.universal_engine.llm_factory import LLMFactory

# Import CherryAI components
from core.app_components.realtime_streaming_handler import (
    StreamChunk, StreamState, StreamSession
)

logger = logging.getLogger(__name__)

class CherryAIUniversalEngineIntegration:
    """CherryAIì™€ Universal Engineì„ ì—°ê²°í•˜ëŠ” í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # Universal Engine ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.llm_factory = LLMFactory()
        self.agent_pool = AgentPool()
        
        # SmartQueryRouter ì´ˆê¸°í™”
        self.query_router = SmartQueryRouter(
            llm_factory=self.llm_factory,
            agent_pool=self.agent_pool
        )
        
        # ì„¤ì •
        self.config = {
            'stream_chunk_delay_ms': 1,  # 0.001ì´ˆ ì§€ì—°
            'enable_langfuse_tracing': True,
            'user_id': '2055186',  # EMP_NO
            'max_retries': 3
        }
        
        logger.info("ğŸš€ CherryAI Universal Engine Integration ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_query_with_streaming(
        self, 
        query: str, 
        session_id: str,
        streaming_handler: Any
    ) -> AsyncGenerator[StreamChunk, None]:
        """
        ì¿¼ë¦¬ë¥¼ ì²˜ë¦¬í•˜ê³  ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        
        Args:
            query: ì‚¬ìš©ì ì¿¼ë¦¬
            session_id: ìŠ¤íŠ¸ë¦¬ë° ì„¸ì…˜ ID
            streaming_handler: CherryAI ìŠ¤íŠ¸ë¦¬ë° í•¸ë“¤ëŸ¬
        
        Yields:
            StreamChunk: ìŠ¤íŠ¸ë¦¬ë° ì²­í¬
        """
        try:
            # ì„¸ì…˜ ì‹œì‘
            if self.config['enable_langfuse_tracing']:
                global_tracer.start_session(
                    session_id=f"user_query_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{self.config['user_id']}_{query[:30]}"
                )
            
            # ì´ˆê¸° ìƒíƒœ ì²­í¬
            yield StreamChunk(
                chunk_id=f"{session_id}_init",
                content="ğŸ¤” ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                chunk_type="status",
                is_final=False
            )
            
            # SmartQueryRouterë¡œ ì¿¼ë¦¬ ë¼ìš°íŒ…
            async for response in self.query_router.route_query_with_streaming(query):
                if response['type'] == 'status':
                    # ìƒíƒœ ì—…ë°ì´íŠ¸
                    yield StreamChunk(
                        chunk_id=f"{session_id}_{response.get('chunk_id', 'status')}",
                        content=response['content'],
                        chunk_type="status",
                        source_agent=response.get('agent'),
                        is_final=False
                    )
                elif response['type'] == 'progress':
                    # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                    yield StreamChunk(
                        chunk_id=f"{session_id}_progress_{response.get('step', 0)}",
                        content=f"ğŸ“Š {response['content']}",
                        chunk_type="status",
                        source_agent=response.get('agent'),
                        is_final=False
                    )
                elif response['type'] == 'result':
                    # ì‹¤ì œ ê²°ê³¼ í…ìŠ¤íŠ¸
                    content = response['content']
                    # ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë°
                    chunk_size = 100
                    for i in range(0, len(content), chunk_size):
                        chunk_content = content[i:i+chunk_size]
                        yield StreamChunk(
                            chunk_id=f"{session_id}_result_{i}",
                            content=chunk_content,
                            chunk_type="text",
                            source_agent=response.get('agent'),
                            is_final=(i + chunk_size >= len(content))
                        )
                        # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°ì„ ìœ„í•œ ì§€ì—°
                        await asyncio.sleep(self.config['stream_chunk_delay_ms'] / 1000)
                elif response['type'] == 'error':
                    # ì˜¤ë¥˜ ì²˜ë¦¬
                    yield StreamChunk(
                        chunk_id=f"{session_id}_error",
                        content=f"âŒ ì˜¤ë¥˜: {response['content']}",
                        chunk_type="error",
                        is_final=True
                    )
                    break
            
            # ì™„ë£Œ ì²­í¬
            yield StreamChunk(
                chunk_id=f"{session_id}_complete",
                content="âœ¨ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!",
                chunk_type="status",
                is_final=True
            )
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            yield StreamChunk(
                chunk_id=f"{session_id}_error",
                content=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                chunk_type="error",
                is_final=True
            )
        finally:
            # ì„¸ì…˜ ì¢…ë£Œ
            if self.config['enable_langfuse_tracing']:
                global_tracer.end_session()
    
    async def get_agent_status(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ìƒíƒœ ì¡°íšŒ"""
        try:
            # AgentPoolì—ì„œ ìƒíƒœ ì¡°íšŒ
            agent_status = await self.agent_pool.get_all_agent_status()
            
            # ìš”ì•½ ì •ë³´ ìƒì„±
            total_agents = len(agent_status)
            active_agents = sum(1 for a in agent_status.values() if a.get('status') == 'active')
            
            return {
                'summary': {
                    'total_agents': total_agents,
                    'active_agents': active_agents,
                    'inactive_agents': total_agents - active_agents,
                    'success_rate': f"{(active_agents/total_agents)*100:.1f}%" if total_agents > 0 else "0%"
                },
                'agents': agent_status,
                'timestamp': datetime.now().isoformat()
            }
        except Exception as e:
            logger.error(f"ì—ì´ì „íŠ¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
    
    def update_configuration(self, config: Dict[str, Any]):
        """ì„¤ì • ì—…ë°ì´íŠ¸"""
        self.config.update(config)
        logger.info(f"ì„¤ì • ì—…ë°ì´íŠ¸ ì™„ë£Œ: {config}")

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_integration_instance = None

def get_cherry_ai_integration():
    """ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _integration_instance
    if _integration_instance is None:
        _integration_instance = CherryAIUniversalEngineIntegration()
    return _integration_instance

async def process_query_with_universal_engine(
    query: str,
    session_id: str,
    streaming_handler: Any
) -> AsyncGenerator[StreamChunk, None]:
    """
    Universal Engineì„ ì‚¬ìš©í•˜ì—¬ ì¿¼ë¦¬ ì²˜ë¦¬
    
    ì™¸ë¶€ì—ì„œ ì‰½ê²Œ í˜¸ì¶œí•  ìˆ˜ ìˆëŠ” í—¬í¼ í•¨ìˆ˜
    """
    integration = get_cherry_ai_integration()
    async for chunk in integration.process_query_with_streaming(
        query, session_id, streaming_handler
    ):
        yield chunk