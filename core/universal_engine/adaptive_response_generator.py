"""
Adaptive Response Generator - ì ì‘í˜• ì‘ë‹µ ìƒì„±ê¸°

ìš”êµ¬ì‚¬í•­ 17ì— ë”°ë¥¸ êµ¬í˜„:
- ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì„¤ëª… ìƒì„± ë¡œì§
- ì ì§„ì  ì •ë³´ ê³µê°œ ë©”ì»¤ë‹ˆì¦˜
- ëŒ€í™”í˜• ëª…í™•í™” ì§ˆë¬¸ ìƒì„±
- í›„ì† ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œ
"""

import asyncio
import logging
from typing import Any, Dict, List, Optional
from datetime import datetime

from ..llm_factory import LLMFactory

logger = logging.getLogger(__name__)


class AdaptiveResponseGenerator:
    """
    ì ì‘í˜• ì‘ë‹µ ìƒì„±ê¸°
    - ì‚¬ìš©ì ìˆ˜ì¤€ì— ë§ëŠ” ë™ì  ì‘ë‹µ ìƒì„±
    - ì ì§„ì  ì •ë³´ ê³µê°œ
    - ëŒ€í™”í˜• ìƒí˜¸ì‘ìš© ì§€ì›
    """
    
    def __init__(self):
        """AdaptiveResponseGenerator ì´ˆê¸°í™”"""
        self.llm_client = LLMFactory.create_llm()
        self.response_templates = self._initialize_response_patterns()
        logger.info("AdaptiveResponseGenerator initialized")
    
    def _initialize_response_patterns(self) -> Dict:
        """ì‘ë‹µ íŒ¨í„´ ì´ˆê¸°í™” - í•˜ë“œì½”ë”© ì—†ì´ ë™ì  ìƒì„±"""
        return {
            'progressive_disclosure': {
                'initial': "í•µì‹¬ ì •ë³´ ë¨¼ì € ì œê³µ",
                'detailed': "ê´€ì‹¬ ìˆëŠ” ë¶€ë¶„ ìƒì„¸ ì„¤ëª…",
                'expert': "ì „ë¬¸ì  ê¹Šì´ ìˆëŠ” ë¶„ì„"
            },
            'clarification': {
                'ambiguous': "ëª…í™•í•˜ì§€ ì•Šì€ ë¶€ë¶„ ì§ˆë¬¸",
                'confirmation': "ì´í•´ í™•ì¸ ì§ˆë¬¸",
                'exploration': "ì¶”ê°€ íƒìƒ‰ ì œì•ˆ"
            }
        }
    
    async def generate_adaptive_response(
        self, 
        knowledge_result: Dict, 
        user_profile: Dict, 
        interaction_context: Dict
    ) -> Dict:
        """
        ì‚¬ìš©ì ë§ì¶¤í˜• ì ì‘ ì‘ë‹µ ìƒì„±
        
        Args:
            knowledge_result: ì§€ì‹ í†µí•© ê²°ê³¼
            user_profile: ì‚¬ìš©ì í”„ë¡œí•„ ì •ë³´
            interaction_context: ìƒí˜¸ì‘ìš© ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            ì ì‘í˜• ì‘ë‹µ ê²°ê³¼
        """
        logger.info("Generating adaptive response")
        
        try:
            # 1. ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì„¤ëª… ì „ëµ ê²°ì •
            explanation_strategy = await self._determine_explanation_strategy(
                user_profile, knowledge_result
            )
            
            # 2. í•µì‹¬ ì‘ë‹µ ìƒì„±
            core_response = await self._generate_core_response(
                knowledge_result, explanation_strategy
            )
            
            # 3. ì ì§„ì  ê³µê°œ ì˜µì…˜ ìƒì„±
            progressive_options = await self._create_progressive_disclosure_options(
                core_response, user_profile
            )
            
            # 4. ëŒ€í™”í˜• ìš”ì†Œ ì¶”ê°€
            interactive_elements = await self._add_interactive_elements(
                core_response, knowledge_result, user_profile
            )
            
            # 5. í›„ì† ì¶”ì²œ ìƒì„±
            follow_up_recommendations = await self._generate_follow_up_recommendations(
                knowledge_result, interaction_context
            )
            
            return {
                'core_response': core_response,
                'progressive_options': progressive_options,
                'interactive_elements': interactive_elements,
                'follow_up_recommendations': follow_up_recommendations,
                'metadata': {
                    'explanation_strategy': explanation_strategy,
                    'user_level': user_profile.get('expertise_level', 'unknown'),
                    'timestamp': datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"Error in adaptive response generation: {e}")
            raise
    
    async def _determine_explanation_strategy(self, user_profile: Dict, knowledge_result: Dict) -> Dict:
        """
        ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì„¤ëª… ì „ëµ ê²°ì •
        """
        prompt = f"""
        ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ì§€ì‹ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ì„¤ëª… ì „ëµì„ ê²°ì •í•˜ì„¸ìš”.
        
        ì‚¬ìš©ì í”„ë¡œí•„: {user_profile}
        ì§€ì‹ ë³µì¡ë„: {self._assess_knowledge_complexity(knowledge_result)}
        
        ë‹¤ìŒì„ ê³ ë ¤í•˜ì—¬ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”:
        1. ì‚¬ìš©ìì˜ ì „ë¬¸ì„± ìˆ˜ì¤€
        2. ì„ í˜¸í•˜ëŠ” í•™ìŠµ ìŠ¤íƒ€ì¼
        3. ë„ë©”ì¸ ì¹œìˆ™ë„
        4. ìƒí˜¸ì‘ìš© ì„ í˜¸ë„
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "primary_approach": "educational|informative|consultative|collaborative",
            "explanation_depth": {{
                "initial": "shallow|medium|deep",
                "maximum": "medium|deep|expert"
            }},
            "language_style": {{
                "technicality": "low|medium|high",
                "formality": "casual|balanced|formal",
                "use_analogies": true/false,
                "use_examples": true/false
            }},
            "interaction_mode": {{
                "proactive_clarification": true/false,
                "offer_alternatives": true/false,
                "encourage_exploration": true/false
            }},
            "pacing": "fast|moderate|slow"
        }}
        """
        
        response = await self.llm_client.agenerate(prompt)
        return self._parse_json_response(response)
    
    async def _generate_core_response(self, knowledge_result: Dict, strategy: Dict) -> Dict:
        """
        í•µì‹¬ ì‘ë‹µ ìƒì„±
        """
        prompt = f"""
        ë‹¤ìŒ ì§€ì‹ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  í•µì‹¬ ì‘ë‹µì„ ìƒì„±í•˜ì„¸ìš”.
        
        ì§€ì‹ ê²°ê³¼: {knowledge_result.get('refined_result', {})}
        ì„¤ëª… ì „ëµ: {strategy}
        
        ì „ëµì— ë”°ë¼:
        - ê¸°ìˆ  ìˆ˜ì¤€: {strategy.get('language_style', {}).get('technicality', 'medium')}
        - í˜•ì‹ì„±: {strategy.get('language_style', {}).get('formality', 'balanced')}
        - ìœ ì¶” ì‚¬ìš©: {strategy.get('language_style', {}).get('use_analogies', False)}
        - ì˜ˆì‹œ ì‚¬ìš©: {strategy.get('language_style', {}).get('use_examples', False)}
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "summary": "í•œ ë¬¸ì¥ ìš”ì•½",
            "main_insights": [
                {{
                    "insight": "í•µì‹¬ ì¸ì‚¬ì´íŠ¸ 1",
                    "explanation": "ì„¤ëª…",
                    "confidence": "high|medium|low"
                }}
            ],
            "key_findings": [
                {{
                    "finding": "ì£¼ìš” ë°œê²¬ 1",
                    "evidence": "ê·¼ê±°",
                    "implication": "ì‹œì‚¬ì "
                }}
            ],
            "recommendations": [
                {{
                    "action": "ê¶Œì¥ ì¡°ì¹˜ 1",
                    "rationale": "ì´ìœ ",
                    "priority": "high|medium|low"
                }}
            ],
            "caveats": ["ì£¼ì˜ì‚¬í•­ 1", "ì£¼ì˜ì‚¬í•­ 2"]
        }}
        """
        
        response = await self.llm_client.agenerate(prompt)
        return self._parse_json_response(response)
    
    async def _create_progressive_disclosure_options(self, core_response: Dict, user_profile: Dict) -> Dict:
        """
        ì ì§„ì  ì •ë³´ ê³µê°œ ì˜µì…˜ ìƒì„±
        """
        prompt = f"""
        í•µì‹¬ ì‘ë‹µì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìê°€ ì„ íƒì ìœ¼ë¡œ ë” ê¹Šì´ íƒìƒ‰í•  ìˆ˜ ìˆëŠ” ì˜µì…˜ì„ ìƒì„±í•˜ì„¸ìš”.
        
        í•µì‹¬ ì‘ë‹µ: {core_response}
        ì‚¬ìš©ì ìˆ˜ì¤€: {user_profile.get('expertise_level', 'unknown')}
        
        ì‚¬ìš©ìê°€ ê´€ì‹¬ ìˆì„ ë§Œí•œ ì¶”ê°€ ì •ë³´ë¥¼ ë‹¨ê³„ë³„ë¡œ ì œê³µí•˜ì„¸ìš”.
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "explore_deeper": [
                {{
                    "topic": "ê¹Šì´ íƒìƒ‰í•  ì£¼ì œ 1",
                    "teaser": "ì´ ì£¼ì œì— ëŒ€í•œ í¥ë¯¸ë¡œìš´ í•œ ì¤„ ì„¤ëª…",
                    "depth_level": "intermediate|advanced|expert",
                    "estimated_time": "2-3ë¶„"
                }}
            ],
            "see_examples": [
                {{
                    "example_type": "ì‹¤ì œ ì‚¬ë¡€",
                    "description": "ì‚¬ë¡€ ì„¤ëª…",
                    "relevance": "ì´ ì‚¬ë¡€ê°€ ë„ì›€ì´ ë˜ëŠ” ì´ìœ "
                }}
            ],
            "technical_details": [
                {{
                    "aspect": "ê¸°ìˆ ì  ì¸¡ë©´ 1",
                    "summary": "ê°„ë‹¨í•œ ì„¤ëª…",
                    "warning": "ë³µì¡ë„ ê²½ê³  (ìˆë‹¤ë©´)"
                }}
            ],
            "related_topics": [
                {{
                    "topic": "ê´€ë ¨ ì£¼ì œ 1",
                    "connection": "í˜„ì¬ ì£¼ì œì™€ì˜ ì—°ê´€ì„±",
                    "value": "íƒìƒ‰í•  ê°€ì¹˜"
                }}
            ]
        }}
        """
        
        response = await self.llm_client.agenerate(prompt)
        return self._parse_json_response(response)
    
    async def _add_interactive_elements(self, core_response: Dict, knowledge_result: Dict, user_profile: Dict) -> Dict:
        """
        ëŒ€í™”í˜• ìš”ì†Œ ì¶”ê°€
        """
        prompt = f"""
        ì‘ë‹µì— ì‚¬ìš©ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ì´‰ì§„í•  ìš”ì†Œë¥¼ ì¶”ê°€í•˜ì„¸ìš”.
        
        í•µì‹¬ ì‘ë‹µ: {core_response}
        ë¶ˆí™•ì‹¤í•œ ì˜ì—­: {knowledge_result.get('refined_result', {}).get('confidence_assessment', {}).get('needs_validation', [])}
        ì‚¬ìš©ì í”„ë¡œí•„: {user_profile}
        
        ëŒ€í™”ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆëŠ” ìš”ì†Œë¥¼ ìƒì„±í•˜ì„¸ìš”.
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "clarification_questions": [
                {{
                    "question": "ëª…í™•í™” ì§ˆë¬¸ 1",
                    "purpose": "ì´ ì§ˆë¬¸ì˜ ëª©ì ",
                    "options": ["ì„ íƒì§€ 1", "ì„ íƒì§€ 2", "ì„ íƒì§€ 3"]
                }}
            ],
            "interactive_prompts": [
                {{
                    "prompt": "ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ 1",
                    "action_type": "explore|validate|customize",
                    "expected_input": "ì˜ˆìƒ ì…ë ¥ ìœ í˜•"
                }}
            ],
            "feedback_requests": [
                {{
                    "aspect": "í”¼ë“œë°± ìš”ì²­ ì¸¡ë©´",
                    "question": "í”¼ë“œë°± ì§ˆë¬¸",
                    "scale": "binary|scale|open"
                }}
            ],
            "quick_actions": [
                {{
                    "action": "ë¹ ë¥¸ ì‘ì—… 1",
                    "description": "ì‘ì—… ì„¤ëª…",
                    "icon": "ğŸ”|ğŸ“Š|ğŸ’¡|ğŸ¯"
                }}
            ]
        }}
        """
        
        response = await self.llm_client.agenerate(prompt)
        return self._parse_json_response(response)
    
    async def _generate_follow_up_recommendations(self, knowledge_result: Dict, interaction_context: Dict) -> Dict:
        """
        í›„ì† ë¶„ì„ ì¶”ì²œ ìƒì„±
        """
        prompt = f"""
        í˜„ì¬ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ë„ì›€ì´ ë  í›„ì† ë¶„ì„ì„ ì¶”ì²œí•˜ì„¸ìš”.
        
        í˜„ì¬ ë¶„ì„ ê²°ê³¼: {knowledge_result.get('refined_result', {})}
        ìƒí˜¸ì‘ìš© ì´ë ¥: {interaction_context}
        
        ë‹¤ì–‘í•œ ê´€ì ì—ì„œ í›„ì† ë¶„ì„ì„ ì œì•ˆí•˜ì„¸ìš”.
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "immediate_next_steps": [
                {{
                    "action": "ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¶„ì„ 1",
                    "value": "ì˜ˆìƒ ê°€ì¹˜",
                    "effort": "low|medium|high",
                    "prerequisites": ["ì „ì œì¡°ê±´ 1", "ì „ì œì¡°ê±´ 2"]
                }}
            ],
            "deeper_analysis": [
                {{
                    "analysis_type": "ì‹¬í™” ë¶„ì„ ìœ í˜• 1",
                    "description": "ë¶„ì„ ì„¤ëª…",
                    "expected_insights": ["ì˜ˆìƒ ì¸ì‚¬ì´íŠ¸ 1", "ì˜ˆìƒ ì¸ì‚¬ì´íŠ¸ 2"],
                    "complexity": "medium|high|expert"
                }}
            ],
            "alternative_perspectives": [
                {{
                    "perspective": "ëŒ€ì•ˆì  ê´€ì  1",
                    "rationale": "ì´ ê´€ì ì˜ ê°€ì¹˜",
                    "approach": "ì ‘ê·¼ ë°©ë²•"
                }}
            ],
            "long_term_exploration": [
                {{
                    "topic": "ì¥ê¸° íƒìƒ‰ ì£¼ì œ 1",
                    "timeline": "ì˜ˆìƒ ê¸°ê°„",
                    "potential_impact": "ì ì¬ì  ì˜í–¥"
                }}
            ]
        }}
        """
        
        response = await self.llm_client.agenerate(prompt)
        return self._parse_json_response(response)
    
    def _assess_knowledge_complexity(self, knowledge_result: Dict) -> str:
        """ì§€ì‹ ë³µì¡ë„ í‰ê°€"""
        # ê°„ë‹¨í•œ ë³µì¡ë„ í‰ê°€ ë¡œì§
        refined = knowledge_result.get('refined_result', {})
        insights = refined.get('refined_insights', {})
        
        complexity_score = 0
        complexity_score += len(insights.get('enhanced_patterns', []))
        complexity_score += len(insights.get('deeper_analysis', []))
        complexity_score += len(refined.get('practical_recommendations', {}).get('strategic_considerations', []))
        
        if complexity_score < 5:
            return "low"
        elif complexity_score < 10:
            return "medium"
        else:
            return "high"
    
    def _parse_json_response(self, response: str) -> Dict:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        import json
        try:
            if "```json" in response:
                json_start = response.find("```json") + 7
                json_end = response.find("```", json_start)
                json_str = response[json_start:json_end].strip()
            else:
                json_str = response.strip()
            
            return json.loads(json_str)
        except Exception as e:
            logger.warning(f"Failed to parse JSON response: {e}")
            return {
                'raw_response': response,
                'parse_error': str(e)
            }