"""
LLM First ê²€ì¦ ì‹œìŠ¤í…œ (LLM First Validator)
Phase 3.4: ì‹¤ì‹œê°„ LLM First ì›ì¹™ ì¤€ìˆ˜ ê²€ì¦ ë° ë³´ì¥

í•µì‹¬ ê¸°ëŠ¥:
- ì‹¤ì‹œê°„ LLM First ì¤€ìˆ˜ë„ ê²€ì¦
- ìë™ ìœ„ë°˜ íƒì§€ ë° ì•Œë¦¼
- í’ˆì§ˆ ë³´ì¦ ë©”ì»¤ë‹ˆì¦˜
- ì¤€ìˆ˜ë„ ì ìˆ˜ ì‹¤ì‹œê°„ ê³„ì‚°
- ìˆ˜ì • ì œì•ˆ ë° ê°€ì´ë“œ
- ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ë° ë³´ê³ 
"""

import asyncio
import json
import logging
import time
import statistics
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Callable, Set, Tuple
from datetime import datetime, timedelta
from collections import defaultdict, deque
from enum import Enum
from pathlib import Path
import sqlite3

# ë¶„ì„ ê´€ë ¨ imports
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)

class ViolationSeverity(Enum):
    """ìœ„ë°˜ ì‹¬ê°ë„"""
    CRITICAL = "critical"     # ì¦‰ì‹œ ìˆ˜ì • í•„ìš”
    HIGH = "high"            # ë†’ì€ ìš°ì„ ìˆœìœ„
    MEDIUM = "medium"        # ë³´í†µ ìš°ì„ ìˆœìœ„
    LOW = "low"              # ë‚®ì€ ìš°ì„ ìˆœìœ„
    INFORMATIONAL = "informational"  # ì •ë³´ì„±

class ComplianceArea(Enum):
    """ì¤€ìˆ˜ ì˜ì—­"""
    DATA_ANALYSIS = "data_analysis"
    STRATEGY_SELECTION = "strategy_selection"
    INSIGHT_GENERATION = "insight_generation"
    RECOMMENDATION = "recommendation"
    VISUALIZATION = "visualization"
    ERROR_HANDLING = "error_handling"

class ValidationStatus(Enum):
    """ê²€ì¦ ìƒíƒœ"""
    COMPLIANT = "compliant"
    NON_COMPLIANT = "non_compliant"
    PARTIALLY_COMPLIANT = "partially_compliant"
    UNDER_REVIEW = "under_review"

@dataclass
class ComplianceViolation:
    """ì¤€ìˆ˜ ìœ„ë°˜ ì •ë³´"""
    id: str
    timestamp: datetime
    area: ComplianceArea
    severity: ViolationSeverity
    description: str
    llm_usage_ratio: float
    hardcoded_elements: List[str]
    suggested_fix: str
    impact_score: float
    confidence: float
    context: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ComplianceReport:
    """ì¤€ìˆ˜ ë³´ê³ ì„œ"""
    timestamp: datetime
    overall_score: float
    area_scores: Dict[ComplianceArea, float]
    violations: List[ComplianceViolation]
    improvements: List[str]
    trends: Dict[str, float]
    recommendations: List[str]
    next_review_date: datetime

class LLMFirstRuleEngine:
    """LLM First ê·œì¹™ ì—”ì§„"""
    
    def __init__(self):
        # ì¤€ìˆ˜ ê·œì¹™ ì •ì˜
        self.compliance_rules = {
            ComplianceArea.DATA_ANALYSIS: {
                "min_llm_ratio": 0.8,  # 80% ì´ìƒ LLM ì‚¬ìš©
                "max_hardcoded_ratio": 0.2,  # 20% ì´í•˜ í•˜ë“œì½”ë”©
                "required_llm_steps": ["data_profiling", "pattern_discovery", "insight_generation"],
                "forbidden_patterns": ["fixed_thresholds", "static_rules", "hardcoded_conditions"]
            },
            ComplianceArea.STRATEGY_SELECTION: {
                "min_llm_ratio": 0.9,  # 90% ì´ìƒ LLM ê¸°ë°˜ ì„ íƒ
                "dynamic_selection": True,
                "context_awareness": True,
                "forbidden_patterns": ["fixed_strategy_mapping", "rule_based_selection"]
            },
            ComplianceArea.INSIGHT_GENERATION: {
                "min_llm_ratio": 0.95,  # 95% ì´ìƒ LLM ìƒì„±
                "template_usage_limit": 0.1,  # 10% ì´í•˜ í…œí”Œë¦¿ ì‚¬ìš©
                "required_qualities": ["novelty", "contextual_relevance", "actionability"]
            },
            ComplianceArea.RECOMMENDATION: {
                "min_llm_ratio": 0.85,  # 85% ì´ìƒ LLM ê¸°ë°˜
                "personalization": True,
                "context_specific": True
            },
            ComplianceArea.VISUALIZATION: {
                "dynamic_chart_selection": True,
                "llm_guided_design": True,
                "context_appropriate": True
            },
            ComplianceArea.ERROR_HANDLING: {
                "intelligent_recovery": True,
                "llm_guided_fallback": True,
                "user_friendly_messages": True
            }
        }
        
        # ìœ„ë°˜ íŒ¨í„´ ì •ì˜
        self.violation_patterns = {
            "hardcoded_values": [
                r"if\s+.*==\s*['\"].*['\"]",  # í•˜ë“œì½”ë”©ëœ ë¬¸ìì—´ ë¹„êµ
                r"threshold\s*=\s*\d+\.?\d*",  # í•˜ë“œì½”ë”©ëœ ì„ê³„ê°’
                r"mapping\s*=\s*\{.*['\"].*['\"].*\}",  # í•˜ë“œì½”ë”©ëœ ë§¤í•‘
            ],
            "static_rules": [
                r"if\s+.*\s+in\s+\[.*['\"].*['\"].*\]",  # ì •ì  ë¦¬ìŠ¤íŠ¸ ì²´í¬
                r"switch.*case",  # switch-case íŒ¨í„´
                r"rules\s*=\s*\[.*\]",  # ì •ì  ê·œì¹™ ë¦¬ìŠ¤íŠ¸
            ],
            "template_responses": [
                r"return\s+[f]?['\"].*{.*}.*['\"]",  # í…œí”Œë¦¿ ì‘ë‹µ
                r"message\s*=\s*[f]?['\"].*['\"]\.format",  # ë¬¸ìì—´ í¬ë§·íŒ…
                r"template\s*=\s*['\"].*['\"]",  # í…œí”Œë¦¿ ì •ì˜
            ]
        }
        
        # ì¤€ìˆ˜ë„ ê°€ì¤‘ì¹˜
        self.area_weights = {
            ComplianceArea.DATA_ANALYSIS: 0.25,
            ComplianceArea.STRATEGY_SELECTION: 0.20,
            ComplianceArea.INSIGHT_GENERATION: 0.20,
            ComplianceArea.RECOMMENDATION: 0.15,
            ComplianceArea.VISUALIZATION: 0.10,
            ComplianceArea.ERROR_HANDLING: 0.10
        }
    
    def evaluate_compliance(self, analysis_data: Dict[str, Any]) -> Dict[ComplianceArea, float]:
        """ì¤€ìˆ˜ë„ í‰ê°€"""
        area_scores = {}
        
        for area, rules in self.compliance_rules.items():
            area_score = self._evaluate_area_compliance(area, rules, analysis_data)
            area_scores[area] = area_score
        
        return area_scores
    
    def _evaluate_area_compliance(self, area: ComplianceArea, rules: Dict[str, Any], 
                                 analysis_data: Dict[str, Any]) -> float:
        """ì˜ì—­ë³„ ì¤€ìˆ˜ë„ í‰ê°€"""
        compliance_factors = []
        
        # LLM ì‚¬ìš© ë¹„ìœ¨ ì²´í¬
        if "min_llm_ratio" in rules:
            llm_ratio = analysis_data.get("llm_usage_ratio", 0.0)
            ratio_score = min(1.0, llm_ratio / rules["min_llm_ratio"])
            compliance_factors.append(ratio_score)
        
        # í•˜ë“œì½”ë”© ë¹„ìœ¨ ì²´í¬
        if "max_hardcoded_ratio" in rules:
            hardcoded_ratio = analysis_data.get("hardcoded_ratio", 0.0)
            hardcode_score = max(0.0, 1.0 - (hardcoded_ratio / rules["max_hardcoded_ratio"]))
            compliance_factors.append(hardcode_score)
        
        # í•„ìˆ˜ LLM ë‹¨ê³„ ì²´í¬
        if "required_llm_steps" in rules:
            required_steps = set(rules["required_llm_steps"])
            completed_steps = set(analysis_data.get("llm_steps", []))
            step_coverage = len(completed_steps & required_steps) / len(required_steps)
            compliance_factors.append(step_coverage)
        
        # ê¸ˆì§€ íŒ¨í„´ ì²´í¬
        if "forbidden_patterns" in rules:
            forbidden_count = analysis_data.get("forbidden_pattern_count", 0)
            pattern_score = max(0.0, 1.0 - (forbidden_count * 0.2))  # íŒ¨í„´ë‹¹ 20% ê°ì 
            compliance_factors.append(pattern_score)
        
        # ë™ì  íŠ¹ì„± ì²´í¬
        if "dynamic_selection" in rules and rules["dynamic_selection"]:
            dynamic_score = 1.0 if analysis_data.get("is_dynamic", False) else 0.5
            compliance_factors.append(dynamic_score)
        
        # ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì²´í¬
        if "context_awareness" in rules and rules["context_awareness"]:
            context_score = analysis_data.get("context_awareness_score", 0.5)
            compliance_factors.append(context_score)
        
        return statistics.mean(compliance_factors) if compliance_factors else 0.5
    
    def detect_violations(self, analysis_data: Dict[str, Any]) -> List[ComplianceViolation]:
        """ìœ„ë°˜ íƒì§€"""
        violations = []
        
        for area, rules in self.compliance_rules.items():
            area_violations = self._detect_area_violations(area, rules, analysis_data)
            violations.extend(area_violations)
        
        return violations
    
    def _detect_area_violations(self, area: ComplianceArea, rules: Dict[str, Any],
                               analysis_data: Dict[str, Any]) -> List[ComplianceViolation]:
        """ì˜ì—­ë³„ ìœ„ë°˜ íƒì§€"""
        violations = []
        
        # LLM ë¹„ìœ¨ ìœ„ë°˜
        if "min_llm_ratio" in rules:
            llm_ratio = analysis_data.get("llm_usage_ratio", 0.0)
            if llm_ratio < rules["min_llm_ratio"]:
                violation = ComplianceViolation(
                    id=f"llm_ratio_{area.value}_{int(time.time())}",
                    timestamp=datetime.now(),
                    area=area,
                    severity=ViolationSeverity.HIGH,
                    description=f"LLM ì‚¬ìš© ë¹„ìœ¨ ë¶€ì¡±: {llm_ratio:.2%} < {rules['min_llm_ratio']:.2%}",
                    llm_usage_ratio=llm_ratio,
                    hardcoded_elements=[],
                    suggested_fix=f"LLM ì‚¬ìš© ë¹„ìœ¨ì„ {rules['min_llm_ratio']:.2%} ì´ìƒìœ¼ë¡œ ì¦ê°€",
                    impact_score=0.8,
                    confidence=0.9
                )
                violations.append(violation)
        
        # í•˜ë“œì½”ë”© ë¹„ìœ¨ ìœ„ë°˜
        if "max_hardcoded_ratio" in rules:
            hardcoded_ratio = analysis_data.get("hardcoded_ratio", 0.0)
            if hardcoded_ratio > rules["max_hardcoded_ratio"]:
                hardcoded_elements = analysis_data.get("hardcoded_elements", [])
                violation = ComplianceViolation(
                    id=f"hardcode_{area.value}_{int(time.time())}",
                    timestamp=datetime.now(),
                    area=area,
                    severity=ViolationSeverity.CRITICAL,
                    description=f"í•˜ë“œì½”ë”© ë¹„ìœ¨ ì´ˆê³¼: {hardcoded_ratio:.2%} > {rules['max_hardcoded_ratio']:.2%}",
                    llm_usage_ratio=analysis_data.get("llm_usage_ratio", 0.0),
                    hardcoded_elements=hardcoded_elements,
                    suggested_fix="í•˜ë“œì½”ë”©ëœ ìš”ì†Œë¥¼ LLM ê¸°ë°˜ ë™ì  ë¡œì§ìœ¼ë¡œ ëŒ€ì²´",
                    impact_score=0.9,
                    confidence=0.95
                )
                violations.append(violation)
        
        # í•„ìˆ˜ ë‹¨ê³„ ëˆ„ë½
        if "required_llm_steps" in rules:
            required_steps = set(rules["required_llm_steps"])
            completed_steps = set(analysis_data.get("llm_steps", []))
            missing_steps = required_steps - completed_steps
            
            if missing_steps:
                violation = ComplianceViolation(
                    id=f"missing_steps_{area.value}_{int(time.time())}",
                    timestamp=datetime.now(),
                    area=area,
                    severity=ViolationSeverity.MEDIUM,
                    description=f"í•„ìˆ˜ LLM ë‹¨ê³„ ëˆ„ë½: {', '.join(missing_steps)}",
                    llm_usage_ratio=analysis_data.get("llm_usage_ratio", 0.0),
                    hardcoded_elements=[],
                    suggested_fix=f"ëˆ„ë½ëœ LLM ë‹¨ê³„ ì¶”ê°€: {', '.join(missing_steps)}",
                    impact_score=0.6,
                    confidence=0.8
                )
                violations.append(violation)
        
        return violations

class ComplianceMonitor:
    """ì¤€ìˆ˜ë„ ëª¨ë‹ˆí„°"""
    
    def __init__(self):
        self.rule_engine = LLMFirstRuleEngine()
        self.llm_client = AsyncOpenAI()
        
        # ëª¨ë‹ˆí„°ë§ ìƒíƒœ
        self.monitoring_active = False
        self.monitoring_interval = 60  # 60ì´ˆë§ˆë‹¤ ì²´í¬
        
        # ì•Œë¦¼ ì„¤ì •
        self.alert_thresholds = {
            ViolationSeverity.CRITICAL: 0.0,  # ì¦‰ì‹œ ì•Œë¦¼
            ViolationSeverity.HIGH: 1.0,      # 1ë¶„ í›„ ì•Œë¦¼
            ViolationSeverity.MEDIUM: 5.0,    # 5ë¶„ í›„ ì•Œë¦¼
            ViolationSeverity.LOW: 30.0       # 30ë¶„ í›„ ì•Œë¦¼
        }
        
        # ìœ„ë°˜ ê¸°ë¡
        self.violation_history: deque = deque(maxlen=1000)
        self.compliance_trends: deque = deque(maxlen=100)
        
        # ì½œë°± í•¨ìˆ˜ë“¤
        self.violation_callbacks: List[Callable] = []
        self.improvement_callbacks: List[Callable] = []
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        self.db_path = Path("core/validation/compliance.db")
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._initialize_database()
    
    def _initialize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ìœ„ë°˜ ê¸°ë¡ í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS violations (
                    id TEXT PRIMARY KEY,
                    timestamp DATETIME,
                    area TEXT,
                    severity TEXT,
                    description TEXT,
                    llm_usage_ratio REAL,
                    hardcoded_elements TEXT,
                    suggested_fix TEXT,
                    impact_score REAL,
                    confidence REAL,
                    resolved BOOLEAN DEFAULT FALSE,
                    resolution_timestamp DATETIME
                )
            """)
            
            # ì¤€ìˆ˜ë„ ê¸°ë¡ í…Œì´ë¸”
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS compliance_history (
                    timestamp DATETIME PRIMARY KEY,
                    overall_score REAL,
                    area_scores TEXT,
                    violations_count INTEGER,
                    improvements_count INTEGER
                )
            """)
            
            conn.commit()
    
    async def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.monitoring_active:
            logger.warning("ëª¨ë‹ˆí„°ë§ì´ ì´ë¯¸ í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            return
        
        self.monitoring_active = True
        logger.info("ğŸ” LLM First ì¤€ìˆ˜ë„ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        # ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì‹œì‘
        asyncio.create_task(self._monitoring_loop())
    
    async def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        logger.info("â¹ï¸ LLM First ì¤€ìˆ˜ë„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    async def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.monitoring_active:
            try:
                # í˜„ì¬ ë¶„ì„ ìƒíƒœ ì²´í¬
                await self._check_current_compliance()
                
                # íŠ¸ë Œë“œ ë¶„ì„
                await self._analyze_compliance_trends()
                
                # ëŒ€ê¸°
                await asyncio.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(30)  # ì˜¤ë¥˜ ì‹œ 30ì´ˆ ëŒ€ê¸°
    
    async def validate_analysis(self, analysis_data: Dict[str, Any]) -> ComplianceReport:
        """ë¶„ì„ ì¤€ìˆ˜ë„ ê²€ì¦"""
        logger.info("ğŸ“Š ë¶„ì„ ì¤€ìˆ˜ë„ ê²€ì¦ ì‹œì‘")
        
        # ì¤€ìˆ˜ë„ í‰ê°€
        area_scores = self.rule_engine.evaluate_compliance(analysis_data)
        overall_score = self._calculate_overall_score(area_scores)
        
        # ìœ„ë°˜ íƒì§€
        violations = self.rule_engine.detect_violations(analysis_data)
        
        # LLM ê¸°ë°˜ ê°œì„  ì œì•ˆ
        improvements = await self._generate_improvements(analysis_data, violations)
        
        # íŠ¸ë Œë“œ ë¶„ì„
        trends = self._analyze_trends()
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = await self._generate_recommendations(area_scores, violations)
        
        # ë³´ê³ ì„œ ìƒì„±
        report = ComplianceReport(
            timestamp=datetime.now(),
            overall_score=overall_score,
            area_scores=area_scores,
            violations=violations,
            improvements=improvements,
            trends=trends,
            recommendations=recommendations,
            next_review_date=datetime.now() + timedelta(hours=24)
        )
        
        # ê¸°ë¡ ì €ì¥
        await self._save_compliance_record(report)
        
        # ìœ„ë°˜ ì•Œë¦¼
        await self._handle_violations(violations)
        
        logger.info(f"âœ… ì¤€ìˆ˜ë„ ê²€ì¦ ì™„ë£Œ: {overall_score:.1f}/100")
        return report
    
    def _calculate_overall_score(self, area_scores: Dict[ComplianceArea, float]) -> float:
        """ì „ì²´ ì¤€ìˆ˜ë„ ì ìˆ˜ ê³„ì‚°"""
        weighted_sum = sum(
            score * self.rule_engine.area_weights[area] 
            for area, score in area_scores.items()
        )
        return weighted_sum * 100  # 0-100 ìŠ¤ì¼€ì¼
    
    async def _generate_improvements(self, analysis_data: Dict[str, Any], 
                                   violations: List[ComplianceViolation]) -> List[str]:
        """ê°œì„  ì œì•ˆ ìƒì„± (LLM ê¸°ë°˜)"""
        if not violations:
            return ["í˜„ì¬ LLM First ì›ì¹™ì„ ì˜ ì¤€ìˆ˜í•˜ê³  ìˆìŠµë‹ˆë‹¤."]
        
        # ìœ„ë°˜ ìš”ì•½
        violation_summary = self._summarize_violations(violations)
        
        improvement_prompt = f"""
LLM First ì›ì¹™ ê°œì„  ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ìœ„ë°˜ ì‚¬í•­ì— ëŒ€í•œ êµ¬ì²´ì ì¸ ê°œì„  ì œì•ˆì„ í•´ì£¼ì„¸ìš”.

ìœ„ë°˜ ìš”ì•½:
{violation_summary}

í˜„ì¬ LLM ì‚¬ìš© ë¹„ìœ¨: {analysis_data.get('llm_usage_ratio', 0):.2%}
í•˜ë“œì½”ë”© ë¹„ìœ¨: {analysis_data.get('hardcoded_ratio', 0):.2%}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ê°œì„  ì œì•ˆì„ í•´ì£¼ì„¸ìš”:
1. ì¦‰ì‹œ ê°œì„  ê°€ëŠ¥í•œ ì‚¬í•­ (3ê°œ)
2. ì¤‘ì¥ê¸° ê°œì„  ê³„íš (2ê°œ)
3. LLM í™œìš©ë„ ì¦ëŒ€ ë°©ì•ˆ (2ê°œ)

ì‹¤ìš©ì ì´ê³  êµ¬ì²´ì ì¸ ì œì•ˆì„ í•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": improvement_prompt}],
                max_tokens=600,
                temperature=0.3
            )
            
            improvement_text = response.choices[0].message.content
            improvements = self._parse_improvements(improvement_text)
            
        except Exception as e:
            logger.warning(f"LLM ê°œì„  ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
            improvements = self._fallback_improvements(violations)
        
        return improvements
    
    def _summarize_violations(self, violations: List[ComplianceViolation]) -> str:
        """ìœ„ë°˜ ìš”ì•½"""
        summary_parts = []
        
        # ì‹¬ê°ë„ë³„ ê°œìˆ˜
        severity_counts = defaultdict(int)
        for violation in violations:
            severity_counts[violation.severity] += 1
        
        for severity, count in severity_counts.items():
            summary_parts.append(f"- {severity.value}: {count}ê°œ")
        
        # ì£¼ìš” ìœ„ë°˜ ì‚¬í•­
        critical_violations = [v for v in violations if v.severity == ViolationSeverity.CRITICAL]
        if critical_violations:
            summary_parts.append("\nì£¼ìš” ìœ„ë°˜:")
            for violation in critical_violations[:3]:
                summary_parts.append(f"- {violation.description}")
        
        return "\n".join(summary_parts)
    
    def _parse_improvements(self, improvement_text: str) -> List[str]:
        """ê°œì„  ì œì•ˆ íŒŒì‹±"""
        improvements = []
        
        lines = improvement_text.split('\n')
        current_improvement = ""
        
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                if line[0].isdigit() or line.startswith('-'):
                    if current_improvement:
                        improvements.append(current_improvement.strip())
                    current_improvement = line
                else:
                    current_improvement += " " + line
        
        if current_improvement:
            improvements.append(current_improvement.strip())
        
        return improvements[:10]  # ìµœëŒ€ 10ê°œ
    
    def _fallback_improvements(self, violations: List[ComplianceViolation]) -> List[str]:
        """í´ë°± ê°œì„  ì œì•ˆ"""
        improvements = []
        
        # ìœ„ë°˜ ìœ í˜•ë³„ ê¸°ë³¸ ì œì•ˆ
        if any(v.severity == ViolationSeverity.CRITICAL for v in violations):
            improvements.append("1. í•˜ë“œì½”ë”©ëœ ë¡œì§ì„ LLM ê¸°ë°˜ ë™ì  ë¶„ì„ìœ¼ë¡œ ì¦‰ì‹œ ëŒ€ì²´")
        
        if any("llm_ratio" in v.id for v in violations):
            improvements.append("2. ë¶„ì„ ë‹¨ê³„ë³„ LLM í™œìš©ë„ë¥¼ 90% ì´ìƒìœ¼ë¡œ ì¦ëŒ€")
        
        improvements.extend([
            "3. í…œí”Œë¦¿ ì‘ë‹µì„ LLM ìƒì„± ì‘ë‹µìœ¼ë¡œ ì „í™˜",
            "4. ì •ì  ê·œì¹™ ê¸°ë°˜ ë¡œì§ì„ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ LLM íŒë‹¨ìœ¼ë¡œ ë³€ê²½",
            "5. ì§€ì†ì ì¸ LLM First ì›ì¹™ ì¤€ìˆ˜ ëª¨ë‹ˆí„°ë§ ê°•í™”"
        ])
        
        return improvements
    
    async def _generate_recommendations(self, area_scores: Dict[ComplianceArea, float],
                                      violations: List[ComplianceViolation]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        low_score_areas = [area for area, score in area_scores.items() if score < 0.7]
        
        if low_score_areas:
            recommendations.append(f"ìš°ì„  ê°œì„  ì˜ì—­: {', '.join([area.value for area in low_score_areas])}")
        
        # ìœ„ë°˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        critical_count = len([v for v in violations if v.severity == ViolationSeverity.CRITICAL])
        if critical_count > 0:
            recommendations.append(f"ê¸´ê¸‰ ìˆ˜ì • í•„ìš”: {critical_count}ê°œ ì‹¬ê°í•œ ìœ„ë°˜ ì‚¬í•­")
        
        # ì¼ë°˜ì  ê¶Œì¥ì‚¬í•­
        recommendations.extend([
            "ì •ê¸°ì ì¸ LLM First ì›ì¹™ êµìœ¡ ë° ë¦¬ë·° ìˆ˜í–‰",
            "ì½”ë“œ ë¦¬ë·° ì‹œ LLM First ì²´í¬ë¦¬ìŠ¤íŠ¸ í™œìš©",
            "ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ ì‹œ LLM First ì„¤ê³„ ê°€ì´ë“œ ì ìš©"
        ])
        
        return recommendations
    
    def _analyze_trends(self) -> Dict[str, float]:
        """íŠ¸ë Œë“œ ë¶„ì„"""
        if len(self.compliance_trends) < 2:
            return {"trend": 0.0, "confidence": 0.0}
        
        # ìµœê·¼ ì ìˆ˜ ë³€í™” ë¶„ì„
        recent_scores = [record["overall_score"] for record in self.compliance_trends]
        
        if len(recent_scores) >= 5:
            early_avg = statistics.mean(recent_scores[:len(recent_scores)//2])
            recent_avg = statistics.mean(recent_scores[len(recent_scores)//2:])
            trend = recent_avg - early_avg
        else:
            trend = recent_scores[-1] - recent_scores[0] if len(recent_scores) > 1 else 0.0
        
        confidence = min(1.0, len(recent_scores) / 10)  # 10ê°œ ê¸°ë¡ì—ì„œ ìµœëŒ€ ì‹ ë¢°ë„
        
        return {
            "overall_trend": trend,
            "trend_confidence": confidence,
            "improvement_rate": max(0, trend) / 30 if trend > 0 else 0  # 30ì¼ ê¸°ì¤€ ê°œì„ ë¥ 
        }
    
    async def _save_compliance_record(self, report: ComplianceReport):
        """ì¤€ìˆ˜ë„ ê¸°ë¡ ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ìœ„ë°˜ ê¸°ë¡ ì €ì¥
            for violation in report.violations:
                cursor.execute("""
                    INSERT OR REPLACE INTO violations 
                    (id, timestamp, area, severity, description, llm_usage_ratio,
                     hardcoded_elements, suggested_fix, impact_score, confidence)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    violation.id,
                    violation.timestamp,
                    violation.area.value,
                    violation.severity.value,
                    violation.description,
                    violation.llm_usage_ratio,
                    json.dumps(violation.hardcoded_elements),
                    violation.suggested_fix,
                    violation.impact_score,
                    violation.confidence
                ))
            
            # ì¤€ìˆ˜ë„ ê¸°ë¡ ì €ì¥
            cursor.execute("""
                INSERT OR REPLACE INTO compliance_history
                (timestamp, overall_score, area_scores, violations_count, improvements_count)
                VALUES (?, ?, ?, ?, ?)
            """, (
                report.timestamp,
                report.overall_score,
                json.dumps({area.value: score for area, score in report.area_scores.items()}),
                len(report.violations),
                len(report.improvements)
            ))
            
            conn.commit()
        
        # ë©”ëª¨ë¦¬ ê¸°ë¡ ì—…ë°ì´íŠ¸
        self.compliance_trends.append({
            "timestamp": report.timestamp,
            "overall_score": report.overall_score,
            "violations_count": len(report.violations)
        })
    
    async def _handle_violations(self, violations: List[ComplianceViolation]):
        """ìœ„ë°˜ ì²˜ë¦¬"""
        for violation in violations:
            # ì‹¬ê°ë„ì— ë”°ë¥¸ ì•Œë¦¼
            await self._send_violation_alert(violation)
            
            # ì½œë°± ì‹¤í–‰
            for callback in self.violation_callbacks:
                try:
                    await callback(violation)
                except Exception as e:
                    logger.error(f"ìœ„ë°˜ ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    async def _send_violation_alert(self, violation: ComplianceViolation):
        """ìœ„ë°˜ ì•Œë¦¼ ë°œì†¡"""
        # ì‹¬ê°ë„ì— ë”°ë¥¸ ì¦‰ì‹œì„± ê²°ì •
        delay = self.alert_thresholds.get(violation.severity, 0.0)
        
        if delay > 0:
            await asyncio.sleep(delay * 60)  # ë¶„ ë‹¨ìœ„ë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
        
        logger.warning(f"ğŸš¨ LLM First ìœ„ë°˜ ì•Œë¦¼ ({violation.severity.value}): {violation.description}")
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì´ë©”ì¼, Slack ë“±ìœ¼ë¡œ ì•Œë¦¼ ë°œì†¡
        alert_message = f"""
LLM First ì›ì¹™ ìœ„ë°˜ ë°œìƒ

ìœ„ë°˜ ID: {violation.id}
ì˜ì—­: {violation.area.value}
ì‹¬ê°ë„: {violation.severity.value}
ì„¤ëª…: {violation.description}
LLM ì‚¬ìš© ë¹„ìœ¨: {violation.llm_usage_ratio:.2%}
ì œì•ˆ ìˆ˜ì •: {violation.suggested_fix}

ì¦‰ì‹œ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
"""
        
        # ë¡œê·¸ë¡œ ì•Œë¦¼ (ì‹¤ì œë¡œëŠ” ì™¸ë¶€ ì‹œìŠ¤í…œ ì—°ë™)
        logger.info(f"ğŸ“§ ì•Œë¦¼ ë°œì†¡: {alert_message}")
    
    async def _check_current_compliance(self):
        """í˜„ì¬ ì¤€ìˆ˜ë„ ì²´í¬"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë¶„ì„ì˜ ìƒíƒœë¥¼ ì²´í¬
        # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
        pass
    
    async def _analyze_compliance_trends(self):
        """ì¤€ìˆ˜ë„ íŠ¸ë Œë“œ ë¶„ì„"""
        if len(self.compliance_trends) >= 5:
            trends = self._analyze_trends()
            
            # ì§€ì†ì  í•˜ë½ íŠ¸ë Œë“œ ê°ì§€
            if trends["overall_trend"] < -5:  # 5ì  ì´ìƒ í•˜ë½
                logger.warning("ğŸ“‰ LLM First ì¤€ìˆ˜ë„ í•˜ë½ íŠ¸ë Œë“œ ê°ì§€")
                
                # ê°œì„  ì½œë°± ì‹¤í–‰
                for callback in self.improvement_callbacks:
                    try:
                        await callback(trends)
                    except Exception as e:
                        logger.error(f"ê°œì„  ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}")
    
    def add_violation_callback(self, callback: Callable):
        """ìœ„ë°˜ ì½œë°± ì¶”ê°€"""
        self.violation_callbacks.append(callback)
    
    def add_improvement_callback(self, callback: Callable):
        """ê°œì„  ì½œë°± ì¶”ê°€"""
        self.improvement_callbacks.append(callback)
    
    def get_compliance_dashboard(self) -> Dict[str, Any]:
        """ì¤€ìˆ˜ë„ ëŒ€ì‹œë³´ë“œ ë°ì´í„°"""
        current_score = self.compliance_trends[-1]["overall_score"] if self.compliance_trends else 0
        recent_violations = list(self.violation_history)[-10:]
        trends = self._analyze_trends()
        
        return {
            "current_score": current_score,
            "trend": trends["overall_trend"],
            "recent_violations": len(recent_violations),
            "monitoring_active": self.monitoring_active,
            "last_check": datetime.now().isoformat(),
            "historical_data": {
                "scores": [record["overall_score"] for record in self.compliance_trends],
                "timestamps": [record["timestamp"].isoformat() for record in self.compliance_trends]
            }
        }

class LLMFirstQualityAssurance:
    """LLM First í’ˆì§ˆ ë³´ì¦"""
    
    def __init__(self):
        self.compliance_monitor = ComplianceMonitor()
        self.quality_gates = {
            "development": 70.0,    # ê°œë°œ ë‹¨ê³„ ìµœì†Œ ì ìˆ˜
            "testing": 80.0,        # í…ŒìŠ¤íŠ¸ ë‹¨ê³„ ìµœì†Œ ì ìˆ˜
            "production": 90.0      # í”„ë¡œë•ì…˜ ë°°í¬ ìµœì†Œ ì ìˆ˜
        }
        
        # í’ˆì§ˆ ì²´í¬ íˆìŠ¤í† ë¦¬
        self.quality_history: deque = deque(maxlen=100)
        
    async def quality_gate_check(self, stage: str, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬"""
        logger.info(f"ğŸ›¡ï¸ {stage} ë‹¨ê³„ í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬")
        
        # ì¤€ìˆ˜ë„ ê²€ì¦
        compliance_report = await self.compliance_monitor.validate_analysis(analysis_data)
        
        # í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼ ì—¬ë¶€
        minimum_score = self.quality_gates.get(stage, 70.0)
        passed = compliance_report.overall_score >= minimum_score
        
        quality_result = {
            "stage": stage,
            "passed": passed,
            "score": compliance_report.overall_score,
            "minimum_score": minimum_score,
            "violations": len(compliance_report.violations),
            "critical_violations": len([v for v in compliance_report.violations 
                                      if v.severity == ViolationSeverity.CRITICAL]),
            "recommendations": compliance_report.recommendations,
            "next_steps": self._determine_next_steps(passed, compliance_report)
        }
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.quality_history.append({
            "timestamp": datetime.now(),
            "stage": stage,
            "result": quality_result
        })
        
        if passed:
            logger.info(f"âœ… {stage} í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼: {compliance_report.overall_score:.1f}/{minimum_score}")
        else:
            logger.warning(f"âŒ {stage} í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤íŒ¨: {compliance_report.overall_score:.1f}/{minimum_score}")
        
        return quality_result
    
    def _determine_next_steps(self, passed: bool, report: ComplianceReport) -> List[str]:
        """ë‹¤ìŒ ë‹¨ê³„ ê²°ì •"""
        if passed:
            return [
                "í’ˆì§ˆ ê²Œì´íŠ¸ í†µê³¼ - ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰ ê°€ëŠ¥",
                "ì§€ì†ì ì¸ LLM First ì›ì¹™ ì¤€ìˆ˜ ìœ ì§€",
                "ì •ê¸°ì ì¸ ì¤€ìˆ˜ë„ ëª¨ë‹ˆí„°ë§ ìˆ˜í–‰"
            ]
        else:
            next_steps = ["í’ˆì§ˆ ê²Œì´íŠ¸ ì‹¤íŒ¨ - ìˆ˜ì • í›„ ì¬ê²€í†  í•„ìš”"]
            
            # ì‹¬ê°í•œ ìœ„ë°˜ì´ ìˆëŠ” ê²½ìš°
            critical_violations = [v for v in report.violations if v.severity == ViolationSeverity.CRITICAL]
            if critical_violations:
                next_steps.append(f"ìš°ì„  ìˆ˜ì •: {len(critical_violations)}ê°œ ì‹¬ê°í•œ ìœ„ë°˜ ì‚¬í•­")
            
            # ê°œì„  ì œì•ˆ ì¶”ê°€
            next_steps.extend(report.improvements[:3])
            
            return next_steps
    
    async def continuous_quality_monitoring(self):
        """ì§€ì†ì  í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""
        await self.compliance_monitor.start_monitoring()
        
        # í’ˆì§ˆ ê°œì„  ì½œë°± ë“±ë¡
        self.compliance_monitor.add_improvement_callback(self._handle_quality_degradation)
        
    async def _handle_quality_degradation(self, trends: Dict[str, float]):
        """í’ˆì§ˆ ì €í•˜ ì²˜ë¦¬"""
        logger.warning("ğŸ“Š LLM First í’ˆì§ˆ ì €í•˜ ê°ì§€ - ê°œì„  ì¡°ì¹˜ ì‹œì‘")
        
        # ìë™ ê°œì„  ì¡°ì¹˜
        improvement_actions = [
            "LLM First ì›ì¹™ ì¬êµìœ¡ ìŠ¤ì¼€ì¤„ë§",
            "ì½”ë“œ ë¦¬ë·° ê°•í™”",
            "ìë™í™”ëœ ì¤€ìˆ˜ë„ ì²´í¬ í™œì„±í™”",
            "ê°œë°œíŒ€ ì•Œë¦¼ ë°œì†¡"
        ]
        
        for action in improvement_actions:
            logger.info(f"ğŸ”§ ê°œì„  ì¡°ì¹˜: {action}")
    
    def get_quality_metrics(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        if not self.quality_history:
            return {"status": "no_data"}
        
        recent_checks = list(self.quality_history)[-10:]
        
        # í†µê³¼ìœ¨ ê³„ì‚°
        passed_count = sum(1 for check in recent_checks if check["result"]["passed"])
        pass_rate = passed_count / len(recent_checks)
        
        # í‰ê·  ì ìˆ˜
        avg_score = statistics.mean(check["result"]["score"] for check in recent_checks)
        
        # íŠ¸ë Œë“œ
        scores = [check["result"]["score"] for check in recent_checks]
        trend = scores[-1] - scores[0] if len(scores) > 1 else 0
        
        return {
            "pass_rate": pass_rate,
            "average_score": avg_score,
            "trend": trend,
            "recent_checks": len(recent_checks),
            "quality_gates": self.quality_gates,
            "monitoring_active": self.compliance_monitor.monitoring_active
        }


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
async def test_llm_first_validator():
    """LLM First ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    # í’ˆì§ˆ ë³´ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    qa_system = LLMFirstQualityAssurance()
    
    # í…ŒìŠ¤íŠ¸ ë¶„ì„ ë°ì´í„°
    test_analysis_data = {
        "llm_usage_ratio": 0.75,  # 75% LLM ì‚¬ìš©
        "hardcoded_ratio": 0.30,  # 30% í•˜ë“œì½”ë”© (ë†’ìŒ)
        "llm_steps": ["data_profiling", "insight_generation"],  # pattern_discovery ëˆ„ë½
        "forbidden_pattern_count": 2,
        "is_dynamic": True,
        "context_awareness_score": 0.8,
        "hardcoded_elements": ["threshold = 0.5", "if status == 'active'"]
    }
    
    print("ğŸ” LLM First ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # ê°œë°œ ë‹¨ê³„ í’ˆì§ˆ ê²Œì´íŠ¸ ì²´í¬
    dev_result = await qa_system.quality_gate_check("development", test_analysis_data)
    
    print(f"\nğŸ“Š ê°œë°œ ë‹¨ê³„ í’ˆì§ˆ ê²Œì´íŠ¸ ê²°ê³¼:")
    print(f"   í†µê³¼ ì—¬ë¶€: {'âœ… í†µê³¼' if dev_result['passed'] else 'âŒ ì‹¤íŒ¨'}")
    print(f"   ì ìˆ˜: {dev_result['score']:.1f}/{dev_result['minimum_score']}")
    print(f"   ìœ„ë°˜ ìˆ˜: {dev_result['violations']}ê°œ (ì‹¬ê°: {dev_result['critical_violations']}ê°œ)")
    
    print(f"\nğŸ’¡ ì£¼ìš” ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(dev_result['recommendations'][:3], 1):
        print(f"   {i}. {rec}")
    
    # ì§€ì†ì  ëª¨ë‹ˆí„°ë§ ì‹œì‘
    await qa_system.continuous_quality_monitoring()
    
    print(f"\nğŸ“ˆ í’ˆì§ˆ ë©”íŠ¸ë¦­:")
    metrics = qa_system.get_quality_metrics()
    if metrics.get("status") != "no_data":
        print(f"   í†µê³¼ìœ¨: {metrics['pass_rate']:.2%}")
        print(f"   í‰ê·  ì ìˆ˜: {metrics['average_score']:.1f}")
        print(f"   ëª¨ë‹ˆí„°ë§ í™œì„±: {'âœ…' if metrics['monitoring_active'] else 'âŒ'}")
    
    # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
    await qa_system.compliance_monitor.stop_monitoring()

if __name__ == "__main__":
    asyncio.run(test_llm_first_validator()) 