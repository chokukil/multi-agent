#!/usr/bin/env python3
"""
ğŸš€ CherryAI Main Application Engine

ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ ì—”ì§„
main.pyì—ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ë¶„ë¦¬í•˜ì—¬ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•˜ê³  LLM First ì›ì¹™ì„ ì¤€ìˆ˜

Key Features:
- A2A + MCP í†µí•© ì²˜ë¦¬
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê´€ë¦¬
- íŒŒì¼ ì²˜ë¦¬ ë° ë°ì´í„° ì¤€ë¹„
- ì¿¼ë¦¬ ì²˜ë¦¬ ë° ì‘ë‹µ ìƒì„±
- ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- ì—ëŸ¬ ì²˜ë¦¬ ë° ë³µêµ¬

Architecture:
- Business Layer: í•µì‹¬ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§
- Integration Layer: A2A + MCP í†µí•©
- Processing Layer: ë°ì´í„° ë° ì¿¼ë¦¬ ì²˜ë¦¬
- Orchestration Layer: ì—ì´ì „íŠ¸ í˜‘ì—… ê´€ë¦¬
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Callable, AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum

# í”„ë¡œì íŠ¸ ì„í¬íŠ¸
from core.app_components.main_app_controller import (
    initialize_app_controller, 
    get_app_controller,
    SystemStatus
)
from core.app_components.realtime_streaming_handler import (
    get_streaming_handler,
    process_query_with_streaming
)
from core.app_components.file_upload_processor import (
    get_file_upload_processor,
    process_and_prepare_files_for_a2a
)
from core.shared_knowledge_bank import (
    get_shared_knowledge_bank,
    add_user_file_knowledge,
    search_relevant_knowledge
)
from core.llm_first_engine import (
    get_llm_first_engine,
    analyze_intent,
    make_decision,
    assess_quality,
    DecisionType,
    UserIntent,
    DynamicDecision,
    QualityAssessment
)

logger = logging.getLogger(__name__)

class ProcessingStage(Enum):
    """ì²˜ë¦¬ ë‹¨ê³„"""
    INITIALIZATION = "initialization"
    INTENT_ANALYSIS = "intent_analysis"
    FILE_PROCESSING = "file_processing"
    AGENT_SELECTION = "agent_selection"
    EXECUTION = "execution"
    QUALITY_CHECK = "quality_check"
    COMPLETION = "completion"

@dataclass
class ProcessingContext:
    """ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸"""
    request_id: str
    user_input: str
    uploaded_files: List[Any] = field(default_factory=list)
    processed_files: List[Any] = field(default_factory=list)
    user_intent: Optional[UserIntent] = None
    agent_decision: Optional[DynamicDecision] = None
    execution_result: Optional[str] = None
    quality_assessment: Optional[QualityAssessment] = None
    stage: ProcessingStage = ProcessingStage.INITIALIZATION
    start_time: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ProcessingResult:
    """ì²˜ë¦¬ ê²°ê³¼"""
    request_id: str
    success: bool
    response: str
    processing_time: float
    stages_completed: List[ProcessingStage]
    quality_score: float = 0.0
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

class CherryAIMainEngine:
    """
    ğŸš€ CherryAI ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì—”ì§„
    
    LLM First ì›ì¹™ì„ ì¤€ìˆ˜í•˜ë©° ëª¨ë“  ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ ì²˜ë¦¬
    """
    
    def __init__(self, 
                 config_manager = None,
                 session_manager = None):
        """
        ë©”ì¸ ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            config_manager: ì„¤ì • ê´€ë¦¬ì
            session_manager: ì„¸ì…˜ ê´€ë¦¬ì
        """
        self.config_manager = config_manager
        self.session_manager = session_manager
        
        # ì—”ì§„ ìƒíƒœ
        self.engine_state = {
            "initialized": False,
            "last_health_check": None,
            "total_requests": 0,
            "successful_requests": 0,
            "error_count": 0,
            "average_processing_time": 0.0
        }
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.app_controller = None
        self.streaming_handler = None
        self.file_processor = None
        self.knowledge_bank = None
        self.llm_engine = None
        
        logger.info("ğŸš€ CherryAI Main Engine ì´ˆê¸°í™” ì‹œì‘")

    async def initialize(self) -> bool:
        """ì—”ì§„ ì´ˆê¸°í™”"""
        try:
            # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
            self.app_controller = initialize_app_controller()
            self.streaming_handler = get_streaming_handler()
            self.file_processor = get_file_upload_processor()
            self.knowledge_bank = get_shared_knowledge_bank()
            self.llm_engine = get_llm_first_engine()
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            self.engine_state["initialized"] = True
            self.engine_state["last_health_check"] = datetime.now()
            
            logger.info("âœ… CherryAI Main Engine ì´ˆê¸°í™” ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False

    async def process_user_request(self, 
                                 user_input: str, 
                                 uploaded_files: List[Any] = None) -> AsyncGenerator[str, None]:
        """
        ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬ (LLM First)
        
        Args:
            user_input: ì‚¬ìš©ì ì…ë ¥
            uploaded_files: ì—…ë¡œë“œëœ íŒŒì¼ë“¤
            
        Yields:
            ì²˜ë¦¬ ê³¼ì •ì˜ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
        """
        # ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = ProcessingContext(
            request_id=str(uuid.uuid4()),
            user_input=user_input,
            uploaded_files=uploaded_files or []
        )
        
        self.engine_state["total_requests"] += 1
        
        try:
            # Stage 1: ì‚¬ìš©ì ì˜ë„ ë¶„ì„ (LLM First)
            yield "ğŸ§  ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
            context = await self._analyze_user_intent(context)
            yield f"âœ… ì˜ë„ ë¶„ì„ ì™„ë£Œ: {context.user_intent.primary_intent}"
            
            # Stage 2: íŒŒì¼ ì²˜ë¦¬ (ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°)
            if context.uploaded_files:
                yield f"ğŸ“ {len(context.uploaded_files)}ê°œ íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
                context = await self._process_files(context)
                yield f"âœ… íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {len(context.processed_files)}ê°œ íŒŒì¼ ì¤€ë¹„ë¨"
            
            # Stage 3: ì—ì´ì „íŠ¸ ì„ íƒ (LLM First)
            yield "ğŸ¤– ìµœì ì˜ ì—ì´ì „íŠ¸ë¥¼ ì„ íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤..."
            context = await self._select_agents(context)
            yield f"âœ… ì—ì´ì „íŠ¸ ì„ íƒ ì™„ë£Œ: {context.agent_decision.decision}"
            
            # Stage 4: ì‹¤í–‰
            yield "ğŸš€ A2A + MCP í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì´ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
            async for execution_chunk in self._execute_with_agents(context):
                context.execution_result = execution_chunk
                yield execution_chunk
            
            # Stage 5: í’ˆì§ˆ ê²€ì¦ (LLM First)
            yield "ğŸ“Š ê²°ê³¼ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
            context = await self._assess_quality(context)
            
            # ìµœì¢… ê²°ê³¼ ë°˜í™˜
            final_result = await self._finalize_result(context)
            
            if final_result.success:
                self.engine_state["successful_requests"] += 1
            else:
                self.engine_state["error_count"] += 1
            
            # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            processing_time = (datetime.now() - context.start_time).total_seconds()
            self._update_performance_metrics(processing_time)
            
        except Exception as e:
            error_msg = f"âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(f"ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            self.engine_state["error_count"] += 1
            yield error_msg

    async def _analyze_user_intent(self, context: ProcessingContext) -> ProcessingContext:
        """ì‚¬ìš©ì ì˜ë„ ë¶„ì„ (LLM First)"""
        context.stage = ProcessingStage.INTENT_ANALYSIS
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        analysis_context = {
            "has_files": len(context.uploaded_files) > 0,
            "file_count": len(context.uploaded_files),
            "platform": "CherryAI",
            "capabilities": ["data_analysis", "visualization", "ml", "web_scraping"]
        }
        
        # LLM First ì˜ë„ ë¶„ì„
        context.user_intent = await analyze_intent(context.user_input, analysis_context)
        
        # ì§€ì‹ ë±…í¬ì— ì¿¼ë¦¬ ì €ì¥
        await self.knowledge_bank.add_knowledge(
            content=context.user_input,
            knowledge_type="user_query",
            source_agent="user",
            title=f"ì‚¬ìš©ì ìš”ì²­ #{context.request_id[:8]}",
            metadata={"intent": context.user_intent.primary_intent}
        )
        
        return context

    async def _process_files(self, context: ProcessingContext) -> ProcessingContext:
        """íŒŒì¼ ì²˜ë¦¬"""
        context.stage = ProcessingStage.FILE_PROCESSING
        
        try:
            # íŒŒì¼ ì²˜ë¦¬
            context.processed_files = process_and_prepare_files_for_a2a(context.uploaded_files)
            
            # ì§€ì‹ ë±…í¬ì— íŒŒì¼ ì •ë³´ ì €ì¥
            for i, file in enumerate(context.uploaded_files):
                file_content = file.getvalue().decode('utf-8') if hasattr(file, 'getvalue') else str(file)
                await add_user_file_knowledge(
                    file_content=file_content[:1000],  # ì²« 1000ìë§Œ ì €ì¥
                    filename=getattr(file, 'name', f'file_{i}'),
                    session_id=context.request_id
                )
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            context.metadata["file_processing_error"] = str(e)
        
        return context

    async def _select_agents(self, context: ProcessingContext) -> ProcessingContext:
        """ì—ì´ì „íŠ¸ ì„ íƒ (LLM First)"""
        context.stage = ProcessingStage.AGENT_SELECTION
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ì •ë³´
        available_agents = [
            "pandas_collaboration_hub",
            "data_cleaning",
            "data_loader", 
            "data_visualization",
            "eda_tools",
            "feature_engineering",
            "h2o_ml",
            "mlflow_tools",
            "sql_database",
            "data_wrangling"
        ]
        
        # ì˜ì‚¬ê²°ì • ì»¨í…ìŠ¤íŠ¸
        decision_context = {
            "user_intent": context.user_intent.primary_intent,
            "complexity": context.user_intent.complexity_level,
            "has_data": len(context.processed_files) > 0,
            "data_requirements": context.user_intent.data_requirements,
            "expected_outputs": context.user_intent.expected_outputs
        }
        
        # LLM First ì—ì´ì „íŠ¸ ì„ íƒ
        context.agent_decision = await make_decision(
            DecisionType.AGENT_SELECTION,
            decision_context,
            available_agents
        )
        
        return context

    async def _execute_with_agents(self, context: ProcessingContext) -> AsyncGenerator[str, None]:
        """ì—ì´ì „íŠ¸ì™€ í•¨ê»˜ ì‹¤í–‰ - ì‹¤ì œ SSE ìŠ¤íŠ¸ë¦¬ë° í†µí•©"""
        context.stage = ProcessingStage.EXECUTION
        
        try:
            # Unified Message Brokerë¡œ ì‹¤ì œ SSE ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
            from core.streaming.unified_message_broker import get_unified_message_broker
            broker = get_unified_message_broker()
            
            # ì„¸ì…˜ ìƒì„±
            session_id = await broker.create_session(context.user_input)
            
            # ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ìœ¼ë¡œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
            async for broker_event in broker.orchestrate_multi_agent_query(
                session_id=session_id,
                user_query=context.user_input,
                required_capabilities=context.agent_decision.required_capabilities if context.agent_decision else None
            ):
                # ë¸Œë¡œì»¤ ì´ë²¤íŠ¸ë¥¼ UIìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                if broker_event.get('event') == 'orchestration_start':
                    agents = broker_event.get('data', {}).get('selected_agents', [])
                    yield f"ğŸ¤– ì„ íƒëœ ì—ì´ì „íŠ¸: {', '.join(agents)}"
                    
                elif broker_event.get('event') == 'agent_response':
                    data = broker_event.get('data', {})
                    agent_name = data.get('agent', 'unknown')
                    content = data.get('content', '')
                    
                    if content.strip():
                        yield f"ğŸ“Š {agent_name}: {content}"
                        
                elif broker_event.get('event') == 'stream_chunk':
                    chunk_content = broker_event.get('data', {}).get('content', '')
                    if chunk_content.strip():
                        yield chunk_content
                        
                elif broker_event.get('event') == 'error':
                    error_msg = broker_event.get('data', {}).get('error', 'Unknown error')
                    yield f"âŒ ì˜¤ë¥˜: {error_msg}"
                    
        except Exception as e:
            error_msg = f"ì‹¤í–‰ ì˜¤ë¥˜: {str(e)}"
            logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            yield error_msg

    async def _assess_quality(self, context: ProcessingContext) -> ProcessingContext:
        """í’ˆì§ˆ í‰ê°€ (LLM First)"""
        context.stage = ProcessingStage.QUALITY_CHECK
        
        if context.execution_result:
            # í’ˆì§ˆ í‰ê°€ ê¸°ì¤€
            quality_criteria = [
                "ì‚¬ìš©ì ìš”ì²­ ì¶©ì¡±ë„",
                "ê²°ê³¼ì˜ ì •í™•ì„±",
                "ì„¤ëª…ì˜ ëª…í™•ì„±",
                "ì‹¤ìš©ì  ê°€ì¹˜",
                "ì™„ì „ì„±"
            ]
            
            # í‰ê°€ ì»¨í…ìŠ¤íŠ¸
            assessment_context = {
                "user_intent": context.user_intent.primary_intent,
                "original_request": context.user_input,
                "processing_time": (datetime.now() - context.start_time).total_seconds()
            }
            
            # LLM First í’ˆì§ˆ í‰ê°€
            context.quality_assessment = await assess_quality(
                context.execution_result,
                quality_criteria,
                assessment_context
            )
        
        return context

    async def _finalize_result(self, context: ProcessingContext) -> ProcessingResult:
        """ê²°ê³¼ ë§ˆë¬´ë¦¬"""
        context.stage = ProcessingStage.COMPLETION
        
        processing_time = (datetime.now() - context.start_time).total_seconds()
        
        # ì™„ë£Œëœ ë‹¨ê³„ë“¤
        stages_completed = [
            ProcessingStage.INITIALIZATION,
            ProcessingStage.INTENT_ANALYSIS
        ]
        
        if context.uploaded_files:
            stages_completed.append(ProcessingStage.FILE_PROCESSING)
        
        stages_completed.extend([
            ProcessingStage.AGENT_SELECTION,
            ProcessingStage.EXECUTION,
            ProcessingStage.QUALITY_CHECK,
            ProcessingStage.COMPLETION
        ])
        
        # ì„±ê³µ ì—¬ë¶€ íŒë‹¨
        success = (
            context.execution_result is not None and
            len(context.execution_result.strip()) > 50 and
            "ì˜¤ë¥˜" not in context.execution_result
        )
        
        # í’ˆì§ˆ ì ìˆ˜
        quality_score = (
            context.quality_assessment.overall_score 
            if context.quality_assessment else 0.5
        )
        
        return ProcessingResult(
            request_id=context.request_id,
            success=success,
            response=context.execution_result or "ì²˜ë¦¬ ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            processing_time=processing_time,
            stages_completed=stages_completed,
            quality_score=quality_score,
            metadata={
                "intent": context.user_intent.primary_intent if context.user_intent else "unknown",
                "complexity": context.user_intent.complexity_level if context.user_intent else "unknown",
                "agent_selected": context.agent_decision.decision if context.agent_decision else "none",
                "files_processed": len(context.processed_files)
            }
        )

    def _update_performance_metrics(self, processing_time: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # í‰ê·  ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
        total_requests = self.engine_state["total_requests"]
        current_avg = self.engine_state["average_processing_time"]
        
        new_avg = ((current_avg * (total_requests - 1)) + processing_time) / total_requests
        self.engine_state["average_processing_time"] = new_avg

    async def get_system_status(self) -> SystemStatus:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        if self.app_controller:
            return self.app_controller.get_system_status()
        else:
            # ê¸°ë³¸ ìƒíƒœ ë°˜í™˜
            return SystemStatus(
                overall_health=50.0,
                a2a_agents_count=0,
                mcp_tools_count=0,
                last_check=datetime.now()
            )

    async def get_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        success_rate = (
            self.engine_state["successful_requests"] / self.engine_state["total_requests"]
            if self.engine_state["total_requests"] > 0 else 0.0
        )
        
        return {
            "initialized": self.engine_state["initialized"],
            "total_requests": self.engine_state["total_requests"],
            "successful_requests": self.engine_state["successful_requests"],
            "error_count": self.engine_state["error_count"],
            "success_rate": success_rate,
            "average_processing_time": self.engine_state["average_processing_time"],
            "last_health_check": self.engine_state["last_health_check"].isoformat() if self.engine_state["last_health_check"] else None
        }

    async def health_check(self) -> Dict[str, Any]:
        """í—¬ìŠ¤ ì²´í¬"""
        try:
            # ê° ì»´í¬ë„ŒíŠ¸ ìƒíƒœ í™•ì¸
            health_status = {
                "engine_initialized": self.engine_state["initialized"],
                "app_controller": self.app_controller is not None,
                "streaming_handler": self.streaming_handler is not None,
                "file_processor": self.file_processor is not None,
                "knowledge_bank": self.knowledge_bank is not None,
                "llm_engine": self.llm_engine is not None
            }
            
            # ì „ì²´ ê±´ê°•ë„ ê³„ì‚°
            healthy_components = sum(health_status.values())
            total_components = len(health_status)
            overall_health = (healthy_components / total_components) * 100
            
            self.engine_state["last_health_check"] = datetime.now()
            
            return {
                "overall_health": overall_health,
                "component_status": health_status,
                "last_check": self.engine_state["last_health_check"].isoformat(),
                "error_count": self.engine_state["error_count"],
                "uptime_status": "healthy" if overall_health > 80 else "degraded"
            }
            
        except Exception as e:
            logger.error(f"í—¬ìŠ¤ ì²´í¬ ì˜¤ë¥˜: {e}")
            return {
                "overall_health": 0.0,
                "error": str(e),
                "last_check": datetime.now().isoformat(),
                "uptime_status": "critical"
            }

    async def restart_components(self) -> bool:
        """ì»´í¬ë„ŒíŠ¸ ì¬ì‹œì‘"""
        try:
            logger.info("ğŸ”„ ì—”ì§„ ì»´í¬ë„ŒíŠ¸ ì¬ì‹œì‘ ì¤‘...")
            
            # ì»´í¬ë„ŒíŠ¸ ì¬ì´ˆê¸°í™”
            success = await self.initialize()
            
            if success:
                logger.info("âœ… ì—”ì§„ ì»´í¬ë„ŒíŠ¸ ì¬ì‹œì‘ ì™„ë£Œ")
            else:
                logger.error("âŒ ì—”ì§„ ì»´í¬ë„ŒíŠ¸ ì¬ì‹œì‘ ì‹¤íŒ¨")
            
            return success
            
        except Exception as e:
            logger.error(f"ì»´í¬ë„ŒíŠ¸ ì¬ì‹œì‘ ì˜¤ë¥˜: {e}")
            return False

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_global_main_engine: Optional[CherryAIMainEngine] = None

def get_main_engine() -> CherryAIMainEngine:
    """ì „ì—­ ë©”ì¸ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ì¡°íšŒ"""
    global _global_main_engine
    if _global_main_engine is None:
        _global_main_engine = CherryAIMainEngine()
    return _global_main_engine

def initialize_main_engine(**kwargs) -> CherryAIMainEngine:
    """ë©”ì¸ ì—”ì§„ ì´ˆê¸°í™”"""
    global _global_main_engine
    _global_main_engine = CherryAIMainEngine(**kwargs)
    return _global_main_engine

async def initialize_and_start_engine() -> CherryAIMainEngine:
    """ì—”ì§„ ì´ˆê¸°í™” ë° ì‹œì‘"""
    engine = get_main_engine()
    await engine.initialize()
    return engine 