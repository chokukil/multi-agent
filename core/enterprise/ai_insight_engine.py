"""
AI-Based Insight Engine
Phase 4.3: ê³ ê¸‰ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸

í•µì‹¬ ê¸°ëŠ¥:
- ìë™ íŒ¨í„´ ë°œê²¬ ë° ë¶„ì„
- ì˜ˆì¸¡ ë¶„ì„ ë° ëª¨ë¸ë§
- ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìƒì„±
- ì‹¤ì‹œê°„ ì´ìƒ ê°ì§€
- íŠ¸ë Œë“œ ë¶„ì„ ë° ì˜ˆì¸¡
- ìë™ ë³´ê³ ì„œ ìƒì„±
"""

import asyncio
import json
import logging
import time
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from enum import Enum
import sqlite3
from pathlib import Path
import pickle
import warnings
from collections import defaultdict
import scipy.stats as stats
from sklearn.ensemble import IsolationForest, RandomForestRegressor
from sklearn.cluster import DBSCAN, KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
import networkx as nx
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.holtwinters import ExponentialSmoothing

logger = logging.getLogger(__name__)

class InsightType(Enum):
    """ì¸ì‚¬ì´íŠ¸ ìœ í˜•"""
    PATTERN = "pattern"                    # íŒ¨í„´ ë°œê²¬
    ANOMALY = "anomaly"                   # ì´ìƒ ê°ì§€
    TREND = "trend"                       # íŠ¸ë Œë“œ ë¶„ì„
    CORRELATION = "correlation"            # ìƒê´€ê´€ê³„ ë¶„ì„
    PREDICTION = "prediction"             # ì˜ˆì¸¡ ë¶„ì„
    CLUSTER = "cluster"                   # í´ëŸ¬ìŠ¤í„° ë¶„ì„
    CLASSIFICATION = "classification"      # ë¶„ë¥˜ ë¶„ì„
    BUSINESS_RULE = "business_rule"       # ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™
    OPTIMIZATION = "optimization"         # ìµœì í™” ì œì•ˆ

class SeverityLevel(Enum):
    """ì‹¬ê°ë„ ìˆ˜ì¤€"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"

class AnalysisMethod(Enum):
    """ë¶„ì„ ë°©ë²•"""
    STATISTICAL = "statistical"
    MACHINE_LEARNING = "machine_learning"
    TIME_SERIES = "time_series"
    GRAPH_ANALYSIS = "graph_analysis"
    RULE_BASED = "rule_based"

@dataclass
class Insight:
    """ì¸ì‚¬ì´íŠ¸ ì •ë³´"""
    insight_id: str
    insight_type: InsightType
    title: str
    description: str
    severity: SeverityLevel
    confidence_score: float  # 0.0 - 1.0
    method: AnalysisMethod
    data_points: List[Dict[str, Any]]
    recommendations: List[str]
    visualizations: List[Dict[str, Any]] = field(default_factory=list)
    business_impact: Optional[str] = None
    created_at: datetime = field(default_factory=datetime.now)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PatternAnalysis:
    """íŒ¨í„´ ë¶„ì„ ê²°ê³¼"""
    pattern_id: str
    pattern_type: str
    frequency: int
    strength: float  # íŒ¨í„´ ê°•ë„
    variables: List[str]
    pattern_data: Dict[str, Any]
    statistical_significance: float

@dataclass
class AnomalyDetection:
    """ì´ìƒ ê°ì§€ ê²°ê³¼"""
    anomaly_id: str
    detection_method: str
    anomaly_score: float
    affected_features: List[str]
    context: Dict[str, Any]
    timestamp: datetime

@dataclass
class TrendAnalysis:
    """íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼"""
    trend_id: str
    direction: str  # increasing, decreasing, stable, cyclical
    strength: float
    seasonality: Optional[Dict[str, Any]]
    forecast_data: List[Dict[str, Any]]
    trend_equation: Optional[str]

class PatternDiscovery:
    """íŒ¨í„´ ë°œê²¬ ì—”ì§„"""
    
    def __init__(self):
        self.discovered_patterns = {}
        
    async def discover_patterns(self, df: pd.DataFrame) -> List[PatternAnalysis]:
        """ë°ì´í„°ì—ì„œ íŒ¨í„´ ìë™ ë°œê²¬"""
        patterns = []
        
        # 1. í†µê³„ì  íŒ¨í„´ ë°œê²¬
        statistical_patterns = await self._discover_statistical_patterns(df)
        patterns.extend(statistical_patterns)
        
        # 2. ì‹œê³„ì—´ íŒ¨í„´ ë°œê²¬
        if 'date' in df.columns or 'datetime' in df.columns or 'timestamp' in df.columns:
            time_patterns = await self._discover_time_patterns(df)
            patterns.extend(time_patterns)
        
        # 3. í´ëŸ¬ìŠ¤í„°ë§ íŒ¨í„´ ë°œê²¬
        cluster_patterns = await self._discover_cluster_patterns(df)
        patterns.extend(cluster_patterns)
        
        # 4. ìƒê´€ê´€ê³„ íŒ¨í„´ ë°œê²¬
        correlation_patterns = await self._discover_correlation_patterns(df)
        patterns.extend(correlation_patterns)
        
        logger.info(f"ë°œê²¬ëœ íŒ¨í„´ ê°œìˆ˜: {len(patterns)}")
        return patterns
    
    async def _discover_statistical_patterns(self, df: pd.DataFrame) -> List[PatternAnalysis]:
        """í†µê³„ì  íŒ¨í„´ ë°œê²¬"""
        patterns = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df[col].dropna().empty:
                continue
                
            # ì •ê·œì„± ê²€ì •
            _, p_value = stats.normaltest(df[col].dropna())
            if p_value > 0.05:
                pattern = PatternAnalysis(
                    pattern_id=f"normal_dist_{col}",
                    pattern_type="normal_distribution",
                    frequency=len(df),
                    strength=1 - p_value,
                    variables=[col],
                    pattern_data={"p_value": p_value, "column": col},
                    statistical_significance=p_value
                )
                patterns.append(pattern)
            
            # ì´ìƒì¹˜ íŒ¨í„´
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
            
            if len(outliers) > 0:
                pattern = PatternAnalysis(
                    pattern_id=f"outliers_{col}",
                    pattern_type="outlier_pattern",
                    frequency=len(outliers),
                    strength=len(outliers) / len(df),
                    variables=[col],
                    pattern_data={
                        "outlier_count": len(outliers),
                        "outlier_percentage": len(outliers) / len(df) * 100,
                        "Q1": Q1, "Q3": Q3, "IQR": IQR
                    },
                    statistical_significance=0.05
                )
                patterns.append(pattern)
        
        return patterns
    
    async def _discover_time_patterns(self, df: pd.DataFrame) -> List[PatternAnalysis]:
        """ì‹œê³„ì—´ íŒ¨í„´ ë°œê²¬"""
        patterns = []
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
        date_cols = []
        for col in df.columns:
            if 'date' in col.lower() or 'time' in col.lower():
                try:
                    pd.to_datetime(df[col])
                    date_cols.append(col)
                except:
                    continue
        
        if not date_cols:
            return patterns
        
        date_col = date_cols[0]
        df_time = df.copy()
        df_time[date_col] = pd.to_datetime(df_time[date_col])
        df_time = df_time.sort_values(date_col)
        
        numeric_cols = df_time.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df_time[col].dropna().empty:
                continue
                
            # ê³„ì ˆì„± ë¶„ì„
            try:
                ts_data = df_time.set_index(date_col)[col].dropna()
                if len(ts_data) > 10:  # ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸
                    decomposition = seasonal_decompose(ts_data, period=min(7, len(ts_data)//2))
                    
                    seasonal_strength = np.var(decomposition.seasonal) / np.var(ts_data)
                    
                    if seasonal_strength > 0.1:  # 10% ì´ìƒì˜ ê³„ì ˆì„±
                        pattern = PatternAnalysis(
                            pattern_id=f"seasonality_{col}",
                            pattern_type="seasonal_pattern",
                            frequency=7,  # ì£¼ê°„ íŒ¨í„´ ê°€ì •
                            strength=seasonal_strength,
                            variables=[col, date_col],
                            pattern_data={
                                "seasonal_strength": seasonal_strength,
                                "trend_component": decomposition.trend.mean(),
                                "seasonal_component": decomposition.seasonal.mean()
                            },
                            statistical_significance=0.05
                        )
                        patterns.append(pattern)
            except Exception as e:
                logger.warning(f"ì‹œê³„ì—´ ë¶„ì„ ì‹¤íŒ¨ {col}: {e}")
        
        return patterns
    
    async def _discover_cluster_patterns(self, df: pd.DataFrame) -> List[PatternAnalysis]:
        """í´ëŸ¬ìŠ¤í„°ë§ íŒ¨í„´ ë°œê²¬"""
        patterns = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) < 2:
            return patterns
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        df_numeric = df[numeric_cols].dropna()
        if len(df_numeric) < 10:
            return patterns
        
        scaler = StandardScaler()
        scaled_data = scaler.fit_transform(df_numeric)
        
        # K-means í´ëŸ¬ìŠ¤í„°ë§
        best_k = 2
        best_score = -1
        
        for k in range(2, min(10, len(df_numeric)//5)):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(scaled_data)
            score = silhouette_score(scaled_data, cluster_labels)
            
            if score > best_score:
                best_score = score
                best_k = k
        
        if best_score > 0.3:  # ì ì ˆí•œ í´ëŸ¬ìŠ¤í„°ë§
            kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(scaled_data)
            
            pattern = PatternAnalysis(
                pattern_id=f"clusters_{best_k}",
                pattern_type="cluster_pattern",
                frequency=best_k,
                strength=best_score,
                variables=list(numeric_cols),
                pattern_data={
                    "n_clusters": best_k,
                    "silhouette_score": best_score,
                    "cluster_centers": kmeans.cluster_centers_.tolist()
                },
                statistical_significance=0.05
            )
            patterns.append(pattern)
        
        return patterns
    
    async def _discover_correlation_patterns(self, df: pd.DataFrame) -> List[PatternAnalysis]:
        """ìƒê´€ê´€ê³„ íŒ¨í„´ ë°œê²¬"""
        patterns = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) < 2:
            return patterns
        
        correlation_matrix = df[numeric_cols].corr()
        
        # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸°
        strong_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:  # ê°•í•œ ìƒê´€ê´€ê³„
                    col1 = correlation_matrix.columns[i]
                    col2 = correlation_matrix.columns[j]
                    strong_correlations.append((col1, col2, corr_value))
        
        for col1, col2, corr_value in strong_correlations:
            pattern = PatternAnalysis(
                pattern_id=f"correlation_{col1}_{col2}",
                pattern_type="correlation_pattern",
                frequency=len(df),
                strength=abs(corr_value),
                variables=[col1, col2],
                pattern_data={
                    "correlation_coefficient": corr_value,
                    "relationship_type": "positive" if corr_value > 0 else "negative"
                },
                statistical_significance=0.05
            )
            patterns.append(pattern)
        
        return patterns

class AnomalyDetector:
    """ì´ìƒ ê°ì§€ ì—”ì§„"""
    
    def __init__(self):
        self.detection_models = {}
        
    async def detect_anomalies(self, df: pd.DataFrame) -> List[AnomalyDetection]:
        """ë°ì´í„°ì—ì„œ ì´ìƒ ê°ì§€"""
        anomalies = []
        
        # 1. í†µê³„ì  ì´ìƒ ê°ì§€
        statistical_anomalies = await self._detect_statistical_anomalies(df)
        anomalies.extend(statistical_anomalies)
        
        # 2. ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ ê°ì§€
        ml_anomalies = await self._detect_ml_anomalies(df)
        anomalies.extend(ml_anomalies)
        
        # 3. ì‹œê³„ì—´ ì´ìƒ ê°ì§€
        time_anomalies = await self._detect_time_series_anomalies(df)
        anomalies.extend(time_anomalies)
        
        logger.info(f"ê°ì§€ëœ ì´ìƒ ê°œìˆ˜: {len(anomalies)}")
        return anomalies
    
    async def _detect_statistical_anomalies(self, df: pd.DataFrame) -> List[AnomalyDetection]:
        """í†µê³„ì  ì´ìƒ ê°ì§€"""
        anomalies = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df[col].dropna().empty:
                continue
                
            # Z-score ê¸°ë°˜ ì´ìƒ ê°ì§€
            z_scores = np.abs(stats.zscore(df[col].dropna()))
            outlier_indices = np.where(z_scores > 3)[0]
            
            for idx in outlier_indices:
                anomaly = AnomalyDetection(
                    anomaly_id=f"zscore_{col}_{idx}",
                    detection_method="z_score",
                    anomaly_score=z_scores[idx],
                    affected_features=[col],
                    context={
                        "value": df[col].iloc[idx],
                        "z_score": z_scores[idx],
                        "mean": df[col].mean(),
                        "std": df[col].std()
                    },
                    timestamp=datetime.now()
                )
                anomalies.append(anomaly)
        
        return anomalies
    
    async def _detect_ml_anomalies(self, df: pd.DataFrame) -> List[AnomalyDetection]:
        """ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì´ìƒ ê°ì§€"""
        anomalies = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) < 2:
            return anomalies
        
        df_numeric = df[numeric_cols].dropna()
        if len(df_numeric) < 10:
            return anomalies
        
        # Isolation Forest ì‚¬ìš©
        isolation_forest = IsolationForest(contamination=0.1, random_state=42)
        outlier_predictions = isolation_forest.fit_predict(df_numeric)
        outlier_scores = isolation_forest.score_samples(df_numeric)
        
        outlier_indices = np.where(outlier_predictions == -1)[0]
        
        for idx in outlier_indices:
            anomaly = AnomalyDetection(
                anomaly_id=f"isolation_forest_{idx}",
                detection_method="isolation_forest",
                anomaly_score=abs(outlier_scores[idx]),
                affected_features=list(numeric_cols),
                context={
                    "row_index": idx,
                    "anomaly_score": outlier_scores[idx],
                    "data_point": df_numeric.iloc[idx].to_dict()
                },
                timestamp=datetime.now()
            )
            anomalies.append(anomaly)
        
        return anomalies
    
    async def _detect_time_series_anomalies(self, df: pd.DataFrame) -> List[AnomalyDetection]:
        """ì‹œê³„ì—´ ì´ìƒ ê°ì§€"""
        anomalies = []
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
        date_cols = []
        for col in df.columns:
            if 'date' in col.lower() or 'time' in col.lower():
                try:
                    pd.to_datetime(df[col])
                    date_cols.append(col)
                except:
                    continue
        
        if not date_cols:
            return anomalies
        
        date_col = date_cols[0]
        df_time = df.copy()
        df_time[date_col] = pd.to_datetime(df_time[date_col])
        df_time = df_time.sort_values(date_col)
        
        numeric_cols = df_time.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df_time[col].dropna().empty:
                continue
                
            ts_data = df_time.set_index(date_col)[col].dropna()
            if len(ts_data) < 10:
                continue
            
            # ì´ë™í‰ê·  ê¸°ë°˜ ì´ìƒ ê°ì§€
            window_size = min(7, len(ts_data)//3)
            rolling_mean = ts_data.rolling(window=window_size).mean()
            rolling_std = ts_data.rolling(window=window_size).std()
            
            # 3 ì‹œê·¸ë§ˆ ê·œì¹™
            upper_bound = rolling_mean + 3 * rolling_std
            lower_bound = rolling_mean - 3 * rolling_std
            
            anomaly_indices = ts_data[(ts_data > upper_bound) | (ts_data < lower_bound)].index
            
            for timestamp in anomaly_indices:
                anomaly = AnomalyDetection(
                    anomaly_id=f"time_series_{col}_{timestamp}",
                    detection_method="time_series_threshold",
                    anomaly_score=abs(ts_data[timestamp] - rolling_mean[timestamp]) / rolling_std[timestamp],
                    affected_features=[col],
                    context={
                        "timestamp": timestamp,
                        "value": ts_data[timestamp],
                        "expected_range": [lower_bound[timestamp], upper_bound[timestamp]],
                        "rolling_mean": rolling_mean[timestamp]
                    },
                    timestamp=datetime.now()
                )
                anomalies.append(anomaly)
        
        return anomalies

class TrendAnalyzer:
    """íŠ¸ë Œë“œ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self):
        self.trend_models = {}
        
    async def analyze_trends(self, df: pd.DataFrame) -> List[TrendAnalysis]:
        """íŠ¸ë Œë“œ ë¶„ì„ ìˆ˜í–‰"""
        trends = []
        
        # ë‚ ì§œ ì»¬ëŸ¼ ì°¾ê¸°
        date_cols = []
        for col in df.columns:
            if 'date' in col.lower() or 'time' in col.lower():
                try:
                    pd.to_datetime(df[col])
                    date_cols.append(col)
                except:
                    continue
        
        if not date_cols:
            logger.warning("ë‚ ì§œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ íŠ¸ë Œë“œ ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return trends
        
        date_col = date_cols[0]
        df_time = df.copy()
        df_time[date_col] = pd.to_datetime(df_time[date_col])
        df_time = df_time.sort_values(date_col)
        
        numeric_cols = df_time.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            if df_time[col].dropna().empty:
                continue
                
            trend = await self._analyze_single_trend(df_time, date_col, col)
            if trend:
                trends.append(trend)
        
        return trends
    
    async def _analyze_single_trend(self, df: pd.DataFrame, date_col: str, value_col: str) -> Optional[TrendAnalysis]:
        """ë‹¨ì¼ ë³€ìˆ˜ì˜ íŠ¸ë Œë“œ ë¶„ì„"""
        ts_data = df.set_index(date_col)[value_col].dropna()
        
        if len(ts_data) < 5:
            return None
        
        # ì„ í˜• íŠ¸ë Œë“œ ë¶„ì„
        x = np.arange(len(ts_data))
        y = ts_data.values
        
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        
        # íŠ¸ë Œë“œ ë°©í–¥ ê²°ì •
        if abs(slope) < std_err:
            direction = "stable"
            strength = 0.0
        elif slope > 0:
            direction = "increasing"
            strength = abs(r_value)
        else:
            direction = "decreasing"
            strength = abs(r_value)
        
        # ê³„ì ˆì„± ë¶„ì„
        seasonality = None
        if len(ts_data) > 10:
            try:
                decomposition = seasonal_decompose(ts_data, period=min(7, len(ts_data)//2))
                seasonal_strength = np.var(decomposition.seasonal) / np.var(ts_data)
                
                if seasonal_strength > 0.1:
                    seasonality = {
                        "strength": seasonal_strength,
                        "period": 7,
                        "component": decomposition.seasonal.mean()
                    }
            except Exception:
                pass
        
        # ì˜ˆì¸¡ ë°ì´í„° ìƒì„±
        forecast_data = []
        future_x = np.arange(len(ts_data), len(ts_data) + 5)
        future_y = slope * future_x + intercept
        
        for i, pred_value in enumerate(future_y):
            forecast_data.append({
                "step": i + 1,
                "predicted_value": pred_value,
                "confidence_interval": [pred_value - 2*std_err, pred_value + 2*std_err]
            })
        
        return TrendAnalysis(
            trend_id=f"trend_{value_col}",
            direction=direction,
            strength=strength,
            seasonality=seasonality,
            forecast_data=forecast_data,
            trend_equation=f"y = {slope:.4f}x + {intercept:.4f}"
        )

class BusinessInsightGenerator:
    """ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        self.insight_rules = self._initialize_business_rules()
        
    def _initialize_business_rules(self) -> Dict[str, Callable]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì´ˆê¸°í™”"""
        return {
            "revenue_analysis": self._analyze_revenue_patterns,
            "customer_analysis": self._analyze_customer_patterns,
            "performance_analysis": self._analyze_performance_metrics,
            "risk_analysis": self._analyze_risk_factors,
            "efficiency_analysis": self._analyze_efficiency_metrics
        }
    
    async def generate_insights(self, df: pd.DataFrame, patterns: List[PatternAnalysis], 
                              anomalies: List[AnomalyDetection], trends: List[TrendAnalysis]) -> List[Insight]:
        """ì¢…í•©ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # 1. íŒ¨í„´ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        pattern_insights = await self._generate_pattern_insights(df, patterns)
        insights.extend(pattern_insights)
        
        # 2. ì´ìƒ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        anomaly_insights = await self._generate_anomaly_insights(df, anomalies)
        insights.extend(anomaly_insights)
        
        # 3. íŠ¸ë Œë“œ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        trend_insights = await self._generate_trend_insights(df, trends)
        insights.extend(trend_insights)
        
        # 4. ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
        business_insights = await self._generate_business_rule_insights(df)
        insights.extend(business_insights)
        
        # ì¸ì‚¬ì´íŠ¸ ìš°ì„ ìˆœìœ„ ì •ë ¬
        insights.sort(key=lambda x: (x.severity.value, -x.confidence_score))
        
        return insights
    
    async def _generate_pattern_insights(self, df: pd.DataFrame, patterns: List[PatternAnalysis]) -> List[Insight]:
        """íŒ¨í„´ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        for pattern in patterns:
            if pattern.pattern_type == "correlation_pattern":
                insight = Insight(
                    insight_id=f"insight_corr_{pattern.pattern_id}",
                    insight_type=InsightType.CORRELATION,
                    title=f"ê°•í•œ ìƒê´€ê´€ê³„ ë°œê²¬: {' vs '.join(pattern.variables)}",
                    description=f"{pattern.variables[0]}ê³¼ {pattern.variables[1]} ì‚¬ì´ì— {pattern.strength:.2f}ì˜ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.",
                    severity=SeverityLevel.MEDIUM,
                    confidence_score=pattern.strength,
                    method=AnalysisMethod.STATISTICAL,
                    data_points=[pattern.pattern_data],
                    recommendations=[
                        f"{pattern.variables[0]}ì„ ì¡°ì •í•˜ì—¬ {pattern.variables[1]}ì— ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                        "ì´ ê´€ê³„ë¥¼ í™œìš©í•œ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œì„ ê³ ë ¤í•´ë³´ì„¸ìš”."
                    ],
                    business_impact="ë³€ìˆ˜ ê°„ ê´€ê³„ë¥¼ ì´í•´í•˜ì—¬ ë” ë‚˜ì€ ì˜ì‚¬ê²°ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤."
                )
                insights.append(insight)
            
            elif pattern.pattern_type == "seasonal_pattern":
                insight = Insight(
                    insight_id=f"insight_seasonal_{pattern.pattern_id}",
                    insight_type=InsightType.TREND,
                    title=f"ê³„ì ˆì„± íŒ¨í„´ ë°œê²¬: {pattern.variables[0]}",
                    description=f"{pattern.variables[0]}ì—ì„œ ì£¼ê¸°ì ì¸ íŒ¨í„´ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤ (ê°•ë„: {pattern.strength:.2f})",
                    severity=SeverityLevel.MEDIUM,
                    confidence_score=pattern.strength,
                    method=AnalysisMethod.TIME_SERIES,
                    data_points=[pattern.pattern_data],
                    recommendations=[
                        "ê³„ì ˆì„±ì„ ê³ ë ¤í•œ ì˜ˆì¸¡ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”.",
                        "ì£¼ê¸°ì ì¸ ë³€ë™ì— ë§ì¶° ë¦¬ì†ŒìŠ¤ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”."
                    ],
                    business_impact="ê³„ì ˆì„±ì„ ê³ ë ¤í•œ ê³„íšìœ¼ë¡œ íš¨ìœ¨ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                insights.append(insight)
        
        return insights
    
    async def _generate_anomaly_insights(self, df: pd.DataFrame, anomalies: List[AnomalyDetection]) -> List[Insight]:
        """ì´ìƒ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ì´ìƒ ë¹ˆë„ë³„ ê·¸ë£¹í™”
        anomaly_counts = defaultdict(int)
        for anomaly in anomalies:
            for feature in anomaly.affected_features:
                anomaly_counts[feature] += 1
        
        for feature, count in anomaly_counts.items():
            if count > len(df) * 0.05:  # 5% ì´ìƒì˜ ì´ìƒ
                insight = Insight(
                    insight_id=f"insight_anomaly_{feature}",
                    insight_type=InsightType.ANOMALY,
                    title=f"ë†’ì€ ì´ìƒ ë¹ˆë„ ê°ì§€: {feature}",
                    description=f"{feature}ì—ì„œ {count}ê°œì˜ ì´ìƒê°’ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ ({count/len(df)*100:.1f}%)",
                    severity=SeverityLevel.HIGH if count > len(df) * 0.1 else SeverityLevel.MEDIUM,
                    confidence_score=min(count / len(df) * 10, 1.0),
                    method=AnalysisMethod.MACHINE_LEARNING,
                    data_points=[{"feature": feature, "anomaly_count": count, "percentage": count/len(df)*100}],
                    recommendations=[
                        f"{feature}ì˜ ë°ì´í„° í’ˆì§ˆì„ ì ê²€í•˜ì„¸ìš”.",
                        "ì´ìƒê°’ì˜ ì›ì¸ì„ ì¡°ì‚¬í•˜ê³  ë°ì´í„° ìˆ˜ì§‘ í”„ë¡œì„¸ìŠ¤ë¥¼ ê°œì„ í•˜ì„¸ìš”.",
                        "ì´ìƒê°’ ì²˜ë¦¬ ì „ëµì„ ìˆ˜ë¦½í•˜ì„¸ìš”."
                    ],
                    business_impact="ë°ì´í„° í’ˆì§ˆ ê°œì„ ìœ¼ë¡œ ë¶„ì„ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                insights.append(insight)
        
        return insights
    
    async def _generate_trend_insights(self, df: pd.DataFrame, trends: List[TrendAnalysis]) -> List[Insight]:
        """íŠ¸ë Œë“œ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        for trend in trends:
            if trend.strength > 0.7:  # ê°•í•œ íŠ¸ë Œë“œ
                insight = Insight(
                    insight_id=f"insight_trend_{trend.trend_id}",
                    insight_type=InsightType.TREND,
                    title=f"ê°•í•œ {trend.direction} íŠ¸ë Œë“œ ê°ì§€",
                    description=f"ë³€ìˆ˜ì—ì„œ {trend.direction} ë°©í–¥ì˜ ê°•í•œ íŠ¸ë Œë“œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ (ê°•ë„: {trend.strength:.2f})",
                    severity=SeverityLevel.HIGH if trend.strength > 0.85 else SeverityLevel.MEDIUM,
                    confidence_score=trend.strength,
                    method=AnalysisMethod.TIME_SERIES,
                    data_points=[{
                        "trend_equation": trend.trend_equation,
                        "direction": trend.direction,
                        "strength": trend.strength,
                        "forecast": trend.forecast_data[:3]  # ì²« 3ê°œ ì˜ˆì¸¡
                    }],
                    recommendations=[
                        f"í˜„ì¬ {trend.direction} íŠ¸ë Œë“œë¥¼ ê³ ë ¤í•œ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”.",
                        "íŠ¸ë Œë“œ ì§€ì†ì„±ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ì ì ˆí•œ ì¡°ì¹˜ë¥¼ ì·¨í•˜ì„¸ìš”.",
                        "ì˜ˆì¸¡ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ë¯¸ë˜ ê³„íšì„ ì„¸ìš°ì„¸ìš”."
                    ],
                    business_impact="íŠ¸ë Œë“œë¥¼ í™œìš©í•œ ì „ëµì  ê³„íšìœ¼ë¡œ ê²½ìŸ ìš°ìœ„ë¥¼ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
                insights.append(insight)
        
        return insights
    
    async def _generate_business_rule_insights(self, df: pd.DataFrame) -> List[Insight]:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []
        
        # ë§¤ì¶œ/ìˆ˜ìµ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
        revenue_cols = [col for col in df.columns if any(keyword in col.lower() 
                       for keyword in ['revenue', 'sales', 'income', 'profit', 'ë§¤ì¶œ', 'ìˆ˜ìµ', 'íŒë§¤'])]
        
        for col in revenue_cols:
            if pd.api.types.is_numeric_dtype(df[col]):
                revenue_insights = await self._analyze_revenue_patterns(df, col)
                insights.extend(revenue_insights)
        
        # ê³ ê° ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
        customer_cols = [col for col in df.columns if any(keyword in col.lower() 
                        for keyword in ['customer', 'client', 'user', 'ê³ ê°', 'ì‚¬ìš©ì'])]
        
        if customer_cols:
            customer_insights = await self._analyze_customer_patterns(df, customer_cols)
            insights.extend(customer_insights)
        
        return insights
    
    async def _analyze_revenue_patterns(self, df: pd.DataFrame, revenue_col: str) -> List[Insight]:
        """ë§¤ì¶œ íŒ¨í„´ ë¶„ì„"""
        insights = []
        
        if df[revenue_col].dropna().empty:
            return insights
        
        revenue_data = df[revenue_col].dropna()
        
        # ë§¤ì¶œ ë¶„í¬ ë¶„ì„
        q25, q50, q75 = revenue_data.quantile([0.25, 0.5, 0.75])
        
        # ë†’ì€ ìˆ˜ìµ êµ¬ê°„ ë¶„ì„
        high_revenue_threshold = q75 + 1.5 * (q75 - q25)
        high_revenue_count = len(revenue_data[revenue_data > high_revenue_threshold])
        
        if high_revenue_count > 0:
            insight = Insight(
                insight_id=f"insight_high_revenue_{revenue_col}",
                insight_type=InsightType.BUSINESS_RULE,
                title="ê³ ìˆ˜ìµ ê¸°íšŒ ì‹ë³„",
                description=f"{high_revenue_count}ê°œì˜ ê³ ìˆ˜ìµ ì¼€ì´ìŠ¤ê°€ ì‹ë³„ë˜ì—ˆìŠµë‹ˆë‹¤ (ì„ê³„ê°’: {high_revenue_threshold:.2f})",
                severity=SeverityLevel.HIGH,
                confidence_score=0.8,
                method=AnalysisMethod.STATISTICAL,
                data_points=[{
                    "high_revenue_count": high_revenue_count,
                    "threshold": high_revenue_threshold,
                    "percentage": high_revenue_count / len(revenue_data) * 100
                }],
                recommendations=[
                    "ê³ ìˆ˜ìµ ì¼€ì´ìŠ¤ì˜ ê³µí†µ íŠ¹ì„±ì„ ë¶„ì„í•˜ì„¸ìš”.",
                    "ì„±ê³µ íŒ¨í„´ì„ ë‹¤ë¥¸ ì¼€ì´ìŠ¤ì— ì ìš©í•´ë³´ì„¸ìš”.",
                    "ê³ ìˆ˜ìµ ê³ ê°/ì œí’ˆì— ì§‘ì¤‘í•˜ëŠ” ì „ëµì„ ê³ ë ¤í•˜ì„¸ìš”."
                ],
                business_impact="ê³ ìˆ˜ìµ íŒ¨í„´ ë³µì œë¡œ ì „ì²´ ìˆ˜ìµì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            insights.append(insight)
        
        return insights
    
    async def _analyze_customer_patterns(self, df: pd.DataFrame, customer_cols: List[str]) -> List[Insight]:
        """ê³ ê° íŒ¨í„´ ë¶„ì„"""
        insights = []
        
        # ê³ ê° ìˆ˜ ë˜ëŠ” ê³ ê° ID ê¸°ë°˜ ë¶„ì„
        for col in customer_cols:
            if 'id' in col.lower():
                unique_customers = df[col].nunique()
                total_records = len(df)
                
                if unique_customers > 0:
                    avg_records_per_customer = total_records / unique_customers
                    
                    insight = Insight(
                        insight_id=f"insight_customer_activity_{col}",
                        insight_type=InsightType.BUSINESS_RULE,
                        title="ê³ ê° í™œë™ íŒ¨í„´ ë¶„ì„",
                        description=f"ê³ ê°ë‹¹ í‰ê·  {avg_records_per_customer:.1f}ê°œì˜ ê¸°ë¡ì´ ìˆìŠµë‹ˆë‹¤",
                        severity=SeverityLevel.MEDIUM,
                        confidence_score=0.7,
                        method=AnalysisMethod.STATISTICAL,
                        data_points=[{
                            "unique_customers": unique_customers,
                            "total_records": total_records,
                            "avg_records_per_customer": avg_records_per_customer
                        }],
                        recommendations=[
                            "ê³ ê° ì„¸ë¶„í™” ì „ëµì„ ê°œë°œí•˜ì„¸ìš”.",
                            "í™œë™ì´ ë§ì€ ê³ ê°ê³¼ ì ì€ ê³ ê°ì„ êµ¬ë¶„í•˜ì—¬ ê´€ë¦¬í•˜ì„¸ìš”.",
                            "ê³ ê° ìƒì• ê°€ì¹˜(CLV) ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”."
                        ],
                        business_impact="ê³ ê°ë³„ ë§ì¶¤ ì „ëµìœ¼ë¡œ ê³ ê° ë§Œì¡±ë„ì™€ ìˆ˜ìµì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                    )
                    insights.append(insight)
        
        return insights
    
    async def _analyze_performance_metrics(self, df: pd.DataFrame) -> List[Insight]:
        """ì„±ëŠ¥ ì§€í‘œ ë¶„ì„"""
        # êµ¬ì²´ì ì¸ ì„±ëŠ¥ ì§€í‘œ ë¶„ì„ ë¡œì§ êµ¬í˜„
        return []
    
    async def _analyze_risk_factors(self, df: pd.DataFrame) -> List[Insight]:
        """ìœ„í—˜ ìš”ì†Œ ë¶„ì„"""
        # êµ¬ì²´ì ì¸ ìœ„í—˜ ë¶„ì„ ë¡œì§ êµ¬í˜„
        return []
    
    async def _analyze_efficiency_metrics(self, df: pd.DataFrame) -> List[Insight]:
        """íš¨ìœ¨ì„± ì§€í‘œ ë¶„ì„"""
        # êµ¬ì²´ì ì¸ íš¨ìœ¨ì„± ë¶„ì„ ë¡œì§ êµ¬í˜„
        return []

class InsightDatabase:
    """ì¸ì‚¬ì´íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""
    
    def __init__(self, db_path: str = "core/enterprise/insights.db"):
        self.db_path = db_path
        self._initialize_database()
    
    def _initialize_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        Path(self.db_path).parent.mkdir(parents=True, exist_ok=True)
        
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS insights (
                    insight_id TEXT PRIMARY KEY,
                    insight_type TEXT NOT NULL,
                    title TEXT NOT NULL,
                    description TEXT NOT NULL,
                    severity TEXT NOT NULL,
                    confidence_score REAL NOT NULL,
                    method TEXT NOT NULL,
                    data_points TEXT NOT NULL,
                    recommendations TEXT NOT NULL,
                    business_impact TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    metadata TEXT
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS insight_performance (
                    insight_id TEXT,
                    execution_time_ms REAL,
                    dataset_size INTEGER,
                    accuracy_score REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY (insight_id) REFERENCES insights (insight_id)
                )
            """)
    
    def save_insight(self, insight: Insight) -> bool:
        """ì¸ì‚¬ì´íŠ¸ ì €ì¥"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("""
                    INSERT OR REPLACE INTO insights 
                    (insight_id, insight_type, title, description, severity, 
                     confidence_score, method, data_points, recommendations, 
                     business_impact, created_at, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    insight.insight_id,
                    insight.insight_type.value,
                    insight.title,
                    insight.description,
                    insight.severity.value,
                    insight.confidence_score,
                    insight.method.value,
                    json.dumps(insight.data_points),
                    json.dumps(insight.recommendations),
                    insight.business_impact,
                    insight.created_at.isoformat(),
                    json.dumps(insight.metadata)
                ))
            return True
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def get_insights(self, limit: int = 100) -> List[Insight]:
        """ì €ì¥ëœ ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ"""
        insights = []
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT * FROM insights 
                    ORDER BY created_at DESC 
                    LIMIT ?
                """, (limit,))
                
                for row in cursor.fetchall():
                    insight = Insight(
                        insight_id=row[0],
                        insight_type=InsightType(row[1]),
                        title=row[2],
                        description=row[3],
                        severity=SeverityLevel(row[4]),
                        confidence_score=row[5],
                        method=AnalysisMethod(row[6]),
                        data_points=json.loads(row[7]),
                        recommendations=json.loads(row[8]),
                        business_impact=row[9],
                        created_at=datetime.fromisoformat(row[10]),
                        metadata=json.loads(row[11]) if row[11] else {}
                    )
                    insights.append(insight)
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        return insights

class AIInsightEngine:
    """AI ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì—”ì§„ í†µí•© í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.pattern_discovery = PatternDiscovery()
        self.anomaly_detector = AnomalyDetector()
        self.trend_analyzer = TrendAnalyzer()
        self.insight_generator = BusinessInsightGenerator()
        self.database = InsightDatabase()
        
    async def analyze_data(self, df: pd.DataFrame, save_results: bool = True) -> Dict[str, Any]:
        """ë°ì´í„° ì¢…í•© ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        start_time = time.time()
        
        logger.info(f"ë°ì´í„° ë¶„ì„ ì‹œì‘: {df.shape}")
        
        try:
            # 1. íŒ¨í„´ ë°œê²¬
            patterns = await self.pattern_discovery.discover_patterns(df)
            
            # 2. ì´ìƒ ê°ì§€
            anomalies = await self.anomaly_detector.detect_anomalies(df)
            
            # 3. íŠ¸ë Œë“œ ë¶„ì„
            trends = await self.trend_analyzer.analyze_trends(df)
            
            # 4. ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = await self.insight_generator.generate_insights(df, patterns, anomalies, trends)
            
            # 5. ê²°ê³¼ ì €ì¥
            if save_results:
                for insight in insights:
                    self.database.save_insight(insight)
            
            execution_time = (time.time() - start_time) * 1000
            
            result = {
                "status": "success",
                "execution_time_ms": execution_time,
                "dataset_info": {
                    "shape": df.shape,
                    "columns": list(df.columns),
                    "memory_usage_mb": df.memory_usage(deep=True).sum() / 1024 / 1024
                },
                "analysis_results": {
                    "patterns_discovered": len(patterns),
                    "anomalies_detected": len(anomalies),
                    "trends_identified": len(trends),
                    "insights_generated": len(insights)
                },
                "patterns": [self._serialize_pattern(p) for p in patterns],
                "anomalies": [self._serialize_anomaly(a) for a in anomalies],
                "trends": [self._serialize_trend(t) for t in trends],
                "insights": [self._serialize_insight(i) for i in insights]
            }
            
            logger.info(f"ë¶„ì„ ì™„ë£Œ: {execution_time:.1f}ms, ì¸ì‚¬ì´íŠ¸ {len(insights)}ê°œ ìƒì„±")
            return result
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "status": "error",
                "error_message": str(e),
                "execution_time_ms": (time.time() - start_time) * 1000
            }
    
    def _serialize_pattern(self, pattern: PatternAnalysis) -> Dict[str, Any]:
        """íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ì§ë ¬í™”"""
        return {
            "pattern_id": pattern.pattern_id,
            "pattern_type": pattern.pattern_type,
            "frequency": pattern.frequency,
            "strength": pattern.strength,
            "variables": pattern.variables,
            "pattern_data": pattern.pattern_data,
            "statistical_significance": pattern.statistical_significance
        }
    
    def _serialize_anomaly(self, anomaly: AnomalyDetection) -> Dict[str, Any]:
        """ì´ìƒ ê°ì§€ ê²°ê³¼ ì§ë ¬í™”"""
        return {
            "anomaly_id": anomaly.anomaly_id,
            "detection_method": anomaly.detection_method,
            "anomaly_score": anomaly.anomaly_score,
            "affected_features": anomaly.affected_features,
            "context": anomaly.context,
            "timestamp": anomaly.timestamp.isoformat()
        }
    
    def _serialize_trend(self, trend: TrendAnalysis) -> Dict[str, Any]:
        """íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ ì§ë ¬í™”"""
        return {
            "trend_id": trend.trend_id,
            "direction": trend.direction,
            "strength": trend.strength,
            "seasonality": trend.seasonality,
            "forecast_data": trend.forecast_data,
            "trend_equation": trend.trend_equation
        }
    
    def _serialize_insight(self, insight: Insight) -> Dict[str, Any]:
        """ì¸ì‚¬ì´íŠ¸ ì§ë ¬í™”"""
        return {
            "insight_id": insight.insight_id,
            "insight_type": insight.insight_type.value,
            "title": insight.title,
            "description": insight.description,
            "severity": insight.severity.value,
            "confidence_score": insight.confidence_score,
            "method": insight.method.value,
            "data_points": insight.data_points,
            "recommendations": insight.recommendations,
            "business_impact": insight.business_impact,
            "created_at": insight.created_at.isoformat()
        }
    
    def get_insight_dashboard(self) -> Dict[str, Any]:
        """ì¸ì‚¬ì´íŠ¸ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        insights = self.database.get_insights(limit=50)
        
        # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
        severity_counts = defaultdict(int)
        type_counts = defaultdict(int)
        
        for insight in insights:
            severity_counts[insight.severity.value] += 1
            type_counts[insight.insight_type.value] += 1
        
        # ìµœê·¼ ì¤‘ìš” ì¸ì‚¬ì´íŠ¸
        critical_insights = [i for i in insights if i.severity in [SeverityLevel.CRITICAL, SeverityLevel.HIGH]]
        
        return {
            "total_insights": len(insights),
            "severity_distribution": dict(severity_counts),
            "type_distribution": dict(type_counts),
            "recent_critical_insights": [self._serialize_insight(i) for i in critical_insights[:5]],
            "average_confidence": np.mean([i.confidence_score for i in insights]) if insights else 0,
            "last_analysis": insights[0].created_at.isoformat() if insights else None
        }

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_ai_insight_engine = None

def get_ai_insight_engine() -> AIInsightEngine:
    """AI ì¸ì‚¬ì´íŠ¸ ì—”ì§„ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ai_insight_engine
    if _ai_insight_engine is None:
        _ai_insight_engine = AIInsightEngine()
    return _ai_insight_engine

# í¸ì˜ í•¨ìˆ˜ë“¤
async def analyze_data_insights(df: pd.DataFrame) -> Dict[str, Any]:
    """ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ë¶„ì„ í¸ì˜ í•¨ìˆ˜"""
    engine = get_ai_insight_engine()
    return await engine.analyze_data(df)

def get_insight_dashboard() -> Dict[str, Any]:
    """ì¸ì‚¬ì´íŠ¸ ëŒ€ì‹œë³´ë“œ í¸ì˜ í•¨ìˆ˜"""
    engine = get_ai_insight_engine()
    return engine.get_insight_dashboard()

async def test_ai_insight_engine():
    """AI ì¸ì‚¬ì´íŠ¸ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª AI Insight Engine í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    np.random.seed(42)
    n_samples = 1000
    
    dates = pd.date_range('2023-01-01', periods=n_samples, freq='D')
    revenue = 1000 + 50 * np.sin(np.arange(n_samples) * 2 * np.pi / 365) + np.random.normal(0, 100, n_samples)
    customers = np.random.poisson(50, n_samples)
    
    # ì´ìƒê°’ ì¶”ê°€
    revenue[100] = 5000  # ì´ìƒê°’
    revenue[200] = -500  # ì´ìƒê°’
    
    test_df = pd.DataFrame({
        'date': dates,
        'revenue': revenue,
        'customers': customers,
        'profit_margin': revenue * 0.2 + np.random.normal(0, 50, n_samples),
        'customer_id': [f'CUST_{i%100}' for i in range(n_samples)]
    })
    
    try:
        # ë¶„ì„ ì‹¤í–‰
        result = await analyze_data_insights(test_df)
        
        print(f"âœ… ë¶„ì„ ì™„ë£Œ: {result['status']}")
        print(f"ğŸ“Š ì‹¤í–‰ ì‹œê°„: {result['execution_time_ms']:.1f}ms")
        print(f"ğŸ“ˆ íŒ¨í„´ ë°œê²¬: {result['analysis_results']['patterns_discovered']}ê°œ")
        print(f"ğŸš¨ ì´ìƒ ê°ì§€: {result['analysis_results']['anomalies_detected']}ê°œ")
        print(f"ğŸ“Š íŠ¸ë Œë“œ: {result['analysis_results']['trends_identified']}ê°œ")
        print(f"ğŸ’¡ ì¸ì‚¬ì´íŠ¸: {result['analysis_results']['insights_generated']}ê°œ")
        
        # ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸
        dashboard = get_insight_dashboard()
        print(f"ğŸ“‹ ëŒ€ì‹œë³´ë“œ ì¸ì‚¬ì´íŠ¸ ì´ê³„: {dashboard['total_insights']}ê°œ")
        
        print("âœ… AI Insight Engine í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    asyncio.run(test_ai_insight_engine()) 