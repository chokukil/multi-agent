#!/usr/bin/env python3
"""
ğŸ’ CherryAI íŒŒì¼ ì—…ë¡œë“œ í”„ë¡œì„¸ì„œ

ì—…ë¡œë“œëœ íŒŒì¼ì„ A2A ì‹œìŠ¤í…œì— ì—°ë™í•˜ëŠ” ë¡œì§
- íŒŒì¼ ì—…ë¡œë“œ ê²€ì¦ (CSV, Excel, JSON)
- A2A ì‹œìŠ¤í…œìœ¼ë¡œ íŒŒì¼ ì „ë‹¬
- íŒŒì¼ ë©”íƒ€ë°ì´í„° ê´€ë¦¬
- íŒŒì¼ í”„ë¦¬ë·° ê¸°ëŠ¥
"""

import io
import json
import os
import uuid
from datetime import datetime
from typing import Dict, Any, List, Optional, Union, Tuple
from dataclasses import dataclass, field
import logging

import pandas as pd
import streamlit as st

logger = logging.getLogger(__name__)

SUPPORTED_FILE_TYPES = {
    'csv': 'text/csv',
    'xlsx': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
    'xls': 'application/vnd.ms-excel',
    'json': 'application/json'
}

@dataclass
class FileMetadata:
    """íŒŒì¼ ë©”íƒ€ë°ì´í„°"""
    file_id: str
    filename: str
    file_type: str
    file_size: int
    upload_time: datetime = field(default_factory=datetime.now)
    rows: Optional[int] = None
    columns: Optional[int] = None
    column_names: Optional[List[str]] = None
    data_types: Optional[Dict[str, str]] = None
    preview_data: Optional[Dict[str, Any]] = None
    processing_status: str = "uploaded"  # uploaded, processing, processed, error
    error_message: Optional[str] = None

@dataclass
class ProcessedFile:
    """ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´"""
    metadata: FileMetadata
    dataframe: Optional[pd.DataFrame] = None
    raw_data: Optional[Any] = None
    a2a_ready: bool = False

class FileUploadProcessor:
    """íŒŒì¼ ì—…ë¡œë“œ í”„ë¡œì„¸ì„œ"""
    
    def __init__(self):
        # ì²˜ë¦¬ëœ íŒŒì¼ë“¤ ì €ì¥
        self.processed_files: Dict[str, ProcessedFile] = {}
        
        # ì„¤ì •
        self.config = {
            'max_file_size_mb': 100,
            'max_preview_rows': 5,
            'max_columns_display': 10,
            'auto_detect_encoding': True,
            'validate_data_quality': True,
            'enable_file_caching': True
        }
        
        # í†µê³„
        self.stats = {
            'total_uploads': 0,
            'successful_uploads': 0,
            'failed_uploads': 0,
            'total_size_mb': 0.0
        }
    
    def validate_uploaded_files(self, uploaded_files: List[Any]) -> List[Tuple[bool, str, Any]]:
        """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ê²€ì¦"""
        validation_results = []
        
        for file in uploaded_files:
            try:
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size_mb = file.size / (1024 * 1024)
                if file_size_mb > self.config['max_file_size_mb']:
                    validation_results.append((
                        False, 
                        f"íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤: {file_size_mb:.1f}MB (ìµœëŒ€ {self.config['max_file_size_mb']}MB)",
                        file
                    ))
                    continue
                
                # íŒŒì¼ íƒ€ì… í™•ì¸
                file_extension = file.name.split('.')[-1].lower()
                if file_extension not in SUPPORTED_FILE_TYPES:
                    validation_results.append((
                        False,
                        f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}",
                        file
                    ))
                    continue
                
                validation_results.append((True, "ê²€ì¦ ì„±ê³µ", file))
                
            except Exception as e:
                validation_results.append((
                    False,
                    f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
                    file
                ))
        
        return validation_results
    
    def process_uploaded_files(self, uploaded_files: List[Any]) -> List[ProcessedFile]:
        """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ ì²˜ë¦¬"""
        processed_files = []
        
        # íŒŒì¼ ê²€ì¦
        validation_results = self.validate_uploaded_files(uploaded_files)
        
        for is_valid, message, file in validation_results:
            self.stats['total_uploads'] += 1
            
            if not is_valid:
                logger.error(f"íŒŒì¼ ê²€ì¦ ì‹¤íŒ¨: {message}")
                self.stats['failed_uploads'] += 1
                continue
            
            try:
                # íŒŒì¼ ì²˜ë¦¬
                processed_file = self._process_single_file(file)
                processed_files.append(processed_file)
                
                # ìºì‹œì— ì €ì¥
                self.processed_files[processed_file.metadata.file_id] = processed_file
                
                self.stats['successful_uploads'] += 1
                self.stats['total_size_mb'] += file.size / (1024 * 1024)
                
                logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file.name}")
                
            except Exception as e:
                logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {file.name} - {str(e)}")
                self.stats['failed_uploads'] += 1
        
        return processed_files
    
    def _process_single_file(self, file) -> ProcessedFile:
        """ê°œë³„ íŒŒì¼ ì²˜ë¦¬"""
        # íŒŒì¼ ë©”íƒ€ë°ì´í„° ìƒì„±
        file_id = str(uuid.uuid4())
        file_extension = file.name.split('.')[-1].lower()
        
        metadata = FileMetadata(
            file_id=file_id,
            filename=file.name,
            file_type=file_extension,
            file_size=file.size,
            processing_status="processing"
        )
        
        try:
            # íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬
            if file_extension == 'csv':
                df, raw_data = self._process_csv_file(file)
            elif file_extension in ['xlsx', 'xls']:
                df, raw_data = self._process_excel_file(file)
            elif file_extension == 'json':
                df, raw_data = self._process_json_file(file)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ íƒ€ì…: {file_extension}")
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            if df is not None:
                metadata.rows = len(df)
                metadata.columns = len(df.columns)
                metadata.column_names = df.columns.tolist()
                metadata.data_types = df.dtypes.astype(str).to_dict()
                metadata.preview_data = self._create_preview_data(df)
            
            metadata.processing_status = "processed"
            
            # ProcessedFile ê°ì²´ ìƒì„±
            processed_file = ProcessedFile(
                metadata=metadata,
                dataframe=df,
                raw_data=raw_data,
                a2a_ready=True
            )
            
            return processed_file
            
        except Exception as e:
            metadata.processing_status = "error"
            metadata.error_message = str(e)
            
            return ProcessedFile(
                metadata=metadata,
                a2a_ready=False
            )
    
    def _process_csv_file(self, file) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """CSV íŒŒì¼ ì²˜ë¦¬"""
        try:
            # ì¸ì½”ë”© ìë™ ê°ì§€
            if self.config['auto_detect_encoding']:
                try:
                    df = pd.read_csv(file, encoding='utf-8')
                except UnicodeDecodeError:
                    try:
                        df = pd.read_csv(file, encoding='cp949')
                    except UnicodeDecodeError:
                        df = pd.read_csv(file, encoding='latin-1')
            else:
                df = pd.read_csv(file)
            
            # Raw ë°ì´í„° ì •ë³´
            raw_data = {
                'encoding': 'auto-detected',
                'separator': ',',
                'header_row': 0,
                'total_rows': len(df),
                'total_columns': len(df.columns)
            }
            
            return df, raw_data
            
        except Exception as e:
            raise Exception(f"CSV íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    def _process_excel_file(self, file) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """Excel íŒŒì¼ ì²˜ë¦¬"""
        try:
            # Excel íŒŒì¼ ì½ê¸° (ì²« ë²ˆì§¸ ì‹œíŠ¸)
            excel_file = pd.ExcelFile(file)
            sheet_names = excel_file.sheet_names
            
            # ì²« ë²ˆì§¸ ì‹œíŠ¸ ì½ê¸°
            df = pd.read_excel(file, sheet_name=0)
            
            # Raw ë°ì´í„° ì •ë³´
            raw_data = {
                'sheet_names': sheet_names,
                'active_sheet': sheet_names[0] if sheet_names else 'Sheet1',
                'total_sheets': len(sheet_names),
                'total_rows': len(df),
                'total_columns': len(df.columns)
            }
            
            return df, raw_data
            
        except Exception as e:
            raise Exception(f"Excel íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    def _process_json_file(self, file) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """JSON íŒŒì¼ ì²˜ë¦¬"""
        try:
            # JSON ë°ì´í„° ì½ê¸°
            json_data = json.load(file)
            
            # DataFrameìœ¼ë¡œ ë³€í™˜ ì‹œë„
            if isinstance(json_data, list):
                df = pd.DataFrame(json_data)
            elif isinstance(json_data, dict):
                if 'data' in json_data:
                    df = pd.DataFrame(json_data['data'])
                else:
                    df = pd.DataFrame([json_data])
            else:
                raise ValueError("JSON í˜•ì‹ì„ DataFrameìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # Raw ë°ì´í„° ì •ë³´
            raw_data = {
                'json_type': type(json_data).__name__,
                'structure': 'list' if isinstance(json_data, list) else 'object',
                'total_rows': len(df),
                'total_columns': len(df.columns)
            }
            
            return df, raw_data
            
        except Exception as e:
            raise Exception(f"JSON íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
    
    def _create_preview_data(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë¯¸ë¦¬ë³´ê¸° ë°ì´í„° ìƒì„±"""
        try:
            max_rows = self.config['max_preview_rows']
            max_cols = self.config['max_columns_display']
            
            # ë¯¸ë¦¬ë³´ê¸°ìš© DataFrame
            preview_df = df.head(max_rows)
            if len(df.columns) > max_cols:
                preview_df = preview_df.iloc[:, :max_cols]
            
            # ê¸°ë³¸ í†µê³„
            basic_stats = {
                'total_rows': len(df),
                'total_columns': len(df.columns),
                'memory_usage_mb': round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
                'null_counts': df.isnull().sum().to_dict(),
                'numeric_columns': df.select_dtypes(include=['number']).columns.tolist(),
                'categorical_columns': df.select_dtypes(include=['object']).columns.tolist()
            }
            
            return {
                'preview_table': preview_df.to_dict('records'),
                'column_info': [
                    {
                        'name': col,
                        'type': str(df[col].dtype),
                        'null_count': int(df[col].isnull().sum()),
                        'unique_count': int(df[col].nunique())
                    }
                    for col in df.columns[:max_cols]
                ],
                'basic_stats': basic_stats
            }
            
        except Exception as e:
            logger.error(f"ë¯¸ë¦¬ë³´ê¸° ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    def prepare_for_a2a_system(self, file_ids: List[str]) -> Dict[str, Any]:
        """A2A ì‹œìŠ¤í…œìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„° ì¤€ë¹„"""
        a2a_data = {
            'files': [],
            'metadata': {
                'total_files': len(file_ids),
                'preparation_time': datetime.now().isoformat(),
                'processor_version': '1.0.0'
            }
        }
        
        for file_id in file_ids:
            if file_id not in self.processed_files:
                logger.warning(f"íŒŒì¼ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
                continue
            
            processed_file = self.processed_files[file_id]
            
            if not processed_file.a2a_ready:
                logger.warning(f"A2A ì¤€ë¹„ê°€ ë˜ì§€ ì•Šì€ íŒŒì¼: {file_id}")
                continue
            
            # A2A ì‹œìŠ¤í…œìš© íŒŒì¼ ë°ì´í„°
            file_data = {
                'file_id': file_id,
                'filename': processed_file.metadata.filename,
                'file_type': processed_file.metadata.file_type,
                'rows': processed_file.metadata.rows,
                'columns': processed_file.metadata.columns,
                'column_names': processed_file.metadata.column_names,
                'data_types': processed_file.metadata.data_types,
                'dataframe_json': processed_file.dataframe.to_json(orient='records') if processed_file.dataframe is not None else None,
                'raw_data_info': processed_file.raw_data
            }
            
            a2a_data['files'].append(file_data)
        
        return a2a_data
    
    def render_file_preview(self, file_id: str) -> None:
        """íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ë Œë”ë§"""
        if file_id not in self.processed_files:
            st.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_id}")
            return
        
        processed_file = self.processed_files[file_id]
        metadata = processed_file.metadata
        
        # íŒŒì¼ ì •ë³´ í—¤ë”
        st.markdown(f"""
        ### ğŸ“„ {metadata.filename}
        - **íŒŒì¼ íƒ€ì…**: {metadata.file_type.upper()}
        - **í¬ê¸°**: {metadata.file_size / 1024:.1f} KB
        - **í–‰ ìˆ˜**: {metadata.rows:,}ê°œ
        - **ì—´ ìˆ˜**: {metadata.columns}ê°œ
        - **ìƒíƒœ**: {metadata.processing_status}
        """)
        
        if metadata.processing_status == "error":
            st.error(f"âŒ ì²˜ë¦¬ ì˜¤ë¥˜: {metadata.error_message}")
            return
        
        if metadata.preview_data and processed_file.dataframe is not None:
            # ë¯¸ë¦¬ë³´ê¸° í…Œì´ë¸”
            st.markdown("#### ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
            preview_df = pd.DataFrame(metadata.preview_data['preview_table'])
            st.dataframe(preview_df, use_container_width=True)
            
            # ì»¬ëŸ¼ ì •ë³´
            st.markdown("#### ğŸ“‹ ì»¬ëŸ¼ ì •ë³´")
            col_info = metadata.preview_data['column_info']
            col_df = pd.DataFrame(col_info)
            st.dataframe(col_df, use_container_width=True)
            
            # ê¸°ë³¸ í†µê³„
            basic_stats = metadata.preview_data['basic_stats']
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", f"{basic_stats['memory_usage_mb']} MB")
            with col2:
                st.metric("ìˆ«ìí˜• ì»¬ëŸ¼", len(basic_stats['numeric_columns']))
            with col3:
                st.metric("í…ìŠ¤íŠ¸ ì»¬ëŸ¼", len(basic_stats['categorical_columns']))
    
    def get_upload_stats(self) -> Dict[str, Any]:
        """ì—…ë¡œë“œ í†µê³„ ë°˜í™˜"""
        success_rate = 0.0
        if self.stats['total_uploads'] > 0:
            success_rate = (self.stats['successful_uploads'] / self.stats['total_uploads']) * 100
        
        return {
            "total_uploads": self.stats['total_uploads'],
            "successful_uploads": self.stats['successful_uploads'],
            "failed_uploads": self.stats['failed_uploads'],
            "success_rate": round(success_rate, 1),
            "total_size_mb": round(self.stats['total_size_mb'], 2),
            "processed_files_count": len(self.processed_files),
            "supported_formats": list(SUPPORTED_FILE_TYPES.keys())
        }

# ì „ì—­ íŒŒì¼ ì—…ë¡œë“œ í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤
_file_processor = None

def get_file_upload_processor() -> FileUploadProcessor:
    """íŒŒì¼ ì—…ë¡œë“œ í”„ë¡œì„¸ì„œ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _file_processor
    if _file_processor is None:
        _file_processor = FileUploadProcessor()
    return _file_processor

def process_and_prepare_files_for_a2a(uploaded_files: List[Any]) -> Optional[Dict[str, Any]]:
    """íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  A2A ì‹œìŠ¤í…œìœ¼ë¡œ ì „ë‹¬í•  ë°ì´í„° ì¤€ë¹„"""
    if not uploaded_files:
        return None
    
    processor = get_file_upload_processor()
    
    try:
        # íŒŒì¼ ì²˜ë¦¬
        processed_files = processor.process_uploaded_files(uploaded_files)
        
        if not processed_files:
            st.warning("ì²˜ë¦¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # A2A ì‹œìŠ¤í…œìš© ë°ì´í„° ì¤€ë¹„
        file_ids = [pf.metadata.file_id for pf in processed_files if pf.a2a_ready]
        a2a_data = processor.prepare_for_a2a_system(file_ids)
        
        return a2a_data
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì²˜ë¦¬ ë° A2A ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None 