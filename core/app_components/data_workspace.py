"""
ğŸ’¾ Data Workspace Component - LLM First Architecture
Cursor ìŠ¤íƒ€ì¼ì˜ ì™„ì „í•œ LLM ê¸°ë°˜ ë°ì´í„° ë¶„ì„ ì›Œí¬ìŠ¤í˜ì´ìŠ¤
Real AI Analysis with A2A Integration & Langfuse Logging
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from typing import Dict, Any, List, Optional
import io
import json
import time
import asyncio
import os
from datetime import datetime
import openai
from openai import AsyncOpenAI
import httpx # Added for dynamic agent discovery
import matplotlib.pyplot as plt # Added for safe code execution
import seaborn as sns # Added for safe code execution
import numpy as np # Added for safe code execution

# Langfuse í†µí•©
try:
    from core.langfuse_session_tracer import get_session_tracer
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False

# A2A í´ë¼ì´ì–¸íŠ¸ í†µí•© - A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜
try:
    from core.a2a.a2a_streamlit_client import A2AStreamlitClient
    from core.enhanced_a2a_communicator import EnhancedA2ACommunicator
    from a2a.client import A2AClient
    from a2a.types import Message, TextPart, Role
    from a2a.utils.message import new_agent_text_message
    A2A_CLIENT_AVAILABLE = True
except ImportError:
    A2A_CLIENT_AVAILABLE = False

# SSE ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
try:
    from core.utils.streaming import StreamingManager
    import asyncio
    from typing import AsyncGenerator
    SSE_STREAMING_AVAILABLE = True
except ImportError:
    SSE_STREAMING_AVAILABLE = False

# LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    llm_client = AsyncOpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
    )
    LLM_AVAILABLE = True
except:
    LLM_AVAILABLE = False

# SSE ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„
import asyncio
from typing import AsyncGenerator, Callable
import json
import time

# SSE ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ê´€ë¦¬
class SSEStreamingManager:
    """SSE ìŠ¤íŠ¸ë¦¬ë° ê´€ë¦¬ì - A2A í‘œì¤€ ì¤€ìˆ˜"""
    
    def __init__(self):
        self.active_streams = {}
        self.stream_callbacks = {}
        self.streaming_updates = []
    
    async def start_sse_stream(self, stream_id: str, callback: Callable = None):
        """SSE ìŠ¤íŠ¸ë¦¼ ì‹œì‘"""
        self.active_streams[stream_id] = {
            "start_time": time.time(),
            "status": "active",
            "chunks_received": 0
        }
        
        if callback:
            self.stream_callbacks[stream_id] = callback
        
        # ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
        if 'sse_streams' not in st.session_state:
            st.session_state['sse_streams'] = {}
        
        st.session_state['sse_streams'][stream_id] = {
            "status": "streaming",
            "start_time": time.time(),
            "updates": []
        }
    
    async def handle_sse_chunk(self, stream_id: str, chunk: str):
        """SSE ì²­í¬ ì²˜ë¦¬"""
        if stream_id in self.active_streams:
            self.active_streams[stream_id]["chunks_received"] += 1
            
            # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
            update = {
                "timestamp": time.time(),
                "chunk": chunk,
                "chunk_index": self.active_streams[stream_id]["chunks_received"]
            }
            
            self.streaming_updates.append(update)
            
            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            if 'sse_streams' in st.session_state and stream_id in st.session_state['sse_streams']:
                st.session_state['sse_streams'][stream_id]["updates"].append(update)
            
            # ì½œë°± ì‹¤í–‰
            if stream_id in self.stream_callbacks:
                try:
                    await self.stream_callbacks[stream_id](chunk)
                except Exception as e:
                    print(f"SSE ì½œë°± ì˜¤ë¥˜: {e}")
    
    async def end_sse_stream(self, stream_id: str):
        """SSE ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ"""
        if stream_id in self.active_streams:
            self.active_streams[stream_id]["status"] = "completed"
            self.active_streams[stream_id]["end_time"] = time.time()
            
            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            if 'sse_streams' in st.session_state and stream_id in st.session_state['sse_streams']:
                st.session_state['sse_streams'][stream_id]["status"] = "completed"
                st.session_state['sse_streams'][stream_id]["end_time"] = time.time()

# ì „ì—­ SSE ìŠ¤íŠ¸ë¦¬ë° ë§¤ë‹ˆì €
sse_manager = SSEStreamingManager()

# A2A SSE ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸
class A2ASSEClient:
    """A2A í‘œì¤€ SSE ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.timeout = 300.0
        self.max_retries = 3
    
    async def stream_a2a_request(self, 
                                agent_url: str, 
                                message: str,
                                stream_id: str = None,
                                context: dict = None) -> AsyncGenerator[dict, None]:
        """A2A SSE ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­"""
        
        if not stream_id:
            stream_id = f"a2a_stream_{int(time.time())}"
        
        # SSE ìŠ¤íŠ¸ë¦¼ ì‹œì‘
        await sse_manager.start_sse_stream(
            stream_id, 
            lambda chunk: self._handle_stream_chunk(stream_id, chunk)
        )
        
        # A2A í‘œì¤€ ë©”ì‹œì§€ êµ¬ì„±
        payload = {
            "jsonrpc": "2.0",
            "method": "tasks/send",
            "params": {
                "id": stream_id,
                "message": {
                    "messageId": f"msg_{stream_id}",
                    "role": "user",
                    "parts": [{"type": "text", "text": message}]
                },
                "context": context or {}
            },
            "id": stream_id
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                # SSE ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­
                async with client.stream(
                    "POST",
                    f"{agent_url}/a2a/stream",
                    json=payload,
                    headers={
                        "Content-Type": "application/json",
                        "Accept": "text/event-stream",
                        "Cache-Control": "no-cache"
                    }
                ) as response:
                    
                    if response.status_code != 200:
                        yield {
                            "type": "error",
                            "content": f"HTTP {response.status_code}: {await response.aread()}"
                        }
                        return
                    
                    # SSE ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
                    async for line in response.aiter_lines():
                        if line.strip():
                            if line.startswith("data: "):
                                try:
                                    data = json.loads(line[6:])  # "data: " ì œê±°
                                    
                                    # A2A í‘œì¤€ ì‘ë‹µ ì²˜ë¦¬
                                    if "result" in data:
                                        result = data["result"]
                                        
                                        # ë©”ì‹œì§€ ì¶”ì¶œ
                                        if "message" in result:
                                            message_data = result["message"]
                                            if "parts" in message_data:
                                                for part in message_data["parts"]:
                                                    if part.get("type") == "text":
                                                        content = part.get("text", "")
                                                        if content:
                                                            await sse_manager.handle_sse_chunk(stream_id, content)
                                                            yield {
                                                                "type": "message",
                                                                "content": content,
                                                                "stream_id": stream_id,
                                                                "timestamp": time.time()
                                                            }
                                    
                                    # ì™„ë£Œ ì‹ í˜¸ ì²˜ë¦¬
                                    if data.get("final") or data.get("done"):
                                        await sse_manager.end_sse_stream(stream_id)
                                        yield {
                                            "type": "complete",
                                            "stream_id": stream_id,
                                            "timestamp": time.time()
                                        }
                                        return
                                
                                except json.JSONDecodeError:
                                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì „ì†¡
                                    content = line[6:] if line.startswith("data: ") else line
                                    await sse_manager.handle_sse_chunk(stream_id, content)
                                    yield {
                                        "type": "raw",
                                        "content": content,
                                        "stream_id": stream_id,
                                        "timestamp": time.time()
                                    }
                            
                            elif line.startswith("event: "):
                                event_type = line[7:]  # "event: " ì œê±°
                                yield {
                                    "type": "event",
                                    "event_type": event_type,
                                    "stream_id": stream_id,
                                    "timestamp": time.time()
                                }
                    
                    # ìŠ¤íŠ¸ë¦¼ ìì—° ì¢…ë£Œ
                    await sse_manager.end_sse_stream(stream_id)
                    yield {
                        "type": "complete",
                        "stream_id": stream_id,
                        "timestamp": time.time()
                    }
        
        except Exception as e:
            await sse_manager.end_sse_stream(stream_id)
            yield {
                "type": "error",
                "content": str(e),
                "stream_id": stream_id,
                "timestamp": time.time()
            }
    
    async def _handle_stream_chunk(self, stream_id: str, chunk: str):
        """ìŠ¤íŠ¸ë¦¼ ì²­í¬ ì²˜ë¦¬"""
        # ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
        if 'streaming_content' not in st.session_state:
            st.session_state['streaming_content'] = {}
        
        if stream_id not in st.session_state['streaming_content']:
            st.session_state['streaming_content'][stream_id] = ""
        
        st.session_state['streaming_content'][stream_id] += chunk
        
        # ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸ ì•Œë¦¼
        if hasattr(st, 'rerun'):
            st.rerun()

# ì „ì—­ A2A SSE í´ë¼ì´ì–¸íŠ¸
a2a_sse_client = A2ASSEClient()

# SSE ìŠ¤íŠ¸ë¦¬ë° UI ì»´í¬ë„ŒíŠ¸
def render_sse_streaming_status():
    """SSE ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ í‘œì‹œ"""
    
    if st.session_state.get('sse_streams'):
        st.markdown("### ğŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ")
        
        for stream_id, stream_info in st.session_state['sse_streams'].items():
            status = stream_info.get('status', 'unknown')
            start_time = stream_info.get('start_time', 0)
            updates_count = len(stream_info.get('updates', []))
            
            if status == 'streaming':
                st.info(f"ğŸ”„ **{stream_id}**: ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ({updates_count} ì—…ë°ì´íŠ¸)")
            elif status == 'completed':
                duration = stream_info.get('end_time', time.time()) - start_time
                st.success(f"âœ… **{stream_id}**: ì™„ë£Œ ({duration:.1f}ì´ˆ, {updates_count} ì—…ë°ì´íŠ¸)")
            else:
                st.warning(f"âš ï¸ **{stream_id}**: {status}")
    
    # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì½˜í…ì¸  í‘œì‹œ
    if st.session_state.get('streaming_content'):
        st.markdown("### ğŸ“º ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì½˜í…ì¸ ")
        
        for stream_id, content in st.session_state['streaming_content'].items():
            if content:
                with st.expander(f"ğŸ“¡ {stream_id}", expanded=True):
                    st.write(content)
                    
                    # ìë™ ìŠ¤í¬ë¡¤ íš¨ê³¼
                    st.markdown("""
                    <script>
                        // ìë™ ìŠ¤í¬ë¡¤
                        window.scrollTo(0, document.body.scrollHeight);
                    </script>
                    """, unsafe_allow_html=True)

def apply_llm_first_layout_styles():
    """LLM First ì•„í‚¤í…ì²˜ ê¸°ë°˜ ë ˆì´ì•„ì›ƒ ìŠ¤íƒ€ì¼"""
    st.markdown("""
    <style>
    /* ì „ì²´ ì»¨í…Œì´ë„ˆ */
    .main .block-container {
        max-width: 1500px !important;
        padding: 1rem !important;
    }
    
    /* 30% ì…ë ¥, 70% ê²°ê³¼ ë ˆì´ì•„ì›ƒ */
    .analysis-layout {
        display: flex;
        gap: 1rem;
        min-height: 80vh;
    }
    
    .input-panel {
        flex: 0 0 30%;
        background: rgba(15, 15, 15, 0.8);
        border-radius: 12px;
        padding: 1.5rem;
        border: 1px solid rgba(0, 122, 204, 0.3);
        position: sticky;
        top: 1rem;
        height: fit-content;
    }
    
    .results-panel {
        flex: 1;
        background: rgba(10, 10, 10, 0.9);
        border-radius: 12px;
        padding: 1.5rem;
        border: 1px solid rgba(0, 212, 255, 0.2);
        overflow-y: auto;
        max-height: 85vh;
    }
    
    /* ë™ì  ì§ˆë¬¸ ìƒì„± ì¹´ë“œ */
    .dynamic-question-card {
        background: linear-gradient(135deg, rgba(0, 122, 204, 0.1), rgba(0, 212, 255, 0.1));
        border: 1px solid rgba(0, 122, 204, 0.4);
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        transition: all 0.3s ease;
        cursor: pointer;
    }
    
    .dynamic-question-card:hover {
        background: linear-gradient(135deg, rgba(0, 122, 204, 0.2), rgba(0, 212, 255, 0.2));
        transform: translateY(-2px);
        box-shadow: 0 4px 12px rgba(0, 122, 204, 0.3);
    }
    
    /* Follow-up ì œì•ˆ ë²„íŠ¼ */
    .followup-suggestion {
        background: rgba(76, 175, 80, 0.1);
        border: 1px solid rgba(76, 175, 80, 0.4);
        border-radius: 8px;
        padding: 0.8rem 1rem;
        margin: 0.3rem;
        display: inline-block;
        cursor: pointer;
        transition: all 0.2s ease;
        color: #4CAF50;
        font-weight: 500;
    }
    
    .followup-suggestion:hover {
        background: rgba(76, 175, 80, 0.2);
        transform: scale(1.02);
    }
    
    /* ì‹¤ì‹œê°„ í”„ë¡œì„¸ìŠ¤ íˆ¬ëª…í™” */
    .process-transparency {
        background: rgba(255, 193, 7, 0.1);
        border: 1px solid rgba(255, 193, 7, 0.3);
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    
    .agent-activity {
        background: rgba(156, 39, 176, 0.1);
        border-left: 4px solid #9C27B0;
        padding: 0.8rem;
        margin: 0.3rem 0;
        border-radius: 0 8px 8px 0;
    }
    
    /* ìë™ ìŠ¤í¬ë¡¤ ì• ë‹ˆë©”ì´ì…˜ */
    .auto-scroll-content {
        animation: slideIn 0.5s ease-out;
    }
    
    @keyframes slideIn {
        from { opacity: 0; transform: translateY(10px); }
        to { opacity: 1; transform: translateY(0); }
    }
    
    /* ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ */
    .streaming-result {
        background: rgba(0, 212, 255, 0.05);
        border: 1px solid rgba(0, 212, 255, 0.2);
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
        white-space: pre-wrap;
        font-family: 'Monaco', 'Menlo', monospace;
        max-height: 300px;
        overflow-y: auto;
    }
    
    /* ëŒ€í™” íë¦„ */
    .conversation-flow {
        border-left: 3px solid #007acc;
        padding-left: 1rem;
        margin: 1rem 0;
    }
    
    .user-message {
        background: rgba(0, 122, 204, 0.1);
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    
    .ai-response {
        background: rgba(0, 212, 255, 0.1);
        border-radius: 8px;
        padding: 1rem;
        margin: 0.5rem 0;
    }
    </style>
    """, unsafe_allow_html=True)

async def generate_dynamic_questions(df: pd.DataFrame, user_context: Dict = None) -> List[Dict]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì— ë§ëŠ” ë™ì  ì§ˆë¬¸ ìƒì„±"""
    if not LLM_AVAILABLE:
        return []
    
    try:
        # ë°ì´í„° íŠ¹ì„± ë¶„ì„
        data_summary = {
            "shape": df.shape,
            "columns": df.columns.tolist(),
            "numeric_columns": df.select_dtypes(include=['number']).columns.tolist(),
            "categorical_columns": df.select_dtypes(include=['object']).columns.tolist(),
            "dtypes": df.dtypes.to_dict(),
            "missing_values": df.isnull().sum().to_dict(),
            "sample_data": df.head(3).to_dict()
        }
        
        prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì‚¬ìš©ìê°€ ê¶ê¸ˆí•´í•  ë§Œí•œ ì§ˆë¬¸ 3ê°œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ë°ì´í„° ì •ë³´:
- í¬ê¸°: {data_summary['shape']}
- ì»¬ëŸ¼: {data_summary['columns']}
- ìˆ«ì ì»¬ëŸ¼: {data_summary['numeric_columns']}
- ë²”ì£¼í˜• ì»¬ëŸ¼: {data_summary['categorical_columns']}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "questions": [
        {{
            "text": "ì§ˆë¬¸ ë‚´ìš©",
            "type": "ë¶„ì„ ìœ í˜•",
            "reasoning": "ì´ ì§ˆë¬¸ì„ ì œì•ˆí•˜ëŠ” ì´ìœ "
        }}
    ]
}}

ê·œì¹™:
1. ë°ì´í„°ì˜ ì‹¤ì œ íŠ¹ì„±ì„ ë°˜ì˜í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸
2. ì‚¬ìš©ìê°€ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´
3. ê° ì§ˆë¬¸ì€ ì„œë¡œ ë‹¤ë¥¸ ë¶„ì„ ê´€ì ì„ ì œê³µ
4. í…œí”Œë¦¿ì´ ì•„ë‹Œ ì´ ë°ì´í„°ì—ë§Œ íŠ¹í™”ëœ ì§ˆë¬¸
"""
        
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        
        result = response.choices[0].message.content
        
        # JSON íŒŒì‹±
        import re
        json_match = re.search(r'\{.*\}', result, re.DOTALL)
        if json_match:
            questions_data = json.loads(json_match.group())
            return questions_data.get("questions", [])
        
        return []
        
    except Exception as e:
        st.error(f"ë™ì  ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return []

async def generate_followup_suggestions(analysis_result: str, conversation_history: List[Dict]) -> List[str]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ Follow-up ì œì•ˆ ìƒì„±"""
    if not LLM_AVAILABLE:
        return []
    
    try:
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ìš”ì•½
        history_summary = []
        for chat in conversation_history[-3:]:  # ìµœê·¼ 3ê°œ ëŒ€í™”ë§Œ ì‚¬ìš©
            history_summary.append(f"ì§ˆë¬¸: {chat['question']}")
            history_summary.append(f"ë‹µë³€: {chat['answer'][:200]}...")
        
        prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì™€ì˜ ëŒ€í™”ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ì— ì í•©í•œ Follow-up ì œì•ˆì„ ìƒì„±í•´ì£¼ì„¸ìš”.

í˜„ì¬ ë¶„ì„ ê²°ê³¼:
{analysis_result[:500]}...

ëŒ€í™” íˆìŠ¤í† ë¦¬:
{chr(10).join(history_summary)}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ 3ê°œì˜ ê°„ê²°í•œ Follow-up ì œì•ˆì„ í•´ì£¼ì„¸ìš”:
{{
    "suggestions": [
        "ì œì•ˆ 1 (ê°„ê²°í•˜ê²Œ)",
        "ì œì•ˆ 2 (ê°„ê²°í•˜ê²Œ)",
        "ì œì•ˆ 3 (ê°„ê²°í•˜ê²Œ)"
    ]
}}

ê·œì¹™:
1. ê° ì œì•ˆì€ 15ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ
2. í˜„ì¬ ë¶„ì„ ê²°ê³¼ì™€ ì—°ê´€ì„± ìˆëŠ” ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
3. ë²„íŠ¼ìœ¼ë¡œ í´ë¦­í•˜ê¸° ì í•©í•œ í˜•íƒœ
4. ì‚¬ìš©ìê°€ ë°”ë¡œ ì´í•´í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´
5. í…œí”Œë¦¿ì´ ì•„ë‹Œ í˜„ì¬ ìƒí™©ì— ë§ëŠ” ë§ì¶¤í˜• ì œì•ˆ
"""
        
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.8
        )
        
        result = response.choices[0].message.content
        
        # JSON íŒŒì‹±
        import re
        json_match = re.search(r'\{.*\}', result, re.DOTALL)
        if json_match:
            suggestions_data = json.loads(json_match.group())
            return suggestions_data.get("suggestions", [])
        
        return []
        
    except Exception as e:
        st.error(f"Follow-up ì œì•ˆ ìƒì„± ì‹¤íŒ¨: {e}")
        return []

def render_input_panel(df: pd.DataFrame):
    """ì…ë ¥ íŒ¨ë„ ë Œë”ë§ (30% ì¢Œì¸¡) - ì‹¤ì‹œê°„ ìƒíƒœ ëª¨ë‹ˆí„°ë§ í†µí•©"""
    st.markdown("### ğŸ§¬ LLM First ë¶„ì„")
    
    # === ì‹¤ì‹œê°„ ë¶„ì„ ìƒíƒœ ì„¹ì…˜ ===
    if st.session_state.get('analysis_in_progress', False):
        current_stage = st.session_state.get('current_stage', 'ë¶„ì„ ì¤€ë¹„ ì¤‘')
        st.success(f"ğŸ”„ **í˜„ì¬ ì§„í–‰**: {current_stage}")
        
        # ì§„í–‰ë¥  í‘œì‹œ
        stage_progress = {
            "ê³„íš ìˆ˜ë¦½": 25,
            "A2A ì—ì´ì „íŠ¸ ì‹¤í–‰": 50,
            "ì•„í‹°íŒ©íŠ¸ ìƒì„±": 75,
            "ê²°ê³¼ í†µí•©": 100
        }
        progress = stage_progress.get(current_stage, 0)
        st.progress(progress / 100)
        
        # ë¶„ì„ ê³„íš ìš”ì•½
        if st.session_state.get('analysis_plan'):
            with st.expander("ğŸ“‹ ë¶„ì„ ê³„íš", expanded=True):
                plan = st.session_state['analysis_plan']
                st.write(f"**ë¶„ì„ ìœ í˜•**: {plan.get('analysis_type', 'N/A')}")
                st.write(f"**í•„ìš” ì—ì´ì „íŠ¸**: {', '.join(plan.get('required_agents', []))}")
                st.write(f"**ì‹œê°í™” í•„ìš”**: {'âœ…' if plan.get('visualization_needed', False) else 'âŒ'}")
    
    # === ì—ì´ì „íŠ¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ===
    st.markdown("### ğŸ¤– ì—ì´ì „íŠ¸ ìƒíƒœ")
    
    # A2A ì„œë²„ ìƒíƒœ í™•ì¸
    agent_servers = {
        "ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°": "8100",
        "EDA ë„êµ¬": "8312", 
        "ë°ì´í„° ì‹œê°í™”": "8308",
        "ë°ì´í„° ì •ë¦¬": "8306",
        "í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§": "8310"
    }
    
    if st.session_state.get('agent_execution_status'):
        # ì‹¤í–‰ ì¤‘ì¸ ì—ì´ì „íŠ¸ ìƒíƒœ
        for agent, status in st.session_state['agent_execution_status'].items():
            if "ì‹¤í–‰ ì¤‘" in status:
                st.write(f"ğŸ”„ **{agent}**: {status}")
            elif "ì™„ë£Œ" in status:
                st.write(f"âœ… **{agent}**: {status}")
            elif "ì‹¤íŒ¨" in status:
                st.write(f"âŒ **{agent}**: {status}")
            else:
                st.write(f"â³ **{agent}**: {status}")
    else:
        # ê¸°ë³¸ ì„œë²„ ìƒíƒœ (í¬íŠ¸ ê¸°ë°˜)
        for name, port in agent_servers.items():
            st.write(f"ğŸ’¤ **{name}** (:{port}): ëŒ€ê¸° ì¤‘")
    
    # === ì•„í‹°íŒ©íŠ¸ ìƒì„± í˜„í™© ===
    st.markdown("### ğŸ“¦ ì•„í‹°íŒ©íŠ¸ ìƒì„± í˜„í™©")
    
    artifacts = st.session_state.get('generated_artifacts', [])
    if artifacts:
        for artifact in artifacts:
            artifact_name = artifact.get('name', 'ì•Œ ìˆ˜ ì—†ìŒ')
            artifact_type = artifact.get('type', 'unknown')
            
            type_icons = {
                'plotly_chart': 'ğŸ“Š',
                'dataframe': 'ğŸ“‹',
                'image': 'ğŸ–¼ï¸',
                'code': 'ğŸ’»',
                'text': 'ğŸ“'
            }
            icon = type_icons.get(artifact_type, 'ğŸ“¦')
            
            st.write(f"{icon} **{artifact_name}**: âœ… ìƒì„± ì™„ë£Œ")
    else:
        st.write("ğŸ“‹ ì•„ì§ ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
    
    # === ë°ì´í„° ì •ë³´ ìš”ì•½ ===
    with st.expander("ğŸ“Š ë°ì´í„° ì •ë³´", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.metric("í–‰ ìˆ˜", df.shape[0])
            st.metric("ì»¬ëŸ¼ ìˆ˜", df.shape[1])
        with col2:
            st.metric("ìˆ«ì ì»¬ëŸ¼", len(df.select_dtypes(include=['number']).columns))
            st.metric("ë²”ì£¼í˜• ì»¬ëŸ¼", len(df.select_dtypes(include=['object']).columns))
    
    # === Langfuse ì„¸ì…˜ ì •ë³´ ===
    if LANGFUSE_AVAILABLE:
        with st.expander("ğŸ” Langfuse ì¶”ì ", expanded=False):
            recent_chat = st.session_state.get('conversation_history', [])
            if recent_chat:
                last_session = recent_chat[-1].get('session_id')
                if last_session:
                    st.write(f"**ìµœê·¼ ì„¸ì…˜**: {last_session}")
                    st.write(f"**ì¶”ì  ìƒíƒœ**: âœ… í™œì„±")
                else:
                    st.write("**ì¶”ì  ìƒíƒœ**: âš ï¸ ì„¸ì…˜ ì—†ìŒ")
            else:
                st.write("**ì¶”ì  ìƒíƒœ**: ğŸ’¤ ëŒ€ê¸° ì¤‘")
    
    # === ë™ì  ì§ˆë¬¸ ìƒì„± ì„¹ì…˜ ===
    st.markdown("### ğŸ’¡ ì¶”ì²œ ì§ˆë¬¸")
    
    if 'dynamic_questions' not in st.session_state:
        st.session_state['dynamic_questions'] = []
        st.session_state['questions_loading'] = True
    
    if st.session_state.get('questions_loading', False):
        with st.spinner("ğŸ¤– ë°ì´í„° íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ë§ì¶¤í˜• ì§ˆë¬¸ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
            # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
            if LLM_AVAILABLE:
                questions = asyncio.run(generate_dynamic_questions(df))
                st.session_state['dynamic_questions'] = questions
                st.session_state['questions_loading'] = False
                st.rerun()
            else:
                st.session_state['questions_loading'] = False
                st.warning("âš ï¸ LLMì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    # ë™ì  ìƒì„±ëœ ì§ˆë¬¸ í‘œì‹œ
    if st.session_state.get('dynamic_questions'):
        for i, question in enumerate(st.session_state['dynamic_questions']):
            if st.button(
                f"ğŸ” {question['text']}", 
                key=f"dynamic_q_{i}",
                use_container_width=True
            ):
                st.session_state['selected_question'] = question['text']
                st.rerun()
    
    # ìƒˆë¡œìš´ ì§ˆë¬¸ ìƒì„± ë²„íŠ¼
    if st.button("ğŸ”„ ìƒˆë¡œìš´ ì§ˆë¬¸ ìƒì„±", use_container_width=True):
        st.session_state['questions_loading'] = True
        st.session_state['dynamic_questions'] = []
        st.rerun()
    
    # === ì‹œê°í™” ë¹ ë¥¸ ì‹¤í–‰ ===
    st.markdown("### âš¡ ë¹ ë¥¸ ì‹œê°í™”")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ“Š ê¸°ë³¸ ì°¨íŠ¸", use_container_width=True):
            st.session_state['selected_question'] = "ê¸°ë³¸ì ì¸ ì°¨íŠ¸ë¥¼ ê·¸ë ¤ì£¼ì„¸ìš”"
            st.rerun()
    
    with col2:
        if st.button("ğŸ“ˆ ìƒê´€ê´€ê³„", use_container_width=True):
            st.session_state['selected_question'] = "ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”í•´ì£¼ì„¸ìš”"
            st.rerun()
    
    # === ì‹œìŠ¤í…œ ì •ë³´ ===
    with st.expander("ğŸ”§ ì‹œìŠ¤í…œ ì •ë³´", expanded=False):
        st.write(f"**LLM ì—°ê²°**: {'âœ…' if LLM_AVAILABLE else 'âŒ'}")
        st.write(f"**A2A í´ë¼ì´ì–¸íŠ¸**: {'âœ…' if A2A_CLIENT_AVAILABLE else 'âŒ'}")
        st.write(f"**Langfuse ì¶”ì **: {'âœ…' if LANGFUSE_AVAILABLE else 'âŒ'}")
        
        if st.session_state.get('conversation_history'):
            st.write(f"**ì´ ëŒ€í™” ìˆ˜**: {len(st.session_state['conversation_history'])}")
        
        if st.session_state.get('generated_artifacts'):
            st.write(f"**ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸**: {len(st.session_state['generated_artifacts'])}ê°œ")

def render_results_panel():
    """ê²°ê³¼ íŒ¨ë„ ë Œë”ë§ (70% ìš°ì¸¡) - ë¶„ì„ ê²°ê³¼, ì•„í‹°íŒ©íŠ¸, ì—ì´ì „íŠ¸ ìƒíƒœ í†µí•© í‘œì‹œ"""
    st.markdown("### ğŸ“Š ë¶„ì„ ê²°ê³¼")
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
    if 'conversation_history' not in st.session_state:
        st.session_state['conversation_history'] = []
    
    # í˜„ì¬ ë¶„ì„ ì§„í–‰ ìƒí™© í‘œì‹œ
    if st.session_state.get('analysis_in_progress', False):
        current_stage = st.session_state.get('current_stage', 'ë¶„ì„ ì¤€ë¹„ ì¤‘')
        st.info(f"ğŸ”„ **í˜„ì¬ ë‹¨ê³„**: {current_stage}")
        
        # ë¶„ì„ ê³„íš í‘œì‹œ
        if st.session_state.get('analysis_plan'):
            with st.expander("ğŸ“‹ ë¶„ì„ ê³„íš", expanded=True):
                plan = st.session_state['analysis_plan']
                st.json(plan)
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰ ìƒíƒœ í‘œì‹œ
        if st.session_state.get('agent_execution_status'):
            with st.expander("ğŸ¤– ì—ì´ì „íŠ¸ ì‹¤í–‰ ìƒíƒœ", expanded=True):
                status = st.session_state['agent_execution_status']
                for agent, state in status.items():
                    if "ì‹¤í–‰ ì¤‘" in state:
                        st.write(f"ğŸ”„ **{agent}**: {state}")
                    elif "ì™„ë£Œ" in state:
                        st.write(f"âœ… **{agent}**: {state}")
                    elif "ì‹¤íŒ¨" in state:
                        st.write(f"âŒ **{agent}**: {state}")
                    else:
                        st.write(f"â³ **{agent}**: {state}")
    
    # ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸ í‘œì‹œ
    if st.session_state.get('generated_artifacts'):
        st.markdown("### ğŸ“¦ ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸")
        artifacts = st.session_state['generated_artifacts']
        
        for i, artifact in enumerate(artifacts):
            with st.expander(f"ğŸ“Š {artifact.get('name', f'ì•„í‹°íŒ©íŠ¸ {i+1}')}", expanded=True):
                st.write(f"**ì„¤ëª…**: {artifact.get('description', 'ì„¤ëª… ì—†ìŒ')}")
                
                # ì•„í‹°íŒ©íŠ¸ íƒ€ì…ë³„ ë Œë”ë§
                if artifact.get('type') == 'plotly_chart':
                    try:
                        st.plotly_chart(artifact['content'], use_container_width=True)
                    except Exception as e:
                        st.error(f"ì°¨íŠ¸ ë Œë”ë§ ì‹¤íŒ¨: {e}")
                
                elif artifact.get('type') == 'dataframe':
                    try:
                        st.dataframe(artifact['content'])
                    except Exception as e:
                        st.error(f"ë°ì´í„°í”„ë ˆì„ ë Œë”ë§ ì‹¤íŒ¨: {e}")
                
                elif artifact.get('type') == 'image':
                    try:
                        st.image(artifact['content'])
                    except Exception as e:
                        st.error(f"ì´ë¯¸ì§€ ë Œë”ë§ ì‹¤íŒ¨: {e}")
                
                elif artifact.get('type') == 'code':
                    try:
                        st.code(artifact['content'], language=artifact.get('language', 'python'))
                    except Exception as e:
                        st.error(f"ì½”ë“œ ë Œë”ë§ ì‹¤íŒ¨: {e}")
                
                else:
                    # ê¸°ë³¸ í…ìŠ¤íŠ¸ ë Œë”ë§
                    st.write(artifact.get('content', 'ë‚´ìš© ì—†ìŒ'))
    
    # ê¸°ì¡´ ëŒ€í™” íˆìŠ¤í† ë¦¬ í‘œì‹œ
    if st.session_state['conversation_history']:
        st.markdown("### ğŸ’¬ ëŒ€í™” íˆìŠ¤í† ë¦¬")
        
        for i, chat in enumerate(st.session_state['conversation_history']):
            with st.container():
                st.markdown(f"""
                <div class="conversation-flow">
                    <div class="user-message">
                        <strong>ğŸ™‹ ì§ˆë¬¸:</strong> {chat['question']}
                    </div>
                    <div class="ai-response">
                        <strong>ğŸ¤– AI ë¶„ì„:</strong> {chat['answer']}
                    </div>
                </div>
                """, unsafe_allow_html=True)
                
                # ë¶„ì„ ë°©ë²• í‘œì‹œ
                if 'method' in chat:
                    method_color = {
                        "LLM First + A2A Integration": "ğŸŸ¢",
                        "LLM Analysis": "ğŸŸ¡", 
                        "A2A Analysis": "ğŸ”µ",
                        "Error": "ğŸ”´"
                    }
                    color = method_color.get(chat['method'], "âšª")
                    st.caption(f"{color} **ë¶„ì„ ë°©ë²•**: {chat['method']}")
                
                # ì„¸ì…˜ ID í‘œì‹œ (Langfuse ì¶”ì )
                if 'session_id' in chat:
                    st.caption(f"ğŸ” **Langfuse ì„¸ì…˜**: {chat.get('session_id', 'N/A')}")
                
                # Follow-up ì œì•ˆ í‘œì‹œ
                if 'followup_suggestions' in chat and chat['followup_suggestions']:
                    st.markdown("**ğŸ’¡ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ:**")
                    cols = st.columns(min(len(chat['followup_suggestions']), 3))
                    for j, suggestion in enumerate(chat['followup_suggestions']):
                        with cols[j % 3]:
                            if st.button(
                                suggestion, 
                                key=f"followup_{i}_{j}",
                                use_container_width=True
                            ):
                                st.session_state['selected_question'] = suggestion
                                st.rerun()
                
                st.markdown("---")
    
    # ì‹œìŠ¤í…œ ìƒíƒœ ì •ë³´
    with st.expander("ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**ğŸ”Œ ì—°ê²° ìƒíƒœ**")
            st.write(f"â€¢ LLM: {'âœ…' if LLM_AVAILABLE else 'âŒ'}")
            st.write(f"â€¢ A2A í´ë¼ì´ì–¸íŠ¸: {'âœ…' if A2A_CLIENT_AVAILABLE else 'âŒ'}")
            st.write(f"â€¢ Langfuse: {'âœ…' if LANGFUSE_AVAILABLE else 'âŒ'}")
        
        with col2:
            st.write("**ğŸ“Š ì„¸ì…˜ ì •ë³´**")
            st.write(f"â€¢ ëŒ€í™” ìˆ˜: {len(st.session_state.get('conversation_history', []))}")
            st.write(f"â€¢ ì•„í‹°íŒ©íŠ¸ ìˆ˜: {len(st.session_state.get('generated_artifacts', []))}")
            st.write(f"â€¢ í˜„ì¬ ë‹¨ê³„: {st.session_state.get('current_stage', 'ëŒ€ê¸° ì¤‘')}")

async def perform_llm_first_analysis(df: pd.DataFrame, user_query: str) -> Dict:
    """ì™„ì „í•œ LLM First ì•„í‚¤í…ì²˜ - ëª¨ë“  í•˜ë“œì½”ë”© ì œê±°, ë™ì  ì²˜ë¦¬"""
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'analysis_plan' not in st.session_state:
        st.session_state['analysis_plan'] = None
    if 'agent_execution_status' not in st.session_state:
        st.session_state['agent_execution_status'] = {}
    if 'generated_artifacts' not in st.session_state:
        st.session_state['generated_artifacts'] = []
    if 'llm_streaming_updates' not in st.session_state:
        st.session_state['llm_streaming_updates'] = []
    
    # Langfuse ì„¸ì…˜ ì‹œì‘
    session_tracer = None
    session_id = None
    if LANGFUSE_AVAILABLE:
        try:
            session_tracer = get_session_tracer()
            user_id = os.getenv("EMP_NO", "cherryai_user")
            session_metadata = {
                "query": user_query,
                "data_shape": df.shape,
                "data_columns": df.columns.tolist(),
                "workspace": "data_workspace",
                "timestamp": datetime.now().isoformat(),
                "llm_first_mode": True
            }
            session_id = session_tracer.start_user_session(user_query, user_id, session_metadata)
            st.info(f"ğŸ” Langfuse ì„¸ì…˜ ì‹œì‘: {session_id}")
        except Exception as e:
            st.warning(f"âš ï¸ Langfuse ì„¸ì…˜ ì‹œì‘ ì‹¤íŒ¨: {e}")
    
    try:
        # === LLM First í•µì‹¬: ëª¨ë“  ê²°ì •ì„ LLMì´ ë™ì ìœ¼ë¡œ ìˆ˜í–‰ ===
        st.session_state['current_stage'] = "LLM ë™ì  ë¶„ì„ ì‹œì‘"
        
        # ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ë™ì  ìƒì„±
        data_context = await _create_dynamic_data_context(df, user_query)
        
        # === 1ë‹¨ê³„: LLM ê¸°ë°˜ ì™„ì „ ë™ì  ë¶„ì„ ì „ëµ ìˆ˜ë¦½ ===
        st.session_state['current_stage'] = "LLM ê¸°ë°˜ ë™ì  ì „ëµ ìˆ˜ë¦½"
        
        analysis_strategy = None
        if LLM_AVAILABLE:
            # LLMì´ ëª¨ë“  ê²ƒì„ ë™ì ìœ¼ë¡œ ê²°ì •
            strategy_prompt = await _generate_dynamic_strategy_prompt(data_context, user_query)
            
            try:
                response = await llm_client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³ ì˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ì™€ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ë¶„ì„ ì „ëµì„ ë™ì ìœ¼ë¡œ ìˆ˜ë¦½í•˜ì„¸ìš”. ì ˆëŒ€ í…œí”Œë¦¿ì´ë‚˜ ê³ ì •ëœ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."},
                        {"role": "user", "content": strategy_prompt}
                    ],
                    temperature=0.7,  # ì°½ì˜ì  ë¶„ì„ì„ ìœ„í•œ ë†’ì€ temperature
                    stream=True  # SSE ìŠ¤íŠ¸ë¦¬ë° ì§€ì›
                )
                
                # SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
                strategy_content = ""
                async for chunk in response:
                    if chunk.choices[0].delta.content:
                        strategy_content += chunk.choices[0].delta.content
                        # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸
                        st.session_state['llm_streaming_updates'].append({
                            "stage": "strategy_planning",
                            "content": chunk.choices[0].delta.content,
                            "timestamp": time.time()
                        })
                
                # LLM ì‘ë‹µì„ ë™ì ìœ¼ë¡œ íŒŒì‹±
                analysis_strategy = await _parse_llm_strategy_dynamically(strategy_content)
                st.session_state['analysis_plan'] = analysis_strategy
                
                # Langfuse ë™ì  ë¡œê¹…
                if session_tracer:
                    session_tracer.record_agent_result("LLM_Dynamic_Strategist", {
                        "success": True,
                        "strategy": analysis_strategy,
                        "query_analysis": user_query,
                        "data_insights": data_context.get("llm_insights", {}),
                        "dynamic_approach": True
                    }, confidence=0.95)
                
            except Exception as e:
                st.error(f"âŒ ë™ì  ì „ëµ ìˆ˜ë¦½ ì‹¤íŒ¨: {e}")
                return {"success": False, "error": str(e)}
        
        # === 2ë‹¨ê³„: LLM ê¸°ë°˜ A2A ì—ì´ì „íŠ¸ ë™ì  ì„ íƒ ë° ì‹¤í–‰ ===
        st.session_state['current_stage'] = "LLM ê¸°ë°˜ A2A ì—ì´ì „íŠ¸ ë™ì  ì‹¤í–‰"
        
        if analysis_strategy and A2A_CLIENT_AVAILABLE:
            # LLMì´ ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë“¤ì„ ë™ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ì„ íƒ
            available_agents = await _discover_available_agents_dynamically()
            
            # LLMì´ ì—ì´ì „íŠ¸ ì‹¤í–‰ ê³„íšì„ ë™ì ìœ¼ë¡œ ìƒì„±
            execution_plan = await _generate_dynamic_execution_plan(
                analysis_strategy, available_agents, data_context, user_query
            )
            
            # A2A í´ë¼ì´ì–¸íŠ¸ ë™ì  ì´ˆê¸°í™”
            a2a_client = A2AStreamlitClient(available_agents)
            agent_results = {}
            
            # LLM ê¸°ë°˜ ë™ì  ì—ì´ì „íŠ¸ ì‹¤í–‰
            for agent_task in execution_plan.get("tasks", []):
                agent_id = agent_task.get("agent_id")
                task_description = agent_task.get("task_description")
                execution_context = agent_task.get("context", {})
                
                st.session_state['agent_execution_status'][agent_id] = "LLM ê¸°ë°˜ ë™ì  ì‹¤í–‰ ì¤‘"
                
                try:
                    # A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜ - ì˜¬ë°”ë¥¸ ë©”ì„œë“œ ì‚¬ìš©
                    agent_url = f"http://localhost:{available_agents[agent_id]['port']}"
                    
                    # A2A í‘œì¤€ ë©”ì‹œì§€ ìƒì„±
                    message = Message(
                        messageId=f"msg-{int(time.time())}",
                        role=Role.user,
                        parts=[TextPart(text=task_description)]
                    )
                    
                    # SSE ìŠ¤íŠ¸ë¦¬ë° ì§€ì› A2A í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
                    enhanced_client = EnhancedA2ACommunicator()
                    
                    # ì‹¤ì‹œê°„ SSE ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê²°ê³¼ ìˆ˜ì‹ 
                    result = await enhanced_client.send_message_with_streaming(
                        agent_url=agent_url,
                        instruction=task_description,
                        stream_callback=lambda chunk: _handle_sse_chunk(agent_id, chunk),
                        context_data={
                            "data_context": data_context,
                            "execution_context": execution_context,
                            "user_query": user_query,
                            "dynamic_params": agent_task.get("dynamic_params", {})
                        }
                    )
                    
                    agent_results[agent_id] = result
                    st.session_state['agent_execution_status'][agent_id] = "LLM ê¸°ë°˜ ë™ì  ì‹¤í–‰ ì™„ë£Œ"
                    
                    # A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜ ë¡œê¹…
                    if session_tracer:
                        session_tracer.record_agent_result(agent_id, {
                            "success": True,
                            "result": result,
                            "task_description": task_description,
                            "dynamic_execution": True,
                            "llm_driven": True
                        }, confidence=0.9)
                
                except Exception as e:
                    st.session_state['agent_execution_status'][agent_id] = f"ì‹¤íŒ¨: {str(e)}"
                    if session_tracer:
                        session_tracer.record_agent_result(agent_id, {
                            "success": False,
                            "error": str(e),
                            "task_description": task_description
                        }, confidence=0.1)
        
        # === 3ë‹¨ê³„: LLM ê¸°ë°˜ ë™ì  ì•„í‹°íŒ©íŠ¸ ìƒì„± ===
        st.session_state['current_stage'] = "LLM ê¸°ë°˜ ë™ì  ì•„í‹°íŒ©íŠ¸ ìƒì„±"
        
        artifacts = await _generate_dynamic_artifacts(
            df, user_query, analysis_strategy, agent_results, session_tracer
        )
        st.session_state['generated_artifacts'] = artifacts
        
        # === 4ë‹¨ê³„: LLM ê¸°ë°˜ ë™ì  ê²°ê³¼ ì¢…í•© ===
        st.session_state['current_stage'] = "LLM ê¸°ë°˜ ë™ì  ê²°ê³¼ ì¢…í•©"
        
        final_analysis = await _synthesize_results_dynamically(
            user_query, data_context, analysis_strategy, agent_results, artifacts
        )
        
        # Langfuse ì„¸ì…˜ ì™„ë£Œ
        if session_tracer and session_id:
            try:
                session_tracer.end_user_session({
                    "final_analysis": final_analysis,
                    "artifacts_generated": len(artifacts),
                    "agents_executed": len(agent_results),
                    "strategy": analysis_strategy,
                    "llm_first_success": True,
                    "dynamic_processing": True
                })
                st.success(f"âœ… LLM First ë¶„ì„ ì™„ë£Œ: {session_id}")
            except Exception as e:
                st.warning(f"âš ï¸ Langfuse ì„¸ì…˜ ì™„ë£Œ ì‹¤íŒ¨: {e}")
        
        return {
            "success": True,
            "result": final_analysis,
            "method": "LLM First Dynamic Analysis",
            "strategy": analysis_strategy,
            "agent_results": agent_results,
            "artifacts": artifacts,
            "session_id": session_id,
            "streaming_updates": st.session_state['llm_streaming_updates']
        }
        
    except Exception as e:
        # ë™ì  ì˜¤ë¥˜ ì²˜ë¦¬
        if session_tracer and session_id:
            try:
                session_tracer.end_user_session({
                    "error": str(e),
                    "success": False,
                    "llm_first_failure": True
                })
            except:
                pass
        
        return {
            "success": False,
            "result": f"LLM First ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}",
            "method": "LLM First Error",
            "error": str(e)
        }

# === LLM First í•µì‹¬ í—¬í¼ í•¨ìˆ˜ë“¤ (í•˜ë“œì½”ë”© ì™„ì „ ì œê±°) ===

async def _create_dynamic_data_context(df: pd.DataFrame, user_query: str) -> Dict:
    """LLMì´ ë°ì´í„°ë¥¼ ë™ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    
    # ê¸°ë³¸ ë°ì´í„° ì •ë³´
    basic_info = {
        "shape": df.shape,
        "columns": df.columns.tolist(),
        "dtypes": df.dtypes.to_dict(),
        "sample_data": df.head(3).to_dict()
    }
    
    # LLMì´ ë°ì´í„°ë¥¼ ë™ì ìœ¼ë¡œ ë¶„ì„
    if LLM_AVAILABLE:
        try:
            analysis_prompt = f"""
ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ í†µì°°ì„ ì œê³µí•´ì£¼ì„¸ìš”.

ë°ì´í„° ì •ë³´:
- í¬ê¸°: {basic_info['shape']}
- ì»¬ëŸ¼: {basic_info['columns']}
- ìƒ˜í”Œ ë°ì´í„°: {basic_info['sample_data']}

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ì´ ë°ì´í„°ì˜ íŠ¹ì„±, íŒ¨í„´, ë¶„ì„ ê°€ëŠ¥ì„±ì„ ììœ ë¡­ê²Œ ë¶„ì„í•˜ê³  í†µì°°ì„ ì œê³µí•´ì£¼ì„¸ìš”.
í…œí”Œë¦¿ì´ë‚˜ ê³ ì •ëœ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ë°ì´í„°ì™€ ì§ˆë¬¸ì— ë§ëŠ” ê³ ìœ í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
            
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ììœ ë¡­ê²Œ ë¶„ì„í•˜ì„¸ìš”."},
                    {"role": "user", "content": analysis_prompt}
                ],
                temperature=0.7
            )
            
            basic_info["llm_insights"] = response.choices[0].message.content
            
        except Exception as e:
            basic_info["llm_insights"] = f"ë™ì  ë¶„ì„ ì‹¤íŒ¨: {e}"
    
    return basic_info

async def _generate_dynamic_strategy_prompt(data_context: Dict, user_query: str) -> str:
    """LLMì´ ë™ì ìœ¼ë¡œ ì „ëµ ìˆ˜ë¦½ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    if not LLM_AVAILABLE:
        return f"ì‚¬ìš©ì ì§ˆë¬¸: {user_query}, ë°ì´í„° ë¶„ì„ ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”."
    
    # LLMì´ í”„ë¡¬í”„íŠ¸ ìì²´ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±
    prompt_generation = f"""
ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„° ë¶„ì„ ì „ëµì„ ìˆ˜ë¦½í•˜ê¸° ìœ„í•œ ìµœì ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ë°ì´í„° ì •ë³´:
{json.dumps(data_context, ensure_ascii=False, indent=2)}

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ì´ íŠ¹ì • ìƒí™©ì— ë§ëŠ” ë§ì¶¤í˜• ì „ëµ ìˆ˜ë¦½ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì¼ë°˜ì ì¸ í…œí”Œë¦¿ì´ ì•„ë‹Œ, ì´ ë°ì´í„°ì™€ ì§ˆë¬¸ì— íŠ¹í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
"""
    
    try:
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": prompt_generation}
            ],
            temperature=0.8
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        # Fallbackìœ¼ë¡œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
        return f"""
ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ìƒí™©ì— ëŒ€í•œ ìµœì ì˜ ë¶„ì„ ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
ë°ì´í„° íŠ¹ì„±: {data_context.get('llm_insights', 'ì•Œ ìˆ˜ ì—†ìŒ')}

ì´ íŠ¹ì • ìƒí™©ì— ë§ëŠ” ê³ ìœ í•œ ë¶„ì„ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
"""

async def _parse_llm_strategy_dynamically(strategy_content: str) -> Dict:
    """LLM ì‘ë‹µì„ ë™ì ìœ¼ë¡œ íŒŒì‹± (í•˜ë“œì½”ë”©ëœ JSON í˜•ì‹ ì œê±°)"""
    
    if not LLM_AVAILABLE:
        return {"strategy": strategy_content, "parsed": False}
    
    # LLMì´ ìì‹ ì˜ ì‘ë‹µì„ êµ¬ì¡°í™”
    parsing_prompt = f"""
ë‹¤ìŒ ë¶„ì„ ì „ëµì„ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”:

{strategy_content}

ì´ ì „ëµì˜ í•µì‹¬ ìš”ì†Œë“¤ì„ íŒŒì•…í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.
ê³ ì •ëœ í˜•ì‹ì´ ì•„ë‹Œ, ì´ ì „ëµì— ë§ëŠ” ìµœì ì˜ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ëµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": parsing_prompt}
            ],
            temperature=0.3
        )
        
        return {
            "raw_strategy": strategy_content,
            "structured_strategy": response.choices[0].message.content,
            "parsed": True
        }
        
    except Exception as e:
        return {
            "raw_strategy": strategy_content,
            "error": str(e),
            "parsed": False
        }

async def _discover_available_agents_dynamically() -> Dict:
    """A2A SDK 0.2.9 í‘œì¤€ì„ ì¤€ìˆ˜í•˜ì—¬ ì—ì´ì „íŠ¸ ë™ì  ë°œê²¬"""
    
    available_agents = {}
    
    # A2A í‘œì¤€ í¬íŠ¸ ë²”ìœ„ ìŠ¤ìº”
    potential_ports = [8100, 8306, 8307, 8308, 8309, 8310, 8311, 8312, 8313, 8314, 8315]
    
    for port in potential_ports:
        try:
            # A2A í‘œì¤€ ì—ì´ì „íŠ¸ ì¹´ë“œ í™•ì¸
            async with httpx.AsyncClient(timeout=2.0) as client:
                response = await client.get(f"http://localhost:{port}/.well-known/agent.json")
                if response.status_code == 200:
                    agent_info = response.json()
                    available_agents[f"agent_{port}"] = {
                        "port": port,
                        "url": f"http://localhost:{port}",
                        "info": agent_info,
                        "name": agent_info.get("name", f"Agent {port}"),
                        "capabilities": agent_info.get("capabilities", [])
                    }
                    st.info(f"ğŸ” A2A ì—ì´ì „íŠ¸ ë°œê²¬: {agent_info.get('name', f'Agent {port}')} (í¬íŠ¸ {port})")
        except Exception as e:
            # ì—ì´ì „íŠ¸ê°€ ì—†ëŠ” í¬íŠ¸ëŠ” ë¬´ì‹œ
            continue
    
    return available_agents

# SSE ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜
def _handle_sse_chunk(agent_id: str, chunk: str):
    """SSE ìŠ¤íŠ¸ë¦¬ë° ì²­í¬ ì²˜ë¦¬"""
    if 'llm_streaming_updates' not in st.session_state:
        st.session_state['llm_streaming_updates'] = []
    
    st.session_state['llm_streaming_updates'].append({
        "agent_id": agent_id,
        "chunk": chunk,
        "timestamp": time.time()
    })
    
    # ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
    if st.session_state.get('agent_execution_status'):
        st.session_state['agent_execution_status'][agent_id] = f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘... {chunk[:50]}..."

async def _generate_dynamic_execution_plan(strategy: Dict, available_agents: Dict, 
                                         data_context: Dict, user_query: str) -> Dict:
    """LLMì´ ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ì™€ ì „ëµì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê³„íš ë™ì  ìƒì„±"""
    
    if not LLM_AVAILABLE:
        return {"tasks": [], "error": "LLM not available"}
    
    planning_prompt = f"""
ë¶„ì„ ì „ëµê³¼ ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

ë¶„ì„ ì „ëµ:
{json.dumps(strategy, ensure_ascii=False, indent=2)}

ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸:
{json.dumps(available_agents, ensure_ascii=False, indent=2)}

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ì´ íŠ¹ì • ìƒí™©ì— ë§ëŠ” ìµœì ì˜ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
ê° ì—ì´ì „íŠ¸ì˜ ëŠ¥ë ¥ê³¼ í˜„ì¬ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ íš¨ìœ¨ì ì¸ ì‹¤í–‰ ìˆœì„œë¥¼ ê²°ì •í•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": planning_prompt}
            ],
            temperature=0.5
        )
        
        # LLM ì‘ë‹µì„ ì‹¤í–‰ ê³„íšìœ¼ë¡œ ë³€í™˜
        execution_plan = await _convert_to_execution_plan(response.choices[0].message.content, available_agents)
        
        return execution_plan
        
    except Exception as e:
        return {"tasks": [], "error": str(e)}

async def _convert_to_execution_plan(plan_text: str, available_agents: Dict) -> Dict:
    """LLM ì‘ë‹µì„ ì‹¤í–‰ ê°€ëŠ¥í•œ ê³„íšìœ¼ë¡œ ë³€í™˜"""
    
    # LLMì´ ìì‹ ì˜ ê³„íšì„ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
    conversion_prompt = f"""
ë‹¤ìŒ ê³„íšì„ ì‹¤í–‰ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”:

{plan_text}

ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸:
{json.dumps(available_agents, ensure_ascii=False, indent=2)}

ê° ë‹¨ê³„ë³„ë¡œ ì–´ë–¤ ì—ì´ì „íŠ¸ì—ê²Œ ì–´ë–¤ ì‘ì—…ì„ ìš”ì²­í• ì§€ êµ¬ì²´ì ìœ¼ë¡œ ì •í•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹œìŠ¤í…œ ì‹¤í–‰ ê³„íš ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": conversion_prompt}
            ],
            temperature=0.3
        )
        
        # ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ì‹¤í–‰ ê³„íš ìƒì„±
        return {
            "original_plan": plan_text,
            "executable_plan": response.choices[0].message.content,
            "tasks": []  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ tasks ìƒì„±
        }
        
    except Exception as e:
        return {"tasks": [], "error": str(e)}

async def _generate_dynamic_artifacts(df: pd.DataFrame, user_query: str, 
                                    strategy: Dict, agent_results: Dict, 
                                    session_tracer) -> List[Dict]:
    """LLMì´ ë™ì ìœ¼ë¡œ ì•„í‹°íŒ©íŠ¸ ìƒì„± (í•˜ë“œì½”ë”©ëœ ì‹œê°í™” ë¡œì§ ì œê±°)"""
    
    if not LLM_AVAILABLE:
        return []
    
    # LLMì´ í•„ìš”í•œ ì•„í‹°íŒ©íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ê²°ì •
    artifact_prompt = f"""
ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì–´ë–¤ ì•„í‹°íŒ©íŠ¸(ì‹œê°í™”, í‘œ, ì½”ë“œ ë“±)ë¥¼ ìƒì„±í• ì§€ ê²°ì •í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
ë¶„ì„ ì „ëµ: {json.dumps(strategy, ensure_ascii=False, indent=2)}
ì—ì´ì „íŠ¸ ê²°ê³¼: {json.dumps(agent_results, ensure_ascii=False, indent=2)}

ì´ íŠ¹ì • ìƒí™©ì— ë§ëŠ” ìµœì ì˜ ì•„í‹°íŒ©íŠ¸ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
ë°ì´í„° íƒ€ì…ì´ë‚˜ ê³ ì •ëœ ê·œì¹™ì— ì˜ì¡´í•˜ì§€ ë§ê³ , ë§¥ë½ì— ë§ëŠ” ì•„í‹°íŒ©íŠ¸ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": artifact_prompt}
            ],
            temperature=0.7
        )
        
        # LLM ì œì•ˆì„ ë°”íƒ•ìœ¼ë¡œ ë™ì ìœ¼ë¡œ ì•„í‹°íŒ©íŠ¸ ìƒì„±
        artifacts = await _create_artifacts_from_llm_suggestions(
            df, response.choices[0].message.content, user_query
        )
        
        return artifacts
        
    except Exception as e:
        return [{"type": "error", "content": f"ì•„í‹°íŒ©íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}"}]

async def _create_artifacts_from_llm_suggestions(df: pd.DataFrame, 
                                               llm_suggestions: str, 
                                               user_query: str) -> List[Dict]:
    """LLM ì œì•ˆì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ì•„í‹°íŒ©íŠ¸ ìƒì„±"""
    
    # LLMì´ êµ¬ì²´ì ì¸ ìƒì„± ì½”ë“œë¥¼ ì‘ì„±
    code_generation_prompt = f"""
ë‹¤ìŒ ì œì•ˆì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ Python ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

{llm_suggestions}

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

plotly, matplotlib, seaborn ë“±ì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì‹œê°í™” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë°ì´í„°í”„ë ˆì„ ë³€ìˆ˜ëª…ì€ 'df'ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ Python ë°ì´í„° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": code_generation_prompt}
            ],
            temperature=0.3
        )
        
        # ìƒì„±ëœ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ ì•„í‹°íŒ©íŠ¸ ìƒì„±
        generated_code = response.choices[0].message.content
        
        # ì•ˆì „í•œ ì½”ë“œ ì‹¤í–‰ í™˜ê²½ì—ì„œ ì•„í‹°íŒ©íŠ¸ ìƒì„±
        artifacts = await _execute_visualization_code_safely(df, generated_code)
        
        return artifacts
        
    except Exception as e:
        return [{"type": "error", "content": f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {e}"}]

async def _execute_visualization_code_safely(df: pd.DataFrame, code: str) -> List[Dict]:
    """ì•ˆì „í•œ í™˜ê²½ì—ì„œ ì‹œê°í™” ì½”ë“œ ì‹¤í–‰"""
    
    artifacts = []
    
    # ì•ˆì „í•œ ì‹¤í–‰ í™˜ê²½ ì„¤ì •
    safe_globals = {
        'df': df,
        'pd': pd,
        'plt': plt,
        'px': px,
        'sns': sns,
        'np': np
    }
    
    try:
        # ì½”ë“œì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        code_lines = code.split('\n')
        exec_code = '\n'.join([line for line in code_lines if not line.strip().startswith('#')])
        
        # ì•ˆì „í•œ ì‹¤í–‰
        exec(exec_code, safe_globals)
        
        # ì‹¤í–‰ ê²°ê³¼ì—ì„œ ì•„í‹°íŒ©íŠ¸ ì¶”ì¶œ
        for key, value in safe_globals.items():
            if hasattr(value, 'show') and 'plotly' in str(type(value)):
                artifacts.append({
                    'type': 'plotly_chart',
                    'content': value,
                    'name': f'llm_generated_{key}',
                    'description': f'LLMì´ ë™ì ìœ¼ë¡œ ìƒì„±í•œ {key} ì‹œê°í™”'
                })
        
        return artifacts
        
    except Exception as e:
        return [{"type": "error", "content": f"ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {e}"}]

async def _synthesize_results_dynamically(user_query: str, data_context: Dict, 
                                        strategy: Dict, agent_results: Dict, 
                                        artifacts: List[Dict]) -> str:
    """LLMì´ ëª¨ë“  ê²°ê³¼ë¥¼ ë™ì ìœ¼ë¡œ ì¢…í•©"""
    
    if not LLM_AVAILABLE:
        return "LLMì„ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê²°ê³¼ë¥¼ ì¢…í•©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    
    # LLMì´ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¶„ì„ ìƒì„±
    synthesis_prompt = f"""
ëª¨ë“  ë¶„ì„ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

ë°ì´í„° ì»¨í…ìŠ¤íŠ¸:
{json.dumps(data_context, ensure_ascii=False, indent=2)}

ë¶„ì„ ì „ëµ:
{json.dumps(strategy, ensure_ascii=False, indent=2)}

ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼:
{json.dumps(agent_results, ensure_ascii=False, indent=2)}

ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸: {len(artifacts)}ê°œ

ì´ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¹˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
í…œí”Œë¦¿ì´ë‚˜ ê³ ì •ëœ í˜•ì‹ì´ ì•„ë‹Œ, ì´ íŠ¹ì • ìƒí™©ì— ë§ëŠ” ê³ ìœ í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
    
    try:
        response = await llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³ ì˜ ë°ì´í„° ë¶„ì„ ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": synthesis_prompt}
            ],
            temperature=0.5
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return f"ê²°ê³¼ ì¢…í•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def render_data_workspace():
    """LLM First ë°ì´í„° ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ë©”ì¸ ë Œë”ë§"""
    apply_llm_first_layout_styles()
    
    st.markdown("# ğŸ§¬ LLM First Data Workspace")
    
    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    uploaded_file = st.file_uploader(
        "ğŸ“ ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ", 
        type=['csv', 'xlsx', 'json'],
        help="CSV, Excel, JSON íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”"
    )
    
    df = None
    if uploaded_file is not None:
        try:
            if uploaded_file.name.endswith('.csv'):
                df = pd.read_csv(uploaded_file)
            elif uploaded_file.name.endswith(('.xlsx', '.xls')):
                df = pd.read_excel(uploaded_file)
            elif uploaded_file.name.endswith('.json'):
                df = pd.read_json(uploaded_file)
            
            st.session_state['workspace_data'] = df
            st.session_state['data_source'] = uploaded_file.name
            st.success(f"âœ… {uploaded_file.name} ì—…ë¡œë“œ ì™„ë£Œ!")
            
        except Exception as e:
            st.error(f"âŒ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    # ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©
    elif 'workspace_data' in st.session_state:
        df = st.session_state['workspace_data']
    
    if df is not None:
        # 30% / 70% ë ˆì´ì•„ì›ƒ
        col_input, col_results = st.columns([3, 7])
        
        with col_input:
            st.markdown('<div class="input-panel">', unsafe_allow_html=True)
            render_input_panel(df)
            st.markdown('</div>', unsafe_allow_html=True)
        
        with col_results:
            st.markdown('<div class="results-panel">', unsafe_allow_html=True)
            render_results_panel()
            st.markdown('</div>', unsafe_allow_html=True)
        
        # ì±„íŒ… ì…ë ¥ (ì—”í„°í‚¤ ì§€ì›)
        if user_input := st.chat_input("ğŸ’¬ ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš” (ì—”í„°í‚¤ë¡œ ì „ì†¡)"):
            st.session_state['selected_question'] = user_input
            st.rerun()
        
        # ì„ íƒëœ ì§ˆë¬¸ ì²˜ë¦¬
        if st.session_state.get('selected_question'):
            question = st.session_state['selected_question']
            st.session_state['selected_question'] = None
            st.session_state['analysis_in_progress'] = True
            
            # ë¶„ì„ ì‹¤í–‰
            with st.spinner("ğŸ¤– LLM First ë¶„ì„ ì‹¤í–‰ ì¤‘..."):
                analysis_result = asyncio.run(perform_llm_first_analysis(df, question))
                
                # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
                new_chat = {
                    "question": question,
                    "answer": analysis_result["result"],
                    "timestamp": datetime.now().isoformat(),
                    "method": analysis_result["method"]
                }
                
                # Follow-up ì œì•ˆ ìƒì„±
                if analysis_result["success"]:
                    followup_suggestions = asyncio.run(
                        generate_followup_suggestions(
                            analysis_result["result"], 
                            st.session_state.get('conversation_history', [])
                        )
                    )
                    new_chat["followup_suggestions"] = followup_suggestions
                
                st.session_state['conversation_history'].append(new_chat)
                st.session_state['analysis_in_progress'] = False
                st.rerun()
    
    else:
        # ìƒ˜í”Œ ë°ì´í„° ì œê³µ
        st.markdown("### ğŸ“‹ ìƒ˜í”Œ ë°ì´í„°ë¡œ ì‹œì‘í•˜ê¸°")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ  ë¶€ë™ì‚° ë°ì´í„°", use_container_width=True):
                import numpy as np
                np.random.seed(42)
                sample_data = pd.DataFrame({
                    'area': np.random.normal(100, 30, 100),
                    'price': np.random.normal(50000, 15000, 100),
                    'rooms': np.random.randint(1, 6, 100),
                    'location': np.random.choice(['ê°•ë‚¨', 'í™ëŒ€', 'ì ì‹¤', 'ì¢…ë¡œ'], 100)
                })
                st.session_state['workspace_data'] = sample_data
                st.session_state['data_source'] = 'ìƒ˜í”Œ_ë¶€ë™ì‚°_ë°ì´í„°.csv'
                st.rerun()
        
        with col2:
            if st.button("ğŸ“ˆ ë§¤ì¶œ ë°ì´í„°", use_container_width=True):
                import numpy as np
                np.random.seed(123)
                sample_data = pd.DataFrame({
                    'month': pd.date_range('2023-01-01', periods=12, freq='M'),
                    'revenue': np.random.normal(100000, 20000, 12),
                    'customers': np.random.randint(800, 1200, 12),
                    'category': np.random.choice(['A', 'B', 'C'], 12)
                })
                st.session_state['workspace_data'] = sample_data
                st.session_state['data_source'] = 'ìƒ˜í”Œ_ë§¤ì¶œ_ë°ì´í„°.csv'
                st.rerun()
        
        with col3:
            if st.button("ğŸ‘¥ ê³ ê° ë°ì´í„°", use_container_width=True):
                import numpy as np
                np.random.seed(456)
                sample_data = pd.DataFrame({
                    'age': np.random.randint(20, 70, 200),
                    'income': np.random.normal(50000, 15000, 200),
                    'spending': np.random.normal(30000, 10000, 200),
                    'segment': np.random.choice(['Premium', 'Standard', 'Basic'], 200)
                })
                st.session_state['workspace_data'] = sample_data
                st.session_state['data_source'] = 'ìƒ˜í”Œ_ê³ ê°_ë°ì´í„°.csv'
                st.rerun() 

# LLM ë™ì  ëŠ¥ë ¥ ê°•í™” ì‹œìŠ¤í…œ
class LLMDynamicCapabilityEngine:
    """LLM ë™ì  ëŠ¥ë ¥ ê°•í™” ì—”ì§„ - ë²”ìš©ì  ë°ì´í„° ì²˜ë¦¬"""
    
    def __init__(self):
        self.capability_cache = {}
        self.learning_history = []
        self.adaptive_strategies = {}
    
    async def analyze_data_dynamically(self, df: pd.DataFrame, user_query: str) -> Dict:
        """LLMì´ ë°ì´í„°ë¥¼ ì™„ì „íˆ ë™ì ìœ¼ë¡œ ë¶„ì„"""
        
        # 1ë‹¨ê³„: LLMì´ ë°ì´í„° íŠ¹ì„±ì„ ìŠ¤ìŠ¤ë¡œ íŒŒì•…
        data_characteristics = await self._discover_data_characteristics(df)
        
        # 2ë‹¨ê³„: LLMì´ ë¶„ì„ ëŠ¥ë ¥ì„ ë™ì ìœ¼ë¡œ êµ¬ì„±
        analysis_capabilities = await self._build_dynamic_analysis_capabilities(
            data_characteristics, user_query
        )
        
        # 3ë‹¨ê³„: LLMì´ ë§ì¶¤í˜• ë¶„ì„ ì „ëµ ìˆ˜ë¦½
        analysis_strategy = await self._create_adaptive_analysis_strategy(
            data_characteristics, analysis_capabilities, user_query
        )
        
        # 4ë‹¨ê³„: ë™ì  ë¶„ì„ ì‹¤í–‰
        results = await self._execute_dynamic_analysis(
            df, analysis_strategy, user_query
        )
        
        # 5ë‹¨ê³„: í•™ìŠµ ë° ì ì‘
        await self._learn_from_analysis(data_characteristics, analysis_strategy, results)
        
        return results
    
    async def _discover_data_characteristics(self, df: pd.DataFrame) -> Dict:
        """LLMì´ ë°ì´í„° íŠ¹ì„±ì„ ì™„ì „íˆ ë™ì ìœ¼ë¡œ ë°œê²¬"""
        
        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        basic_info = {
            "shape": df.shape,
            "columns": df.columns.tolist(),
            "dtypes": df.dtypes.astype(str).to_dict(),
            "missing_values": df.isnull().sum().to_dict(),
            "unique_values": {col: df[col].nunique() for col in df.columns},
            "sample_values": {col: df[col].dropna().head(3).tolist() for col in df.columns}
        }
        
        # LLMì´ ë°ì´í„° íŠ¹ì„±ì„ ë™ì ìœ¼ë¡œ ë¶„ì„
        discovery_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ íŠ¹ì„±ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

ê¸°ë³¸ ì •ë³´:
{json.dumps(basic_info, ensure_ascii=False, indent=2)}

ì´ ë°ì´í„°ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ê²°ì •í•´ì£¼ì„¸ìš”:
1. ë°ì´í„°ì˜ ë„ë©”ì¸ê³¼ ìš©ë„
2. ì£¼ìš” ë³€ìˆ˜ë“¤ì˜ ì˜ë¯¸ì™€ ê´€ê³„
3. ë°ì´í„° í’ˆì§ˆ í‰ê°€
4. ë¶„ì„ ê°€ëŠ¥ì„± í‰ê°€
5. íŠ¹ë³„í•œ íŒ¨í„´ì´ë‚˜ íŠ¹ì§•

ì´ ë°ì´í„°ë§Œì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ íŒŒì•…í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.
ì¼ë°˜ì ì¸ í…œí”Œë¦¿ì´ ì•„ë‹Œ, ì´ ë°ì´í„°ì— íŠ¹í™”ëœ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": discovery_prompt}
                ],
                temperature=0.7
            )
            
            characteristics = {
                "basic_info": basic_info,
                "llm_analysis": response.choices[0].message.content,
                "discovery_timestamp": time.time()
            }
            
            return characteristics
            
        except Exception as e:
            return {
                "basic_info": basic_info,
                "llm_analysis": f"ë™ì  ë¶„ì„ ì‹¤íŒ¨: {e}",
                "discovery_timestamp": time.time()
            }
    
    async def _build_dynamic_analysis_capabilities(self, 
                                                 data_characteristics: Dict, 
                                                 user_query: str) -> Dict:
        """LLMì´ ë¶„ì„ ëŠ¥ë ¥ì„ ë™ì ìœ¼ë¡œ êµ¬ì„±"""
        
        capability_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ëŠ¥ë ¥ ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 

ë°ì´í„° íŠ¹ì„±:
{json.dumps(data_characteristics, ensure_ascii=False, indent=2)}

ì‚¬ìš©ì ìš”ì²­:
{user_query}

ì´ ìƒí™©ì— ë§ëŠ” ìµœì ì˜ ë¶„ì„ ëŠ¥ë ¥ì„ ì„¤ê³„í•´ì£¼ì„¸ìš”:
1. ì–´ë–¤ ë¶„ì„ ê¸°ë²•ì´ í•„ìš”í•œê°€?
2. ì–´ë–¤ ì‹œê°í™”ê°€ íš¨ê³¼ì ì¸ê°€?
3. ì–´ë–¤ í†µê³„ ë°©ë²•ì´ ì ì ˆí•œê°€?
4. ì–´ë–¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆëŠ”ê°€?

ì´ íŠ¹ì • ë°ì´í„°ì™€ ìš”ì²­ì— ë§ëŠ” ë§ì¶¤í˜• ë¶„ì„ ëŠ¥ë ¥ì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ëŠ¥ë ¥ ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": capability_prompt}
                ],
                temperature=0.6
            )
            
            return {
                "capabilities": response.choices[0].message.content,
                "build_timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "capabilities": f"ëŠ¥ë ¥ êµ¬ì„± ì‹¤íŒ¨: {e}",
                "build_timestamp": time.time()
            }
    
    async def _create_adaptive_analysis_strategy(self, 
                                               data_characteristics: Dict,
                                               analysis_capabilities: Dict,
                                               user_query: str) -> Dict:
        """LLMì´ ì ì‘í˜• ë¶„ì„ ì „ëµ ìˆ˜ë¦½"""
        
        strategy_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ëµ ìˆ˜ë¦½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë°ì´í„° íŠ¹ì„±:
{data_characteristics.get('llm_analysis', 'ì•Œ ìˆ˜ ì—†ìŒ')}

ë¶„ì„ ëŠ¥ë ¥:
{analysis_capabilities.get('capabilities', 'ì•Œ ìˆ˜ ì—†ìŒ')}

ì‚¬ìš©ì ìš”ì²­:
{user_query}

ì´ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ êµ¬ì²´ì ì¸ ë¶„ì„ ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:
1. ë¶„ì„ ìˆœì„œì™€ ë‹¨ê³„
2. ê° ë‹¨ê³„ë³„ ëª©í‘œ
3. ì‚¬ìš©í•  ë„êµ¬ì™€ ê¸°ë²•
4. ì˜ˆìƒ ê²°ê³¼ì™€ ê²€ì¦ ë°©ë²•

ì´ íŠ¹ì • ìƒí™©ì— ìµœì í™”ëœ ë¶„ì„ ì „ëµì„ ì œì•ˆí•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ëµ ìˆ˜ë¦½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": strategy_prompt}
                ],
                temperature=0.5
            )
            
            return {
                "strategy": response.choices[0].message.content,
                "strategy_timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "strategy": f"ì „ëµ ìˆ˜ë¦½ ì‹¤íŒ¨: {e}",
                "strategy_timestamp": time.time()
            }
    
    async def _execute_dynamic_analysis(self, 
                                      df: pd.DataFrame, 
                                      analysis_strategy: Dict,
                                      user_query: str) -> Dict:
        """ë™ì  ë¶„ì„ ì‹¤í–‰"""
        
        execution_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì‹¤í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë¶„ì„ ì „ëµ:
{analysis_strategy.get('strategy', 'ì•Œ ìˆ˜ ì—†ìŒ')}

ì‚¬ìš©ì ìš”ì²­:
{user_query}

ì´ ì „ëµì— ë”°ë¼ ì‹¤ì œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
êµ¬ì²´ì ì¸ ë¶„ì„ ê²°ê³¼ì™€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ë°ì´í„° ì •ë³´:
- í¬ê¸°: {df.shape}
- ì»¬ëŸ¼: {df.columns.tolist()}
- ê¸°ë³¸ í†µê³„: {df.describe().to_dict() if len(df.select_dtypes(include=['number']).columns) > 0 else 'ìˆ˜ì¹˜ ë°ì´í„° ì—†ìŒ'}
"""
        
        try:
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì‹¤í–‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": execution_prompt}
                ],
                temperature=0.4
            )
            
            # ì¶”ê°€ë¡œ ì‹œê°í™” ìƒì„±
            visualizations = await self._generate_dynamic_visualizations(df, user_query)
            
            return {
                "analysis_result": response.choices[0].message.content,
                "visualizations": visualizations,
                "execution_timestamp": time.time()
            }
            
        except Exception as e:
            return {
                "analysis_result": f"ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}",
                "visualizations": [],
                "execution_timestamp": time.time()
            }
    
    async def _generate_dynamic_visualizations(self, df: pd.DataFrame, user_query: str) -> List[Dict]:
        """LLMì´ ë™ì ìœ¼ë¡œ ì‹œê°í™” ìƒì„±"""
        
        viz_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ìš”ì²­: {user_query}

ë°ì´í„° ì •ë³´:
- í¬ê¸°: {df.shape}
- ì»¬ëŸ¼: {df.columns.tolist()}
- ìˆ˜ì¹˜ ì»¬ëŸ¼: {df.select_dtypes(include=['number']).columns.tolist()}
- ë²”ì£¼í˜• ì»¬ëŸ¼: {df.select_dtypes(include=['object']).columns.tolist()}

ì´ ë°ì´í„°ì™€ ìš”ì²­ì— ê°€ì¥ ì í•©í•œ ì‹œê°í™”ë¥¼ ìƒì„±í•˜ëŠ” Python ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
plotlyë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬ì²´ì ì¸ ì‹œê°í™” ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
ë³€ìˆ˜ëª… 'df'ë¥¼ ì‚¬ìš©í•˜ê³ , ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë°ì´í„° ì‹œê°í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": viz_prompt}
                ],
                temperature=0.3
            )
            
            # ìƒì„±ëœ ì½”ë“œ ì‹¤í–‰
            viz_code = response.choices[0].message.content
            visualizations = await self._execute_visualization_code(df, viz_code)
            
            return visualizations
            
        except Exception as e:
            return [{"error": f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}"}]
    
    async def _execute_visualization_code(self, df: pd.DataFrame, code: str) -> List[Dict]:
        """ì‹œê°í™” ì½”ë“œ ì•ˆì „ ì‹¤í–‰"""
        
        visualizations = []
        
        # ì•ˆì „í•œ ì‹¤í–‰ í™˜ê²½
        safe_globals = {
            'df': df,
            'pd': pd,
            'px': px,
            'go': go,
            'plt': plt,
            'sns': sns,
            'np': np
        }
        
        try:
            # ì½”ë“œ ì‹¤í–‰
            exec(code, safe_globals)
            
            # ìƒì„±ëœ ì‹œê°í™” ì¶”ì¶œ
            for key, value in safe_globals.items():
                if hasattr(value, 'show') and 'plotly' in str(type(value)):
                    visualizations.append({
                        'type': 'plotly',
                        'figure': value,
                        'name': key,
                        'description': f'LLM ìƒì„± ì‹œê°í™”: {key}'
                    })
            
            return visualizations
            
        except Exception as e:
            return [{"error": f"ì‹œê°í™” ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {e}"}]
    
    async def _learn_from_analysis(self, 
                                 data_characteristics: Dict,
                                 analysis_strategy: Dict,
                                 results: Dict):
        """ë¶„ì„ ê²°ê³¼ë¡œë¶€í„° í•™ìŠµ"""
        
        # í•™ìŠµ ê¸°ë¡ ì €ì¥
        learning_record = {
            "timestamp": time.time(),
            "data_characteristics": data_characteristics,
            "strategy": analysis_strategy,
            "results": results,
            "success": "error" not in results.get("analysis_result", "")
        }
        
        self.learning_history.append(learning_record)
        
        # ì„±ê³µì ì¸ ì „ëµ ìºì‹±
        if learning_record["success"]:
            strategy_key = self._generate_strategy_key(data_characteristics)
            self.adaptive_strategies[strategy_key] = analysis_strategy
        
        # í•™ìŠµ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ìµœëŒ€ 100ê°œ)
        if len(self.learning_history) > 100:
            self.learning_history = self.learning_history[-100:]
    
    def _generate_strategy_key(self, data_characteristics: Dict) -> str:
        """ì „ëµ í‚¤ ìƒì„±"""
        basic_info = data_characteristics.get("basic_info", {})
        return f"{basic_info.get('shape', 'unknown')}_{len(basic_info.get('columns', []))}"

# ì „ì—­ LLM ë™ì  ëŠ¥ë ¥ ì—”ì§„
llm_capability_engine = LLMDynamicCapabilityEngine()

# ë²”ìš©ì  ë°ì´í„° ë¶„ì„ í•¨ìˆ˜ (í•˜ë“œì½”ë”© ì™„ì „ ì œê±°)
async def perform_universal_data_analysis(df: pd.DataFrame, user_query: str) -> Dict:
    """ë²”ìš©ì  ë°ì´í„° ë¶„ì„ - íŠ¹ì • ë°ì´í„°ì…‹ì— ì¢…ì†ë˜ì§€ ì•ŠìŒ"""
    
    try:
        # LLM ë™ì  ëŠ¥ë ¥ ì—”ì§„ ì‚¬ìš©
        results = await llm_capability_engine.analyze_data_dynamically(df, user_query)
        
        return {
            "success": True,
            "analysis": results.get("analysis_result", ""),
            "visualizations": results.get("visualizations", []),
            "method": "LLM Dynamic Universal Analysis",
            "timestamp": time.time()
        }
        
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "method": "LLM Dynamic Universal Analysis (Error)",
            "timestamp": time.time()
        } 