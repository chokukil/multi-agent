# File: utils/streaming.py
# Location: ./utils/streaming.py

import logging
import json
import re
from typing import List, Dict, Any, Tuple, Callable, Optional, AsyncGenerator
from langchain_core.messages import AIMessage, AIMessageChunk, ToolMessage
import asyncio
import time
import streamlit as st
from datetime import datetime
from contextlib import asynccontextmanager

async def astream_graph_with_callbacks(
    graph, 
    inputs: dict, 
    callbacks: list, 
    timeout: int = 300,
    config: dict = None
) -> AsyncGenerator[Any, None]:
    """
    ê·¸ë˜í”„ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ë©° ì½œë°±ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    
    Args:
        graph: LangGraph ì›Œí¬í”Œë¡œìš°
        inputs: ì…ë ¥ ë°ì´í„°
        callbacks: ì½œë°± ë¦¬ìŠ¤íŠ¸
        timeout: íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        config: ì‹¤í–‰ ì„¤ì • (ì˜µì…˜)
    """
    
    @asynccontextmanager
    async def safe_stream_context():
        """ì•ˆì „í•œ ìŠ¤íŠ¸ë¦¬ë° ì»¨í…ìŠ¤íŠ¸"""
        stream_generator = None
        try:
            # ì„¤ì • ì¤€ë¹„
            stream_config = config or {}
            if callbacks:
                stream_config["callbacks"] = callbacks
            
            # ğŸ†• íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ìŠ¤íŠ¸ë¦¼ ìƒì„±
            if stream_config:
                stream_generator = graph.astream(
                    inputs, 
                    config=stream_config,
                    stream_mode="values"
                )
            else:
                stream_generator = graph.astream(
                    inputs,
                    stream_mode="values"
                )
            yield stream_generator
            
        except asyncio.CancelledError:
            logging.warning("ğŸš« Stream was cancelled")
            raise
        except Exception as e:
            logging.error(f"âŒ Stream error: {e}")
            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì²˜ë¦¬
            if stream_generator:
                try:
                    await stream_generator.aclose()
                except Exception as close_error:
                    logging.error(f"Error closing stream: {close_error}")
            raise
        finally:
            # ì •ë¦¬ ì‘ì—…
            if stream_generator:
                try:
                    await stream_generator.aclose()
                except Exception:
                    pass  # ì´ë¯¸ ë‹«í˜”ì„ ìˆ˜ ìˆìŒ
    
    try:
        # ğŸ†• íƒ€ì„ì•„ì›ƒê³¼ í•¨ê»˜ ì•ˆì „í•œ ìŠ¤íŠ¸ë¦¬ë°
        async with asyncio.timeout(timeout):
            async with safe_stream_context() as stream:
                async for chunk in stream:
                    try:
                        # ê° ì²­í¬ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
                        if chunk is not None:
                            # ì½œë°± ì‹¤í–‰
                            for callback in callbacks:
                                try:
                                    if hasattr(callback, 'on_chunk'):
                                        callback.on_chunk(chunk)
                                    elif callable(callback):
                                        callback(chunk)
                                except Exception as cb_error:
                                    logging.error(f"Callback error: {cb_error}")
                            
                            yield chunk
                            
                        # ğŸ†• ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œê°„ í™•ë³´
                        await asyncio.sleep(0.01)  # ì§§ì€ ëŒ€ê¸°ë¡œ ë‹¤ë¥¸ íƒœìŠ¤í¬ì— ì‹œê°„ ì œê³µ
                        
                    except asyncio.CancelledError:
                        logging.warning("ğŸš« Chunk processing cancelled")
                        break
                    except Exception as chunk_error:
                        logging.error(f"âŒ Error processing chunk: {chunk_error}")
                        # ì²­í¬ ì—ëŸ¬ëŠ” ìŠ¤íŠ¸ë¦¼ì„ ì¤‘ë‹¨í•˜ì§€ ì•Šê³  ê³„ì† ì§„í–‰
                        continue
                        
    except asyncio.TimeoutError:
        logging.error(f"â° Stream timeout after {timeout} seconds")
        # íƒ€ì„ì•„ì›ƒ ì‹œ ë§ˆì§€ë§‰ ìƒíƒœ ë°˜í™˜
        yield {
            "messages": inputs.get("messages", []) + [
                {"role": "assistant", "content": f"â° Analysis timed out after {timeout} seconds. Please try a simpler request or increase timeout."}
            ],
            "next_action": "final_responder",
            "timeout_occurred": True
        }
        
    except asyncio.CancelledError:
        logging.warning("ğŸš« Stream was cancelled by user")
        yield {
            "messages": inputs.get("messages", []) + [
                {"role": "assistant", "content": "ğŸš« Analysis was cancelled by user."}
            ],
            "next_action": "final_responder",
            "cancelled": True
        }
        
    except Exception as e:
        logging.error(f"âŒ Critical stream error: {e}")
        # ì¹˜ëª…ì  ì—ëŸ¬ ì‹œ ì—ëŸ¬ ì‘ë‹µ ë°˜í™˜
        yield {
            "messages": inputs.get("messages", []) + [
                {"role": "assistant", "content": f"âŒ Analysis failed due to error: {str(e)}"}
            ],
            "next_action": "final_responder",
            "error_occurred": True,
            "error_details": str(e)
        }


async def safe_callback_execution(callback: Callable, data: Any, timeout: float = 5.0) -> bool:
    """
    ì½œë°±ì„ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
    
    Args:
        callback: ì‹¤í–‰í•  ì½œë°± í•¨ìˆ˜
        data: ì½œë°±ì— ì „ë‹¬í•  ë°ì´í„°
        timeout: ì½œë°± ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ
        
    Returns:
        ì„±ê³µ ì—¬ë¶€
    """
    try:
        # ğŸ†• ì½œë°± ì‹¤í–‰ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        async with asyncio.timeout(timeout):
            if asyncio.iscoroutinefunction(callback):
                await callback(data)
            else:
                # ë™ê¸° í•¨ìˆ˜ëŠ” ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, callback, data)
        return True
        
    except asyncio.TimeoutError:
        logging.warning(f"â° Callback timeout after {timeout} seconds")
        return False
    except Exception as e:
        logging.error(f"âŒ Callback error: {e}")
        return False


class StreamingManager:
    """ìŠ¤íŠ¸ë¦¬ë° ê´€ë¦¬ì - ì•ˆì •ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬"""
    
    def __init__(self, max_concurrent_callbacks: int = 3):
        self.max_concurrent_callbacks = max_concurrent_callbacks
        self.active_callbacks = 0
        self.callback_semaphore = asyncio.Semaphore(max_concurrent_callbacks)
        
    async def process_with_callbacks(
        self, 
        graph, 
        inputs: dict, 
        callbacks: list, 
        timeout: int = 300,
        callback_timeout: float = 5.0
    ) -> AsyncGenerator[Any, None]:
        """
        ì½œë°±ê³¼ í•¨ê»˜ ì•ˆì „í•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        
        Args:
            graph: LangGraph ì›Œí¬í”Œë¡œìš°
            inputs: ì…ë ¥ ë°ì´í„°
            callbacks: ì½œë°± ë¦¬ìŠ¤íŠ¸
            timeout: ì „ì²´ íƒ€ì„ì•„ì›ƒ
            callback_timeout: ê°œë³„ ì½œë°± íƒ€ì„ì•„ì›ƒ
        """
        
        # ğŸ†• ì½œë°± ë˜í¼ - ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ
        async def safe_callback_wrapper(callback, data):
            async with self.callback_semaphore:
                self.active_callbacks += 1
                try:
                    success = await safe_callback_execution(callback, data, callback_timeout)
                    if not success:
                        logging.warning(f"Callback {getattr(callback, '__name__', 'unknown')} failed")
                finally:
                    self.active_callbacks -= 1
        
        # ğŸ†• ê°œì„ ëœ ì½œë°± ì²˜ë¦¬
        enhanced_callbacks = []
        for callback in callbacks:
            if callable(callback):
                # ì›ë˜ ì½œë°±ì„ ì•ˆì „í•œ ë˜í¼ë¡œ ê°ì‹¸ê¸°
                async def wrapped_callback(data, original_callback=callback):
                    await safe_callback_wrapper(original_callback, data)
                enhanced_callbacks.append(wrapped_callback)
            else:
                enhanced_callbacks.append(callback)
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        async for chunk in astream_graph_with_callbacks(
            graph, inputs, enhanced_callbacks, timeout
        ):
            yield chunk
            
            # ğŸ†• ì½œë°± ì²˜ë¦¬ ìƒíƒœ ëª¨ë‹ˆí„°ë§
            if self.active_callbacks > self.max_concurrent_callbacks * 0.8:
                logging.warning(f"High callback load: {self.active_callbacks}/{self.max_concurrent_callbacks}")
                await asyncio.sleep(0.1)  # ì•½ê°„ì˜ ë°±í”„ë ˆì…” ì ìš©


# ì „ì—­ ìŠ¤íŠ¸ë¦¬ë° ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
streaming_manager = StreamingManager()


def create_timeout_aware_callback(original_callback, name: str = "unnamed"):
    """
    íƒ€ì„ì•„ì›ƒì„ ì¸ì‹í•˜ëŠ” ì½œë°± ë˜í¼ ìƒì„±
    
    Args:
        original_callback: ì›ë˜ ì½œë°± í•¨ìˆ˜
        name: ì½œë°± ì´ë¦„ (ë””ë²„ê¹…ìš©)
        
    Returns:
        ë˜í•‘ëœ ì½œë°±
    """
    
    def timeout_aware_callback(data):
        try:
            # ğŸ†• ë¹ ë¥¸ íƒ€ì„ì•„ì›ƒ ì²´í¬
            if hasattr(data, 'get') and data.get('timeout_occurred'):
                logging.info(f"Skipping callback {name} due to timeout")
                return
                
            if hasattr(data, 'get') and data.get('cancelled'):
                logging.info(f"Skipping callback {name} due to cancellation")
                return
                
            # ì›ë˜ ì½œë°± ì‹¤í–‰
            return original_callback(data)
            
        except Exception as e:
            logging.error(f"Error in timeout-aware callback {name}: {e}")
            
    timeout_aware_callback.__name__ = f"timeout_aware_{name}"
    return timeout_aware_callback

def get_plan_execute_streaming_callback(tool_activity_placeholder) -> Callable:
    """Plan-Execute íŒ¨í„´ì— ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¸ë“¤ëŸ¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    # ì‹¤í–‰ ìƒíƒœ ì¶”ì 
    execution_state = {
        "current_executor": None,
        "current_step": 0,
        "total_steps": 0,
        "tool_outputs": [],
        "plan": [],
        "step_results": {},
        "data_transformations": [],
        "final_response": None,  # ğŸ†• ìµœì¢… ì‘ë‹µ ì €ì¥
        "plan_displayed": False,  # ğŸ†• ê³„íš ì¤‘ë³µ í‘œì‹œ ë°©ì§€
        "final_response_displayed": False  # ğŸ†• ìµœì¢… ì‘ë‹µ ì¤‘ë³µ í‘œì‹œ ë°©ì§€
    }
    
    # ğŸ†• ë„êµ¬ í™œë™ ë²„í¼ (ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°)
    tool_buf = []
    
    def flush_tool():
        """ë„êµ¬ í™œë™ ë²„í¼ë¥¼ UIì— í‘œì‹œ"""
        if tool_buf:
            content = "".join(tool_buf)
            tool_activity_placeholder.markdown(content)
            tool_buf.clear()
    
    def extract_python_code(content: str) -> Optional[str]:
        """ë©”ì‹œì§€ ë‚´ìš©ì—ì„œ Python ì½”ë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        # python_repl_ast ë„êµ¬ í˜¸ì¶œ íŒ¨í„´ ì°¾ê¸°
        python_patterns = [
            r'```python\n(.*?)```',
            r'python_repl_ast\((.*?)\)',
            r'"code":\s*"(.*?)"',
            r"'code':\s*'(.*?)'"
        ]
        
        for pattern in python_patterns:
            matches = re.findall(pattern, content, re.DOTALL | re.IGNORECASE)
            if matches:
                return matches[0].strip()
        
        return None
    
    def format_tool_output(content: str) -> str:
        """ë„êµ¬ ì¶œë ¥ì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·í•©ë‹ˆë‹¤."""
        # Python ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ í¬ë§·
        if "```python" in content:
            return content
        
        # ê¸´ ì¶œë ¥ ì¤„ì´ê¸°
        lines = content.split('\n')
        if len(lines) > 20:
            return '\n'.join(lines[:10]) + f"\n... ({len(lines) - 20} more lines) ...\n" + '\n'.join(lines[-10:])
        
        return content
    
    def render_tool_activity():
        """ë„êµ¬ í™œë™ì„ ë Œë”ë§í•©ë‹ˆë‹¤."""
        if not execution_state["tool_outputs"]:
            tool_activity_placeholder.empty()
            return
        
        content_parts = []
        
        for output in execution_state["tool_outputs"]:
            node = output["node"]
            content = output["content"]
            timestamp = output.get("timestamp", "")
            
            if node == "planner":
                content_parts.append("### ğŸ“‹ **ê³„íš ìˆ˜ë¦½**")
                if isinstance(content, dict) and "plan" in content:
                    for i, step in enumerate(content["plan"], 1):
                        content_parts.append(f"{i}. {step.get('task', step)}")
                else:
                    content_parts.append(str(content))
                content_parts.append("")
                
            elif node == "router":
                content_parts.append("### ğŸ”€ **ë¼ìš°í„°**")
                content_parts.append(str(content))
                content_parts.append("")
                
            elif node == "replanner":
                content_parts.append("### ğŸ”„ **ì¬ê³„íš**")
                content_parts.append(str(content))
                content_parts.append("")
                
            elif "executor" in node.lower() or node in ["Data_Preprocessor", "EDA_Specialist", "Visualization_Expert", "ML_Engineer", "Statistical_Analyst", "Report_Writer"]:
                # Executor í™œë™
                content_parts.append(f"### ğŸ’¼ **{node}**")
                
                # Python ì½”ë“œ ì¶”ì¶œ ë° í‘œì‹œ
                python_code = extract_python_code(str(content))
                if python_code:
                    content_parts.append("**ì‹¤í–‰ëœ Python ì½”ë“œ:**")
                    content_parts.append("```python")
                    content_parts.append(python_code)
                    content_parts.append("```")
                
                # ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ í‘œì‹œ
                formatted_content = format_tool_output(str(content))
                if formatted_content and python_code != formatted_content:
                    content_parts.append("**ì‹¤í–‰ ê²°ê³¼:**")
                    content_parts.append("```")
                    content_parts.append(formatted_content)
                    content_parts.append("```")
                
                content_parts.append("")
        
        # ì „ì²´ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í‘œì‹œ
        full_content = "\n".join(content_parts)
        tool_activity_placeholder.markdown(full_content)
    
    def callback(msg: Dict[str, Any]):
        """ë©”ì¸ ì½œë°± í•¨ìˆ˜"""
        node = msg.get("node", "")
        content = msg.get("content")
        
        if not content:
            return
        
        logging.debug(f"Streaming callback: node={node}, content_type={type(content)}")
        
        # ë©”ì‹œì§€ ë‚´ìš© ì¶”ì¶œ
        if hasattr(content, "content"):
            message_content = content.content
        elif isinstance(content, dict):
            message_content = content
        else:
            message_content = str(content)
        
        # ğŸ†• Final Responder ì²˜ë¦¬ ì¶”ê°€
        if node == "final_responder" or node == "Final_Responder":
            logging.info("ğŸ¯ Processing final_responder content")
            
            # ìµœì¢… ì‘ë‹µì„ ì €ì¥í•˜ê³  UIì— í‘œì‹œ
            if hasattr(content, "content"):
                final_content = content.content
            elif isinstance(content, str):
                final_content = content
            elif hasattr(content, "messages") and content.messages:
                # messagesì—ì„œ ìµœì¢… ì‘ë‹µ ì¶”ì¶œ
                for msg in reversed(content.messages):
                    if hasattr(msg, "content") and msg.content.strip():
                        final_content = msg.content
                        break
                else:
                    final_content = "ìµœì¢… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            else:
                final_content = "ìµœì¢… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            
            # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
            execution_state["final_response"] = final_content
            
            # UIì— ì¦‰ì‹œ í‘œì‹œ
            try:
                if hasattr(st, 'session_state'):
                    st.session_state['final_response'] = final_content
                    st.session_state['final_response_timestamp'] = time.time()
                    logging.info("âœ… Final response saved to session state")
                    
                    # ğŸ†• UI í”Œë ˆì´ìŠ¤í™€ë”ì— ì§ì ‘ í‘œì‹œ ì‹œë„
                    try:
                        # í˜„ì¬ í™œì„± í…ìŠ¤íŠ¸ í”Œë ˆì´ìŠ¤í™€ë” ì°¾ê¸°
                        if hasattr(st.session_state, 'current_text_placeholder'):
                            st.session_state.current_text_placeholder.markdown(final_content)
                            logging.info("âœ… Final response displayed directly to UI")
                    except Exception as ui_e:
                        logging.warning(f"Could not display final response directly: {ui_e}")
                        
            except Exception as e:
                logging.warning(f"Failed to save final response to session: {e}")
            
            # ğŸ†• ìµœì¢… ì‘ë‹µ ì¤‘ë³µ í‘œì‹œ ë°©ì§€
            if not execution_state["final_response_displayed"]:
                execution_state["final_response_displayed"] = True
                
                # ë„êµ¬ í™œë™ í‘œì‹œ
                tool_buf.append(f"\nğŸ¯ **ìµœì¢… ì‘ë‹µ ìƒì„± ì™„ë£Œ**\n")
                tool_buf.append(f"ì‘ë‹µ ê¸¸ì´: {len(final_content)} ë¬¸ì\n")
                tool_buf.append(f"ì‹œê°„: {time.strftime('%H:%M:%S')}\n")
                flush_tool()
            else:
                logging.debug("ğŸ¯ Final response already displayed, skipping duplicate")
            
            # ğŸ†• ìµœì¢… ì‘ë‹µì— ëŒ€í•œ ìë™ ì•„í‹°íŒ©íŠ¸ ìƒì„±
            try:
                from ui.artifact_manager import auto_detect_artifacts, notify_artifact_creation
                created_artifacts = auto_detect_artifacts(final_content, "Final_Responder")
                if created_artifacts:
                    notify_artifact_creation(created_artifacts)
                    logging.info(f"ğŸ¨ Created {len(created_artifacts)} final report artifacts")
            except ImportError:
                logging.warning("Artifact manager not available for final report")
            except Exception as e:
                logging.warning(f"Could not create final report artifacts: {e}")
            
            logging.info(f"âœ… Final response processed: {len(final_content)} characters")
            
            return  # Final responderëŠ” tool activityì— í‘œì‹œí•˜ì§€ ì•ŠìŒ
        
        # Plan-Execute íŠ¹ë³„ ì²˜ë¦¬
        if node == "planner" and isinstance(content, dict) and "plan" in content:
            # ğŸ†• ê³„íš ì¤‘ë³µ í‘œì‹œ ë°©ì§€
            if not execution_state["plan_displayed"]:
                execution_state["plan"] = content["plan"]
                execution_state["total_steps"] = len(content["plan"])
                execution_state["plan_displayed"] = True
                
                # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                st.session_state.current_plan = content["plan"]
                st.session_state.current_step = 0
                
                # ğŸ”¥ ê³„íš ìƒì„± UI ì•ˆì •í™” (í•œ ë²ˆë§Œ í‘œì‹œ)
                logging.info(f"ğŸ“‹ Plan created with {len(content['plan'])} steps")
                tool_buf.append(f"\nğŸ“‹ **ì‹¤í–‰ ê³„íš ìƒì„± ì™„ë£Œ** ({len(content['plan'])}ë‹¨ê³„)\n")
                for i, step in enumerate(content["plan"]):
                    tool_buf.append(f"  {i+1}. {step.get('task', 'Unknown task')}\n")
                tool_buf.append("\n")
                flush_tool()
            else:
                logging.debug("ğŸ“‹ Plan already displayed, skipping duplicate")
            
        elif node == "router":
            # í˜„ì¬ ë‹¨ê³„ ì—…ë°ì´íŠ¸
            current_step = st.session_state.get("current_step", 0)
            if current_step < len(execution_state["plan"]):
                st.session_state.current_step = current_step
                
                # ğŸ”¥ ë¼ìš°í„° ì •ë³´ í‘œì‹œ
                logging.info(f"ğŸ”€ Router: Step {current_step + 1} routing")
                tool_buf.append(f"\nğŸ”€ **Step {current_step + 1} ì‹œì‘**\n")
                flush_tool()
                
        elif "executor" in node.lower() or node in ["Data_Validator", "Preprocessing_Expert", "EDA_Analyst", "Visualization_Expert", "ML_Specialist", "Statistical_Analyst", "Report_Generator"]:
            # Executor ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬
            current_step = st.session_state.get("current_step", 0)
            
            # ë‹¨ê³„ ê²°ê³¼ ì—…ë°ì´íŠ¸
            if "step_results" not in st.session_state:
                st.session_state.step_results = {}
                
            # ì‘ì—… ì™„ë£Œ ì—¬ë¶€ í™•ì¸
            completed = "TASK COMPLETED:" in str(message_content)
            
            # ğŸ”¥ ì‹¤í–‰ ìƒíƒœ í‘œì‹œ ê°œì„ 
            if completed:
                logging.info(f"âœ… {node}: Step {current_step + 1} completed")
                tool_buf.append(f"\nâœ… **{node}**: Step {current_step + 1} ì™„ë£Œ\n")
                tool_buf.append(f"ğŸ“ ê²°ê³¼: {str(message_content).split('TASK COMPLETED:')[-1].strip()[:100]}...\n")
            else:
                logging.info(f"ğŸ”„ {node}: Step {current_step + 1} in progress")
                tool_buf.append(f"\nğŸ”„ **{node}**: Step {current_step + 1} ì‹¤í–‰ ì¤‘...\n")
            
            flush_tool()
            
            st.session_state.step_results[current_step] = {
                "executor": node,
                "completed": completed,
                "timestamp": datetime.now().isoformat(),
                "content": str(message_content)[:500]  # ìš”ì•½ ì €ì¥
            }
            
            # ìë™ ì•„í‹°íŒ©íŠ¸ ê°ì§€ ë° ìƒì„±
            if isinstance(message_content, str):
                try:
                    from ui.artifact_manager import auto_detect_artifacts, notify_artifact_creation
                    created_artifacts = auto_detect_artifacts(message_content, node)
                    if created_artifacts:
                        notify_artifact_creation(created_artifacts)
                        logging.info(f"ğŸ¨ Created {len(created_artifacts)} artifacts for {node}")
                except ImportError:
                    logging.warning("Artifact manager not available")
                except Exception as e:
                    logging.warning(f"Could not create artifacts: {e}")
        
        elif node == "replanner":
            # ì¬ê³„íš ë‹¨ê³„ - ë‹¤ìŒ ë‹¨ê³„ë¡œ ì´ë™
            current_step = st.session_state.get("current_step", 0)
            
            # ğŸ”¥ ì¬ê³„íš ì •ë³´ í‘œì‹œ ê°œì„ 
            logging.info(f"ğŸ”„ Replanner: Evaluating step {current_step + 1}")
            tool_buf.append(f"\nğŸ”„ **ì¬ê³„íš**: Step {current_step + 1} í‰ê°€ ì¤‘...\n")
            flush_tool()
        
        # ë„êµ¬ í™œë™ ê¸°ë¡ì— ì¶”ê°€
        execution_state["tool_outputs"].append({
            "node": node,
            "content": message_content,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        })
        
        # ìµœëŒ€ 50ê°œ í•­ëª©ë§Œ ìœ ì§€ (ë©”ëª¨ë¦¬ ì ˆì•½)
        if len(execution_state["tool_outputs"]) > 50:
            execution_state["tool_outputs"] = execution_state["tool_outputs"][-50:]
        
        # UI ì—…ë°ì´íŠ¸
        render_tool_activity()
    
    return callback

def get_streaming_callback(text_placeholder, tool_placeholder) -> Tuple:
    """ê¸°ì¡´ í˜¸í™˜ì„±ì„ ìœ„í•œ ë ˆê±°ì‹œ í•¨ìˆ˜"""
    text_buf: List[str] = []
    tool_buf: List[str] = []
    
    # ì‹¤í–‰ ìƒíƒœ ì¶”ì 
    execution_state = {
        "current_executor": None,
        "current_step": 0,
        "total_steps": 0
    }
    
    def flush_txt():
        text_placeholder.write("".join(text_buf))
    
    def flush_tool():
        tool_placeholder.empty()
        tool_placeholder.write("".join(tool_buf))
    
    def callback(msg: Dict[str, Any]):
        node = msg.get("node", "")
        content = msg.get("content")
        
        logging.debug(f"Callback: node={node}, content_type={type(content)}")
        
        # Plan-Execute ë…¸ë“œë³„ ì²˜ë¦¬
        if node == "planner":
            if isinstance(content, dict) and "plan" in content:
                tool_buf.append("\nğŸ“‹ **Execution Plan Created**\n")
                for step in content["plan"]:
                    tool_buf.append(f"  {step['step']}. {step['task']}\n")
                flush_tool()
                
        elif node == "router":
            if hasattr(content, "content"):
                tool_buf.append(f"\nğŸ”€ **Router**: {content.content}\n")
                flush_tool()
                
        elif node == "replanner":
            if hasattr(content, "content"):
                tool_buf.append(f"\nğŸ”„ **Re-planner**: {content.content}\n")
                flush_tool()
                
        elif node == "final_responder" or node == "Final_Responder":
            # ğŸ†• Final Responderì˜ ë‚´ìš©ì€ í…ìŠ¤íŠ¸ ì˜ì—­ìœ¼ë¡œ
            if hasattr(content, "content"):
                text_buf.append(content.content)
                flush_txt()
            elif isinstance(content, str):
                text_buf.append(content)
                flush_txt()
            
            # ì„¸ì…˜ ìƒíƒœì—ë„ ì €ì¥
            st.session_state.final_response = content.content if hasattr(content, "content") else str(content)
            logging.info("ğŸ¯ Final response stored in session state")
                
        # ê¸°íƒ€ executor ë…¸ë“œë“¤
        elif any(executor in node.lower() for executor in ["executor", "analyst", "specialist", "expert", "engineer", "writer", "generator", "preprocessor", "validator"]):
            if hasattr(content, "content"):
                tool_buf.append(f"\nğŸ¤– **{node}**: {content.content[:200]}...\n")
                flush_tool()
    
    return callback, flush_txt, flush_tool