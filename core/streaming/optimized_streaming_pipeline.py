"""
ìµœì í™”ëœ ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸
Phase 2.3: A2A SSE ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ ìµœì í™”

ê¸°ëŠ¥:
- ì ì‘ì  ì²­í¬ í¬ê¸° ì¡°ì •
- ë°±í”„ë ˆì…” ì²˜ë¦¬ ë° íë¦„ ì œì–´
- ì§€ëŠ¥í˜• ë²„í¼ë§ ì „ëµ
- ìŠ¤íŠ¸ë¦¬ë° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§
- ë„¤íŠ¸ì›Œí¬ ì¡°ê±´ ì ì‘
- ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œì‘
"""

import asyncio
import time
import json
import logging
import statistics
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, AsyncGenerator, Callable, Union
from datetime import datetime, timedelta
from collections import deque
from enum import Enum
import uuid
from pathlib import Path

logger = logging.getLogger(__name__)

class StreamingState(Enum):
    """ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ"""
    INITIALIZING = "initializing"
    STREAMING = "streaming"
    BUFFERING = "buffering"
    BACKPRESSURE = "backpressure"
    ERROR = "error"
    COMPLETED = "completed"
    CANCELLED = "cancelled"

class ChunkType(Enum):
    """ì²­í¬ íƒ€ì…"""
    TEXT = "text"
    JSON = "json"
    BINARY = "binary"
    METADATA = "metadata"
    HEARTBEAT = "heartbeat"
    ERROR = "error"

@dataclass
class StreamChunk:
    """ìŠ¤íŠ¸ë¦¬ë° ì²­í¬"""
    id: str
    type: ChunkType
    data: Any
    timestamp: datetime
    size_bytes: int
    sequence: int
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_sse_format(self) -> str:
        """SSE í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        sse_data = {
            "id": self.id,
            "type": self.type.value,
            "data": self.data,
            "timestamp": self.timestamp.isoformat(),
            "sequence": self.sequence,
            "metadata": self.metadata
        }
        
        json_data = json.dumps(sse_data, ensure_ascii=False, default=str)
        return f"data: {json_data}\n\n"

@dataclass
class StreamingMetrics:
    """ìŠ¤íŠ¸ë¦¬ë° ë©”íŠ¸ë¦­"""
    total_chunks: int = 0
    total_bytes: int = 0
    avg_chunk_size: float = 0.0
    max_chunk_size: int = 0
    min_chunk_size: int = float('inf')
    throughput_bps: float = 0.0
    latency_ms: float = 0.0
    buffer_usage_percent: float = 0.0
    backpressure_events: int = 0
    error_count: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    
    @property
    def duration_seconds(self) -> float:
        """ìŠ¤íŠ¸ë¦¬ë° ì§€ì† ì‹œê°„"""
        if self.start_time and self.end_time:
            return (self.end_time - self.start_time).total_seconds()
        return 0.0
    
    @property
    def avg_throughput_bps(self) -> float:
        """í‰ê·  ì²˜ë¦¬ëŸ‰ (bytes per second)"""
        if self.duration_seconds > 0:
            return self.total_bytes / self.duration_seconds
        return 0.0

@dataclass
class BufferConfig:
    """ë²„í¼ ì„¤ì •"""
    max_buffer_size: int = 1024 * 1024  # 1MB
    low_watermark: float = 0.3  # 30%
    high_watermark: float = 0.8  # 80%
    chunk_timeout_ms: float = 100.0
    max_chunks_in_buffer: int = 1000
    adaptive_sizing: bool = True

@dataclass
class ChunkingConfig:
    """ì²­í‚¹ ì„¤ì •"""
    min_chunk_size: int = 256
    max_chunk_size: int = 8192
    target_chunk_size: int = 2048
    adaptive_chunking: bool = True
    chunk_overlap: int = 0
    compression_threshold: int = 1024

class AdaptiveChunker:
    """ì ì‘ì  ì²­í‚¹ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: ChunkingConfig):
        self.config = config
        self.current_chunk_size = config.target_chunk_size
        self.performance_history: deque = deque(maxlen=50)
        self.network_conditions: Dict[str, float] = {
            "latency_ms": 0.0,
            "bandwidth_bps": 0.0,
            "packet_loss_rate": 0.0
        }
    
    def calculate_optimal_chunk_size(self, content_size: int, network_conditions: Dict[str, float]) -> int:
        """ìµœì  ì²­í¬ í¬ê¸° ê³„ì‚°"""
        self.network_conditions.update(network_conditions)
        
        if not self.config.adaptive_chunking:
            return self.config.target_chunk_size
        
        # ë„¤íŠ¸ì›Œí¬ ì¡°ê±´ ê¸°ë°˜ ì¡°ì •
        latency = network_conditions.get("latency_ms", 50.0)
        bandwidth = network_conditions.get("bandwidth_bps", 1000000.0)  # 1Mbps ê¸°ë³¸ê°’
        
        # ê³ ì§€ì—° ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” í° ì²­í¬ ì‚¬ìš©
        if latency > 100:
            size_factor = min(2.0, latency / 50)
        # ì €ì§€ì—° ë„¤íŠ¸ì›Œí¬ì—ì„œëŠ” ì‘ì€ ì²­í¬ë¡œ ë” ë¹ ë¥¸ ì‘ë‹µì„±
        else:
            size_factor = max(0.5, latency / 100)
        
        # ëŒ€ì—­í­ ê¸°ë°˜ ì¡°ì •
        if bandwidth < 100000:  # 100kbps ë¯¸ë§Œ
            size_factor *= 0.5
        elif bandwidth > 10000000:  # 10Mbps ì´ˆê³¼
            size_factor *= 1.5
        
        # ì»¨í…ì¸  í¬ê¸° ê¸°ë°˜ ì¡°ì •
        if content_size < 1024:  # ì‘ì€ ì»¨í…ì¸ 
            size_factor *= 0.7
        elif content_size > 100000:  # í° ì»¨í…ì¸ 
            size_factor *= 1.3
        
        optimal_size = int(self.config.target_chunk_size * size_factor)
        optimal_size = max(self.config.min_chunk_size, min(self.config.max_chunk_size, optimal_size))
        
        self.current_chunk_size = optimal_size
        return optimal_size
    
    async def chunk_content(self, content: Union[str, bytes], chunk_type: ChunkType) -> AsyncGenerator[StreamChunk, None]:
        """ì»¨í…ì¸ ë¥¼ ìµœì í™”ëœ ì²­í¬ë¡œ ë¶„í• """
        if isinstance(content, str):
            content_bytes = content.encode('utf-8')
        else:
            content_bytes = content
        
        content_size = len(content_bytes)
        chunk_size = self.calculate_optimal_chunk_size(
            content_size, 
            self.network_conditions
        )
        
        sequence = 0
        position = 0
        
        while position < content_size:
            # ì²­í¬ í¬ê¸° ë™ì  ì¡°ì •
            remaining = content_size - position
            current_chunk_size = min(chunk_size, remaining)
            
            # ì˜¤ë²„ë© ê³ ë ¤ (í…ìŠ¤íŠ¸ ê²½ê³„ ë§ì¶”ê¸° ë“±)
            if self.config.chunk_overlap > 0 and position > 0:
                overlap_start = max(0, position - self.config.chunk_overlap)
                chunk_data = content_bytes[overlap_start:position + current_chunk_size]
            else:
                chunk_data = content_bytes[position:position + current_chunk_size]
            
            # í…ìŠ¤íŠ¸ ê²½ê³„ ì¡°ì • (UTF-8 ì•ˆì „ì„±)
            if chunk_type == ChunkType.TEXT and position + current_chunk_size < content_size:
                chunk_data = self._adjust_text_boundary(chunk_data)
            
            chunk = StreamChunk(
                id=str(uuid.uuid4()),
                type=chunk_type,
                data=chunk_data.decode('utf-8') if chunk_type == ChunkType.TEXT else chunk_data,
                timestamp=datetime.now(),
                size_bytes=len(chunk_data),
                sequence=sequence,
                metadata={
                    "total_size": content_size,
                    "position": position,
                    "is_final": position + current_chunk_size >= content_size
                }
            )
            
            yield chunk
            
            position += current_chunk_size
            sequence += 1
    
    def _adjust_text_boundary(self, chunk_data: bytes) -> bytes:
        """í…ìŠ¤íŠ¸ ê²½ê³„ ì¡°ì • (UTF-8 ì•ˆì „)"""
        try:
            # ë§ˆì§€ë§‰ 10ë°”ì´íŠ¸ ë‚´ì—ì„œ ì•ˆì „í•œ UTF-8 ê²½ê³„ ì°¾ê¸°
            for i in range(min(10, len(chunk_data))):
                try:
                    test_chunk = chunk_data[:-i] if i > 0 else chunk_data
                    test_chunk.decode('utf-8')
                    return test_chunk
                except UnicodeDecodeError:
                    continue
        except:
            pass
        
        return chunk_data
    
    def update_performance(self, throughput_bps: float, latency_ms: float):
        """ì„±ëŠ¥ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.performance_history.append({
            "timestamp": datetime.now(),
            "throughput_bps": throughput_bps,
            "latency_ms": latency_ms,
            "chunk_size": self.current_chunk_size
        })
        
        # ì„±ëŠ¥ ê¸°ë°˜ ì²­í¬ í¬ê¸° ë¯¸ì„¸ ì¡°ì •
        if len(self.performance_history) >= 10:
            recent_performance = list(self.performance_history)[-10:]
            avg_latency = statistics.mean([p["latency_ms"] for p in recent_performance])
            avg_throughput = statistics.mean([p["throughput_bps"] for p in recent_performance])
            
            # ì§€ì—°ì‹œê°„ì´ ì¦ê°€í•˜ë©´ ì²­í¬ í¬ê¸° ì¦ê°€ (fewer requests)
            if avg_latency > self.network_conditions.get("latency_ms", 50.0) * 1.2:
                self.current_chunk_size = min(
                    self.config.max_chunk_size,
                    int(self.current_chunk_size * 1.1)
                )
            # ì²˜ë¦¬ëŸ‰ì´ ê°ì†Œí•˜ë©´ ì²­í¬ í¬ê¸° ê°ì†Œ (better responsiveness)
            elif avg_throughput < self.network_conditions.get("bandwidth_bps", 1000000.0) * 0.8:
                self.current_chunk_size = max(
                    self.config.min_chunk_size,
                    int(self.current_chunk_size * 0.9)
                )

class StreamBuffer:
    """ìŠ¤íŠ¸ë¦¬ë° ë²„í¼ ê´€ë¦¬"""
    
    def __init__(self, config: BufferConfig):
        self.config = config
        self.buffer: deque = deque()
        self.buffer_size_bytes = 0
        self.is_draining = False
        self.backpressure_active = False
        self._buffer_lock = asyncio.Lock()
        
        # ë²„í¼ ìƒíƒœ ì¶”ì 
        self.buffer_stats = {
            "max_size_reached": 0,
            "backpressure_events": 0,
            "drain_events": 0,
            "overflow_events": 0
        }
    
    async def add_chunk(self, chunk: StreamChunk) -> bool:
        """ì²­í¬ë¥¼ ë²„í¼ì— ì¶”ê°€"""
        async with self._buffer_lock:
            # ë²„í¼ ìš©ëŸ‰ ì²´í¬
            if self._is_buffer_full():
                self.buffer_stats["overflow_events"] += 1
                logger.warning(f"ë²„í¼ ì˜¤ë²„í”Œë¡œìš°: {self.buffer_size_bytes}/{self.config.max_buffer_size} bytes")
                return False
            
            self.buffer.append(chunk)
            self.buffer_size_bytes += chunk.size_bytes
            
            # ë°±í”„ë ˆì…” ìƒíƒœ í™•ì¸
            self._check_backpressure()
            
            return True
    
    async def get_chunk(self) -> Optional[StreamChunk]:
        """ë²„í¼ì—ì„œ ì²­í¬ ê°€ì ¸ì˜¤ê¸°"""
        async with self._buffer_lock:
            if not self.buffer:
                return None
            
            chunk = self.buffer.popleft()
            self.buffer_size_bytes -= chunk.size_bytes
            
            # ë“œë ˆì¸ ìƒíƒœ í™•ì¸
            self._check_drain_status()
            
            return chunk
    
    async def get_chunks_batch(self, max_count: int = 10) -> List[StreamChunk]:
        """ì—¬ëŸ¬ ì²­í¬ë¥¼ ë°°ì¹˜ë¡œ ê°€ì ¸ì˜¤ê¸°"""
        chunks = []
        
        async with self._buffer_lock:
            for _ in range(min(max_count, len(self.buffer))):
                if self.buffer:
                    chunk = self.buffer.popleft()
                    self.buffer_size_bytes -= chunk.size_bytes
                    chunks.append(chunk)
                else:
                    break
            
            self._check_drain_status()
        
        return chunks
    
    def _is_buffer_full(self) -> bool:
        """ë²„í¼ê°€ ê°€ë“ ì°¼ëŠ”ì§€ í™•ì¸"""
        size_full = self.buffer_size_bytes >= self.config.max_buffer_size
        count_full = len(self.buffer) >= self.config.max_chunks_in_buffer
        return size_full or count_full
    
    def _check_backpressure(self):
        """ë°±í”„ë ˆì…” ìƒíƒœ í™•ì¸"""
        usage_ratio = self.buffer_size_bytes / self.config.max_buffer_size
        
        if usage_ratio > self.config.high_watermark and not self.backpressure_active:
            self.backpressure_active = True
            self.buffer_stats["backpressure_events"] += 1
            logger.info(f"ë°±í”„ë ˆì…” í™œì„±í™”: ë²„í¼ ì‚¬ìš©ëŸ‰ {usage_ratio:.2%}")
        
        elif usage_ratio < self.config.low_watermark and self.backpressure_active:
            self.backpressure_active = False
            logger.info(f"ë°±í”„ë ˆì…” í•´ì œ: ë²„í¼ ì‚¬ìš©ëŸ‰ {usage_ratio:.2%}")
    
    def _check_drain_status(self):
        """ë“œë ˆì¸ ìƒíƒœ í™•ì¸"""
        usage_ratio = self.buffer_size_bytes / self.config.max_buffer_size
        
        if usage_ratio < self.config.low_watermark and self.is_draining:
            self.is_draining = False
            self.buffer_stats["drain_events"] += 1
    
    def get_buffer_status(self) -> Dict[str, Any]:
        """ë²„í¼ ìƒíƒœ ë°˜í™˜"""
        usage_ratio = self.buffer_size_bytes / self.config.max_buffer_size
        
        return {
            "buffer_size_bytes": self.buffer_size_bytes,
            "buffer_chunk_count": len(self.buffer),
            "usage_ratio": usage_ratio,
            "is_draining": self.is_draining,
            "backpressure_active": self.backpressure_active,
            "stats": self.buffer_stats.copy()
        }

class OptimizedStreamingPipeline:
    """ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, 
                 buffer_config: Optional[BufferConfig] = None,
                 chunking_config: Optional[ChunkingConfig] = None):
        # ì„¤ì •
        self.buffer_config = buffer_config or BufferConfig()
        self.chunking_config = chunking_config or ChunkingConfig()
        
        # ì»´í¬ë„ŒíŠ¸
        self.chunker = AdaptiveChunker(self.chunking_config)
        self.buffer = StreamBuffer(self.buffer_config)
        
        # ìƒíƒœ
        self.state = StreamingState.INITIALIZING
        self.session_id = str(uuid.uuid4())
        self.metrics = StreamingMetrics()
        
        # ìŠ¤íŠ¸ë¦¬ë° ì œì–´
        self.is_streaming = False
        self.should_stop = False
        self._consumers: List[Callable] = []
        
        # ë°±í”„ë ˆì…” ì œì–´
        self.backpressure_threshold = 0.8
        self.flow_control_enabled = True
        
        # ì—ëŸ¬ ì²˜ë¦¬
        self.error_count = 0
        self.max_errors = 5
        self.retry_delay = 1.0
        
        # ëª¨ë‹ˆí„°ë§
        self.monitoring_enabled = True
        self.quality_metrics: deque = deque(maxlen=100)
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.results_dir = Path("monitoring/streaming_performance")
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    async def start_stream(self, content_source: AsyncGenerator[str, None]) -> AsyncGenerator[str, None]:
        """ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘"""
        self.state = StreamingState.STREAMING
        self.metrics.start_time = datetime.now()
        self.is_streaming = True
        
        logger.info(f"ğŸš€ ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° ì‹œì‘ (ì„¸ì…˜: {self.session_id})")
        
        try:
            # í”„ë¡œë“€ì„œ íƒœìŠ¤í¬ (ì»¨í…ì¸  ìƒì„± ë° ì²­í‚¹)
            producer_task = asyncio.create_task(
                self._producer_loop(content_source)
            )
            
            # ì»¨ìŠˆë¨¸ íƒœìŠ¤í¬ (SSE ì „ì†¡)
            consumer_task = asyncio.create_task(
                self._consumer_loop()
            )
            
            # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬
            monitoring_task = asyncio.create_task(
                self._monitoring_loop()
            ) if self.monitoring_enabled else None
            
            # SSE ìŠ¤íŠ¸ë¦¼ ìƒì„±
            async for sse_chunk in self._sse_generator():
                if self.should_stop:
                    break
                yield sse_chunk
                
                # ë°±í”„ë ˆì…” ì œì–´
                if self.flow_control_enabled:
                    await self._handle_backpressure()
            
            # ì •ë¦¬
            producer_task.cancel()
            consumer_task.cancel()
            if monitoring_task:
                monitoring_task.cancel()
            
            await self._cleanup_tasks([producer_task, consumer_task, monitoring_task])
            
        except Exception as e:
            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜: {e}")
            self.state = StreamingState.ERROR
            self.error_count += 1
            
            # ì—ëŸ¬ ì²­í¬ ì „ì†¡
            error_chunk = StreamChunk(
                id=str(uuid.uuid4()),
                type=ChunkType.ERROR,
                data={"error": str(e), "session_id": self.session_id},
                timestamp=datetime.now(),
                size_bytes=len(str(e)),
                sequence=-1
            )
            yield error_chunk.to_sse_format()
            
        finally:
            await self._finalize_stream()
    
    async def _producer_loop(self, content_source: AsyncGenerator[str, None]):
        """í”„ë¡œë“€ì„œ ë£¨í”„ - ì»¨í…ì¸  ìƒì„± ë° ì²­í‚¹"""
        try:
            async for content in content_source:
                if self.should_stop:
                    break
                
                # ë°±í”„ë ˆì…” ì²´í¬
                if self.buffer.backpressure_active:
                    await asyncio.sleep(0.1)
                    continue
                
                # ì»¨í…ì¸  ì²­í‚¹
                async for chunk in self.chunker.chunk_content(content, ChunkType.TEXT):
                    success = await self.buffer.add_chunk(chunk)
                    
                    if not success:
                        logger.warning("ë²„í¼ ê°€ë“ì°¸ìœ¼ë¡œ ì²­í¬ ë“œë¡­")
                        await asyncio.sleep(0.05)  # ì§§ì€ ë°±ì˜¤í”„
                        
                        # ì¬ì‹œë„
                        if await self.buffer.add_chunk(chunk):
                            logger.info("ì²­í¬ ì¬ì‹œë„ ì„±ê³µ")
                        else:
                            logger.error("ì²­í¬ ì¬ì‹œë„ ì‹¤íŒ¨")
                            break
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    self.metrics.total_chunks += 1
                    self.metrics.total_bytes += chunk.size_bytes
                    
                    if self.should_stop:
                        break
                        
        except Exception as e:
            logger.error(f"í”„ë¡œë“€ì„œ ë£¨í”„ ì˜¤ë¥˜: {e}")
            self.error_count += 1
    
    async def _consumer_loop(self):
        """ì»¨ìŠˆë¨¸ ë£¨í”„ - ë²„í¼ì—ì„œ ì²­í¬ ì†Œë¹„"""
        try:
            while self.is_streaming and not self.should_stop:
                # ë°°ì¹˜ë¡œ ì²­í¬ ê°€ì ¸ì˜¤ê¸°
                chunks = await self.buffer.get_chunks_batch(max_count=5)
                
                if not chunks:
                    await asyncio.sleep(0.01)  # 10ms ëŒ€ê¸°
                    continue
                
                # ì²­í¬ ì²˜ë¦¬
                for chunk in chunks:
                    await self._process_chunk(chunk)
                
        except Exception as e:
            logger.error(f"ì»¨ìŠˆë¨¸ ë£¨í”„ ì˜¤ë¥˜: {e}")
            self.error_count += 1
    
    async def _sse_generator(self) -> AsyncGenerator[str, None]:
        """SSE í˜•ì‹ìœ¼ë¡œ ì²­í¬ ì „ì†¡"""
        chunk_count = 0
        
        while self.is_streaming and not self.should_stop:
            chunk = await self.buffer.get_chunk()
            
            if chunk is None:
                # í•˜íŠ¸ë¹„íŠ¸ ì „ì†¡ (ì—°ê²° ìœ ì§€)
                if chunk_count % 50 == 0:  # ì£¼ê¸°ì ìœ¼ë¡œ
                    heartbeat = StreamChunk(
                        id=str(uuid.uuid4()),
                        type=ChunkType.HEARTBEAT,
                        data={"status": "alive", "session_id": self.session_id},
                        timestamp=datetime.now(),
                        size_bytes=0,
                        sequence=-1
                    )
                    yield heartbeat.to_sse_format()
                
                await asyncio.sleep(0.01)
                continue
            
            # SSE í˜•ì‹ìœ¼ë¡œ ë³€í™˜ í›„ ì „ì†¡
            sse_data = chunk.to_sse_format()
            yield sse_data
            
            chunk_count += 1
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
            await self._collect_quality_metrics(chunk)
    
    async def _handle_backpressure(self):
        """ë°±í”„ë ˆì…” ì²˜ë¦¬"""
        buffer_status = self.buffer.get_buffer_status()
        
        if buffer_status["backpressure_active"]:
            self.state = StreamingState.BACKPRESSURE
            
            # ì ì‘ì  ì§€ì—°
            usage_ratio = buffer_status["usage_ratio"]
            delay = 0.01 + (usage_ratio - self.backpressure_threshold) * 0.1
            
            await asyncio.sleep(delay)
            
            logger.debug(f"ë°±í”„ë ˆì…” ì²˜ë¦¬: {delay:.3f}ì´ˆ ì§€ì—°")
        
        elif self.state == StreamingState.BACKPRESSURE:
            self.state = StreamingState.STREAMING
    
    async def _process_chunk(self, chunk: StreamChunk):
        """ì²­í¬ ì²˜ë¦¬ (ì „ì²˜ë¦¬, ì••ì¶• ë“±)"""
        try:
            # ì••ì¶• ê³ ë ¤
            if (chunk.size_bytes > self.chunking_config.compression_threshold and
                chunk.type in [ChunkType.TEXT, ChunkType.JSON]):
                # ì—¬ê¸°ì— ì••ì¶• ë¡œì§ ì¶”ê°€ ê°€ëŠ¥
                pass
            
            # í’ˆì§ˆ ê²€ì¦
            await self._validate_chunk_quality(chunk)
            
        except Exception as e:
            logger.error(f"ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            self.error_count += 1
    
    async def _validate_chunk_quality(self, chunk: StreamChunk):
        """ì²­í¬ í’ˆì§ˆ ê²€ì¦"""
        # í¬ê¸° ê²€ì¦
        if chunk.size_bytes > self.chunking_config.max_chunk_size:
            logger.warning(f"ì²­í¬ í¬ê¸° ì´ˆê³¼: {chunk.size_bytes} > {self.chunking_config.max_chunk_size}")
        
        # íƒ€ì… ê²€ì¦
        if chunk.type == ChunkType.TEXT and isinstance(chunk.data, str):
            # UTF-8 ìœ íš¨ì„± ê²€ì‚¬
            try:
                chunk.data.encode('utf-8')
            except UnicodeEncodeError as e:
                logger.error(f"UTF-8 ì¸ì½”ë”© ì˜¤ë¥˜: {e}")
                raise
        
        # ìˆœì„œ ê²€ì¦ (ê¸°ë³¸ì ì¸ ì²´í¬)
        if hasattr(self, '_last_sequence') and chunk.sequence > 0:
            if chunk.sequence != self._last_sequence + 1:
                logger.warning(f"ì²­í¬ ìˆœì„œ ë¶ˆì¼ì¹˜: {chunk.sequence} (ì˜ˆìƒ: {self._last_sequence + 1})")
        
        self._last_sequence = chunk.sequence
    
    async def _collect_quality_metrics(self, chunk: StreamChunk):
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        current_time = datetime.now()
        
        # ì§€ì—°ì‹œê°„ ê³„ì‚°
        latency_ms = (current_time - chunk.timestamp).total_seconds() * 1000
        
        quality_metric = {
            "timestamp": current_time,
            "chunk_id": chunk.id,
            "latency_ms": latency_ms,
            "chunk_size": chunk.size_bytes,
            "buffer_usage": self.buffer.get_buffer_status()["usage_ratio"]
        }
        
        self.quality_metrics.append(quality_metric)
        
        # ì„±ëŠ¥ ì •ë³´ë¥¼ ì²­ì»¤ì— í”¼ë“œë°±
        if len(self.quality_metrics) >= 10:
            recent_metrics = list(self.quality_metrics)[-10:]
            avg_latency = statistics.mean([m["latency_ms"] for m in recent_metrics])
            avg_throughput = self.metrics.avg_throughput_bps
            
            self.chunker.update_performance(avg_throughput, avg_latency)
    
    async def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        try:
            while self.is_streaming and not self.should_stop:
                # ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ë¡œê¹…
                buffer_status = self.buffer.get_buffer_status()
                
                if self.metrics.total_chunks % 100 == 0:  # 100ì²­í¬ë§ˆë‹¤
                    logger.info(
                        f"ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ - ì²­í¬: {self.metrics.total_chunks}, "
                        f"ë²„í¼: {buffer_status['usage_ratio']:.2%}, "
                        f"ìƒíƒœ: {self.state.value}"
                    )
                
                await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ëª¨ë‹ˆí„°ë§
                
        except Exception as e:
            logger.error(f"ëª¨ë‹ˆí„°ë§ ë£¨í”„ ì˜¤ë¥˜: {e}")
    
    async def _cleanup_tasks(self, tasks: List[asyncio.Task]):
        """íƒœìŠ¤í¬ ì •ë¦¬"""
        for task in tasks:
            if task and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass
                except Exception as e:
                    logger.error(f"íƒœìŠ¤í¬ ì •ë¦¬ ì˜¤ë¥˜: {e}")
    
    async def _finalize_stream(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì¢…ë£Œ ì²˜ë¦¬"""
        self.is_streaming = False
        self.metrics.end_time = datetime.now()
        self.state = StreamingState.COMPLETED
        
        # ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°
        self._calculate_final_metrics()
        
        # ê²°ê³¼ ì €ì¥
        await self._save_streaming_results()
        
        logger.info(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ (ì„¸ì…˜: {self.session_id}) - "
                   f"ì²­í¬: {self.metrics.total_chunks}, "
                   f"ì²˜ë¦¬ëŸ‰: {self.metrics.avg_throughput_bps:.0f} bps")
    
    def _calculate_final_metrics(self):
        """ìµœì¢… ë©”íŠ¸ë¦­ ê³„ì‚°"""
        if self.metrics.total_chunks > 0:
            self.metrics.avg_chunk_size = self.metrics.total_bytes / self.metrics.total_chunks
        
        if self.quality_metrics:
            latencies = [m["latency_ms"] for m in self.quality_metrics]
            self.metrics.latency_ms = statistics.mean(latencies)
        
        buffer_status = self.buffer.get_buffer_status()
        self.metrics.buffer_usage_percent = buffer_status["usage_ratio"] * 100
        self.metrics.backpressure_events = buffer_status["stats"]["backpressure_events"]
        self.metrics.error_count = self.error_count
    
    async def _save_streaming_results(self):
        """ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì €ì¥"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results = {
                "session_id": self.session_id,
                "metrics": {
                    "total_chunks": self.metrics.total_chunks,
                    "total_bytes": self.metrics.total_bytes,
                    "avg_chunk_size": self.metrics.avg_chunk_size,
                    "duration_seconds": self.metrics.duration_seconds,
                    "avg_throughput_bps": self.metrics.avg_throughput_bps,
                    "latency_ms": self.metrics.latency_ms,
                    "backpressure_events": self.metrics.backpressure_events,
                    "error_count": self.metrics.error_count
                },
                "buffer_config": {
                    "max_buffer_size": self.buffer_config.max_buffer_size,
                    "high_watermark": self.buffer_config.high_watermark,
                    "low_watermark": self.buffer_config.low_watermark
                },
                "chunking_config": {
                    "target_chunk_size": self.chunking_config.target_chunk_size,
                    "adaptive_chunking": self.chunking_config.adaptive_chunking
                },
                "quality_metrics": list(self.quality_metrics),
                "buffer_stats": self.buffer.get_buffer_status()["stats"]
            }
            
            file_path = self.results_dir / f"streaming_session_{timestamp}.json"
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"ğŸ’¾ ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ì €ì¥: {file_path}")
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def stop_stream(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€"""
        self.should_stop = True
        logger.info(f"â¹ï¸ ìŠ¤íŠ¸ë¦¬ë° ì¤‘ì§€ ìš”ì²­ (ì„¸ì…˜: {self.session_id})")
    
    def get_streaming_status(self) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ë°˜í™˜"""
        buffer_status = self.buffer.get_buffer_status()
        
        return {
            "session_id": self.session_id,
            "state": self.state.value,
            "is_streaming": self.is_streaming,
            "metrics": {
                "total_chunks": self.metrics.total_chunks,
                "total_bytes": self.metrics.total_bytes,
                "avg_throughput_bps": self.metrics.avg_throughput_bps,
                "error_count": self.error_count
            },
            "buffer_status": buffer_status,
            "chunker_status": {
                "current_chunk_size": self.chunker.current_chunk_size,
                "adaptive_enabled": self.chunking_config.adaptive_chunking
            }
        }


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
async def sample_content_generator() -> AsyncGenerator[str, None]:
    """ìƒ˜í”Œ ì»¨í…ì¸  ìƒì„±ê¸°"""
    for i in range(100):
        content = f"ì´ê²ƒì€ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤. ì²­í¬ ë²ˆí˜¸: {i}. " * 5
        yield content
        await asyncio.sleep(0.1)  # 100ms ê°„ê²©

async def test_optimized_streaming():
    """ìµœì í™”ëœ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
    # ì„¤ì •
    buffer_config = BufferConfig(
        max_buffer_size=1024*512,  # 512KB
        high_watermark=0.8,
        low_watermark=0.3
    )
    
    chunking_config = ChunkingConfig(
        target_chunk_size=1024,
        adaptive_chunking=True
    )
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„±
    pipeline = OptimizedStreamingPipeline(buffer_config, chunking_config)
    
    # ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
    content_source = sample_content_generator()
    
    chunk_count = 0
    async for sse_chunk in pipeline.start_stream(content_source):
        print(f"ìˆ˜ì‹ í•œ SSE ì²­í¬: {len(sse_chunk)} bytes")
        chunk_count += 1
        
        if chunk_count >= 50:  # 50ê°œ ì²­í¬ë¡œ ì œí•œ
            pipeline.stop_stream()
            break
    
    # ê²°ê³¼ ì¶œë ¥
    status = pipeline.get_streaming_status()
    print(f"ğŸš€ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ: {status['metrics']['total_chunks']}ê°œ ì²­í¬, "
          f"{status['metrics']['total_bytes']} bytes")

if __name__ == "__main__":
    asyncio.run(test_optimized_streaming()) 