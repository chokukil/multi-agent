#!/usr/bin/env python3
"""
ğŸ“Š Enhanced Log Analyzer for CherryAI Production Environment

í–¥ìƒëœ ë¡œê·¸ ë¶„ì„ ë° íŒ¨í„´ íƒì§€ ì‹œìŠ¤í…œ
- ì‹¤ì‹œê°„ ë¡œê·¸ íŒŒì¼ ëª¨ë‹ˆí„°ë§
- ì—ëŸ¬ íŒ¨í„´ ìë™ íƒì§€
- ì„±ëŠ¥ ë³‘ëª©ì  ë¶„ì„
- ì´ìƒ í–‰ë™ íŒ¨í„´ ê°ì§€
- ë¡œê·¸ ì§‘ê³„ ë° ìš”ì•½
- ìë™ ë¡œê·¸ ë¡œí…Œì´ì…˜
- ì˜ˆì¸¡ì  ë¬¸ì œ íƒì§€

Author: CherryAI Production Team
"""

import os
import re
import json
import time
import threading
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Pattern
from dataclasses import dataclass, asdict, field
from collections import defaultdict, deque
from pathlib import Path
import gzip
import shutil
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# ìš°ë¦¬ ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from core.integrated_alert_system import get_integrated_alert_system, AlertSeverity, AlertCategory
    ALERT_SYSTEM_AVAILABLE = True
except ImportError:
    ALERT_SYSTEM_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class LogEntry:
    """ë¡œê·¸ ì—”íŠ¸ë¦¬"""
    timestamp: datetime
    level: str
    source: str
    message: str
    raw_line: str
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LogPattern:
    """ë¡œê·¸ íŒ¨í„´ ì •ì˜"""
    pattern_id: str
    name: str
    regex: Pattern
    severity: AlertSeverity
    category: str
    description: str
    threshold_count: int = 5
    time_window_minutes: int = 5
    enabled: bool = True


@dataclass
class PatternMatch:
    """íŒ¨í„´ ë§¤ì¹˜ ê²°ê³¼"""
    pattern_id: str
    pattern_name: str
    matched_entries: List[LogEntry]
    count: int
    first_occurrence: datetime
    last_occurrence: datetime
    severity: AlertSeverity
    details: Dict[str, Any] = field(default_factory=dict)


@dataclass
class LogAnalysisReport:
    """ë¡œê·¸ ë¶„ì„ ë³´ê³ ì„œ"""
    start_time: datetime
    end_time: datetime
    total_entries: int
    entries_by_level: Dict[str, int]
    entries_by_source: Dict[str, int]
    pattern_matches: List[PatternMatch]
    performance_metrics: Dict[str, Any]
    anomalies: List[Dict[str, Any]]
    recommendations: List[str]


class LogFileWatcher(FileSystemEventHandler):
    """ë¡œê·¸ íŒŒì¼ ê°ì‹œì"""
    
    def __init__(self, analyzer):
        self.analyzer = analyzer
        super().__init__()
    
    def on_modified(self, event):
        """íŒŒì¼ ìˆ˜ì • ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        if not event.is_directory and event.src_path.endswith('.log'):
            self.analyzer._process_log_file_update(event.src_path)


class EnhancedLogAnalyzer:
    """í–¥ìƒëœ ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self, log_directory: str = "logs"):
        self.log_directory = Path(log_directory)
        self.log_directory.mkdir(exist_ok=True)
        
        # ë¡œê·¸ ì—”íŠ¸ë¦¬ ì €ì¥
        self.recent_entries: deque = deque(maxlen=10000)
        self.pattern_matches: Dict[str, List[PatternMatch]] = defaultdict(list)
        
        # íŒ¨í„´ ì •ì˜
        self.patterns: Dict[str, LogPattern] = {}
        self._initialize_patterns()
        
        # íŒŒì¼ ê°ì‹œ
        self.observer = Observer()
        self.watcher = LogFileWatcher(self)
        self.monitoring_active = False
        
        # ë¶„ì„ ìŠ¤ë ˆë“œ
        self.analysis_thread = None
        self.analysis_interval = 30  # 30ì´ˆë§ˆë‹¤ ë¶„ì„
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_metrics = {
            "processed_lines": 0,
            "processing_time": 0.0,
            "error_count": 0,
            "pattern_matches": 0
        }
        
        # ì•Œë¦¼ ì‹œìŠ¤í…œ ì—°ë™
        if ALERT_SYSTEM_AVAILABLE:
            self.alert_system = get_integrated_alert_system()
        else:
            self.alert_system = None
        
        # íŒŒì¼ ì¶”ì 
        self.file_positions: Dict[str, int] = {}
        
        logger.info(f"ğŸ“Š ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ - ë””ë ‰í† ë¦¬: {self.log_directory}")
    
    def _initialize_patterns(self):
        """ìœ„í—˜í•œ ë¡œê·¸ íŒ¨í„´ ì´ˆê¸°í™”"""
        patterns_config = [
            {
                "pattern_id": "error_burst",
                "name": "ì—ëŸ¬ ë²„ìŠ¤íŠ¸",
                "regex": r"ERROR|CRITICAL|FATAL",
                "severity": AlertSeverity.HIGH,
                "category": "error",
                "description": "ì§§ì€ ì‹œê°„ ë‚´ ë‹¤ìˆ˜ì˜ ì—ëŸ¬ ë°œìƒ",
                "threshold_count": 10,
                "time_window_minutes": 5
            },
            {
                "pattern_id": "memory_error",
                "name": "ë©”ëª¨ë¦¬ ì˜¤ë¥˜",
                "regex": r"MemoryError|OutOfMemoryError|memory|Memory.*failed",
                "severity": AlertSeverity.CRITICAL,
                "category": "system",
                "description": "ë©”ëª¨ë¦¬ ê´€ë ¨ ì˜¤ë¥˜ ê°ì§€",
                "threshold_count": 1,
                "time_window_minutes": 1
            },
            {
                "pattern_id": "connection_timeout",
                "name": "ì—°ê²° íƒ€ì„ì•„ì›ƒ",
                "regex": r"timeout|TimeoutError|Connection.*timeout|Request.*timeout",
                "severity": AlertSeverity.HIGH,
                "category": "network",
                "description": "ë„¤íŠ¸ì›Œí¬ ì—°ê²° íƒ€ì„ì•„ì›ƒ",
                "threshold_count": 5,
                "time_window_minutes": 3
            },
            {
                "pattern_id": "agent_failure",
                "name": "ì—ì´ì „íŠ¸ ì‹¤íŒ¨",
                "regex": r"Agent.*failed|Agent.*error|A2A.*failed|A2A.*error",
                "severity": AlertSeverity.CRITICAL,
                "category": "agent",
                "description": "A2A ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨",
                "threshold_count": 3,
                "time_window_minutes": 2
            },
            {
                "pattern_id": "database_error",
                "name": "ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜",
                "regex": r"DatabaseError|Connection.*refused|SQL.*error|database.*connection",
                "severity": AlertSeverity.HIGH,
                "category": "database",
                "description": "ë°ì´í„°ë² ì´ìŠ¤ ê´€ë ¨ ì˜¤ë¥˜",
                "threshold_count": 3,
                "time_window_minutes": 5
            },
            {
                "pattern_id": "performance_degradation",
                "name": "ì„±ëŠ¥ ì €í•˜",
                "regex": r"slow|performance|optimization|ì²˜ë¦¬.*ëŠë¦¼|ì‘ë‹µ.*ì§€ì—°",
                "severity": AlertSeverity.MEDIUM,
                "category": "performance",
                "description": "ì‹œìŠ¤í…œ ì„±ëŠ¥ ì €í•˜ ì§•í›„",
                "threshold_count": 15,
                "time_window_minutes": 10
            },
            {
                "pattern_id": "security_alert",
                "name": "ë³´ì•ˆ ê²½ê³ ",
                "regex": r"security|Security|unauthorized|Unauthorized|forbidden|Forbidden|attack",
                "severity": AlertSeverity.CRITICAL,
                "category": "security",
                "description": "ë³´ì•ˆ ê´€ë ¨ ê²½ê³ ",
                "threshold_count": 1,
                "time_window_minutes": 1
            },
            {
                "pattern_id": "api_error",
                "name": "API ì˜¤ë¥˜",
                "regex": r"API.*error|HTTP.*[45]\d\d|400|401|403|404|500|502|503|504",
                "severity": AlertSeverity.HIGH,
                "category": "api",
                "description": "API í˜¸ì¶œ ì˜¤ë¥˜",
                "threshold_count": 10,
                "time_window_minutes": 5
            }
        ]
        
        for pattern_config in patterns_config:
            pattern = LogPattern(
                pattern_id=pattern_config["pattern_id"],
                name=pattern_config["name"],
                regex=re.compile(pattern_config["regex"], re.IGNORECASE),
                severity=pattern_config["severity"],
                category=pattern_config["category"],
                description=pattern_config["description"],
                threshold_count=pattern_config["threshold_count"],
                time_window_minutes=pattern_config["time_window_minutes"]
            )
            self.patterns[pattern.pattern_id] = pattern
    
    def start_monitoring(self):
        """ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if not self.monitoring_active:
            self.monitoring_active = True
            
            # íŒŒì¼ ê°ì‹œ ì‹œì‘
            self.observer.schedule(self.watcher, str(self.log_directory), recursive=True)
            self.observer.start()
            
            # ë¶„ì„ ìŠ¤ë ˆë“œ ì‹œì‘
            self.analysis_thread = threading.Thread(target=self._analysis_loop, daemon=True)
            self.analysis_thread.start()
            
            # ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ì´ˆê¸° ì²˜ë¦¬
            self._process_existing_logs()
            
            logger.info("ğŸ” ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    def stop_monitoring(self):
        """ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.monitoring_active = False
        
        if self.observer:
            self.observer.stop()
            self.observer.join()
        
        if self.analysis_thread:
            self.analysis_thread.join(timeout=10)
        
        logger.info("ğŸ›‘ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    def _process_existing_logs(self):
        """ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬"""
        log_files = list(self.log_directory.glob("*.log"))
        
        for log_file in log_files:
            try:
                self._process_log_file(str(log_file))
            except Exception as e:
                logger.error(f"ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {log_file}: {e}")
    
    def _process_log_file_update(self, file_path: str):
        """ë¡œê·¸ íŒŒì¼ ì—…ë°ì´íŠ¸ ì²˜ë¦¬"""
        try:
            self._process_log_file(file_path, incremental=True)
        except Exception as e:
            logger.error(f"ë¡œê·¸ íŒŒì¼ ì—…ë°ì´íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
    
    def _process_log_file(self, file_path: str, incremental: bool = False):
        """ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬"""
        start_time = time.time()
        processed_lines = 0
        
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                # ì¦ë¶„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìœ„ì¹˜ ë³µì›
                if incremental and file_path in self.file_positions:
                    f.seek(self.file_positions[file_path])
                
                for line in f:
                    line = line.strip()
                    if line:
                        entry = self._parse_log_line(line, file_path)
                        if entry:
                            self.recent_entries.append(entry)
                            self._check_patterns(entry)
                            processed_lines += 1
                
                # íŒŒì¼ ìœ„ì¹˜ ì €ì¥
                self.file_positions[file_path] = f.tell()
        
        except Exception as e:
            logger.error(f"ë¡œê·¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
            self.performance_metrics["error_count"] += 1
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        processing_time = time.time() - start_time
        self.performance_metrics["processed_lines"] += processed_lines
        self.performance_metrics["processing_time"] += processing_time
        
        if processed_lines > 0:
            logger.debug(f"ë¡œê·¸ ì²˜ë¦¬ ì™„ë£Œ: {file_path} ({processed_lines}ì¤„, {processing_time:.2f}ì´ˆ)")
    
    def _parse_log_line(self, line: str, source_file: str) -> Optional[LogEntry]:
        """ë¡œê·¸ ë¼ì¸ íŒŒì‹±"""
        try:
            # ë‹¤ì–‘í•œ ë¡œê·¸ í˜•ì‹ ì§€ì›
            patterns = [
                # í‘œì¤€ Python logging í˜•ì‹
                r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3}) - (\w+) - (\w+) - (.+)',
                # ISO ì‹œê°„ í˜•ì‹
                r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}[\.\d]*Z?) (\w+) (.+)',
                # ê°„ë‹¨í•œ ì‹œê°„ í˜•ì‹
                r'(\d{2}:\d{2}:\d{2}) \[(\w+)\] (.+)',
                # íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ëŠ” í˜•ì‹
                r'(\w+): (.+)'
            ]
            
            timestamp = datetime.now()  # ê¸°ë³¸ê°’
            level = "INFO"
            message = line
            
            for pattern in patterns:
                match = re.match(pattern, line)
                if match:
                    groups = match.groups()
                    
                    if len(groups) >= 4:  # í‘œì¤€ Python logging
                        timestamp_str, level, logger_name, message = groups
                        try:
                            timestamp = datetime.strptime(timestamp_str, '%Y-%m-%d %H:%M:%S,%f')
                        except:
                            pass
                    elif len(groups) >= 3:  # ISO ë˜ëŠ” ê°„ë‹¨í•œ í˜•ì‹
                        timestamp_str, level, message = groups
                        try:
                            # ISO í˜•ì‹ ì‹œë„
                            timestamp = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
                        except:
                            try:
                                # ì‹œê°„ë§Œ ìˆëŠ” í˜•ì‹
                                time_part = datetime.strptime(timestamp_str, '%H:%M:%S').time()
                                timestamp = datetime.combine(datetime.now().date(), time_part)
                            except:
                                pass
                    elif len(groups) >= 2:  # ë ˆë²¨ê³¼ ë©”ì‹œì§€ë§Œ
                        level, message = groups
                    
                    break
            
            return LogEntry(
                timestamp=timestamp,
                level=level.upper(),
                source=Path(source_file).name,
                message=message.strip(),
                raw_line=line
            )
        
        except Exception as e:
            logger.debug(f"ë¡œê·¸ ë¼ì¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _check_patterns(self, entry: LogEntry):
        """ë¡œê·¸ ì—”íŠ¸ë¦¬ì— ëŒ€í•œ íŒ¨í„´ ì²´í¬"""
        for pattern_id, pattern in self.patterns.items():
            if not pattern.enabled:
                continue
            
            if pattern.regex.search(entry.message) or pattern.regex.search(entry.raw_line):
                self._record_pattern_match(pattern_id, entry)
    
    def _record_pattern_match(self, pattern_id: str, entry: LogEntry):
        """íŒ¨í„´ ë§¤ì¹˜ ê¸°ë¡"""
        pattern = self.patterns[pattern_id]
        
        # ì‹œê°„ ìœˆë„ìš° ë‚´ì˜ ë§¤ì¹˜ë“¤ë§Œ ìœ ì§€
        cutoff_time = datetime.now() - timedelta(minutes=pattern.time_window_minutes)
        recent_matches = [
            match for match in self.pattern_matches[pattern_id]
            if match.last_occurrence >= cutoff_time
        ]
        
        # ìƒˆë¡œìš´ ë§¤ì¹˜ ì¶”ê°€
        if recent_matches:
            # ê¸°ì¡´ ë§¤ì¹˜ì— ì¶”ê°€
            latest_match = recent_matches[-1]
            latest_match.matched_entries.append(entry)
            latest_match.count += 1
            latest_match.last_occurrence = entry.timestamp
        else:
            # ìƒˆë¡œìš´ ë§¤ì¹˜ ìƒì„±
            new_match = PatternMatch(
                pattern_id=pattern_id,
                pattern_name=pattern.name,
                matched_entries=[entry],
                count=1,
                first_occurrence=entry.timestamp,
                last_occurrence=entry.timestamp,
                severity=pattern.severity
            )
            recent_matches.append(new_match)
        
        self.pattern_matches[pattern_id] = recent_matches
        self.performance_metrics["pattern_matches"] += 1
        
        # ì„ê³„ê°’ ì²´í¬
        latest_match = recent_matches[-1]
        if latest_match.count >= pattern.threshold_count:
            self._trigger_pattern_alert(pattern, latest_match)
    
    def _trigger_pattern_alert(self, pattern: LogPattern, match: PatternMatch):
        """íŒ¨í„´ ê¸°ë°˜ ì•Œë¦¼ íŠ¸ë¦¬ê±°"""
        logger.warning(f"ğŸš¨ íŒ¨í„´ ê°ì§€: {pattern.name} ({match.count}íšŒ in {pattern.time_window_minutes}ë¶„)")
        
        # ì•Œë¦¼ ì‹œìŠ¤í…œì— ì „ì†¡ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” alert_system.create_pattern_alert() ì‚¬ìš©)
        if self.alert_system:
            try:
                # ì—¬ê¸°ì„œ ì•Œë¦¼ ì‹œìŠ¤í…œì— íŒ¨í„´ ê¸°ë°˜ ì•Œë¦¼ ì „ì†¡
                pass
            except Exception as e:
                logger.error(f"íŒ¨í„´ ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
    
    def _analysis_loop(self):
        """ë¶„ì„ ë£¨í”„"""
        while self.monitoring_active:
            try:
                # ì •ê¸°ì ì¸ ë¶„ì„ ìˆ˜í–‰
                self._perform_periodic_analysis()
                
                # ë¡œê·¸ ë¡œí…Œì´ì…˜ ì²´í¬
                self._check_log_rotation()
                
                time.sleep(self.analysis_interval)
                
            except Exception as e:
                logger.error(f"ë¶„ì„ ë£¨í”„ ì˜¤ë¥˜: {e}")
                time.sleep(60)
    
    def _perform_periodic_analysis(self):
        """ì •ê¸°ì ì¸ ë¶„ì„ ìˆ˜í–‰"""
        # ìµœê·¼ ë¡œê·¸ ì—”íŠ¸ë¦¬ ë¶„ì„
        if len(self.recent_entries) < 10:
            return
        
        # ì´ìƒ ì§•í›„ íƒì§€
        anomalies = self._detect_anomalies()
        
        if anomalies:
            logger.info(f"ğŸ” {len(anomalies)}ê°œ ì´ìƒ ì§•í›„ ê°ì§€")
            for anomaly in anomalies:
                logger.warning(f"  - {anomaly['type']}: {anomaly['description']}")
    
    def _detect_anomalies(self) -> List[Dict[str, Any]]:
        """ì´ìƒ ì§•í›„ íƒì§€"""
        anomalies = []
        
        # ìµœê·¼ 30ë¶„ê°„ì˜ ì—”íŠ¸ë¦¬ ë¶„ì„
        cutoff_time = datetime.now() - timedelta(minutes=30)
        recent_entries = [entry for entry in self.recent_entries if entry.timestamp >= cutoff_time]
        
        if not recent_entries:
            return anomalies
        
        # ì—ëŸ¬ìœ¨ ì´ìƒ ì¦ê°€
        error_entries = [entry for entry in recent_entries if entry.level in ['ERROR', 'CRITICAL', 'FATAL']]
        error_rate = len(error_entries) / len(recent_entries) * 100
        
        if error_rate > 10:  # 10% ì´ìƒ ì—ëŸ¬ìœ¨
            anomalies.append({
                "type": "high_error_rate",
                "description": f"ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€: {error_rate:.1f}%",
                "severity": "high",
                "details": {"error_rate": error_rate, "error_count": len(error_entries)}
            })
        
        # ë¡œê·¸ ë³¼ë¥¨ ê¸‰ì¦
        entry_count = len(recent_entries)
        if entry_count > 1000:  # 30ë¶„ì— 1000ê°œ ì´ìƒ
            anomalies.append({
                "type": "high_log_volume",
                "description": f"ë¡œê·¸ ë³¼ë¥¨ ê¸‰ì¦: {entry_count}ê°œ/30ë¶„",
                "severity": "medium",
                "details": {"entry_count": entry_count}
            })
        
        # íŠ¹ì • ì†ŒìŠ¤ì—ì„œì˜ ê³¼ë„í•œ ë¡œê·¸
        source_counts = defaultdict(int)
        for entry in recent_entries:
            source_counts[entry.source] += 1
        
        for source, count in source_counts.items():
            if count > 500:  # í•œ ì†ŒìŠ¤ì—ì„œ 500ê°œ ì´ìƒ
                anomalies.append({
                    "type": "source_log_burst",
                    "description": f"{source}ì—ì„œ ê³¼ë„í•œ ë¡œê·¸: {count}ê°œ",
                    "severity": "medium",
                    "details": {"source": source, "count": count}
                })
        
        return anomalies
    
    def _check_log_rotation(self):
        """ë¡œê·¸ ë¡œí…Œì´ì…˜ ì²´í¬"""
        try:
            for log_file in self.log_directory.glob("*.log"):
                file_size = log_file.stat().st_size
                
                # íŒŒì¼ í¬ê¸°ê°€ 100MB ì´ìƒì´ë©´ ë¡œí…Œì´ì…˜
                if file_size > 100 * 1024 * 1024:
                    self._rotate_log_file(log_file)
        
        except Exception as e:
            logger.error(f"ë¡œê·¸ ë¡œí…Œì´ì…˜ ì²´í¬ ì‹¤íŒ¨: {e}")
    
    def _rotate_log_file(self, log_file: Path):
        """ë¡œê·¸ íŒŒì¼ ë¡œí…Œì´ì…˜"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            rotated_name = f"{log_file.stem}_{timestamp}.log.gz"
            rotated_path = log_file.parent / rotated_name
            
            # ì••ì¶•í•˜ì—¬ ë°±ì—…
            with open(log_file, 'rb') as f_in:
                with gzip.open(rotated_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            # ì›ë³¸ íŒŒì¼ ì´ˆê¸°í™”
            with open(log_file, 'w') as f:
                f.write(f"# Log rotated at {datetime.now().isoformat()}\n")
            
            # íŒŒì¼ ìœ„ì¹˜ ì´ˆê¸°í™”
            if str(log_file) in self.file_positions:
                self.file_positions[str(log_file)] = 0
            
            logger.info(f"ğŸ“¦ ë¡œê·¸ íŒŒì¼ ë¡œí…Œì´ì…˜ ì™„ë£Œ: {log_file} -> {rotated_name}")
        
        except Exception as e:
            logger.error(f"ë¡œê·¸ íŒŒì¼ ë¡œí…Œì´ì…˜ ì‹¤íŒ¨ {log_file}: {e}")
    
    def generate_analysis_report(self, hours: int = 24) -> LogAnalysisReport:
        """ë¡œê·¸ ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""
        end_time = datetime.now()
        start_time = end_time - timedelta(hours=hours)
        
        # í•´ë‹¹ ê¸°ê°„ì˜ ì—”íŠ¸ë¦¬ í•„í„°ë§
        period_entries = [
            entry for entry in self.recent_entries
            if start_time <= entry.timestamp <= end_time
        ]
        
        # ë ˆë²¨ë³„ ì§‘ê³„
        entries_by_level = defaultdict(int)
        for entry in period_entries:
            entries_by_level[entry.level] += 1
        
        # ì†ŒìŠ¤ë³„ ì§‘ê³„
        entries_by_source = defaultdict(int)
        for entry in period_entries:
            entries_by_source[entry.source] += 1
        
        # íŒ¨í„´ ë§¤ì¹˜ ìˆ˜ì§‘
        recent_pattern_matches = []
        for pattern_id, matches in self.pattern_matches.items():
            for match in matches:
                if start_time <= match.first_occurrence <= end_time:
                    recent_pattern_matches.append(match)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(
            period_entries, recent_pattern_matches, entries_by_level
        )
        
        return LogAnalysisReport(
            start_time=start_time,
            end_time=end_time,
            total_entries=len(period_entries),
            entries_by_level=dict(entries_by_level),
            entries_by_source=dict(entries_by_source),
            pattern_matches=recent_pattern_matches,
            performance_metrics=self.performance_metrics.copy(),
            anomalies=self._detect_anomalies(),
            recommendations=recommendations
        )
    
    def _generate_recommendations(self, entries: List[LogEntry], 
                                pattern_matches: List[PatternMatch],
                                entries_by_level: Dict[str, int]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        total_entries = len(entries)
        if total_entries == 0:
            return ["ë¡œê·¸ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."]
        
        # ì—ëŸ¬ìœ¨ ì²´í¬
        error_count = entries_by_level.get('ERROR', 0) + entries_by_level.get('CRITICAL', 0)
        error_rate = (error_count / total_entries) * 100
        
        if error_rate > 5:
            recommendations.append(f"ë†’ì€ ì—ëŸ¬ìœ¨ ê°ì§€ ({error_rate:.1f}%) - ì‹œìŠ¤í…œ ì ê²€ í•„ìš”")
        
        # íŒ¨í„´ ë§¤ì¹˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if pattern_matches:
            critical_patterns = [m for m in pattern_matches if m.severity == AlertSeverity.CRITICAL]
            if critical_patterns:
                recommendations.append("ì‹¬ê°í•œ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤ - ì¦‰ì‹œ ëŒ€ì‘ í•„ìš”")
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if self.performance_metrics["error_count"] > 10:
            recommendations.append("ë¡œê·¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë¹ˆë²ˆí•©ë‹ˆë‹¤ - ë¡œê·¸ í˜•ì‹ í™•ì¸ í•„ìš”")
        
        if not recommendations:
            recommendations.append("ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")
        
        return recommendations
    
    def get_monitoring_status(self) -> Dict[str, Any]:
        """ëª¨ë‹ˆí„°ë§ ìƒíƒœ ë°˜í™˜"""
        return {
            "monitoring_active": self.monitoring_active,
            "recent_entries_count": len(self.recent_entries),
            "patterns_defined": len(self.patterns),
            "active_patterns": sum(1 for p in self.patterns.values() if p.enabled),
            "performance_metrics": self.performance_metrics.copy(),
            "log_files_monitored": len(self.file_positions)
        }


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_log_analyzer_instance = None

def get_enhanced_log_analyzer() -> EnhancedLogAnalyzer:
    """í–¥ìƒëœ ë¡œê·¸ ë¶„ì„ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _log_analyzer_instance
    if _log_analyzer_instance is None:
        _log_analyzer_instance = EnhancedLogAnalyzer()
    return _log_analyzer_instance


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    analyzer = get_enhanced_log_analyzer()
    analyzer.start_monitoring()
    
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        time.sleep(60)
        
        # ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
        report = analyzer.generate_analysis_report(hours=1)
        print(f"\nğŸ“Š ë¡œê·¸ ë¶„ì„ ë³´ê³ ì„œ")
        print(f"ì´ ì—”íŠ¸ë¦¬: {report.total_entries}ê°œ")
        print(f"íŒ¨í„´ ë§¤ì¹˜: {len(report.pattern_matches)}ê°œ")
        print(f"ì´ìƒ ì§•í›„: {len(report.anomalies)}ê°œ")
        
    except KeyboardInterrupt:
        analyzer.stop_monitoring()
        print("ë¡œê·¸ ë¶„ì„ ì‹œìŠ¤í…œ ì¢…ë£Œ") 