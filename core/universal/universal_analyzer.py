"""
ë²”ìš©ì  ë¶„ì„ ì—”ì§„ (Universal Analyzer)
Phase 3.2: ë°ì´í„°ì…‹ ë…ë¦½ì  LLM First ë¶„ì„ ì‹œìŠ¤í…œ

í•µì‹¬ ì›ì¹™:
- ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ë²”ìš©ì  ë¶„ì„
- LLM ê¸°ë°˜ ë™ì  ì „ëµ ìˆ˜ë¦½
- ë©”íƒ€ ëŸ¬ë‹ì„ í†µí•œ ì§€ì†ì  ê°œì„ 
- ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì ì‘í˜• ë¶„ì„
- ì™„ì „í•œ í•˜ë“œì½”ë”© ì œê±°
"""

import asyncio
import json
import logging
import statistics
import numpy as np
import pandas as pd
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Union, Tuple, Callable
from datetime import datetime
from collections import defaultdict
from enum import Enum
from pathlib import Path

# LLM í´ë¼ì´ì–¸íŠ¸
from openai import AsyncOpenAI

logger = logging.getLogger(__name__)

class AnalysisContext(Enum):
    """ë¶„ì„ ì»¨í…ìŠ¤íŠ¸"""
    EXPLORATION = "exploration"         # íƒìƒ‰ì  ë¶„ì„
    PREDICTION = "prediction"          # ì˜ˆì¸¡ ë¶„ì„
    CLASSIFICATION = "classification"  # ë¶„ë¥˜ ë¶„ì„
    CLUSTERING = "clustering"          # í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„
    ANOMALY_DETECTION = "anomaly_detection"  # ì´ìƒ íƒì§€
    TIME_SERIES = "time_series"        # ì‹œê³„ì—´ ë¶„ì„
    ASSOCIATION = "association"        # ì—°ê´€ì„± ë¶„ì„
    CAUSAL = "causal"                 # ì¸ê³¼ê´€ê³„ ë¶„ì„

class DataCharacteristics(Enum):
    """ë°ì´í„° íŠ¹ì„±"""
    NUMERICAL = "numerical"
    CATEGORICAL = "categorical"
    TEXT = "text"
    TEMPORAL = "temporal"
    MIXED = "mixed"
    SPARSE = "sparse"
    HIGH_DIMENSIONAL = "high_dimensional"
    IMBALANCED = "imbalanced"

@dataclass
class DataProfile:
    """ë°ì´í„° í”„ë¡œíŒŒì¼"""
    shape: Tuple[int, int]
    column_types: Dict[str, str]
    missing_rates: Dict[str, float]
    data_characteristics: List[DataCharacteristics]
    statistical_summary: Dict[str, Any]
    quality_score: float
    complexity_level: str
    suggested_contexts: List[AnalysisContext]
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AnalysisStrategy:
    """ë¶„ì„ ì „ëµ"""
    context: AnalysisContext
    priority_steps: List[str]
    techniques: List[str]
    expected_insights: List[str]
    confidence: float
    reasoning: str
    adaptive_parameters: Dict[str, Any] = field(default_factory=dict)

@dataclass
class AnalysisResult:
    """ë¶„ì„ ê²°ê³¼"""
    insights: List[str]
    visualizations: List[Dict[str, Any]]
    statistical_tests: List[Dict[str, Any]]
    recommendations: List[str]
    confidence_scores: Dict[str, float]
    methodology: str
    limitations: List[str]
    next_steps: List[str]
    metadata: Dict[str, Any] = field(default_factory=dict)

class UniversalDataProfiler:
    """ë²”ìš© ë°ì´í„° í”„ë¡œíŒŒì¼ëŸ¬"""
    
    def __init__(self):
        self.llm_client = AsyncOpenAI()
        
        # ë°ì´í„° íŠ¹ì„± íƒì§€ ê·œì¹™ (LLM ë³´ì™„ìš©)
        self.characteristic_detectors = {
            DataCharacteristics.NUMERICAL: self._detect_numerical,
            DataCharacteristics.CATEGORICAL: self._detect_categorical,
            DataCharacteristics.TEXT: self._detect_text,
            DataCharacteristics.TEMPORAL: self._detect_temporal,
            DataCharacteristics.SPARSE: self._detect_sparse,
            DataCharacteristics.HIGH_DIMENSIONAL: self._detect_high_dimensional,
            DataCharacteristics.IMBALANCED: self._detect_imbalanced
        }
    
    async def profile_data(self, data: pd.DataFrame, user_context: str = "") -> DataProfile:
        """ë°ì´í„° í”„ë¡œíŒŒì¼ë§ (ì™„ì „ ë²”ìš©ì )"""
        logger.info(f"ğŸ“Š ë²”ìš© ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ì‹œì‘ (shape: {data.shape})")
        
        # 1. ê¸°ë³¸ êµ¬ì¡° ë¶„ì„
        basic_profile = self._analyze_basic_structure(data)
        
        # 2. ë°ì´í„° íŠ¹ì„± íƒì§€
        characteristics = await self._detect_data_characteristics(data)
        
        # 3. í†µê³„ì  ìš”ì•½ (ë²”ìš©ì )
        statistical_summary = await self._generate_statistical_summary(data)
        
        # 4. í’ˆì§ˆ í‰ê°€
        quality_score = self._assess_data_quality(data, basic_profile)
        
        # 5. LLM ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì œì•ˆ
        suggested_contexts = await self._suggest_analysis_contexts(
            data, characteristics, user_context
        )
        
        # 6. ë³µì¡ë„ í‰ê°€
        complexity_level = self._assess_complexity(data, characteristics)
        
        profile = DataProfile(
            shape=data.shape,
            column_types=basic_profile["column_types"],
            missing_rates=basic_profile["missing_rates"],
            data_characteristics=characteristics,
            statistical_summary=statistical_summary,
            quality_score=quality_score,
            complexity_level=complexity_level,
            suggested_contexts=suggested_contexts,
            metadata={
                "profiling_timestamp": datetime.now().isoformat(),
                "user_context": user_context
            }
        )
        
        logger.info(f"âœ… ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ (í’ˆì§ˆ: {quality_score:.2f}, ë³µì¡ë„: {complexity_level})")
        return profile
    
    def _analyze_basic_structure(self, data: pd.DataFrame) -> Dict[str, Any]:
        """ê¸°ë³¸ êµ¬ì¡° ë¶„ì„"""
        column_types = {}
        missing_rates = {}
        
        for column in data.columns:
            # ë°ì´í„° íƒ€ì… ë¶„ì„ (ë²”ìš©ì )
            dtype = str(data[column].dtype)
            if dtype.startswith(('int', 'float')):
                column_types[column] = 'numeric'
            elif dtype == 'object':
                # í…ìŠ¤íŠ¸ vs ì¹´í…Œê³ ë¦¬ êµ¬ë¶„
                unique_ratio = len(data[column].dropna().unique()) / len(data[column].dropna())
                if unique_ratio < 0.1:  # 10% ë¯¸ë§Œì´ë©´ ì¹´í…Œê³ ë¦¬
                    column_types[column] = 'categorical'
                else:
                    column_types[column] = 'text'
            elif dtype.startswith('datetime'):
                column_types[column] = 'datetime'
            else:
                column_types[column] = 'other'
            
            # ê²°ì¸¡ë¥  ê³„ì‚°
            missing_rates[column] = data[column].isnull().sum() / len(data)
        
        return {
            "column_types": column_types,
            "missing_rates": missing_rates
        }
    
    async def _detect_data_characteristics(self, data: pd.DataFrame) -> List[DataCharacteristics]:
        """ë°ì´í„° íŠ¹ì„± íƒì§€"""
        characteristics = []
        
        # ê° íŠ¹ì„± íƒì§€ê¸° ì‹¤í–‰
        for characteristic, detector in self.characteristic_detectors.items():
            if detector(data):
                characteristics.append(characteristic)
        
        # Mixed íŠ¹ì„± ì²´í¬
        if len(set(data.dtypes.astype(str))) > 2:
            characteristics.append(DataCharacteristics.MIXED)
        
        return characteristics
    
    def _detect_numerical(self, data: pd.DataFrame) -> bool:
        """ìˆ˜ì¹˜í˜• ë°ì´í„° íƒì§€"""
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        return len(numeric_cols) / len(data.columns) > 0.5
    
    def _detect_categorical(self, data: pd.DataFrame) -> bool:
        """ë²”ì£¼í˜• ë°ì´í„° íƒì§€"""
        categorical_cols = 0
        for column in data.columns:
            if data[column].dtype == 'object':
                unique_ratio = len(data[column].dropna().unique()) / len(data[column].dropna())
                if unique_ratio < 0.1:
                    categorical_cols += 1
        return categorical_cols / len(data.columns) > 0.3
    
    def _detect_text(self, data: pd.DataFrame) -> bool:
        """í…ìŠ¤íŠ¸ ë°ì´í„° íƒì§€"""
        text_cols = 0
        for column in data.columns:
            if data[column].dtype == 'object':
                # í‰ê·  ë¬¸ìì—´ ê¸¸ì´ë¡œ í…ìŠ¤íŠ¸ íŒë‹¨
                avg_length = data[column].dropna().astype(str).str.len().mean()
                if avg_length > 20:  # 20ì ì´ìƒì´ë©´ í…ìŠ¤íŠ¸ë¡œ íŒë‹¨
                    text_cols += 1
        return text_cols > 0
    
    def _detect_temporal(self, data: pd.DataFrame) -> bool:
        """ì‹œê³„ì—´ ë°ì´í„° íƒì§€"""
        datetime_cols = data.select_dtypes(include=['datetime64']).columns
        return len(datetime_cols) > 0
    
    def _detect_sparse(self, data: pd.DataFrame) -> bool:
        """í¬ì†Œ ë°ì´í„° íƒì§€"""
        zero_ratio = (data == 0).sum().sum() / data.size
        return zero_ratio > 0.5
    
    def _detect_high_dimensional(self, data: pd.DataFrame) -> bool:
        """ê³ ì°¨ì› ë°ì´í„° íƒì§€"""
        return data.shape[1] > 50
    
    def _detect_imbalanced(self, data: pd.DataFrame) -> bool:
        """ë¶ˆê· í˜• ë°ì´í„° íƒì§€"""
        for column in data.columns:
            if data[column].dtype == 'object':
                value_counts = data[column].value_counts()
                if len(value_counts) > 1:
                    imbalance_ratio = value_counts.iloc[0] / value_counts.iloc[-1]
                    if imbalance_ratio > 10:  # 10:1 ì´ìƒ ë¶ˆê· í˜•
                        return True
        return False
    
    async def _generate_statistical_summary(self, data: pd.DataFrame) -> Dict[str, Any]:
        """í†µê³„ì  ìš”ì•½ ìƒì„± (ë²”ìš©ì )"""
        summary = {}
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ìš”ì•½
        numeric_data = data.select_dtypes(include=[np.number])
        if not numeric_data.empty:
            summary["numeric_summary"] = {
                "count": len(numeric_data.columns),
                "descriptive_stats": numeric_data.describe().to_dict(),
                "correlation_strength": self._assess_correlation_strength(numeric_data),
                "outlier_rates": self._detect_outlier_rates(numeric_data)
            }
        
        # ë²”ì£¼í˜• ì»¬ëŸ¼ ìš”ì•½
        categorical_cols = []
        for column in data.columns:
            if data[column].dtype == 'object':
                unique_ratio = len(data[column].dropna().unique()) / len(data[column].dropna())
                if unique_ratio < 0.1:
                    categorical_cols.append(column)
        
        if categorical_cols:
            summary["categorical_summary"] = {
                "count": len(categorical_cols),
                "unique_values": {col: len(data[col].dropna().unique()) for col in categorical_cols},
                "mode_frequencies": {col: data[col].mode().iloc[0] if not data[col].mode().empty else None 
                                   for col in categorical_cols}
            }
        
        # ì „ì²´ ë°ì´í„° íŠ¹ì„±
        summary["overall"] = {
            "total_rows": len(data),
            "total_columns": len(data.columns),
            "missing_data_percentage": (data.isnull().sum().sum() / data.size) * 100,
            "duplicate_rows": data.duplicated().sum(),
            "memory_usage_mb": data.memory_usage(deep=True).sum() / (1024 * 1024)
        }
        
        return summary
    
    def _assess_correlation_strength(self, numeric_data: pd.DataFrame) -> Dict[str, float]:
        """ìƒê´€ê´€ê³„ ê°•ë„ í‰ê°€"""
        if len(numeric_data.columns) < 2:
            return {"max_correlation": 0.0, "avg_correlation": 0.0}
        
        corr_matrix = numeric_data.corr().abs()
        # ëŒ€ê°ì„  ì œê±°
        corr_values = corr_matrix.values[np.triu_indices_from(corr_matrix.values, k=1)]
        
        return {
            "max_correlation": float(np.max(corr_values)) if len(corr_values) > 0 else 0.0,
            "avg_correlation": float(np.mean(corr_values)) if len(corr_values) > 0 else 0.0
        }
    
    def _detect_outlier_rates(self, numeric_data: pd.DataFrame) -> Dict[str, float]:
        """ì´ìƒì¹˜ ë¹„ìœ¨ íƒì§€"""
        outlier_rates = {}
        
        for column in numeric_data.columns:
            Q1 = numeric_data[column].quantile(0.25)
            Q3 = numeric_data[column].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers = ((numeric_data[column] < lower_bound) | 
                       (numeric_data[column] > upper_bound)).sum()
            outlier_rates[column] = outliers / len(numeric_data[column])
        
        return outlier_rates
    
    async def _suggest_analysis_contexts(self, data: pd.DataFrame, 
                                       characteristics: List[DataCharacteristics],
                                       user_context: str) -> List[AnalysisContext]:
        """ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ ì œì•ˆ (LLM ê¸°ë°˜)"""
        
        # ë°ì´í„° íŠ¹ì„± ìš”ì•½
        data_summary = {
            "shape": data.shape,
            "characteristics": [c.value for c in characteristics],
            "column_types": data.dtypes.astype(str).to_dict(),
            "sample_columns": list(data.columns[:10])  # ì²˜ìŒ 10ê°œ ì»¬ëŸ¼ë§Œ
        }
        
        prompt = f"""
ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë°ì´í„°ì— ê°€ì¥ ì í•©í•œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.

ë°ì´í„° ì •ë³´:
- í˜•íƒœ: {data_summary['shape']} (í–‰ x ì—´)
- íŠ¹ì„±: {', '.join(data_summary['characteristics'])}
- ì»¬ëŸ¼ ì˜ˆì‹œ: {', '.join(data_summary['sample_columns'][:5])}
- ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸: {user_context or 'ì—†ìŒ'}

ê°€ëŠ¥í•œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸:
1. exploration - íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
2. prediction - ì˜ˆì¸¡ ëª¨ë¸ë§
3. classification - ë¶„ë¥˜ ë¶„ì„
4. clustering - í´ëŸ¬ìŠ¤í„°ë§
5. anomaly_detection - ì´ìƒ íƒì§€
6. time_series - ì‹œê³„ì—´ ë¶„ì„
7. association - ì—°ê´€ì„± ë¶„ì„
8. causal - ì¸ê³¼ê´€ê³„ ë¶„ì„

ë°ì´í„° íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìš°ì„ ìˆœìœ„ëŒ€ë¡œ 3ê°œ ì„ íƒí•˜ê³ , ê°ê°ì— ëŒ€í•œ ê°„ë‹¨í•œ ì´ìœ ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
1. context_name: ì´ìœ 
2. context_name: ì´ìœ 
3. context_name: ì´ìœ 
"""
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.3
            )
            
            content = response.choices[0].message.content
            suggested_contexts = self._parse_context_suggestions(content)
            
        except Exception as e:
            logger.warning(f"LLM ì»¨í…ìŠ¤íŠ¸ ì œì•ˆ ì‹¤íŒ¨: {e}")
            # í´ë°±: ë°ì´í„° íŠ¹ì„± ê¸°ë°˜ ê¸°ë³¸ ì œì•ˆ
            suggested_contexts = self._fallback_context_suggestion(characteristics)
        
        return suggested_contexts
    
    def _parse_context_suggestions(self, llm_response: str) -> List[AnalysisContext]:
        """LLM ì‘ë‹µì—ì„œ ì»¨í…ìŠ¤íŠ¸ íŒŒì‹±"""
        contexts = []
        
        context_mapping = {
            "exploration": AnalysisContext.EXPLORATION,
            "prediction": AnalysisContext.PREDICTION,
            "classification": AnalysisContext.CLASSIFICATION,
            "clustering": AnalysisContext.CLUSTERING,
            "anomaly_detection": AnalysisContext.ANOMALY_DETECTION,
            "time_series": AnalysisContext.TIME_SERIES,
            "association": AnalysisContext.ASSOCIATION,
            "causal": AnalysisContext.CAUSAL
        }
        
        for context_name, context_enum in context_mapping.items():
            if context_name in llm_response.lower():
                contexts.append(context_enum)
        
        # ìµœì†Œ 1ê°œëŠ” ë³´ì¥
        if not contexts:
            contexts.append(AnalysisContext.EXPLORATION)
        
        return contexts[:3]  # ìµœëŒ€ 3ê°œ
    
    def _fallback_context_suggestion(self, characteristics: List[DataCharacteristics]) -> List[AnalysisContext]:
        """í´ë°± ì»¨í…ìŠ¤íŠ¸ ì œì•ˆ"""
        contexts = [AnalysisContext.EXPLORATION]  # ê¸°ë³¸ì€ íƒìƒ‰ì  ë¶„ì„
        
        if DataCharacteristics.TEMPORAL in characteristics:
            contexts.append(AnalysisContext.TIME_SERIES)
        
        if DataCharacteristics.NUMERICAL in characteristics:
            contexts.append(AnalysisContext.PREDICTION)
        
        if DataCharacteristics.CATEGORICAL in characteristics:
            contexts.append(AnalysisContext.CLASSIFICATION)
        
        return contexts[:3]
    
    def _assess_data_quality(self, data: pd.DataFrame, basic_profile: Dict[str, Any]) -> float:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€ (0-1)"""
        quality_factors = []
        
        # 1. ì™„ì„±ë„ (ê²°ì¸¡ê°’ ë¹„ìœ¨)
        overall_missing_rate = sum(basic_profile["missing_rates"].values()) / len(basic_profile["missing_rates"])
        completeness_score = 1 - overall_missing_rate
        quality_factors.append(completeness_score)
        
        # 2. ì¼ê´€ì„± (ë°ì´í„° íƒ€ì… ì¼ê´€ì„±)
        consistency_score = 1.0  # ê¸°ë³¸ê°’
        quality_factors.append(consistency_score)
        
        # 3. ìœ íš¨ì„± (ìˆ˜ì¹˜ ë°ì´í„°ì˜ í•©ë¦¬ì„±)
        validity_score = self._assess_validity(data)
        quality_factors.append(validity_score)
        
        # 4. ìœ ë‹ˆí¬ì„± (ì¤‘ë³µ ë°ì´í„° ë¹„ìœ¨)
        duplicate_rate = data.duplicated().sum() / len(data)
        uniqueness_score = 1 - duplicate_rate
        quality_factors.append(uniqueness_score)
        
        return statistics.mean(quality_factors)
    
    def _assess_validity(self, data: pd.DataFrame) -> float:
        """ë°ì´í„° ìœ íš¨ì„± í‰ê°€"""
        validity_scores = []
        
        numeric_data = data.select_dtypes(include=[np.number])
        
        for column in numeric_data.columns:
            col_data = numeric_data[column].dropna()
            if len(col_data) == 0:
                continue
            
            # ë¬´í•œê°’, NaN ì²´í¬
            invalid_count = np.isinf(col_data).sum() + np.isnan(col_data).sum()
            validity_score = 1 - (invalid_count / len(col_data))
            validity_scores.append(validity_score)
        
        return statistics.mean(validity_scores) if validity_scores else 1.0
    
    def _assess_complexity(self, data: pd.DataFrame, characteristics: List[DataCharacteristics]) -> str:
        """ë°ì´í„° ë³µì¡ë„ í‰ê°€"""
        complexity_score = 0
        
        # ì°¨ì›ìˆ˜ ê¸°ë°˜
        if data.shape[1] > 100:
            complexity_score += 3
        elif data.shape[1] > 50:
            complexity_score += 2
        elif data.shape[1] > 20:
            complexity_score += 1
        
        # ë°ì´í„° í¬ê¸° ê¸°ë°˜
        if data.shape[0] > 1000000:
            complexity_score += 3
        elif data.shape[0] > 100000:
            complexity_score += 2
        elif data.shape[0] > 10000:
            complexity_score += 1
        
        # íŠ¹ì„± ê¸°ë°˜
        if DataCharacteristics.HIGH_DIMENSIONAL in characteristics:
            complexity_score += 2
        if DataCharacteristics.TEXT in characteristics:
            complexity_score += 2
        if DataCharacteristics.MIXED in characteristics:
            complexity_score += 1
        
        # ë³µì¡ë„ ë ˆë²¨ ê²°ì •
        if complexity_score >= 6:
            return "very_high"
        elif complexity_score >= 4:
            return "high"
        elif complexity_score >= 2:
            return "medium"
        else:
            return "low"

class UniversalAnalysisEngine:
    """ë²”ìš© ë¶„ì„ ì—”ì§„"""
    
    def __init__(self):
        self.llm_client = AsyncOpenAI()
        self.data_profiler = UniversalDataProfiler()
        
        # ë¶„ì„ ì „ëµ ë©”íƒ€ ë°ì´í„°
        self.strategy_knowledge = {
            AnalysisContext.EXPLORATION: {
                "priority_steps": ["data_overview", "distribution_analysis", "correlation_analysis", "pattern_discovery"],
                "techniques": ["descriptive_statistics", "visualization", "outlier_detection"],
                "expected_insights": ["data_quality", "patterns", "relationships", "anomalies"]
            },
            AnalysisContext.PREDICTION: {
                "priority_steps": ["feature_analysis", "target_correlation", "model_selection", "validation"],
                "techniques": ["regression", "feature_importance", "cross_validation"],
                "expected_insights": ["predictive_features", "model_performance", "feature_importance"]
            },
            AnalysisContext.CLASSIFICATION: {
                "priority_steps": ["class_distribution", "feature_discrimination", "model_comparison"],
                "techniques": ["classification_algorithms", "feature_selection", "performance_metrics"],
                "expected_insights": ["class_separability", "discriminative_features", "classification_accuracy"]
            }
        }
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.results_dir = Path("core/universal/analysis_results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    async def analyze_universally(self, data: pd.DataFrame, 
                                user_query: str = "",
                                preferred_context: Optional[AnalysisContext] = None) -> AnalysisResult:
        """ë²”ìš©ì  ë°ì´í„° ë¶„ì„"""
        logger.info(f"ğŸ”¬ ë²”ìš© ë¶„ì„ ì‹œì‘: {data.shape}")
        
        # 1. ë°ì´í„° í”„ë¡œíŒŒì¼ë§
        profile = await self.data_profiler.profile_data(data, user_query)
        
        # 2. ë¶„ì„ ì „ëµ ìˆ˜ë¦½
        strategy = await self._develop_analysis_strategy(profile, user_query, preferred_context)
        
        # 3. ë¶„ì„ ì‹¤í–‰
        result = await self._execute_analysis(data, profile, strategy)
        
        # 4. ê²°ê³¼ ì €ì¥
        await self._save_analysis_result(result, profile, strategy)
        
        logger.info(f"âœ… ë²”ìš© ë¶„ì„ ì™„ë£Œ: {len(result.insights)}ê°œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ")
        return result
    
    async def _develop_analysis_strategy(self, profile: DataProfile, 
                                       user_query: str,
                                       preferred_context: Optional[AnalysisContext]) -> AnalysisStrategy:
        """ë¶„ì„ ì „ëµ ìˆ˜ë¦½ (LLM ê¸°ë°˜)"""
        
        # ì»¨í…ìŠ¤íŠ¸ ê²°ì •
        if preferred_context:
            context = preferred_context
        else:
            context = profile.suggested_contexts[0] if profile.suggested_contexts else AnalysisContext.EXPLORATION
        
        # LLMì„ í†µí•œ ì „ëµ ìƒì„¸í™”
        strategy_prompt = f"""
ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë°ì´í„°ì— ëŒ€í•œ {context.value} ë¶„ì„ ì „ëµì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.

ë°ì´í„° í”„ë¡œíŒŒì¼:
- í˜•íƒœ: {profile.shape}
- íŠ¹ì„±: {[c.value for c in profile.data_characteristics]}
- í’ˆì§ˆ ì ìˆ˜: {profile.quality_score:.2f}
- ë³µì¡ë„: {profile.complexity_level}

ì‚¬ìš©ì ìš”ì²­: {user_query or 'ì¼ë°˜ì ì¸ ë¶„ì„'}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì „ëµì„ ì œì‹œí•´ì£¼ì„¸ìš”:
1. ìš°ì„ ìˆœìœ„ ë‹¨ê³„ (4-6ê°œ ë‹¨ê³„)
2. ì ìš©í•  ê¸°ë²•ë“¤
3. ì˜ˆìƒë˜ëŠ” ì¸ì‚¬ì´íŠ¸ ìœ í˜•
4. ì´ ì „ëµì˜ ì‹ ë¢°ë„ (0-1)
5. ì „ëµ ì„ íƒ ì´ìœ 

ë²”ìš©ì ì´ê³  ë°ì´í„°ì…‹ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ì ‘ê·¼ë²•ì„ ì‚¬ìš©í•˜ì„¸ìš”.
"""
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": strategy_prompt}],
                max_tokens=800,
                temperature=0.3
            )
            
            llm_strategy = response.choices[0].message.content
            strategy = self._parse_strategy_response(llm_strategy, context)
            
        except Exception as e:
            logger.warning(f"LLM ì „ëµ ìˆ˜ë¦½ ì‹¤íŒ¨: {e}")
            strategy = self._fallback_strategy(context)
        
        return strategy
    
    def _parse_strategy_response(self, llm_response: str, context: AnalysisContext) -> AnalysisStrategy:
        """LLM ì‘ë‹µì—ì„œ ì „ëµ íŒŒì‹±"""
        
        # ê¸°ë³¸ ì „ëµ ê°€ì ¸ì˜¤ê¸°
        base_strategy = self.strategy_knowledge.get(context, {})
        
        # LLM ì‘ë‹µì—ì„œ ì •ë³´ ì¶”ì¶œ (ê°„ë‹¨í•œ íŒŒì‹±)
        lines = llm_response.split('\n')
        
        priority_steps = base_strategy.get("priority_steps", [])
        techniques = base_strategy.get("techniques", [])
        expected_insights = base_strategy.get("expected_insights", [])
        
        # ì‹ ë¢°ë„ ì¶”ì¶œ ì‹œë„
        confidence = 0.8  # ê¸°ë³¸ê°’
        for line in lines:
            if "ì‹ ë¢°ë„" in line or "confidence" in line.lower():
                try:
                    import re
                    numbers = re.findall(r'0\.\d+|\d+', line)
                    if numbers:
                        confidence = float(numbers[0])
                        if confidence > 1:
                            confidence = confidence / 100
                except:
                    pass
        
        return AnalysisStrategy(
            context=context,
            priority_steps=priority_steps,
            techniques=techniques,
            expected_insights=expected_insights,
            confidence=confidence,
            reasoning=llm_response
        )
    
    def _fallback_strategy(self, context: AnalysisContext) -> AnalysisStrategy:
        """í´ë°± ì „ëµ"""
        base_strategy = self.strategy_knowledge.get(context, {})
        
        return AnalysisStrategy(
            context=context,
            priority_steps=base_strategy.get("priority_steps", ["basic_analysis"]),
            techniques=base_strategy.get("techniques", ["descriptive_statistics"]),
            expected_insights=base_strategy.get("expected_insights", ["basic_insights"]),
            confidence=0.6,
            reasoning="ê¸°ë³¸ ì „ëµ ì ìš©"
        )
    
    async def _execute_analysis(self, data: pd.DataFrame, 
                              profile: DataProfile, 
                              strategy: AnalysisStrategy) -> AnalysisResult:
        """ë¶„ì„ ì‹¤í–‰"""
        
        insights = []
        visualizations = []
        statistical_tests = []
        recommendations = []
        confidence_scores = {}
        
        # ê° ë‹¨ê³„ë³„ ë¶„ì„ ì‹¤í–‰
        for step in strategy.priority_steps:
            try:
                step_result = await self._execute_analysis_step(step, data, profile, strategy)
                
                insights.extend(step_result.get("insights", []))
                visualizations.extend(step_result.get("visualizations", []))
                statistical_tests.extend(step_result.get("statistical_tests", []))
                confidence_scores[step] = step_result.get("confidence", 0.5)
                
            except Exception as e:
                logger.error(f"ë¶„ì„ ë‹¨ê³„ '{step}' ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                insights.append(f"ë¶„ì„ ë‹¨ê³„ '{step}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
                confidence_scores[step] = 0.0
        
        # LLM ê¸°ë°˜ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        comprehensive_insights = await self._generate_comprehensive_insights(
            data, profile, strategy, insights
        )
        insights.extend(comprehensive_insights)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = await self._generate_recommendations(data, profile, strategy, insights)
        
        return AnalysisResult(
            insights=insights,
            visualizations=visualizations,
            statistical_tests=statistical_tests,
            recommendations=recommendations,
            confidence_scores=confidence_scores,
            methodology=f"{strategy.context.value} ë¶„ì„ (LLM ê¸°ë°˜ ë²”ìš© ì ‘ê·¼ë²•)",
            limitations=self._identify_limitations(profile, strategy),
            next_steps=self._suggest_next_steps(profile, strategy),
            metadata={
                "analysis_timestamp": datetime.now().isoformat(),
                "strategy_confidence": strategy.confidence,
                "data_quality": profile.quality_score
            }
        )
    
    async def _execute_analysis_step(self, step: str, data: pd.DataFrame, 
                                   profile: DataProfile, strategy: AnalysisStrategy) -> Dict[str, Any]:
        """ê°œë³„ ë¶„ì„ ë‹¨ê³„ ì‹¤í–‰"""
        
        if step == "data_overview":
            return await self._analyze_data_overview(data, profile)
        elif step == "distribution_analysis":
            return await self._analyze_distributions(data, profile)
        elif step == "correlation_analysis":
            return await self._analyze_correlations(data, profile)
        elif step == "pattern_discovery":
            return await self._discover_patterns(data, profile)
        elif step == "feature_analysis":
            return await self._analyze_features(data, profile)
        else:
            # ë²”ìš©ì  ë‹¨ê³„ ì²˜ë¦¬
            return await self._generic_analysis_step(step, data, profile)
    
    async def _analyze_data_overview(self, data: pd.DataFrame, profile: DataProfile) -> Dict[str, Any]:
        """ë°ì´í„° ê°œìš” ë¶„ì„"""
        insights = []
        
        # ê¸°ë³¸ í†µê³„
        insights.append(f"ë°ì´í„°ì…‹ í¬ê¸°: {profile.shape[0]:,}í–‰ Ã— {profile.shape[1]}ì—´")
        insights.append(f"ë°ì´í„° í’ˆì§ˆ ì ìˆ˜: {profile.quality_score:.2f}/1.0")
        insights.append(f"ë³µì¡ë„ ìˆ˜ì¤€: {profile.complexity_level}")
        
        # ê²°ì¸¡ê°’ ë¶„ì„
        high_missing_cols = [col for col, rate in profile.missing_rates.items() if rate > 0.3]
        if high_missing_cols:
            insights.append(f"ë†’ì€ ê²°ì¸¡ë¥ (30% ì´ìƒ) ì»¬ëŸ¼: {', '.join(high_missing_cols[:3])}")
        
        # ë°ì´í„° íŠ¹ì„±
        characteristics_desc = [c.value for c in profile.data_characteristics]
        insights.append(f"ì£¼ìš” ë°ì´í„° íŠ¹ì„±: {', '.join(characteristics_desc)}")
        
        return {
            "insights": insights,
            "confidence": 0.9,
            "visualizations": [],
            "statistical_tests": []
        }
    
    async def _analyze_distributions(self, data: pd.DataFrame, profile: DataProfile) -> Dict[str, Any]:
        """ë¶„í¬ ë¶„ì„"""
        insights = []
        visualizations = []
        
        # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ë¶„í¬ ë¶„ì„
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols[:5]:  # ìµœëŒ€ 5ê°œ ì»¬ëŸ¼
            col_data = data[col].dropna()
            if len(col_data) == 0:
                continue
            
            # ê¸°ë³¸ í†µê³„
            mean_val = col_data.mean()
            median_val = col_data.median()
            std_val = col_data.std()
            
            # ë¶„í¬ íŠ¹ì„± ë¶„ì„
            skewness = col_data.skew()
            if abs(skewness) > 1:
                skew_desc = "highly skewed" if abs(skewness) > 2 else "moderately skewed"
                insights.append(f"{col}: {skew_desc} distribution (skewness: {skewness:.2f})")
            
            # ì´ìƒì¹˜ ë¶„ì„
            Q1 = col_data.quantile(0.25)
            Q3 = col_data.quantile(0.75)
            IQR = Q3 - Q1
            outliers = ((col_data < (Q1 - 1.5 * IQR)) | (col_data > (Q3 + 1.5 * IQR))).sum()
            outlier_rate = outliers / len(col_data)
            
            if outlier_rate > 0.1:
                insights.append(f"{col}: {outlier_rate:.1%} ì´ìƒì¹˜ í¬í•¨ ({outliers}ê°œ)")
            
            # ì‹œê°í™” ì •ë³´ (ì‹¤ì œ ìƒì„±ì€ ë³„ë„)
            visualizations.append({
                "type": "histogram",
                "column": col,
                "description": f"{col} ë¶„í¬ íˆìŠ¤í† ê·¸ë¨"
            })
        
        return {
            "insights": insights,
            "confidence": 0.8,
            "visualizations": visualizations,
            "statistical_tests": []
        }
    
    async def _analyze_correlations(self, data: pd.DataFrame, profile: DataProfile) -> Dict[str, Any]:
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        insights = []
        statistical_tests = []
        
        numeric_data = data.select_dtypes(include=[np.number])
        
        if len(numeric_data.columns) < 2:
            return {
                "insights": ["ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ë¶€ì¡±í•˜ì—¬ ìƒê´€ê´€ê³„ ë¶„ì„ ë¶ˆê°€"],
                "confidence": 0.3,
                "visualizations": [],
                "statistical_tests": []
            }
        
        # ìƒê´€ê´€ê³„ í–‰ë ¬ ê³„ì‚°
        corr_matrix = numeric_data.corr()
        
        # ê°•í•œ ìƒê´€ê´€ê³„ íƒì§€
        strong_correlations = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]
                    strong_correlations.append((col1, col2, corr_val))
        
        if strong_correlations:
            for col1, col2, corr_val in strong_correlations[:3]:  # ìƒìœ„ 3ê°œ
                insights.append(f"ê°•í•œ ìƒê´€ê´€ê³„: {col1} â†” {col2} (r={corr_val:.3f})")
                
                statistical_tests.append({
                    "test": "Pearson correlation",
                    "variables": [col1, col2],
                    "statistic": corr_val,
                    "interpretation": "strong positive correlation" if corr_val > 0 else "strong negative correlation"
                })
        else:
            insights.append("ê°•í•œ ìƒê´€ê´€ê³„(|r| > 0.7)ë¥¼ ê°€ì§„ ë³€ìˆ˜ ìŒ ì—†ìŒ")
        
        return {
            "insights": insights,
            "confidence": 0.7,
            "visualizations": [{"type": "correlation_heatmap", "description": "ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ"}],
            "statistical_tests": statistical_tests
        }
    
    async def _discover_patterns(self, data: pd.DataFrame, profile: DataProfile) -> Dict[str, Any]:
        """íŒ¨í„´ íƒì§€ (LLM ê¸°ë°˜)"""
        insights = []
        
        # ë°ì´í„° ìƒ˜í”Œê³¼ ê¸°ë³¸ í†µê³„ë¡œ LLMì—ê²Œ íŒ¨í„´ íƒì§€ ìš”ì²­
        sample_data = data.head(10).to_dict()
        
        pattern_prompt = f"""
ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë°ì´í„°ì—ì„œ ì˜ë¯¸ ìˆëŠ” íŒ¨í„´ì„ íƒì§€í•´ì£¼ì„¸ìš”.

ë°ì´í„° ìƒ˜í”Œ (ì²˜ìŒ 10í–‰):
{json.dumps(sample_data, indent=2, default=str)}

ë°ì´í„° íŠ¹ì„±:
- í˜•íƒœ: {profile.shape}
- í’ˆì§ˆ: {profile.quality_score:.2f}
- íŠ¹ì„±: {[c.value for c in profile.data_characteristics]}

ë°œê²¬í•  ìˆ˜ ìˆëŠ” íŒ¨í„´ ìœ í˜•:
1. ë¶„í¬ íŒ¨í„´
2. ê·¸ë£¹í™” íŒ¨í„´  
3. ì‹œê°„ì  íŒ¨í„´ (í•´ë‹¹ë˜ëŠ” ê²½ìš°)
4. ë²”ì£¼ë³„ ì°¨ì´ì 
5. ì´ìƒ íŒ¨í„´

ê° íŒ¨í„´ì— ëŒ€í•´ êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ë¯¸ë¥¼ ì œì‹œí•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": pattern_prompt}],
                max_tokens=600,
                temperature=0.4
            )
            
            llm_patterns = response.choices[0].message.content
            
            # LLM ì‘ë‹µì„ ì¸ì‚¬ì´íŠ¸ë¡œ ë³€í™˜
            pattern_insights = llm_patterns.split('\n')
            insights.extend([insight.strip() for insight in pattern_insights if insight.strip()])
            
        except Exception as e:
            logger.warning(f"LLM íŒ¨í„´ íƒì§€ ì‹¤íŒ¨: {e}")
            insights.append("íŒ¨í„´ íƒì§€ë¥¼ ìœ„í•œ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return {
            "insights": insights,
            "confidence": 0.6,
            "visualizations": [],
            "statistical_tests": []
        }
    
    async def _analyze_features(self, data: pd.DataFrame, profile: DataProfile) -> Dict[str, Any]:
        """í”¼ì²˜ ë¶„ì„"""
        insights = []
        
        # í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        
        if len(numeric_cols) > 1:
            # ë³€ë™ê³„ìˆ˜ë¥¼ í†µí•œ í”¼ì²˜ ë³€ë™ì„± ë¶„ì„
            feature_variability = {}
            for col in numeric_cols:
                col_data = data[col].dropna()
                if len(col_data) > 0 and col_data.std() > 0:
                    cv = col_data.std() / abs(col_data.mean()) if col_data.mean() != 0 else 0
                    feature_variability[col] = cv
            
            if feature_variability:
                # ë†’ì€ ë³€ë™ì„± í”¼ì²˜
                high_var_features = sorted(feature_variability.items(), key=lambda x: x[1], reverse=True)[:3]
                insights.append(f"ë†’ì€ ë³€ë™ì„± í”¼ì²˜: {', '.join([f[0] for f in high_var_features])}")
                
                # ë‚®ì€ ë³€ë™ì„± í”¼ì²˜
                low_var_features = sorted(feature_variability.items(), key=lambda x: x[1])[:3]
                insights.append(f"ë‚®ì€ ë³€ë™ì„± í”¼ì²˜: {', '.join([f[0] for f in low_var_features])}")
        
        return {
            "insights": insights,
            "confidence": 0.7,
            "visualizations": [],
            "statistical_tests": []
        }
    
    async def _generic_analysis_step(self, step: str, data: pd.DataFrame, profile: DataProfile) -> Dict[str, Any]:
        """ë²”ìš©ì  ë¶„ì„ ë‹¨ê³„"""
        insights = [f"{step} ë¶„ì„ì´ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤."]
        
        return {
            "insights": insights,
            "confidence": 0.5,
            "visualizations": [],
            "statistical_tests": []
        }
    
    async def _generate_comprehensive_insights(self, data: pd.DataFrame, profile: DataProfile,
                                             strategy: AnalysisStrategy, step_insights: List[str]) -> List[str]:
        """ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„± (LLM ê¸°ë°˜)"""
        
        insights_summary = '\n'.join(step_insights[-10:])  # ìµœê·¼ 10ê°œ ì¸ì‚¬ì´íŠ¸
        
        synthesis_prompt = f"""
ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë“¤ì„ ì¢…í•©í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”.

ë¶„ì„ ì»¨í…ìŠ¤íŠ¸: {strategy.context.value}
ë°ì´í„° íŠ¹ì„±: {profile.shape}, í’ˆì§ˆ {profile.quality_score:.2f}

ê°œë³„ ë¶„ì„ ê²°ê³¼:
{insights_summary}

ìš”ì²­ì‚¬í•­:
1. ê°€ì¥ ì¤‘ìš”í•œ 3-5ê°œì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
2. ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œì˜ ì˜ë¯¸ í•´ì„
3. ë°ì´í„° ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ì œì‹œ

ë²”ìš©ì ì´ê³  ì‹¤ìš©ì ì¸ ê´€ì ì—ì„œ ì¢…í•©í•´ì£¼ì„¸ìš”.
"""
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": synthesis_prompt}],
                max_tokens=500,
                temperature=0.3
            )
            
            comprehensive = response.choices[0].message.content
            return [comprehensive]
            
        except Exception as e:
            logger.warning(f"ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["ë¶„ì„ ê²°ê³¼ ì¢…í•©ì„ ìœ„í•œ ì¶”ê°€ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤."]
    
    async def _generate_recommendations(self, data: pd.DataFrame, profile: DataProfile,
                                      strategy: AnalysisStrategy, insights: List[str]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if profile.quality_score < 0.7:
            recommendations.append("ë°ì´í„° í’ˆì§ˆ ê°œì„  í•„ìš”: ê²°ì¸¡ê°’ ì²˜ë¦¬ ë° ë°ì´í„° ì •ì œ ìš°ì„  ìˆ˜í–‰")
        
        # ë³µì¡ë„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if profile.complexity_level in ["high", "very_high"]:
            recommendations.append("ê³ ë³µì¡ë„ ë°ì´í„°: ì°¨ì› ì¶•ì†Œ ë˜ëŠ” í”¼ì²˜ ì„ íƒ ê¸°ë²• ì ìš© ê³ ë ¤")
        
        # ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if strategy.context == AnalysisContext.EXPLORATION:
            recommendations.append("íƒìƒ‰ì  ë¶„ì„ ì™„ë£Œ í›„ íŠ¹ì • ëª©ì ì˜ ë¶„ì„(ì˜ˆì¸¡, ë¶„ë¥˜) ìˆ˜í–‰ ê¶Œì¥")
        elif strategy.context == AnalysisContext.PREDICTION:
            recommendations.append("ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•ì„ ìœ„í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë° êµì°¨ ê²€ì¦ ìˆ˜í–‰")
        
        # ì¼ë°˜ì  ê¶Œì¥ì‚¬í•­
        recommendations.append("ì§€ì†ì ì¸ ë°ì´í„° ëª¨ë‹ˆí„°ë§ ë° ë¶„ì„ ê²°ê³¼ ê²€ì¦ í•„ìš”")
        
        return recommendations
    
    def _identify_limitations(self, profile: DataProfile, strategy: AnalysisStrategy) -> List[str]:
        """ë¶„ì„ í•œê³„ì  ì‹ë³„"""
        limitations = []
        
        if profile.quality_score < 0.5:
            limitations.append("ë‚®ì€ ë°ì´í„° í’ˆì§ˆë¡œ ì¸í•œ ë¶„ì„ ê²°ê³¼ ì‹ ë¢°ì„± ì œí•œ")
        
        if profile.shape[0] < 100:
            limitations.append("ì‘ì€ ë°ì´í„° í¬ê¸°ë¡œ ì¸í•œ í†µê³„ì  ìœ ì˜ì„± ì œí•œ")
        
        if DataCharacteristics.SPARSE in profile.data_characteristics:
            limitations.append("í¬ì†Œ ë°ì´í„° íŠ¹ì„±ìœ¼ë¡œ ì¸í•œ íŒ¨í„´ íƒì§€ ì–´ë ¤ì›€")
        
        limitations.append("ë²”ìš©ì  ë¶„ì„ ì ‘ê·¼ë²•ìœ¼ë¡œ ì¸í•œ ë„ë©”ì¸ íŠ¹í™” ì¸ì‚¬ì´íŠ¸ ì œí•œ")
        
        return limitations
    
    def _suggest_next_steps(self, profile: DataProfile, strategy: AnalysisStrategy) -> List[str]:
        """ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ"""
        next_steps = []
        
        # ì»¨í…ìŠ¤íŠ¸ë³„ ë‹¤ìŒ ë‹¨ê³„
        if strategy.context == AnalysisContext.EXPLORATION:
            next_steps.append("ëª©í‘œ ë³€ìˆ˜ ì„¤ì • í›„ ì˜ˆì¸¡ ë˜ëŠ” ë¶„ë¥˜ ë¶„ì„ ìˆ˜í–‰")
            
        next_steps.append("ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ ê²°ê³¼ ê²€í†  ë° í•´ì„")
        next_steps.append("ì¶”ê°€ ë°ì´í„° ìˆ˜ì§‘ì„ í†µí•œ ë¶„ì„ í™•ì¥")
        next_steps.append("ì‹œê°í™” ëŒ€ì‹œë³´ë“œ êµ¬ì¶•ìœ¼ë¡œ ì§€ì†ì  ëª¨ë‹ˆí„°ë§")
        
        return next_steps
    
    async def _save_analysis_result(self, result: AnalysisResult, profile: DataProfile, strategy: AnalysisStrategy):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        result_data = {
            "analysis_metadata": {
                "timestamp": datetime.now().isoformat(),
                "strategy": {
                    "context": strategy.context.value,
                    "confidence": strategy.confidence
                },
                "data_profile": {
                    "shape": profile.shape,
                    "quality_score": profile.quality_score,
                    "complexity": profile.complexity_level
                }
            },
            "insights": result.insights,
            "recommendations": result.recommendations,
            "methodology": result.methodology,
            "limitations": result.limitations,
            "next_steps": result.next_steps,
            "confidence_scores": result.confidence_scores
        }
        
        file_path = self.results_dir / f"universal_analysis_{timestamp}.json"
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(result_data, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ’¾ ë¶„ì„ ê²°ê³¼ ì €ì¥: {file_path}")


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
async def test_universal_analyzer():
    """ë²”ìš© ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸"""
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± (ì™„ì „íˆ ë²”ìš©ì )
    np.random.seed(42)
    test_data = pd.DataFrame({
        'numeric_1': np.random.normal(100, 15, 1000),
        'numeric_2': np.random.exponential(2, 1000),
        'categorical_1': np.random.choice(['A', 'B', 'C'], 1000),
        'categorical_2': np.random.choice(['Type1', 'Type2'], 1000, p=[0.7, 0.3]),
        'mixed_data': np.random.choice(['High', 'Medium', 'Low'], 1000)
    })
    
    # ì¼ë¶€ ê²°ì¸¡ê°’ ì¶”ê°€
    test_data.loc[np.random.choice(1000, 50, replace=False), 'numeric_1'] = np.nan
    
    # ë²”ìš© ë¶„ì„ ì‹¤í–‰
    analyzer = UniversalAnalysisEngine()
    
    print("ğŸ”¬ ë²”ìš© ë¶„ì„ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    result = await analyzer.analyze_universally(
        test_data, 
        user_query="ì´ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ íŒ¨í„´ì„ íŒŒì•…í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤"
    )
    
    print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
    print(f"   ë°©ë²•ë¡ : {result.methodology}")
    print(f"   ì¸ì‚¬ì´íŠ¸ ìˆ˜: {len(result.insights)}ê°œ")
    print(f"   ê¶Œì¥ì‚¬í•­ ìˆ˜: {len(result.recommendations)}ê°œ")
    
    print(f"\nğŸ” ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    for i, insight in enumerate(result.insights[:5], 1):
        print(f"   {i}. {insight}")
    
    print(f"\nğŸ’¡ ì£¼ìš” ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(result.recommendations[:3], 1):
        print(f"   {i}. {rec}")

if __name__ == "__main__":
    asyncio.run(test_universal_analyzer()) 