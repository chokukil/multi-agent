"""
í•˜ë“œì½”ë”©ëœ ë¡œì§ íƒì§€ ë° ì œê±° ì‹œìŠ¤í…œ
Phase 3.1: LLM First ì›ì¹™ ì™„ì „ êµ¬í˜„

í•µì‹¬ ê¸°ëŠ¥:
- ì½”ë“œë² ì´ìŠ¤ ì „ì²´ ìŠ¤ìº”
- Rule ê¸°ë°˜ ë¡œì§ íƒì§€
- íŒ¨í„´ ë§¤ì¹­ ì½”ë“œ ì‹ë³„
- í•˜ë“œì½”ë”©ëœ ë°ì´í„°ì…‹ ì˜ì¡´ì„± ê²€ì¶œ
- LLM First ì›ì¹™ ìœ„ë°˜ ë¶„ì„
- ìë™ ë¦¬íŒ©í† ë§ ì œì•ˆ
"""

import ast
import re
import os
import json
import logging
from dataclasses import dataclass, field
from typing import Dict, List, Any, Set, Tuple, Optional
from pathlib import Path
from collections import defaultdict
from enum import Enum
import subprocess

logger = logging.getLogger(__name__)

class ViolationType(Enum):
    """ìœ„ë°˜ ìœ í˜•"""
    HARDCODED_VALUES = "hardcoded_values"
    RULE_BASED_LOGIC = "rule_based_logic"
    PATTERN_MATCHING = "pattern_matching"
    DATASET_DEPENDENCY = "dataset_dependency"
    TEMPLATE_RESPONSE = "template_response"
    FIXED_WORKFLOW = "fixed_workflow"
    CONDITIONAL_HARDCODE = "conditional_hardcode"

class Severity(Enum):
    """ì‹¬ê°ë„"""
    CRITICAL = "critical"     # LLM First ì›ì¹™ ì‹¬ê° ìœ„ë°˜
    HIGH = "high"            # ëª…í™•í•œ ìœ„ë°˜
    MEDIUM = "medium"        # ì ì¬ì  ìœ„ë°˜
    LOW = "low"              # ê°œì„  ê¶Œì¥

@dataclass
class CodeViolation:
    """ì½”ë“œ ìœ„ë°˜ ì •ë³´"""
    file_path: str
    line_number: int
    violation_type: ViolationType
    severity: Severity
    description: str
    code_snippet: str
    suggested_fix: str
    llm_first_impact: float  # 0.0-1.0, LLM First ì›ì¹™ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ë„
    confidence: float        # 0.0-1.0, íƒì§€ ì‹ ë¢°ë„
    context: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ScanResults:
    """ìŠ¤ìº” ê²°ê³¼"""
    total_files_scanned: int
    violations_found: List[CodeViolation]
    violations_by_type: Dict[ViolationType, int]
    violations_by_severity: Dict[Severity, int]
    llm_first_compliance_score: float
    files_with_violations: Set[str]
    suggested_refactoring_priority: List[str]

class HardcodeDetector:
    """í•˜ë“œì½”ë”© íƒì§€ê¸°"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.violations: List[CodeViolation] = []
        
        # ìŠ¤ìº” ëŒ€ìƒ íŒ¨í„´
        self.scan_patterns = {
            "python_files": "**/*.py",
            "exclude_patterns": [
                "**/test_*",
                "**/__pycache__/**",
                "**/.*",
                "**/node_modules/**",
                "**/venv/**",
                "**/.venv/**"
            ]
        }
        
        # í•˜ë“œì½”ë”© íŒ¨í„´ ì •ì˜
        self.hardcode_patterns = {
            # ë°ì´í„°ì…‹ íŠ¹í™” í•˜ë“œì½”ë”©
            "dataset_specific": [
                r'titanic|Titanic',
                r'survived?|Survived',
                r'pclass|Pclass',
                r'embarked|Embarked',
                r'boston.*housing',
                r'iris.*dataset',
                r'wine.*quality',
                r'diabetes.*dataset'
            ],
            
            # Rule ê¸°ë°˜ ë¡œì§
            "rule_based": [
                r'if.*\.lower\(\).*==.*["\'].*["\']',  # ë¬¸ìì—´ ë¹„êµ ê¸°ë°˜ ë¶„ê¸°
                r'if.*in.*\[.*["\'].*["\'].*\]',       # í•˜ë“œì½”ë”©ëœ ë¦¬ìŠ¤íŠ¸ ì²´í¬
                r'elif.*==.*["\'][^"\']*["\']',        # í•˜ë“œì½”ë”©ëœ ê°’ ë¹„êµ
                r'switch.*case',                       # switch-case íŒ¨í„´
                r'mapping\s*=\s*\{.*["\'].*["\']',     # í•˜ë“œì½”ë”©ëœ ë§¤í•‘
            ],
            
            # íŒ¨í„´ ë§¤ì¹­
            "pattern_matching": [
                r'regex.*=.*r["\'].*["\']',           # ì •ê·œì‹ íŒ¨í„´
                r'pattern.*=.*["\'].*["\']',          # íŒ¨í„´ ë³€ìˆ˜
                r'\.match\(.*["\'].*["\']',           # ì§ì ‘ íŒ¨í„´ ë§¤ì¹­
                r'\.search\(.*["\'].*["\']',          # ì •ê·œì‹ ê²€ìƒ‰
                r'startswith\(["\'].*["\']',          # ì ‘ë‘ì‚¬ ë§¤ì¹­
                r'endswith\(["\'].*["\']',            # ì ‘ë¯¸ì‚¬ ë§¤ì¹­
            ],
            
            # í…œí”Œë¦¿ ì‘ë‹µ
            "template_response": [
                r'return\s+[f]?["\'].*ë¶„ì„.*ê²°ê³¼.*["\']',  # í…œí”Œë¦¿ ì‘ë‹µ
                r'response\s*=\s*[f]?["\'].*["\']',       # í•˜ë“œì½”ë”©ëœ ì‘ë‹µ
                r'message\s*=\s*[f]?["\'].*["\']',        # í•˜ë“œì½”ë”©ëœ ë©”ì‹œì§€
                r'\.format\(.*\)',                       # ë¬¸ìì—´ í¬ë§·íŒ…
            ],
            
            # ê³ ì • ì›Œí¬í”Œë¡œìš°
            "fixed_workflow": [
                r'steps\s*=\s*\[.*["\'].*["\'].*\]',     # í•˜ë“œì½”ë”©ëœ ë‹¨ê³„
                r'workflow\s*=\s*\[.*\]',                # ê³ ì • ì›Œí¬í”Œë¡œìš°
                r'pipeline\s*=\s*\[.*\]',                # ê³ ì • íŒŒì´í”„ë¼ì¸
                r'sequence\s*=\s*\[.*["\'].*["\'].*\]',  # ê³ ì • ì‹œí€€ìŠ¤
            ]
        }
        
        # LLM First ìœ„ë°˜ í‚¤ì›Œë“œ
        self.llm_first_violations = {
            "anti_patterns": [
                "hardcoded", "hard_coded", "fixed_logic", "rule_based",
                "pattern_match", "template", "static_response",
                "predefined", "predetermined", "manual_logic"
            ],
            "preferred_patterns": [
                "llm_analyze", "ai_generate", "dynamic_analysis",
                "intelligent_", "adaptive_", "smart_", "auto_",
                "ml_based", "ai_driven", "llm_powered"
            ]
        }
        
        # AST ë¶„ì„ìš© ë…¸ë“œ ë°©ë¬¸ì
        self.ast_visitor = HardcodeASTVisitor()
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.results_dir = Path("core/quality/scan_results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    def scan_codebase(self) -> ScanResults:
        """ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ìŠ¤ìº”"""
        logger.info("ğŸ” ì½”ë“œë² ì´ìŠ¤ í•˜ë“œì½”ë”© íƒì§€ ì‹œì‘...")
        
        self.violations.clear()
        scanned_files = []
        
        # Python íŒŒì¼ ìˆ˜ì§‘
        python_files = self._collect_python_files()
        logger.info(f"ğŸ“ ìŠ¤ìº” ëŒ€ìƒ íŒŒì¼: {len(python_files)}ê°œ")
        
        # ê° íŒŒì¼ ìŠ¤ìº”
        for file_path in python_files:
            try:
                violations = self._scan_file(file_path)
                self.violations.extend(violations)
                scanned_files.append(str(file_path))
                
                if violations:
                    logger.debug(f"âš ï¸  {file_path}: {len(violations)}ê°œ ìœ„ë°˜ ë°œê²¬")
                    
            except Exception as e:
                logger.error(f"âŒ {file_path} ìŠ¤ìº” ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ë¶„ì„
        results = self._analyze_scan_results(scanned_files)
        
        # ê²°ê³¼ ì €ì¥
        self._save_scan_results(results)
        
        logger.info(f"âœ… ìŠ¤ìº” ì™„ë£Œ: {results.total_files_scanned}ê°œ íŒŒì¼, "
                   f"{len(results.violations_found)}ê°œ ìœ„ë°˜ ë°œê²¬")
        
        return results
    
    def _collect_python_files(self) -> List[Path]:
        """Python íŒŒì¼ ìˆ˜ì§‘"""
        python_files = []
        
        for pattern in [self.scan_patterns["python_files"]]:
            for file_path in self.project_root.glob(pattern):
                # ì œì™¸ íŒ¨í„´ ì²´í¬
                should_exclude = False
                for exclude_pattern in self.scan_patterns["exclude_patterns"]:
                    if file_path.match(exclude_pattern):
                        should_exclude = True
                        break
                
                if not should_exclude and file_path.is_file():
                    python_files.append(file_path)
        
        return sorted(python_files)
    
    def _scan_file(self, file_path: Path) -> List[CodeViolation]:
        """ê°œë³„ íŒŒì¼ ìŠ¤ìº”"""
        violations = []
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.split('\n')
            
            # 1. ì •ê·œì‹ ê¸°ë°˜ íŒ¨í„´ íƒì§€
            regex_violations = self._scan_with_regex(file_path, content, lines)
            violations.extend(regex_violations)
            
            # 2. AST ê¸°ë°˜ êµ¬ì¡° ë¶„ì„
            ast_violations = self._scan_with_ast(file_path, content)
            violations.extend(ast_violations)
            
            # 3. LLM First ì›ì¹™ ìœ„ë°˜ íƒì§€
            llm_violations = self._scan_llm_first_violations(file_path, content, lines)
            violations.extend(llm_violations)
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜ {file_path}: {e}")
        
        return violations
    
    def _scan_with_regex(self, file_path: Path, content: str, lines: List[str]) -> List[CodeViolation]:
        """ì •ê·œì‹ ê¸°ë°˜ íŒ¨í„´ ìŠ¤ìº”"""
        violations = []
        
        for violation_type, patterns in self.hardcode_patterns.items():
            for pattern in patterns:
                try:
                    for match in re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE):
                        line_number = content[:match.start()].count('\n') + 1
                        line_content = lines[line_number - 1] if line_number <= len(lines) else ""
                        
                        # ì½”ë©˜íŠ¸ë‚˜ ë…ìŠ¤íŠ¸ë§ ë‚´ë¶€ëŠ” ì œì™¸
                        if self._is_in_comment_or_docstring(line_content, match.group()):
                            continue
                        
                        violation = self._create_violation_from_regex(
                            file_path, line_number, line_content, 
                            violation_type, pattern, match.group()
                        )
                        
                        if violation:
                            violations.append(violation)
                            
                except re.error as e:
                    logger.warning(f"ì •ê·œì‹ ì˜¤ë¥˜ {pattern}: {e}")
        
        return violations
    
    def _scan_with_ast(self, file_path: Path, content: str) -> List[CodeViolation]:
        """AST ê¸°ë°˜ êµ¬ì¡° ë¶„ì„"""
        violations = []
        
        try:
            tree = ast.parse(content)
            self.ast_visitor.reset(str(file_path), content.split('\n'))
            self.ast_visitor.visit(tree)
            violations.extend(self.ast_visitor.violations)
            
        except SyntaxError as e:
            logger.warning(f"AST íŒŒì‹± ì‹¤íŒ¨ {file_path}: {e}")
        except Exception as e:
            logger.error(f"AST ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
        
        return violations
    
    def _scan_llm_first_violations(self, file_path: Path, content: str, lines: List[str]) -> List[CodeViolation]:
        """LLM First ì›ì¹™ ìœ„ë°˜ íƒì§€"""
        violations = []
        
        # ì•ˆí‹° íŒ¨í„´ ê²€ìƒ‰
        for anti_pattern in self.llm_first_violations["anti_patterns"]:
            for match in re.finditer(rf'\b{re.escape(anti_pattern)}\b', content, re.IGNORECASE):
                line_number = content[:match.start()].count('\n') + 1
                line_content = lines[line_number - 1] if line_number <= len(lines) else ""
                
                if self._is_in_comment_or_docstring(line_content, match.group()):
                    continue
                
                violation = CodeViolation(
                    file_path=str(file_path),
                    line_number=line_number,
                    violation_type=ViolationType.RULE_BASED_LOGIC,
                    severity=Severity.HIGH,
                    description=f"LLM First ì›ì¹™ ìœ„ë°˜: '{anti_pattern}' ì•ˆí‹°íŒ¨í„´ ì‚¬ìš©",
                    code_snippet=line_content.strip(),
                    suggested_fix=f"LLM ê¸°ë°˜ ë™ì  ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´ ê²€í† ",
                    llm_first_impact=0.8,
                    confidence=0.7,
                    context={"anti_pattern": anti_pattern}
                )
                violations.append(violation)
        
        # í•¨ìˆ˜ëª…/ë³€ìˆ˜ëª… ë¶„ì„
        function_violations = self._analyze_function_names(file_path, content, lines)
        violations.extend(function_violations)
        
        return violations
    
    def _analyze_function_names(self, file_path: Path, content: str, lines: List[str]) -> List[CodeViolation]:
        """í•¨ìˆ˜ëª…/ë³€ìˆ˜ëª…ì—ì„œ LLM First ìœ„ë°˜ ë¶„ì„"""
        violations = []
        
        # í•¨ìˆ˜ ì •ì˜ íŒ¨í„´
        function_pattern = r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\('
        
        for match in re.finditer(function_pattern, content):
            function_name = match.group(1)
            line_number = content[:match.start()].count('\n') + 1
            line_content = lines[line_number - 1] if line_number <= len(lines) else ""
            
            # Rule ê¸°ë°˜ í•¨ìˆ˜ëª… íŒ¨í„´ ì²´í¬
            rule_based_patterns = [
                'handle_', 'process_', 'parse_', 'validate_',
                'check_', 'verify_', 'match_', 'filter_',
                'transform_', 'convert_', 'format_'
            ]
            
            for pattern in rule_based_patterns:
                if function_name.startswith(pattern) and not any(
                    llm_keyword in function_name.lower() 
                    for llm_keyword in ['llm', 'ai', 'intelligent', 'smart', 'dynamic']
                ):
                    violation = CodeViolation(
                        file_path=str(file_path),
                        line_number=line_number,
                        violation_type=ViolationType.RULE_BASED_LOGIC,
                        severity=Severity.MEDIUM,
                        description=f"Rule ê¸°ë°˜ í•¨ìˆ˜ëª… íŒ¨í„´: '{function_name}'",
                        code_snippet=line_content.strip(),
                        suggested_fix=f"LLM ê¸°ë°˜ í•¨ìˆ˜ë¡œ ë¦¬íŒ©í† ë§ ê²€í†  (ì˜ˆ: ai_{function_name}, llm_{function_name})",
                        llm_first_impact=0.5,
                        confidence=0.6,
                        context={"function_name": function_name, "pattern": pattern}
                    )
                    violations.append(violation)
                    break
        
        return violations
    
    def _create_violation_from_regex(self, file_path: Path, line_number: int, line_content: str,
                                   violation_type: str, pattern: str, matched_text: str) -> Optional[CodeViolation]:
        """ì •ê·œì‹ ë§¤ì¹˜ì—ì„œ ìœ„ë°˜ ê°ì²´ ìƒì„±"""
        
        # ìœ„ë°˜ ìœ í˜•ë³„ ì‹¬ê°ë„ ë° ì„¤ëª… ë§¤í•‘
        type_mapping = {
            "dataset_specific": (ViolationType.DATASET_DEPENDENCY, Severity.CRITICAL, 
                               "ë°ì´í„°ì…‹ íŠ¹í™” í•˜ë“œì½”ë”©", 0.9),
            "rule_based": (ViolationType.RULE_BASED_LOGIC, Severity.HIGH,
                          "Rule ê¸°ë°˜ ë¡œì§", 0.8),
            "pattern_matching": (ViolationType.PATTERN_MATCHING, Severity.HIGH,
                               "íŒ¨í„´ ë§¤ì¹­ ë¡œì§", 0.7),
            "template_response": (ViolationType.TEMPLATE_RESPONSE, Severity.MEDIUM,
                                "í…œí”Œë¦¿ ì‘ë‹µ", 0.6),
            "fixed_workflow": (ViolationType.FIXED_WORKFLOW, Severity.MEDIUM,
                             "ê³ ì • ì›Œí¬í”Œë¡œìš°", 0.6)
        }
        
        if violation_type not in type_mapping:
            return None
        
        vtype, severity, description, impact = type_mapping[violation_type]
        
        # ì œì•ˆëœ ìˆ˜ì • ë°©ë²• ìƒì„±
        suggested_fix = self._generate_suggested_fix(vtype, matched_text)
        
        return CodeViolation(
            file_path=str(file_path),
            line_number=line_number,
            violation_type=vtype,
            severity=severity,
            description=f"{description}: '{matched_text}'",
            code_snippet=line_content.strip(),
            suggested_fix=suggested_fix,
            llm_first_impact=impact,
            confidence=0.8,
            context={"pattern": pattern, "matched_text": matched_text}
        )
    
    def _generate_suggested_fix(self, violation_type: ViolationType, matched_text: str) -> str:
        """ìœ„ë°˜ ìœ í˜•ë³„ ìˆ˜ì • ì œì•ˆ ìƒì„±"""
        
        fixes = {
            ViolationType.DATASET_DEPENDENCY: 
                f"ë²”ìš©ì  ë¶„ì„ ë¡œì§ìœ¼ë¡œ ëŒ€ì²´. íŠ¹ì • ë°ì´í„°ì…‹('{matched_text}')ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” LLM ê¸°ë°˜ ë¶„ì„ êµ¬í˜„",
            ViolationType.RULE_BASED_LOGIC:
                f"ì¡°ê±´ë¬¸ì„ LLM íŒë‹¨ìœ¼ë¡œ ëŒ€ì²´. '{matched_text}' ëŒ€ì‹  LLMì—ê²Œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²°ì • ìœ„ì„",
            ViolationType.PATTERN_MATCHING:
                f"ì •ê·œì‹ ë§¤ì¹­ì„ LLM í…ìŠ¤íŠ¸ ì´í•´ë¡œ ëŒ€ì²´. '{matched_text}' íŒ¨í„´ ëŒ€ì‹  ìì—°ì–´ ì²˜ë¦¬ í™œìš©",
            ViolationType.TEMPLATE_RESPONSE:
                f"ê³ ì • í…œí”Œë¦¿ì„ LLM ìƒì„± ì‘ë‹µìœ¼ë¡œ ëŒ€ì²´. '{matched_text}' ëŒ€ì‹  ë™ì  ì‘ë‹µ ìƒì„±",
            ViolationType.FIXED_WORKFLOW:
                f"ê³ ì • ì›Œí¬í”Œë¡œìš°ë¥¼ ì ì‘í˜•ìœ¼ë¡œ ë³€ê²½. '{matched_text}' ëŒ€ì‹  LLMì´ ìƒí™©ì— ë§ëŠ” ë‹¨ê³„ ê²°ì •",
            ViolationType.HARDCODED_VALUES:
                f"í•˜ë“œì½”ë”©ëœ ê°’ì„ ì„¤ì •ì´ë‚˜ LLM ì¶”ë¡ ìœ¼ë¡œ ëŒ€ì²´. '{matched_text}' ê°’ì„ ë™ì ìœ¼ë¡œ ê²°ì •",
            ViolationType.CONDITIONAL_HARDCODE:
                f"ì¡°ê±´ë¶€ í•˜ë“œì½”ë”©ì„ LLM ê¸°ë°˜ ë™ì  ë¡œì§ìœ¼ë¡œ ëŒ€ì²´"
        }
        
        return fixes.get(violation_type, f"LLM First ì›ì¹™ì— ë”°ë¼ '{matched_text}'ë¥¼ ë™ì  ë¡œì§ìœ¼ë¡œ ëŒ€ì²´ ê²€í† ")
    
    def _is_in_comment_or_docstring(self, line_content: str, matched_text: str) -> bool:
        """ì½”ë©˜íŠ¸ë‚˜ ë…ìŠ¤íŠ¸ë§ ë‚´ë¶€ì¸ì§€ í™•ì¸"""
        stripped_line = line_content.strip()
        
        # ì£¼ì„ ë¼ì¸
        if stripped_line.startswith('#'):
            return True
        
        # ë…ìŠ¤íŠ¸ë§ (ê°„ë‹¨í•œ ì²´í¬)
        if '"""' in line_content or "'''" in line_content:
            return True
        
        # ë¬¸ìì—´ ë¦¬í„°ëŸ´ ë‚´ë¶€ (ê°„ë‹¨í•œ ì²´í¬)
        if (matched_text in line_content and 
            (line_content.count('"') >= 2 or line_content.count("'") >= 2)):
            # ë” ì •í™•í•œ ì²´í¬ê°€ í•„ìš”í•˜ì§€ë§Œ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ì‚¬ìš©
            quote_positions = []
            for i, char in enumerate(line_content):
                if char in ['"', "'"]:
                    quote_positions.append(i)
            
            if len(quote_positions) >= 2:
                text_pos = line_content.find(matched_text)
                if text_pos != -1:
                    for i in range(0, len(quote_positions), 2):
                        if i + 1 < len(quote_positions):
                            start, end = quote_positions[i], quote_positions[i + 1]
                            if start <= text_pos <= end:
                                return True
        
        return False
    
    def _analyze_scan_results(self, scanned_files: List[str]) -> ScanResults:
        """ìŠ¤ìº” ê²°ê³¼ ë¶„ì„"""
        
        # ìœ„ë°˜ ìœ í˜•ë³„ ì§‘ê³„
        violations_by_type = defaultdict(int)
        violations_by_severity = defaultdict(int)
        files_with_violations = set()
        
        for violation in self.violations:
            violations_by_type[violation.violation_type] += 1
            violations_by_severity[violation.severity] += 1
            files_with_violations.add(violation.file_path)
        
        # LLM First ì¤€ìˆ˜ë„ ê³„ì‚°
        llm_first_score = self._calculate_llm_first_compliance()
        
        # ë¦¬íŒ©í† ë§ ìš°ì„ ìˆœìœ„ ê²°ì •
        refactoring_priority = self._determine_refactoring_priority()
        
        return ScanResults(
            total_files_scanned=len(scanned_files),
            violations_found=self.violations,
            violations_by_type=dict(violations_by_type),
            violations_by_severity=dict(violations_by_severity),
            llm_first_compliance_score=llm_first_score,
            files_with_violations=files_with_violations,
            suggested_refactoring_priority=refactoring_priority
        )
    
    def _calculate_llm_first_compliance(self) -> float:
        """LLM First ì¤€ìˆ˜ë„ ê³„ì‚° (0-100ì )"""
        if not self.violations:
            return 100.0
        
        # ìœ„ë°˜ë³„ ê°€ì¤‘ì¹˜ ì ìš©
        total_penalty = 0.0
        total_weight = 0.0
        
        for violation in self.violations:
            # ì‹¬ê°ë„ë³„ ê°€ì¤‘ì¹˜
            severity_weights = {
                Severity.CRITICAL: 1.0,
                Severity.HIGH: 0.7,
                Severity.MEDIUM: 0.4,
                Severity.LOW: 0.2
            }
            
            weight = severity_weights.get(violation.severity, 0.5)
            penalty = violation.llm_first_impact * weight
            
            total_penalty += penalty
            total_weight += weight
        
        # ìœ„ë°˜ ë°€ë„ ê³ ë ¤ (ìœ„ë°˜ ìˆ˜ / ìŠ¤ìº” íŒŒì¼ ìˆ˜)
        violation_density = len(self.violations) / max(len(self.violations), 1)
        density_penalty = min(50.0, violation_density * 10)
        
        # ìµœì¢… ì ìˆ˜ ê³„ì‚°
        avg_penalty = total_penalty / max(total_weight, 1) if total_weight > 0 else 0
        compliance_score = max(0.0, 100.0 - (avg_penalty * 100) - density_penalty)
        
        return round(compliance_score, 1)
    
    def _determine_refactoring_priority(self) -> List[str]:
        """ë¦¬íŒ©í† ë§ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        
        # íŒŒì¼ë³„ ìœ„ë°˜ ì ìˆ˜ ê³„ì‚°
        file_scores = defaultdict(float)
        
        for violation in self.violations:
            severity_multipliers = {
                Severity.CRITICAL: 4.0,
                Severity.HIGH: 3.0,
                Severity.MEDIUM: 2.0,
                Severity.LOW: 1.0
            }
            
            multiplier = severity_multipliers.get(violation.severity, 1.0)
            score = violation.llm_first_impact * multiplier * violation.confidence
            file_scores[violation.file_path] += score
        
        # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        sorted_files = sorted(file_scores.items(), key=lambda x: x[1], reverse=True)
        
        return [file_path for file_path, score in sorted_files[:20]]  # ìƒìœ„ 20ê°œ íŒŒì¼
    
    def _save_scan_results(self, results: ScanResults):
        """ìŠ¤ìº” ê²°ê³¼ ì €ì¥"""
        from datetime import datetime
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ê²°ê³¼ ì €ì¥
        results_data = {
            "scan_timestamp": datetime.now().isoformat(),
            "summary": {
                "total_files_scanned": results.total_files_scanned,
                "total_violations": len(results.violations_found),
                "llm_first_compliance_score": results.llm_first_compliance_score,
                "files_with_violations": len(results.files_with_violations)
            },
            "violations_by_type": {vtype.value: count for vtype, count in results.violations_by_type.items()},
            "violations_by_severity": {severity.value: count for severity, count in results.violations_by_severity.items()},
            "violations": [
                {
                    "file_path": v.file_path,
                    "line_number": v.line_number,
                    "violation_type": v.violation_type.value,
                    "severity": v.severity.value,
                    "description": v.description,
                    "code_snippet": v.code_snippet,
                    "suggested_fix": v.suggested_fix,
                    "llm_first_impact": v.llm_first_impact,
                    "confidence": v.confidence,
                    "context": v.context
                }
                for v in results.violations_found
            ],
            "refactoring_priority": results.suggested_refactoring_priority
        }
        
        json_path = self.results_dir / f"hardcode_scan_results_{timestamp}.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ìŠ¤ìº” ê²°ê³¼ ì €ì¥: {json_path}")
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        self._generate_summary_report(results, timestamp)
    
    def _generate_summary_report(self, results: ScanResults, timestamp: str):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±"""
        report_path = self.results_dir / f"hardcode_summary_report_{timestamp}.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(f"# í•˜ë“œì½”ë”© íƒì§€ ê²°ê³¼ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**ìŠ¤ìº” ì¼ì‹œ**: {timestamp}\n\n")
            
            f.write(f"## ğŸ“Š ì „ì²´ ìš”ì•½\n\n")
            f.write(f"- **ìŠ¤ìº” íŒŒì¼ ìˆ˜**: {results.total_files_scanned}ê°œ\n")
            f.write(f"- **ìœ„ë°˜ ë°œê²¬**: {len(results.violations_found)}ê°œ\n")
            f.write(f"- **LLM First ì¤€ìˆ˜ë„**: {results.llm_first_compliance_score:.1f}/100\n")
            f.write(f"- **ë¬¸ì œ íŒŒì¼ ìˆ˜**: {len(results.files_with_violations)}ê°œ\n\n")
            
            f.write(f"## ğŸ¯ ìœ„ë°˜ ìœ í˜•ë³„ ë¶„í¬\n\n")
            for vtype, count in results.violations_by_type.items():
                f.write(f"- **{vtype.value}**: {count}ê°œ\n")
            f.write("\n")
            
            f.write(f"## âš ï¸ ì‹¬ê°ë„ë³„ ë¶„í¬\n\n")
            for severity, count in results.violations_by_severity.items():
                emoji = {"critical": "ğŸš¨", "high": "âš ï¸", "medium": "âš¡", "low": "ğŸ’¡"}
                f.write(f"- {emoji.get(severity.value, 'â€¢')} **{severity.value}**: {count}ê°œ\n")
            f.write("\n")
            
            f.write(f"## ğŸ”§ ìš°ì„  ë¦¬íŒ©í† ë§ ëŒ€ìƒ\n\n")
            for i, file_path in enumerate(results.suggested_refactoring_priority[:10], 1):
                f.write(f"{i}. `{file_path}`\n")
            f.write("\n")
            
            f.write(f"## ğŸ“‹ ì£¼ìš” ìœ„ë°˜ ì‚¬ë¡€\n\n")
            critical_violations = [v for v in results.violations_found if v.severity == Severity.CRITICAL][:5]
            for violation in critical_violations:
                f.write(f"### {violation.file_path}:{violation.line_number}\n")
                f.write(f"- **ìœ í˜•**: {violation.violation_type.value}\n")
                f.write(f"- **ì„¤ëª…**: {violation.description}\n")
                f.write(f"- **ì½”ë“œ**: `{violation.code_snippet}`\n")
                f.write(f"- **ì œì•ˆ**: {violation.suggested_fix}\n\n")
        
        logger.info(f"ğŸ“ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    
    def generate_refactoring_plan(self, results: ScanResults) -> Dict[str, Any]:
        """ë¦¬íŒ©í† ë§ ê³„íš ìƒì„±"""
        
        plan = {
            "overview": {
                "total_violations": len(results.violations_found),
                "estimated_effort_hours": self._estimate_refactoring_effort(results),
                "priority_files": len(results.suggested_refactoring_priority),
                "expected_compliance_improvement": self._estimate_compliance_improvement(results)
            },
            "phases": [],
            "file_specific_plans": {}
        }
        
        # Phaseë³„ ê³„íš
        critical_violations = [v for v in results.violations_found if v.severity == Severity.CRITICAL]
        high_violations = [v for v in results.violations_found if v.severity == Severity.HIGH]
        
        if critical_violations:
            plan["phases"].append({
                "phase": 1,
                "name": "Critical ìœ„ë°˜ ìˆ˜ì •",
                "violations": len(critical_violations),
                "estimated_hours": len(critical_violations) * 2,
                "description": "LLM First ì›ì¹™ ì‹¬ê° ìœ„ë°˜ ìˆ˜ì •"
            })
        
        if high_violations:
            plan["phases"].append({
                "phase": 2,
                "name": "High ìœ„ë°˜ ìˆ˜ì •",
                "violations": len(high_violations),
                "estimated_hours": len(high_violations) * 1.5,
                "description": "Rule ê¸°ë°˜ ë¡œì§ LLM ì „í™˜"
            })
        
        # íŒŒì¼ë³„ ì„¸ë¶€ ê³„íš
        for file_path in results.suggested_refactoring_priority[:10]:
            file_violations = [v for v in results.violations_found if v.file_path == file_path]
            plan["file_specific_plans"][file_path] = {
                "violation_count": len(file_violations),
                "severity_distribution": {
                    severity.value: len([v for v in file_violations if v.severity == severity])
                    for severity in Severity
                },
                "estimated_hours": len(file_violations) * 1.5,
                "key_changes": [v.suggested_fix for v in file_violations[:3]]
            }
        
        return plan
    
    def _estimate_refactoring_effort(self, results: ScanResults) -> float:
        """ë¦¬íŒ©í† ë§ ë…¸ë ¥ ì‹œê°„ ì¶”ì •"""
        effort_by_severity = {
            Severity.CRITICAL: 3.0,  # 3ì‹œê°„/ê±´
            Severity.HIGH: 2.0,      # 2ì‹œê°„/ê±´
            Severity.MEDIUM: 1.5,    # 1.5ì‹œê°„/ê±´
            Severity.LOW: 0.5        # 0.5ì‹œê°„/ê±´
        }
        
        total_hours = 0.0
        for violation in results.violations_found:
            total_hours += effort_by_severity.get(violation.severity, 1.0)
        
        return round(total_hours, 1)
    
    def _estimate_compliance_improvement(self, results: ScanResults) -> float:
        """ì¤€ìˆ˜ë„ ê°œì„  ì˜ˆìƒì¹˜ ê³„ì‚°"""
        current_score = results.llm_first_compliance_score
        
        # Critical/High ìœ„ë°˜ ìˆ˜ì • ì‹œ ì˜ˆìƒ ê°œì„ 
        critical_count = results.violations_by_severity.get(Severity.CRITICAL, 0)
        high_count = results.violations_by_severity.get(Severity.HIGH, 0)
        
        improvement = (critical_count * 15) + (high_count * 10)  # ì ìˆ˜ ê°œì„  ì¶”ì •
        expected_score = min(100.0, current_score + improvement)
        
        return round(expected_score, 1)


class HardcodeASTVisitor(ast.NodeVisitor):
    """AST ê¸°ë°˜ í•˜ë“œì½”ë”© íƒì§€ ë°©ë¬¸ì"""
    
    def __init__(self):
        self.violations: List[CodeViolation] = []
        self.file_path = ""
        self.lines = []
    
    def reset(self, file_path: str, lines: List[str]):
        """ìƒˆ íŒŒì¼ ë¶„ì„ì„ ìœ„í•œ ì´ˆê¸°í™”"""
        self.violations.clear()
        self.file_path = file_path
        self.lines = lines
    
    def visit_If(self, node: ast.If):
        """if ë¬¸ ë¶„ì„"""
        # í•˜ë“œì½”ë”©ëœ ì¡°ê±´ë¬¸ íƒì§€
        if self._is_hardcoded_condition(node.test):
            line_number = node.lineno
            line_content = self.lines[line_number - 1] if line_number <= len(self.lines) else ""
            
            violation = CodeViolation(
                file_path=self.file_path,
                line_number=line_number,
                violation_type=ViolationType.CONDITIONAL_HARDCODE,
                severity=Severity.HIGH,
                description="í•˜ë“œì½”ë”©ëœ ì¡°ê±´ë¬¸ íƒì§€",
                code_snippet=line_content.strip(),
                suggested_fix="ì¡°ê±´ë¶€ ë¡œì§ì„ LLM ê¸°ë°˜ ë™ì  íŒë‹¨ìœ¼ë¡œ ëŒ€ì²´",
                llm_first_impact=0.7,
                confidence=0.8
            )
            self.violations.append(violation)
        
        self.generic_visit(node)
    
    def visit_Dict(self, node: ast.Dict):
        """ë”•ì…”ë„ˆë¦¬ ë¦¬í„°ëŸ´ ë¶„ì„"""
        # í•˜ë“œì½”ë”©ëœ ë§¤í•‘ íƒì§€
        if self._is_hardcoded_mapping(node):
            line_number = node.lineno
            line_content = self.lines[line_number - 1] if line_number <= len(self.lines) else ""
            
            violation = CodeViolation(
                file_path=self.file_path,
                line_number=line_number,
                violation_type=ViolationType.HARDCODED_VALUES,
                severity=Severity.MEDIUM,
                description="í•˜ë“œì½”ë”©ëœ ë§¤í•‘ ë”•ì…”ë„ˆë¦¬",
                code_snippet=line_content.strip(),
                suggested_fix="ì •ì  ë§¤í•‘ì„ ë™ì  ë¡œì§ ë˜ëŠ” ì„¤ì • íŒŒì¼ë¡œ ëŒ€ì²´",
                llm_first_impact=0.5,
                confidence=0.6
            )
            self.violations.append(violation)
        
        self.generic_visit(node)
    
    def _is_hardcoded_condition(self, node: ast.AST) -> bool:
        """í•˜ë“œì½”ë”©ëœ ì¡°ê±´ì¸ì§€ í™•ì¸"""
        if isinstance(node, ast.Compare):
            # ë¬¸ìì—´ ë¦¬í„°ëŸ´ê³¼ì˜ ë¹„êµ
            for comparator in node.comparators:
                if isinstance(comparator, ast.Constant) and isinstance(comparator.value, str):
                    return True
        
        elif isinstance(node, ast.BoolOp):
            # ë…¼ë¦¬ ì—°ì‚°ì—ì„œ í•˜ë“œì½”ë”©ëœ ê°’ë“¤
            for value in node.values:
                if self._is_hardcoded_condition(value):
                    return True
        
        return False
    
    def _is_hardcoded_mapping(self, node: ast.Dict) -> bool:
        """í•˜ë“œì½”ë”©ëœ ë§¤í•‘ì¸ì§€ í™•ì¸"""
        if len(node.keys) < 3:  # ë„ˆë¬´ ì‘ì€ ë”•ì…”ë„ˆë¦¬ëŠ” ì œì™¸
            return False
        
        # ëª¨ë“  í‚¤ê°€ ë¬¸ìì—´ ë¦¬í„°ëŸ´ì¸ì§€ í™•ì¸
        string_keys = 0
        for key in node.keys:
            if isinstance(key, ast.Constant) and isinstance(key.value, str):
                string_keys += 1
        
        # 70% ì´ìƒì´ ë¬¸ìì—´ í‚¤ë©´ í•˜ë“œì½”ë”©ëœ ë§¤í•‘ìœ¼ë¡œ íŒë‹¨
        return (string_keys / len(node.keys)) > 0.7


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
def main():
    """í•˜ë“œì½”ë”© íƒì§€ ì‹¤í–‰ ì˜ˆì‹œ"""
    detector = HardcodeDetector(".")
    
    print("ğŸ” í•˜ë“œì½”ë”© íƒì§€ ì‹œì‘...")
    results = detector.scan_codebase()
    
    print(f"\nğŸ“Š íƒì§€ ê²°ê³¼:")
    print(f"   ìŠ¤ìº” íŒŒì¼: {results.total_files_scanned}ê°œ")
    print(f"   ìœ„ë°˜ ë°œê²¬: {len(results.violations_found)}ê°œ")
    print(f"   LLM First ì¤€ìˆ˜ë„: {results.llm_first_compliance_score:.1f}/100")
    
    if results.violations_found:
        print(f"\nğŸš¨ ì£¼ìš” ìœ„ë°˜ ì‚¬ë¡€:")
        for violation in results.violations_found[:5]:
            print(f"   â€¢ {violation.file_path}:{violation.line_number} - {violation.description}")
    
    # ë¦¬íŒ©í† ë§ ê³„íš ìƒì„±
    plan = detector.generate_refactoring_plan(results)
    print(f"\nğŸ”§ ë¦¬íŒ©í† ë§ ê³„íš:")
    print(f"   ì˜ˆìƒ ì‘ì—… ì‹œê°„: {plan['overview']['estimated_effort_hours']}ì‹œê°„")
    print(f"   ì˜ˆìƒ ì¤€ìˆ˜ë„ ê°œì„ : {plan['overview']['expected_compliance_improvement']:.1f}/100")

if __name__ == "__main__":
    main() 