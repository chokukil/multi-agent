# File: core/llm_factory.py
# Location: ./core/llm_factory.py

import os
import logging
from typing import Optional, Dict, Any
from langchain_openai import ChatOpenAI

# Ollama import - ÏÉàÎ°úÏö¥ Ìå®ÌÇ§ÏßÄÎ•º Ïö∞ÏÑ† ÏãúÎèÑÌïòÍ≥† fallback
try:
    from langchain_ollama import ChatOllama
    OLLAMA_IMPORT_SOURCE = "langchain_ollama"
except ImportError:
    try:
        from langchain_community.chat_models.ollama import ChatOllama
        OLLAMA_IMPORT_SOURCE = "langchain_community"
        logging.warning("Using deprecated ChatOllama from langchain_community. Consider upgrading to langchain-ollama.")
    except ImportError:
        ChatOllama = None
        OLLAMA_IMPORT_SOURCE = None
        logging.error("ChatOllama not available. Please install langchain-ollama or langchain-community.")

# Langfuse imports - 2.60.8 Î≤ÑÏ†ÑÏóê ÎßûÎäî Ïò¨Î∞îÎ•∏ import Í≤ΩÎ°ú ÏÇ¨Ïö©
try:
    from langfuse import Langfuse
    from langfuse.callback import CallbackHandler  # 2.60.8ÏóêÏÑúÎäî callback Î™®ÎìàÏóêÏÑú import
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    logging.warning("Langfuse not available. Install langfuse for advanced tracing.")

from .utils.config import get_config

# Ollama Î™®Îç∏Î≥Ñ ÎèÑÍµ¨ Ìò∏Ï∂ú Îä•Î†• Îß§Ìïë
OLLAMA_TOOL_CALLING_MODELS = {
    # ÎèÑÍµ¨ Ìò∏Ï∂úÏùÑ Ïûò ÏßÄÏõêÌïòÎäî Î™®Îç∏Îì§
    "qwen2.5:7b", "qwen2.5:14b", "qwen2.5:32b", "qwen2.5:72b",
    "qwen3:8b", "qwen3:14b", "qwen3:32b",
    "llama3.1:8b", "llama3.1:70b", "llama3.1:405b",
    "llama3.2:3b", "llama3.2:1b",
    "mistral:7b", "mistral:latest",
    "mixtral:8x7b", "mixtral:8x22b",
    "gemma2:9b", "gemma2:27b",
    "phi3:3.8b", "phi3:14b",
    "codellama:7b", "codellama:13b", "codellama:34b",
    "deepseek-coder:6.7b", "deepseek-coder:33b"
}

def is_ollama_model_tool_capable(model: str) -> bool:
    """Ollama Î™®Îç∏Ïùò ÎèÑÍµ¨ Ìò∏Ï∂ú Îä•Î†• ÌôïÏù∏"""
    if not model:
        return False
    
    # Ï†ïÌôïÌïú Îß§Ïπ≠ ÎòêÎäî Î∂ÄÎ∂Ñ Îß§Ïπ≠ ÌôïÏù∏
    model_lower = model.lower()
    
    for capable_model in OLLAMA_TOOL_CALLING_MODELS:
        if model_lower == capable_model.lower() or model_lower.startswith(capable_model.split(':')[0].lower()):
            return True
    
    return False

def create_llm_instance(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    temperature: float = 0.7,
    streaming: bool = True,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
    **kwargs
) -> Any:
    """
    ÌÜµÌï© LLM Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ± Ìå©ÌÜ†Î¶¨
    
    Args:
        provider: LLM Ï†úÍ≥µÏûê (OPENAI, OLLAMA)
        model: Î™®Îç∏ Ïù¥Î¶Ñ
        temperature: Ïò®ÎèÑ ÏÑ§Ï†ï
        streaming: Ïä§Ìä∏Î¶¨Î∞ç Ïó¨Î∂Ä
        session_id: ÏÑ∏ÏÖò ID (Streamlit session_stateÏóêÏÑú Ï†ÑÎã¨)
        user_id: ÏÇ¨Ïö©Ïûê ID
        **kwargs: Ï∂îÍ∞Ä ÌååÎùºÎØ∏ÌÑ∞
    
    Returns:
        LLM Ïù∏Ïä§ÌÑ¥Ïä§ (ÎèÑÍµ¨ Ìò∏Ï∂ú Îä•Î†• Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ìè¨Ìï®)
    """
    # Langfuse ÏΩúÎ∞± Ìï∏Îì§Îü¨ Ï¥àÍ∏∞Ìôî - multi_agent_supervisor.py Ìå®ÌÑ¥ ÏÇ¨Ïö©
    if LANGFUSE_AVAILABLE:
        langfuse_config = get_config('langfuse')
        if langfuse_config.get('host') and langfuse_config.get('public_key') and langfuse_config.get('secret_key'):
            try:
                # ÏÑ∏ÏÖò IDÎ•º ÌååÎùºÎØ∏ÌÑ∞ÏóêÏÑú Í∞ÄÏ†∏Ïò§Í±∞ÎÇò ÌôòÍ≤ΩÎ≥ÄÏàò/Í∏∞Î≥∏Í∞í ÏÇ¨Ïö©
                effective_session_id = session_id or os.getenv("THREAD_ID", "default-session")
                effective_user_id = user_id or os.getenv("EMP_NO", "default_user")
                
                # multi_agent_supervisor.pyÏôÄ ÎèôÏùºÌïú Ìå®ÌÑ¥ÏúºÎ°ú CallbackHandler ÏßÅÏ†ë Ï¥àÍ∏∞Ìôî
                handler = CallbackHandler(
                    session_id=effective_session_id,
                    user_id=effective_user_id,
                    metadata={
                        "app_type": "llm_factory",
                        "model": model or "unknown",
                        "provider": provider or "unknown",
                        "temperature": temperature,
                        "session_id": effective_session_id
                    }
                )
                
                # kwargsÏóê ÏΩúÎ∞± Ï∂îÍ∞Ä
                if 'callbacks' not in kwargs:
                    kwargs['callbacks'] = []
                kwargs['callbacks'].append(handler)
                logging.info(f"Langfuse callback handler initialized with session_id: {effective_session_id}")
            except Exception as e:
                logging.error(f"Failed to initialize Langfuse: {e}")

    # ÌôòÍ≤Ω Î≥ÄÏàòÏóêÏÑú Í∏∞Î≥∏Í∞í ÏùΩÍ∏∞
    if provider is None:
        provider = os.getenv("LLM_PROVIDER", "OPENAI")
    
    provider = provider.upper()
    
    try:
        if provider == "OPENAI":
            # OpenAI ÏÑ§Ï†ï
            api_key = os.getenv("OPENAI_API_KEY", "")
            api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
            
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found in environment variables")
            
            if model is None:
                model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            
            # ChatOpenAI Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
            llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                streaming=streaming,
                api_key=api_key,
                base_url=api_base,
                **kwargs
            )
            
            # ÎèÑÍµ¨ Ìò∏Ï∂ú Îä•Î†• Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
            llm._tool_calling_capable = True
            llm._provider = "OPENAI"
            llm._model_name = model
            
            logging.info(f"Created OpenAI LLM: model={model}, temperature={temperature}")
            
        elif provider == "OLLAMA":
            # Ollama ÏÑ§Ï†ï - OLLAMA_BASE_URLÍ≥º OLLAMA_API_BASE Î™®Îëê ÏßÄÏõê
            base_url = os.getenv("OLLAMA_BASE_URL") or os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
            
            if model is None:
                model = os.getenv("OLLAMA_MODEL", "llama2")
            
            # OllamaÎäî Î°úÏª¨ LLMÏù¥ÎØÄÎ°ú Í∏¥ ÌÉÄÏûÑÏïÑÏõÉ ÏÑ§Ï†ï (10Î∂Ñ)
            ollama_timeout = int(os.getenv("OLLAMA_TIMEOUT", "600"))  # 10Î∂Ñ Í∏∞Î≥∏Í∞í
            
            # üÜï Ollama Î™®Îç∏Ïùò ÎèÑÍµ¨ Ìò∏Ï∂ú Îä•Î†• ÌôïÏù∏
            tool_calling_capable = is_ollama_model_tool_capable(model)
            
            # Ollama Î™®Îç∏Î≥Ñ ÌäπÌôî ÏÑ§Ï†ï
            ollama_kwargs = kwargs.copy()
            
            # ÎèÑÍµ¨ Ìò∏Ï∂ú Îä•Î†•Ïù¥ Ï†úÌïúÏ†ÅÏù∏ Î™®Îç∏Ïùò Í≤ΩÏö∞ ÌäπÎ≥Ñ ÏÑ§Ï†ï
            if not tool_calling_capable:
                logging.warning(f"üö® Ollama model '{model}' has limited tool calling capability. Enabling enhanced prompting.")
                # ÎÇÆÏùÄ Ïò®ÎèÑÎ°ú ÏÑ§Ï†ïÌïòÏó¨ Îçî ÏùºÍ¥ÄÎêú Ï∂úÎ†• ÏÉùÏÑ±
                temperature = min(temperature, 0.3)
                
            # ChatOllama Ïù∏Ïä§ÌÑ¥Ïä§ ÏÉùÏÑ±
            llm = ChatOllama(
                model=model,
                temperature=temperature,
                base_url=base_url,
                streaming=streaming,
                request_timeout=ollama_timeout,  # ÏöîÏ≤≠ ÌÉÄÏûÑÏïÑÏõÉ ÏÑ§Ï†ï
                **ollama_kwargs
            )
            
            # ÎèÑÍµ¨ Ìò∏Ï∂ú Îä•Î†• Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
            llm._tool_calling_capable = tool_calling_capable
            llm._provider = "OLLAMA"
            llm._model_name = model
            llm._needs_enhanced_prompting = not tool_calling_capable
            
            if tool_calling_capable:
                logging.info(f"‚úÖ Created Ollama LLM with tool calling: model={model}, base_url={base_url}, timeout={ollama_timeout}s")
            else:
                logging.warning(f"‚ö†Ô∏è Created Ollama LLM with limited tool calling: model={model}, base_url={base_url}, timeout={ollama_timeout}s")
            
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
        
        return llm
        
    except Exception as e:
        logging.error(f"Failed to create LLM instance: {e}")
        raise

def get_llm_capabilities(llm) -> Dict[str, Any]:
    """LLMÏùò Îä•Î†• Ï†ïÎ≥¥ Î∞òÌôò"""
    return {
        "tool_calling_capable": getattr(llm, '_tool_calling_capable', True),
        "provider": getattr(llm, '_provider', 'UNKNOWN'),
        "model_name": getattr(llm, '_model_name', 'unknown'),
        "needs_enhanced_prompting": getattr(llm, '_needs_enhanced_prompting', False)
    }

def validate_llm_config() -> Dict[str, Any]:
    """
    LLM ÏÑ§Ï†ï Í≤ÄÏ¶ù Î∞è Ï†ïÎ≥¥ Î∞òÌôò
    
    Returns:
        ÏÑ§Ï†ï Ï†ïÎ≥¥ ÎîïÏÖîÎÑàÎ¶¨
    """
    provider = os.getenv("LLM_PROVIDER", "OPENAI")
    
    config = {
        "provider": provider,
        "valid": False,
        "error": None
    }
    
    try:
        if provider == "OPENAI":
            api_key = os.getenv("OPENAI_API_KEY", "")
            if not api_key:
                config["error"] = "OPENAI_API_KEY not set"
            else:
                config["valid"] = True
                config["model"] = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
                config["api_base"] = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
                config["tool_calling_capable"] = True
                
        elif provider == "OLLAMA":
            model = os.getenv("OLLAMA_MODEL", "llama2")
            config["valid"] = True
            config["model"] = model
            config["base_url"] = os.getenv("OLLAMA_BASE_URL") or os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
            config["tool_calling_capable"] = is_ollama_model_tool_capable(model)
            
            if not config["tool_calling_capable"]:
                config["warning"] = f"Model '{model}' has limited tool calling capability. Consider using qwen2.5:7b, llama3.1:8b, or other supported models."
            
        else:
            config["error"] = f"Unknown provider: {provider}"
            
    except Exception as e:
        config["error"] = str(e)
    
    return config