# File: core/llm_factory.py
# Location: ./core/llm_factory.py

import os
import logging
from typing import Optional, Dict, Any, List
from langchain_openai import ChatOpenAI

# Ollama import - ğŸ†• langchain_ollama ìš°ì„  ì‚¬ìš© (ë„êµ¬ í˜¸ì¶œ ì§€ì›)
try:
    from langchain_ollama import ChatOllama
    OLLAMA_IMPORT_SOURCE = "langchain_ollama"
    OLLAMA_TOOL_CALLING_SUPPORTED = True
    logging.info("âœ… Using langchain_ollama - Tool calling supported")
except ImportError:
    try:
        from langchain_community.chat_models.ollama import ChatOllama
        OLLAMA_IMPORT_SOURCE = "langchain_community"
        OLLAMA_TOOL_CALLING_SUPPORTED = False
        logging.warning("âš ï¸ Using deprecated langchain_community.ChatOllama - Tool calling NOT supported")
    except ImportError:
        ChatOllama = None
        OLLAMA_IMPORT_SOURCE = None
        OLLAMA_TOOL_CALLING_SUPPORTED = False
        logging.error("âŒ ChatOllama not available. Please install langchain-ollama")

# Ollama í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì ‘ê·¼ìš© (ëª¨ë¸ ëª©ë¡ ë“±)
try:
    import ollama
    OLLAMA_CLIENT_AVAILABLE = True
    logging.info("âœ… Ollama client available")
except ImportError:
    ollama = None
    OLLAMA_CLIENT_AVAILABLE = False
    logging.warning("âš ï¸ Ollama client not available. Install: uv add ollama")

# Langfuse imports - 2.60.8 ë²„ì „ì— ë§ëŠ” ì˜¬ë°”ë¥¸ import ê²½ë¡œ ì‚¬ìš©
try:
    from langfuse import Langfuse
    from langfuse.callback import CallbackHandler  # 2.60.8ì—ì„œëŠ” callback ëª¨ë“ˆì—ì„œ import
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    logging.warning("Langfuse not available. Install langfuse for advanced tracing.")

from core.utils.config import Config

# ğŸ†• Ollama ëª¨ë¸ë³„ ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ ë§¤í•‘ - 2024ë…„ 12ì›” ê¸°ì¤€ ìµœì‹  ì •ë³´
OLLAMA_TOOL_CALLING_MODELS = {
    # âœ… Llama ê³„ì—´ - í™•ì‹¤íˆ ì§€ì›
    "llama3.1:8b", "llama3.1:70b", "llama3.1:405b",
    "llama3.2:1b", "llama3.2:3b", "llama3.2:11b",
    "llama3.3:70b",  # ìƒˆë¡œìš´ ëª¨ë¸
    
    # âœ… Qwen ê³„ì—´ - 2.5 ì´ìƒë§Œ ì§€ì›
    "qwen2.5:0.5b", "qwen2.5:1.5b", "qwen2.5:3b", "qwen2.5:7b", 
    "qwen2.5:14b", "qwen2.5:32b", "qwen2.5:72b",
    "qwen2.5-coder:1.5b", "qwen2.5-coder:7b", "qwen2.5-coder:32b",
    "qwen3:8b",  # ì‚¬ìš©ì í™•ì¸: ë„êµ¬ í˜¸ì¶œ ì§€ì›
    
    # âœ… Mistral ê³„ì—´
    "mistral:7b", "mistral:latest", "mistral-nemo", "mistral-nemo:12b",
    "mixtral:8x7b", "mixtral:8x22b",
    
    # âœ… Google Gemma ê³„ì—´
    "gemma2:2b", "gemma2:9b", "gemma2:27b",
    
    # âœ… Microsoft Phi ê³„ì—´
    "phi3:mini", "phi3:medium", "phi3:3.8b", "phi3:14b",
    
    # âœ… Code ì „ìš© ëª¨ë¸ë“¤
    "codellama:7b", "codellama:13b", "codellama:34b",
    "codegemma:2b", "codegemma:7b",
    "deepseek-coder:1.3b", "deepseek-coder:6.7b", "deepseek-coder:33b",
    "starcoder2:3b", "starcoder2:7b", "starcoder2:15b",
    
    # âœ… íŠ¹ìˆ˜ Function Calling ëª¨ë¸ë“¤
    "firefunction-v2", "firefunction-v2:70b",
    "nexusraven", "nexusraven:13b",
    
    # âœ… ê¸°íƒ€ ì§€ì› ëª¨ë¸ë“¤
    "command-r", "command-r-plus",
    "yi:6b", "yi:9b", "yi:34b",
    "nous-hermes2", "nous-hermes2:10.7b", "nous-hermes2:34b"
}

# ğŸš¨ í™•ì‹¤íˆ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ë“¤ (ë” ìƒì„¸í•˜ê²Œ)
OLLAMA_NON_TOOL_CALLING_MODELS = {
    # êµ¬í˜• Llama ëª¨ë¸ë“¤
    "llama2", "llama2:7b", "llama2:13b", "llama2:70b",
    "llama:7b", "llama:13b", "llama:30b", "llama:65b",
    
    # êµ¬í˜• Qwen ëª¨ë¸ë“¤ (2.5 ì´ì „)
    "qwen", "qwen:7b", "qwen:14b", "qwen:32b", "qwen:72b",
    "qwen1.5", "qwen1.5:7b", "qwen1.5:14b", "qwen1.5:32b", "qwen1.5:72b",
    
    # ê¸°íƒ€ êµ¬í˜•/ë¯¸ì§€ì› ëª¨ë¸ë“¤
    "vicuna", "vicuna:7b", "vicuna:13b", "vicuna:33b",
    "alpaca", "alpaca:7b", "alpaca:13b",
    "orca-mini", "orca-mini:3b", "orca-mini:7b", "orca-mini:13b",
    "wizard-vicuna-uncensored", "wizard-vicuna-uncensored:7b", "wizard-vicuna-uncensored:13b",
    "falcon", "falcon:7b", "falcon:40b", "falcon:180b",
    "mpt", "mpt:7b", "mpt:30b",
    "chatglm3", "chatglm3:6b",
    "zephyr", "zephyr:7b",
    "openchat", "openchat:7b",
    "dolphin-mistral", "dolphin-mistral:7b"
}

# ğŸ¯ ê¶Œì¥ ëª¨ë¸ ëª©ë¡ (ì„±ëŠ¥/ì•ˆì •ì„± ê¸°ì¤€)
RECOMMENDED_OLLAMA_MODELS = {
    "light": {
        "name": "qwen2.5:3b",
        "description": "ê°€ë²¼ìš´ ì‘ì—…ìš© (3GB RAM)",
        "ram_requirement": "4GB",
        "use_case": "ê°„ë‹¨í•œ ì§ˆì˜ì‘ë‹µ, ì½”ë“œ ìƒì„±"
    },
    "balanced": {
        "name": "llama3.1:8b", 
        "description": "ê· í˜•ì¡íŒ ì„±ëŠ¥ (8GB RAM)",
        "ram_requirement": "10GB",
        "use_case": "ë°ì´í„° ë¶„ì„, ë³µì¡í•œ ì¶”ë¡ "
    },
    "powerful": {
        "name": "qwen2.5:14b",
        "description": "ê³ ì„±ëŠ¥ ì‘ì—…ìš© (14GB RAM)", 
        "ram_requirement": "16GB",
        "use_case": "ê³ ê¸‰ ë¶„ì„, ë³µì¡í•œ ì½”ë”©"
    },
    "coding": {
        "name": "qwen2.5-coder:7b",
        "description": "ì½”ë”© ì „ë¬¸ ëª¨ë¸ (7GB RAM)",
        "ram_requirement": "9GB", 
        "use_case": "ì½”ë“œ ìƒì„±, ë””ë²„ê¹…, ë¦¬íŒ©í† ë§"
    }
}

def get_available_ollama_models() -> List[Dict[str, Any]]:
    """Ollama ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤"""
    if not OLLAMA_CLIENT_AVAILABLE:
        return []
    
    try:
        models = ollama.list()
        available_models = []
        
        for model in models.get("models", []):
            model_name = model.get("name", "")
            model_info = {
                "name": model_name,
                "size": model.get("size", 0),
                "modified_at": model.get("modified_at", ""),
                "tool_calling_capable": is_ollama_model_tool_capable(model_name),
                "family": model.get("details", {}).get("family", "unknown")
            }
            available_models.append(model_info)
        
        return available_models
    except Exception as e:
        logging.error(f"Failed to get Ollama models: {e}")
        return []

def test_ollama_connection() -> Dict[str, Any]:
    """Ollama ì„œë²„ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤"""
    if not OLLAMA_CLIENT_AVAILABLE:
        return {
            "connected": False,
            "error": "Ollama client not available. Install: uv add ollama"
        }
    
    try:
        # ê°„ë‹¨í•œ ping í…ŒìŠ¤íŠ¸
        models = ollama.list()
        return {
            "connected": True,
            "model_count": len(models.get("models", [])),
            "server_version": getattr(ollama, "__version__", "unknown")
        }
    except Exception as e:
        return {
            "connected": False,
            "error": f"Connection failed: {str(e)}"
        }

def is_ollama_model_tool_capable(model: str) -> bool:
    """Ollama ëª¨ë¸ì˜ ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ í™•ì¸ - ì—„ê²©í•œ ê²€ì¦"""
    if not model or not OLLAMA_TOOL_CALLING_SUPPORTED:
        return False
    
    model_lower = model.lower().strip()
    
    # ğŸš¨ ëª…ì‹œì ìœ¼ë¡œ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ë“¤ í™•ì¸
    for non_capable_model in OLLAMA_NON_TOOL_CALLING_MODELS:
        if model_lower == non_capable_model.lower() or model_lower.startswith(non_capable_model.split(':')[0].lower()):
            return False
    
    # âœ… ëª…ì‹œì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” ëª¨ë¸ë“¤ í™•ì¸
    for capable_model in OLLAMA_TOOL_CALLING_MODELS:
        if model_lower == capable_model.lower() or model_lower.startswith(capable_model.split(':')[0].lower()):
            return True
    
    # ğŸ¤” ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì€ False ë°˜í™˜ (ë³´ìˆ˜ì  ì ‘ê·¼)
    logging.warning(f"âš ï¸ Unknown model '{model}' - assuming no tool calling support")
    return False

def get_model_recommendation(ram_gb: Optional[int] = None) -> Dict[str, Any]:
    """ì‚¬ìš©ì ì‹œìŠ¤í…œì— ë§ëŠ” ëª¨ë¸ ì¶”ì²œ"""
    if ram_gb is None:
        import psutil
        ram_gb = psutil.virtual_memory().total // (1024**3)
    
    if ram_gb >= 16:
        return RECOMMENDED_OLLAMA_MODELS["powerful"]
    elif ram_gb >= 10:
        return RECOMMENDED_OLLAMA_MODELS["balanced"] 
    elif ram_gb >= 6:
        return RECOMMENDED_OLLAMA_MODELS["light"]
    else:
        return {
            "name": "qwen2.5:0.5b",
            "description": "ì´ˆê²½ëŸ‰ ëª¨ë¸ (0.5GB RAM)",
            "ram_requirement": "2GB",
            "use_case": "ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë§Œ ê°€ëŠ¥",
            "warning": "ì„±ëŠ¥ì´ ë§¤ìš° ì œí•œì ì…ë‹ˆë‹¤"
        }

def create_llm_instance(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    temperature: Optional[float] = None,
    callbacks: Optional[List[Any]] = None
) -> Any:
    """
    LLM ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        provider: LLM ì œê³µì ("OPENAI" ë˜ëŠ” "OLLAMA")
        model: ëª¨ë¸ëª…
        temperature: ì˜¨ë„ ì„¤ì •
        callbacks: ì¶”ê°€ ì½œë°± ë¦¬ìŠ¤íŠ¸
        
    Returns:
        LLM ì¸ìŠ¤í„´ìŠ¤
    """
    config = Config()
    
    # í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì •
    provider = provider or os.getenv("LLM_PROVIDER", "OPENAI")
    temperature = temperature or float(os.getenv("LLM_TEMPERATURE", "0.7"))
    
    # ì½œë°± ì„¤ì • - langfuse ìë™ í¬í•¨
    callback_list = callbacks or []
    
    # Langfuse ì½œë°± ì¶”ê°€ (í™˜ê²½ ë³€ìˆ˜ë¡œ í™œì„±í™”ëœ ê²½ìš°)
    if LANGFUSE_AVAILABLE and os.getenv("LOGGING_PROVIDER") in ["langfuse", "both"]:
        try:
            langfuse_handler = CallbackHandler(
                public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
                secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
                host=os.getenv("LANGFUSE_HOST"),
            )
            callback_list.append(langfuse_handler)
            logging.info("âœ… Langfuse callback automatically added to LLM")
        except Exception as e:
            logging.warning(f"âš ï¸ Failed to add Langfuse callback: {e}")
    
    if provider.upper() == "OPENAI":
        model = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        return ChatOpenAI(
            model=model,
            temperature=temperature,
            api_key=os.getenv("OPENAI_API_KEY"),
            base_url=os.getenv("OPENAI_API_BASE"),
            callbacks=callback_list if callback_list else None
        )
        
    elif provider.upper() == "OLLAMA":
        if not ChatOllama:
            raise ImportError("Ollama not available. Install: uv add langchain-ollama")
        
        model = model or os.getenv("OLLAMA_MODEL", "llama3:8b")
        base_url = os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
        
        return ChatOllama(
            model=model,
            temperature=temperature,
            base_url=base_url,
            callbacks=callback_list if callback_list else None
        )
    
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")

def get_llm_capabilities(llm) -> Dict[str, Any]:
    """LLMì˜ ëŠ¥ë ¥ ì •ë³´ ë°˜í™˜"""
    return {
        "tool_calling_capable": getattr(llm, '_tool_calling_capable', True),
        "provider": getattr(llm, '_provider', 'UNKNOWN'),
        "model_name": getattr(llm, '_model_name', 'unknown'),
        "needs_enhanced_prompting": getattr(llm, '_needs_enhanced_prompting', False)
    }

def validate_llm_config() -> Dict[str, Any]:
    """
    LLM ì„¤ì • ê²€ì¦ ë° ì •ë³´ ë°˜í™˜
    
    Returns:
        ì„¤ì • ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    provider = os.getenv("LLM_PROVIDER", "OPENAI")
    
    config = {
        "provider": provider,
        "valid": False,
        "error": None
    }
    
    try:
        if provider == "OPENAI":
            api_key = os.getenv("OPENAI_API_KEY", "")
            if not api_key:
                config["error"] = "OPENAI_API_KEY not set"
            else:
                config["valid"] = True
                config["model"] = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
                config["api_base"] = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
                config["tool_calling_capable"] = True
                
        elif provider == "OLLAMA":
            model = os.getenv("OLLAMA_MODEL", "llama3.1:8b")  # ğŸ†• ê¸°ë³¸ê°’ ë³€ê²½
            config["valid"] = OLLAMA_TOOL_CALLING_SUPPORTED
            config["model"] = model
            config["base_url"] = os.getenv("OLLAMA_BASE_URL") or os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
            config["tool_calling_capable"] = is_ollama_model_tool_capable(model) and OLLAMA_TOOL_CALLING_SUPPORTED
            config["import_source"] = OLLAMA_IMPORT_SOURCE
            
            if not OLLAMA_TOOL_CALLING_SUPPORTED:
                config["error"] = "langchain-ollama package required for tool calling"
                config["warning"] = "Install: pip install langchain-ollama"
            elif not config["tool_calling_capable"]:
                recommended = get_model_recommendation()
                config["warning"] = f"Model '{model}' has limited tool calling. Recommended: {recommended['name']}"
                config["alternatives"] = ["llama3.1:8b", "qwen2.5:7b", "mistral:7b", "qwen2.5-coder:7b"]
            
            # ğŸ†• Ollama ì—°ê²° ìƒíƒœ ì¶”ê°€ í™•ì¸
            connection_status = test_ollama_connection()
            config["connection"] = connection_status
            config["available_models"] = get_available_ollama_models()
            
        else:
            config["error"] = f"Unknown provider: {provider}"
            
    except Exception as e:
        config["error"] = str(e)
    
    return config

# ğŸ†• ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def get_ollama_status() -> Dict[str, Any]:
    """Ollama ì„œë²„ì˜ ì „ì²´ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
    status = {
        "client_available": OLLAMA_CLIENT_AVAILABLE,
        "langchain_package": OLLAMA_IMPORT_SOURCE,
        "tool_calling_supported": OLLAMA_TOOL_CALLING_SUPPORTED,
        "connection": test_ollama_connection(),
        "available_models": get_available_ollama_models(),
        "recommended_models": RECOMMENDED_OLLAMA_MODELS,
        "current_model": os.getenv("OLLAMA_MODEL", "llama3.1:8b")
    }
    
    # í˜„ì¬ ëª¨ë¸ì˜ ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ í™•ì¸
    current_model = status["current_model"]
    status["current_model_tool_capable"] = is_ollama_model_tool_capable(current_model)
    
    return status

def suggest_ollama_setup() -> Dict[str, Any]:
    """ì‚¬ìš©ìë¥¼ ìœ„í•œ Ollama ì„¤ì • ì œì•ˆ"""
    status = get_ollama_status()
    suggestions = {
        "steps": [],
        "commands": [],
        "warnings": [],
        "next_actions": []
    }
    
    # 1. í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ í™•ì¸
    if not status["client_available"]:
        suggestions["steps"].append("1. Ollama Python í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜")
        suggestions["commands"].append("uv add ollama")
    
    # 2. ì„œë²„ ì—°ê²° í™•ì¸  
    if not status["connection"]["connected"]:
        suggestions["steps"].append("2. Ollama ì„œë²„ ì‹œì‘")
        suggestions["commands"].append("ollama serve")
        suggestions["warnings"].append("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
    
    # 3. ëª¨ë¸ ì„¤ì¹˜ í™•ì¸
    available_models = status["available_models"]
    if not available_models:
        suggestions["steps"].append("3. ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
        recommended = get_model_recommendation()
        suggestions["commands"].append(f"ollama pull {recommended['name']}")
        suggestions["next_actions"].append(f"í™˜ê²½ë³€ìˆ˜ ì„¤ì •: OLLAMA_MODEL={recommended['name']}")
    
    # 4. ë„êµ¬ í˜¸ì¶œ ì§€ì› í™•ì¸
    if not status["current_model_tool_capable"]:
        suggestions["warnings"].append(f"í˜„ì¬ ëª¨ë¸ '{status['current_model']}'ì€ ë„êµ¬ í˜¸ì¶œì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        recommended = get_model_recommendation()
        suggestions["next_actions"].append(f"ê¶Œì¥ ëª¨ë¸ë¡œ ë³€ê²½: {recommended['name']}")
        suggestions["commands"].append(f"ollama pull {recommended['name']}")
    
    return suggestions