# File: core/llm_factory.py
# Location: ./core/llm_factory.py

import os
import logging
from typing import Optional, Dict, Any, List
from langchain_openai import ChatOpenAI

# Ollama import - ğŸ†• langchain_ollama ìš°ì„  ì‚¬ìš© (ë„êµ¬ í˜¸ì¶œ ì§€ì›)
try:
    from langchain_ollama import ChatOllama
    OLLAMA_IMPORT_SOURCE = "langchain_ollama"
    OLLAMA_TOOL_CALLING_SUPPORTED = True
    logging.info("âœ… Using langchain_ollama - Tool calling supported")
except ImportError:
    try:
        from langchain_community.chat_models.ollama import ChatOllama
        OLLAMA_IMPORT_SOURCE = "langchain_community"
        OLLAMA_TOOL_CALLING_SUPPORTED = False
        logging.warning("âš ï¸ Using deprecated langchain_community.ChatOllama - Tool calling NOT supported")
    except ImportError:
        ChatOllama = None
        OLLAMA_IMPORT_SOURCE = None
        OLLAMA_TOOL_CALLING_SUPPORTED = False
        logging.error("âŒ ChatOllama not available. Please install langchain-ollama")

# Ollama í´ë¼ì´ì–¸íŠ¸ ì§ì ‘ ì ‘ê·¼ìš© (ëª¨ë¸ ëª©ë¡ ë“±)
try:
    import ollama
    OLLAMA_CLIENT_AVAILABLE = True
    logging.info("âœ… Ollama client available")
except ImportError:
    ollama = None
    OLLAMA_CLIENT_AVAILABLE = False
    logging.warning("âš ï¸ Ollama client not available. Install: uv add ollama")

# Langfuse imports - 2.60.8 ë²„ì „ì— ë§ëŠ” ì˜¬ë°”ë¥¸ import ê²½ë¡œ ì‚¬ìš©
try:
    from langfuse import Langfuse
    from langfuse.callback import CallbackHandler  # 2.60.8ì—ì„œëŠ” callback ëª¨ë“ˆì—ì„œ import
    LANGFUSE_AVAILABLE = True
except ImportError:
    LANGFUSE_AVAILABLE = False
    logging.warning("Langfuse not available. Install langfuse for advanced tracing.")

from .utils.config import get_config

# ğŸ†• Ollama ëª¨ë¸ë³„ ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ ë§¤í•‘ - 2024ë…„ 12ì›” ê¸°ì¤€ ìµœì‹  ì •ë³´
OLLAMA_TOOL_CALLING_MODELS = {
    # âœ… Llama ê³„ì—´ - í™•ì‹¤íˆ ì§€ì›
    "llama3.1:8b", "llama3.1:70b", "llama3.1:405b",
    "llama3.2:1b", "llama3.2:3b", "llama3.2:11b",
    "llama3.3:70b",  # ìƒˆë¡œìš´ ëª¨ë¸
    
    # âœ… Qwen ê³„ì—´ - 2.5 ì´ìƒë§Œ ì§€ì›
    "qwen2.5:0.5b", "qwen2.5:1.5b", "qwen2.5:3b", "qwen2.5:7b", 
    "qwen2.5:14b", "qwen2.5:32b", "qwen2.5:72b",
    "qwen2.5-coder:1.5b", "qwen2.5-coder:7b", "qwen2.5-coder:32b",
    "qwen3:8b",  # ì‚¬ìš©ì í™•ì¸: ë„êµ¬ í˜¸ì¶œ ì§€ì›
    
    # âœ… Mistral ê³„ì—´
    "mistral:7b", "mistral:latest", "mistral-nemo", "mistral-nemo:12b",
    "mixtral:8x7b", "mixtral:8x22b",
    
    # âœ… Google Gemma ê³„ì—´
    "gemma2:2b", "gemma2:9b", "gemma2:27b",
    
    # âœ… Microsoft Phi ê³„ì—´
    "phi3:mini", "phi3:medium", "phi3:3.8b", "phi3:14b",
    
    # âœ… Code ì „ìš© ëª¨ë¸ë“¤
    "codellama:7b", "codellama:13b", "codellama:34b",
    "codegemma:2b", "codegemma:7b",
    "deepseek-coder:1.3b", "deepseek-coder:6.7b", "deepseek-coder:33b",
    "starcoder2:3b", "starcoder2:7b", "starcoder2:15b",
    
    # âœ… íŠ¹ìˆ˜ Function Calling ëª¨ë¸ë“¤
    "firefunction-v2", "firefunction-v2:70b",
    "nexusraven", "nexusraven:13b",
    
    # âœ… ê¸°íƒ€ ì§€ì› ëª¨ë¸ë“¤
    "command-r", "command-r-plus",
    "yi:6b", "yi:9b", "yi:34b",
    "nous-hermes2", "nous-hermes2:10.7b", "nous-hermes2:34b"
}

# ğŸš¨ í™•ì‹¤íˆ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ë“¤ (ë” ìƒì„¸í•˜ê²Œ)
OLLAMA_NON_TOOL_CALLING_MODELS = {
    # êµ¬í˜• Llama ëª¨ë¸ë“¤
    "llama2", "llama2:7b", "llama2:13b", "llama2:70b",
    "llama:7b", "llama:13b", "llama:30b", "llama:65b",
    
    # êµ¬í˜• Qwen ëª¨ë¸ë“¤ (2.5 ì´ì „)
    "qwen", "qwen:7b", "qwen:14b", "qwen:32b", "qwen:72b",
    "qwen1.5", "qwen1.5:7b", "qwen1.5:14b", "qwen1.5:32b", "qwen1.5:72b",
    
    # ê¸°íƒ€ êµ¬í˜•/ë¯¸ì§€ì› ëª¨ë¸ë“¤
    "vicuna", "vicuna:7b", "vicuna:13b", "vicuna:33b",
    "alpaca", "alpaca:7b", "alpaca:13b",
    "orca-mini", "orca-mini:3b", "orca-mini:7b", "orca-mini:13b",
    "wizard-vicuna-uncensored", "wizard-vicuna-uncensored:7b", "wizard-vicuna-uncensored:13b",
    "falcon", "falcon:7b", "falcon:40b", "falcon:180b",
    "mpt", "mpt:7b", "mpt:30b",
    "chatglm3", "chatglm3:6b",
    "zephyr", "zephyr:7b",
    "openchat", "openchat:7b",
    "dolphin-mistral", "dolphin-mistral:7b"
}

# ğŸ¯ ê¶Œì¥ ëª¨ë¸ ëª©ë¡ (ì„±ëŠ¥/ì•ˆì •ì„± ê¸°ì¤€)
RECOMMENDED_OLLAMA_MODELS = {
    "light": {
        "name": "qwen2.5:3b",
        "description": "ê°€ë²¼ìš´ ì‘ì—…ìš© (3GB RAM)",
        "ram_requirement": "4GB",
        "use_case": "ê°„ë‹¨í•œ ì§ˆì˜ì‘ë‹µ, ì½”ë“œ ìƒì„±"
    },
    "balanced": {
        "name": "llama3.1:8b", 
        "description": "ê· í˜•ì¡íŒ ì„±ëŠ¥ (8GB RAM)",
        "ram_requirement": "10GB",
        "use_case": "ë°ì´í„° ë¶„ì„, ë³µì¡í•œ ì¶”ë¡ "
    },
    "powerful": {
        "name": "qwen2.5:14b",
        "description": "ê³ ì„±ëŠ¥ ì‘ì—…ìš© (14GB RAM)", 
        "ram_requirement": "16GB",
        "use_case": "ê³ ê¸‰ ë¶„ì„, ë³µì¡í•œ ì½”ë”©"
    },
    "coding": {
        "name": "qwen2.5-coder:7b",
        "description": "ì½”ë”© ì „ë¬¸ ëª¨ë¸ (7GB RAM)",
        "ram_requirement": "9GB", 
        "use_case": "ì½”ë“œ ìƒì„±, ë””ë²„ê¹…, ë¦¬íŒ©í† ë§"
    }
}

def get_available_ollama_models() -> List[Dict[str, Any]]:
    """Ollama ì„œë²„ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤"""
    if not OLLAMA_CLIENT_AVAILABLE:
        return []
    
    try:
        models = ollama.list()
        available_models = []
        
        for model in models.get("models", []):
            model_name = model.get("name", "")
            model_info = {
                "name": model_name,
                "size": model.get("size", 0),
                "modified_at": model.get("modified_at", ""),
                "tool_calling_capable": is_ollama_model_tool_capable(model_name),
                "family": model.get("details", {}).get("family", "unknown")
            }
            available_models.append(model_info)
        
        return available_models
    except Exception as e:
        logging.error(f"Failed to get Ollama models: {e}")
        return []

def test_ollama_connection() -> Dict[str, Any]:
    """Ollama ì„œë²„ ì—°ê²°ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤"""
    if not OLLAMA_CLIENT_AVAILABLE:
        return {
            "connected": False,
            "error": "Ollama client not available. Install: uv add ollama"
        }
    
    try:
        # ê°„ë‹¨í•œ ping í…ŒìŠ¤íŠ¸
        models = ollama.list()
        return {
            "connected": True,
            "model_count": len(models.get("models", [])),
            "server_version": getattr(ollama, "__version__", "unknown")
        }
    except Exception as e:
        return {
            "connected": False,
            "error": f"Connection failed: {str(e)}"
        }

def is_ollama_model_tool_capable(model: str) -> bool:
    """Ollama ëª¨ë¸ì˜ ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ í™•ì¸ - ì—„ê²©í•œ ê²€ì¦"""
    if not model or not OLLAMA_TOOL_CALLING_SUPPORTED:
        return False
    
    model_lower = model.lower().strip()
    
    # ğŸš¨ ëª…ì‹œì ìœ¼ë¡œ ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ë“¤ í™•ì¸
    for non_capable_model in OLLAMA_NON_TOOL_CALLING_MODELS:
        if model_lower == non_capable_model.lower() or model_lower.startswith(non_capable_model.split(':')[0].lower()):
            return False
    
    # âœ… ëª…ì‹œì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” ëª¨ë¸ë“¤ í™•ì¸
    for capable_model in OLLAMA_TOOL_CALLING_MODELS:
        if model_lower == capable_model.lower() or model_lower.startswith(capable_model.split(':')[0].lower()):
            return True
    
    # ğŸ¤” ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸ì€ False ë°˜í™˜ (ë³´ìˆ˜ì  ì ‘ê·¼)
    logging.warning(f"âš ï¸ Unknown model '{model}' - assuming no tool calling support")
    return False

def get_model_recommendation(ram_gb: Optional[int] = None) -> Dict[str, Any]:
    """ì‚¬ìš©ì ì‹œìŠ¤í…œì— ë§ëŠ” ëª¨ë¸ ì¶”ì²œ"""
    if ram_gb is None:
        import psutil
        ram_gb = psutil.virtual_memory().total // (1024**3)
    
    if ram_gb >= 16:
        return RECOMMENDED_OLLAMA_MODELS["powerful"]
    elif ram_gb >= 10:
        return RECOMMENDED_OLLAMA_MODELS["balanced"] 
    elif ram_gb >= 6:
        return RECOMMENDED_OLLAMA_MODELS["light"]
    else:
        return {
            "name": "qwen2.5:0.5b",
            "description": "ì´ˆê²½ëŸ‰ ëª¨ë¸ (0.5GB RAM)",
            "ram_requirement": "2GB",
            "use_case": "ê¸°ë³¸ì ì¸ í…ìŠ¤íŠ¸ ì²˜ë¦¬ë§Œ ê°€ëŠ¥",
            "warning": "ì„±ëŠ¥ì´ ë§¤ìš° ì œí•œì ì…ë‹ˆë‹¤"
        }

def create_llm_instance(
    provider: Optional[str] = None,
    model: Optional[str] = None,
    temperature: float = 0.7,
    streaming: bool = True,
    session_id: Optional[str] = None,
    user_id: Optional[str] = None,
    **kwargs
) -> Any:
    """
    í†µí•© LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„± íŒ©í† ë¦¬
    
    Args:
        provider: LLM ì œê³µì (OPENAI, OLLAMA)
        model: ëª¨ë¸ ì´ë¦„
        temperature: ì˜¨ë„ ì„¤ì •
        streaming: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
        session_id: ì„¸ì…˜ ID (Streamlit session_stateì—ì„œ ì „ë‹¬)
        user_id: ì‚¬ìš©ì ID
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„°
    
    Returns:
        LLM ì¸ìŠ¤í„´ìŠ¤ (ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ ë©”íƒ€ë°ì´í„° í¬í•¨)
    """
    # Langfuse ì½œë°± í•¸ë“¤ëŸ¬ ì´ˆê¸°í™” - multi_agent_supervisor.py íŒ¨í„´ ì‚¬ìš©
    if LANGFUSE_AVAILABLE:
        langfuse_config = get_config('langfuse')
        if langfuse_config.get('host') and langfuse_config.get('public_key') and langfuse_config.get('secret_key'):
            try:
                # ì„¸ì…˜ IDë¥¼ íŒŒë¼ë¯¸í„°ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ í™˜ê²½ë³€ìˆ˜/ê¸°ë³¸ê°’ ì‚¬ìš©
                effective_session_id = session_id or os.getenv("THREAD_ID", "default-session")
                effective_user_id = user_id or os.getenv("EMP_NO", "default_user")
                
                # multi_agent_supervisor.pyì™€ ë™ì¼í•œ íŒ¨í„´ìœ¼ë¡œ CallbackHandler ì§ì ‘ ì´ˆê¸°í™”
                handler = CallbackHandler(
                    session_id=effective_session_id,
                    user_id=effective_user_id,
                    metadata={
                        "app_type": "llm_factory",
                        "model": model or "unknown",
                        "provider": provider or "unknown",
                        "temperature": temperature,
                        "session_id": effective_session_id
                    }
                )
                
                # kwargsì— ì½œë°± ì¶”ê°€
                if 'callbacks' not in kwargs:
                    kwargs['callbacks'] = []
                kwargs['callbacks'].append(handler)
                logging.info(f"Langfuse callback handler initialized with session_id: {effective_session_id}")
            except Exception as e:
                logging.error(f"Failed to initialize Langfuse: {e}")

    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê¸°ë³¸ê°’ ì½ê¸°
    if provider is None:
        provider = os.getenv("LLM_PROVIDER", "OPENAI")
    
    provider = provider.upper()
    
    try:
        if provider == "OPENAI":
            # OpenAI ì„¤ì •
            api_key = os.getenv("OPENAI_API_KEY", "")
            api_base = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
            
            if not api_key:
                raise ValueError("OPENAI_API_KEY not found in environment variables")
            
            if model is None:
                model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            
            # ChatOpenAI ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            llm = ChatOpenAI(
                model=model,
                temperature=temperature,
                streaming=streaming,
                api_key=api_key,
                base_url=api_base,
                **kwargs
            )
            
            # ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            llm._tool_calling_capable = True
            llm._provider = "OPENAI"
            llm._model_name = model
            
            logging.info(f"Created OpenAI LLM: model={model}, temperature={temperature}")
            
        elif provider == "OLLAMA":
            # ğŸ†• íŒ¨í‚¤ì§€ ì§€ì› ì—¬ë¶€ ë¨¼ì € í™•ì¸
            if not OLLAMA_TOOL_CALLING_SUPPORTED:
                logging.error("âŒ Current langchain_community.ChatOllama does not support tool calling")
                logging.error("ğŸ’¡ Install langchain-ollama: pip install langchain-ollama")
                raise ValueError("Tool calling requires langchain-ollama package")
            
            # Ollama ì„¤ì • - OLLAMA_BASE_URLê³¼ OLLAMA_API_BASE ëª¨ë‘ ì§€ì›
            base_url = os.getenv("OLLAMA_BASE_URL") or os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
            
            if model is None:
                # ğŸ†• ê¸°ë³¸ ëª¨ë¸ì„ ë„êµ¬ í˜¸ì¶œ ì§€ì› ëª¨ë¸ë¡œ ë³€ê²½
                model = os.getenv("OLLAMA_MODEL", "llama3.1:8b")
            
            # OllamaëŠ” ë¡œì»¬ LLMì´ë¯€ë¡œ ê¸´ íƒ€ì„ì•„ì›ƒ ì„¤ì • (10ë¶„)
            ollama_timeout = int(os.getenv("OLLAMA_TIMEOUT", "600"))  # 10ë¶„ ê¸°ë³¸ê°’
            
            # ğŸ†• Ollama ëª¨ë¸ì˜ ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ í™•ì¸
            tool_calling_capable = is_ollama_model_tool_capable(model)
            
            # ğŸš¨ ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•œë° ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì¸ ê²½ìš° ê²½ê³  ë° ì¶”ì²œ
            if not tool_calling_capable:
                logging.error(f"ğŸš¨ Model '{model}' does NOT support tool calling!")
                recommended = get_model_recommendation()
                logging.error(f"ğŸ’¡ Recommended model: {recommended['name']} ({recommended['description']})")
                logging.error("ğŸ’¡ Available tool-capable models: llama3.1:8b, qwen2.5:7b, mistral:7b, qwen2.5-coder:7b")
                logging.error("ğŸ’¡ Set OLLAMA_MODEL environment variable to a supported model")
                
                # ğŸ”„ ìë™ìœ¼ë¡œ ê¶Œì¥ ëª¨ë¸ë¡œ ëŒ€ì²´ (ì„ íƒì‚¬í•­)
                auto_switch = os.getenv("OLLAMA_AUTO_SWITCH_MODEL", "false").lower() == "true"
                if auto_switch:
                    old_model = model
                    model = recommended['name']
                    logging.warning(f"ğŸ”„ Auto-switching from '{old_model}' to '{model}' for tool calling support")
            
            # Ollama ëª¨ë¸ë³„ íŠ¹í™” ì„¤ì •
            ollama_kwargs = kwargs.copy()
            
            # ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ì´ ì œí•œì ì¸ ëª¨ë¸ì˜ ê²½ìš° íŠ¹ë³„ ì„¤ì •
            if not tool_calling_capable:
                logging.warning(f"ğŸ”§ Limited tool calling model - applying enhanced settings")
                # ë‚®ì€ ì˜¨ë„ë¡œ ì„¤ì •í•˜ì—¬ ë” ì¼ê´€ëœ ì¶œë ¥ ìƒì„±
                temperature = min(temperature, 0.2)
                
            # ğŸ†• ChatOllama ì¸ìŠ¤í„´ìŠ¤ ìƒì„± - format=json ì¶”ê°€ë¡œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ê°•ì œ
            llm = ChatOllama(
                model=model,
                temperature=temperature,
                base_url=base_url,
                streaming=streaming,
                request_timeout=ollama_timeout,  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì„¤ì •
                format="json" if not tool_calling_capable else None,  # ì œí•œì  ëª¨ë¸ì€ JSON ê°•ì œ
                **ollama_kwargs
            )
            
            # ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            llm._tool_calling_capable = tool_calling_capable
            llm._provider = "OLLAMA"
            llm._model_name = model
            llm._needs_enhanced_prompting = not tool_calling_capable
            llm._import_source = OLLAMA_IMPORT_SOURCE
            
            if tool_calling_capable:
                logging.info(f"âœ… Created Ollama LLM with tool calling: model={model}, source={OLLAMA_IMPORT_SOURCE}")
            else:
                logging.warning(f"âš ï¸ Created Ollama LLM with LIMITED tool calling: model={model}")
                logging.warning("ğŸ’¡ Consider upgrading to a tool-calling capable model")
            
        else:
            raise ValueError(f"Unsupported LLM provider: {provider}")
        
        return llm
        
    except Exception as e:
        logging.error(f"Failed to create LLM instance: {e}")
        raise

def get_llm_capabilities(llm) -> Dict[str, Any]:
    """LLMì˜ ëŠ¥ë ¥ ì •ë³´ ë°˜í™˜"""
    return {
        "tool_calling_capable": getattr(llm, '_tool_calling_capable', True),
        "provider": getattr(llm, '_provider', 'UNKNOWN'),
        "model_name": getattr(llm, '_model_name', 'unknown'),
        "needs_enhanced_prompting": getattr(llm, '_needs_enhanced_prompting', False)
    }

def validate_llm_config() -> Dict[str, Any]:
    """
    LLM ì„¤ì • ê²€ì¦ ë° ì •ë³´ ë°˜í™˜
    
    Returns:
        ì„¤ì • ì •ë³´ ë”•ì…”ë„ˆë¦¬
    """
    provider = os.getenv("LLM_PROVIDER", "OPENAI")
    
    config = {
        "provider": provider,
        "valid": False,
        "error": None
    }
    
    try:
        if provider == "OPENAI":
            api_key = os.getenv("OPENAI_API_KEY", "")
            if not api_key:
                config["error"] = "OPENAI_API_KEY not set"
            else:
                config["valid"] = True
                config["model"] = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
                config["api_base"] = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
                config["tool_calling_capable"] = True
                
        elif provider == "OLLAMA":
            model = os.getenv("OLLAMA_MODEL", "llama3.1:8b")  # ğŸ†• ê¸°ë³¸ê°’ ë³€ê²½
            config["valid"] = OLLAMA_TOOL_CALLING_SUPPORTED
            config["model"] = model
            config["base_url"] = os.getenv("OLLAMA_BASE_URL") or os.getenv("OLLAMA_API_BASE", "http://localhost:11434")
            config["tool_calling_capable"] = is_ollama_model_tool_capable(model) and OLLAMA_TOOL_CALLING_SUPPORTED
            config["import_source"] = OLLAMA_IMPORT_SOURCE
            
            if not OLLAMA_TOOL_CALLING_SUPPORTED:
                config["error"] = "langchain-ollama package required for tool calling"
                config["warning"] = "Install: pip install langchain-ollama"
            elif not config["tool_calling_capable"]:
                recommended = get_model_recommendation()
                config["warning"] = f"Model '{model}' has limited tool calling. Recommended: {recommended['name']}"
                config["alternatives"] = ["llama3.1:8b", "qwen2.5:7b", "mistral:7b", "qwen2.5-coder:7b"]
            
            # ğŸ†• Ollama ì—°ê²° ìƒíƒœ ì¶”ê°€ í™•ì¸
            connection_status = test_ollama_connection()
            config["connection"] = connection_status
            config["available_models"] = get_available_ollama_models()
            
        else:
            config["error"] = f"Unknown provider: {provider}"
            
    except Exception as e:
        config["error"] = str(e)
    
    return config

# ğŸ†• ì¶”ê°€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
def get_ollama_status() -> Dict[str, Any]:
    """Ollama ì„œë²„ì˜ ì „ì²´ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
    status = {
        "client_available": OLLAMA_CLIENT_AVAILABLE,
        "langchain_package": OLLAMA_IMPORT_SOURCE,
        "tool_calling_supported": OLLAMA_TOOL_CALLING_SUPPORTED,
        "connection": test_ollama_connection(),
        "available_models": get_available_ollama_models(),
        "recommended_models": RECOMMENDED_OLLAMA_MODELS,
        "current_model": os.getenv("OLLAMA_MODEL", "llama3.1:8b")
    }
    
    # í˜„ì¬ ëª¨ë¸ì˜ ë„êµ¬ í˜¸ì¶œ ëŠ¥ë ¥ í™•ì¸
    current_model = status["current_model"]
    status["current_model_tool_capable"] = is_ollama_model_tool_capable(current_model)
    
    return status

def suggest_ollama_setup() -> Dict[str, Any]:
    """ì‚¬ìš©ìë¥¼ ìœ„í•œ Ollama ì„¤ì • ì œì•ˆ"""
    status = get_ollama_status()
    suggestions = {
        "steps": [],
        "commands": [],
        "warnings": [],
        "next_actions": []
    }
    
    # 1. í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ í™•ì¸
    if not status["client_available"]:
        suggestions["steps"].append("1. Ollama Python í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜")
        suggestions["commands"].append("uv add ollama")
    
    # 2. ì„œë²„ ì—°ê²° í™•ì¸  
    if not status["connection"]["connected"]:
        suggestions["steps"].append("2. Ollama ì„œë²„ ì‹œì‘")
        suggestions["commands"].append("ollama serve")
        suggestions["warnings"].append("Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”")
    
    # 3. ëª¨ë¸ ì„¤ì¹˜ í™•ì¸
    available_models = status["available_models"]
    if not available_models:
        suggestions["steps"].append("3. ì¶”ì²œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ")
        recommended = get_model_recommendation()
        suggestions["commands"].append(f"ollama pull {recommended['name']}")
        suggestions["next_actions"].append(f"í™˜ê²½ë³€ìˆ˜ ì„¤ì •: OLLAMA_MODEL={recommended['name']}")
    
    # 4. ë„êµ¬ í˜¸ì¶œ ì§€ì› í™•ì¸
    if not status["current_model_tool_capable"]:
        suggestions["warnings"].append(f"í˜„ì¬ ëª¨ë¸ '{status['current_model']}'ì€ ë„êµ¬ í˜¸ì¶œì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        recommended = get_model_recommendation()
        suggestions["next_actions"].append(f"ê¶Œì¥ ëª¨ë¸ë¡œ ë³€ê²½: {recommended['name']}")
        suggestions["commands"].append(f"ollama pull {recommended['name']}")
    
    return suggestions