"""
í™•ì¥ì„± ë° ì„±ëŠ¥ ìµœì í™” ì‹œìŠ¤í…œ
Phase 4.2: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ë° ê³ ì„±ëŠ¥ ë¶„ì„

í•µì‹¬ ê¸°ëŠ¥:
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²­í‚¹ ë° ë³‘ë ¬ ì²˜ë¦¬
- ì§€ëŠ¥í˜• ìºì‹± ì‹œìŠ¤í…œ
- ë¡œë“œ ë°¸ëŸ°ì‹± ë° ìì› ë¶„ë°°
- ë©”ëª¨ë¦¬ ìµœì í™” ë° ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
- ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í ê´€ë¦¬
- ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ìë™ ìŠ¤ì¼€ì¼ë§
"""

import asyncio
import multiprocessing
import threading
import time
import psutil
import logging
import pickle
import gc
import hashlib
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from dataclasses import dataclass, field
from typing import Dict, List, Any, Optional, Callable, Union, Tuple, Generator
from datetime import datetime, timedelta
from collections import defaultdict, deque
from enum import Enum
import sqlite3
import pandas as pd
import numpy as np
from pathlib import Path
import redis
import dask.dataframe as dd
import dask.array as da
from dask.distributed import Client, as_completed
import queue
import json

logger = logging.getLogger(__name__)

class ProcessingMode(Enum):
    """ì²˜ë¦¬ ëª¨ë“œ"""
    SEQUENTIAL = "sequential"         # ìˆœì°¨ ì²˜ë¦¬
    PARALLEL_THREAD = "parallel_thread"  # ìŠ¤ë ˆë“œ ë³‘ë ¬
    PARALLEL_PROCESS = "parallel_process"  # í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬
    DISTRIBUTED = "distributed"       # ë¶„ì‚° ì²˜ë¦¬
    ADAPTIVE = "adaptive"             # ì ì‘í˜• ì„ íƒ

class CacheStrategy(Enum):
    """ìºì‹œ ì „ëµ"""
    LRU = "lru"                      # Least Recently Used
    LFU = "lfu"                      # Least Frequently Used
    TTL = "ttl"                      # Time To Live
    ADAPTIVE = "adaptive"             # ì ì‘í˜•

class TaskPriority(Enum):
    """ì‘ì—… ìš°ì„ ìˆœìœ„"""
    CRITICAL = 1
    HIGH = 2
    NORMAL = 3
    LOW = 4
    BACKGROUND = 5

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    cpu_usage_percent: float
    memory_usage_mb: float
    memory_usage_percent: float
    disk_io_read_mb: float
    disk_io_write_mb: float
    network_io_mb: float
    active_connections: int
    queue_size: int
    throughput_ops_per_sec: float
    avg_response_time_ms: float
    error_rate_percent: float
    timestamp: datetime = field(default_factory=datetime.now)

@dataclass
class DataChunk:
    """ë°ì´í„° ì²­í¬"""
    chunk_id: str
    data: Any
    size_bytes: int
    metadata: Dict[str, Any] = field(default_factory=dict)
    processing_time: Optional[float] = None

@dataclass
class Task:
    """ì‘ì—… ì •ì˜"""
    task_id: str
    function: Callable
    args: tuple
    kwargs: dict
    priority: TaskPriority
    created_at: datetime
    timeout_seconds: Optional[float] = None
    retry_count: int = 0
    max_retries: int = 3
    callback: Optional[Callable] = None

class MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""
    
    def __init__(self, max_memory_usage_percent: float = 80.0):
        self.max_memory_usage_percent = max_memory_usage_percent
        self.memory_threshold_mb = psutil.virtual_memory().total * max_memory_usage_percent / 100 / (1024**2)
        self.gc_threshold = 0.9  # GC íŠ¸ë¦¬ê±° ì„ê³„ê°’
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        self.memory_usage_history: deque = deque(maxlen=100)
        self.large_objects: Dict[str, Any] = {}
        
    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        memory = psutil.virtual_memory()
        process = psutil.Process()
        
        return {
            "system_total_mb": memory.total / (1024**2),
            "system_available_mb": memory.available / (1024**2),
            "system_usage_percent": memory.percent,
            "process_usage_mb": process.memory_info().rss / (1024**2),
            "process_usage_percent": process.memory_percent()
        }
    
    def check_memory_pressure(self) -> bool:
        """ë©”ëª¨ë¦¬ ì••ë°• ìƒíƒœ í™•ì¸"""
        memory_info = self.get_memory_usage()
        return memory_info["system_usage_percent"] > self.max_memory_usage_percent
    
    def optimize_memory(self):
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        if self.check_memory_pressure():
            logger.info("ğŸ§¹ ë©”ëª¨ë¦¬ ì••ë°• ê°ì§€ - ìµœì í™” ì‹œì‘")
            
            # 1. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
            collected = gc.collect()
            logger.info(f"   ê°€ë¹„ì§€ ì»¬ë ‰ì…˜: {collected}ê°œ ê°ì²´ ì •ë¦¬")
            
            # 2. ëŒ€ìš©ëŸ‰ ìºì‹œ ê°ì²´ ì •ë¦¬
            self._cleanup_large_objects()
            
            # 3. íŒë‹¤ìŠ¤ ë©”ëª¨ë¦¬ ìµœì í™”
            self._optimize_pandas_memory()
    
    def _cleanup_large_objects(self):
        """ëŒ€ìš©ëŸ‰ ê°ì²´ ì •ë¦¬"""
        removed_count = 0
        for obj_id, obj in list(self.large_objects.items()):
            if self._should_remove_object(obj):
                del self.large_objects[obj_id]
                removed_count += 1
        
        if removed_count > 0:
            logger.info(f"   ëŒ€ìš©ëŸ‰ ê°ì²´ ì •ë¦¬: {removed_count}ê°œ")
    
    def _should_remove_object(self, obj: Any) -> bool:
        """ê°ì²´ ì œê±° ì—¬ë¶€ ê²°ì •"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ì‚¬ìš© ë¹ˆë„ê°€ ë‚®ì€ ê°ì²´ ì œê±°
        return hasattr(obj, '_last_accessed') and \
               (datetime.now() - obj._last_accessed).seconds > 300  # 5ë¶„ ì´ìƒ ë¯¸ì‚¬ìš©
    
    def _optimize_pandas_memory(self):
        """íŒë‹¤ìŠ¤ ë©”ëª¨ë¦¬ ìµœì í™”"""
        # ì „ì—­ íŒë‹¤ìŠ¤ ì„¤ì • ìµœì í™”
        pd.set_option('mode.chained_assignment', None)
        
        # ì»¬ëŸ¼ íƒ€ì… ìµœì í™” í•¨ìˆ˜ ì œê³µ
        def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
            for col in df.columns:
                if df[col].dtype == 'object':
                    try:
                        df[col] = pd.to_numeric(df[col], downcast='integer')
                    except:
                        try:
                            df[col] = df[col].astype('category')
                        except:
                            pass
                elif df[col].dtype == 'float64':
                    df[col] = pd.to_numeric(df[col], downcast='float')
                elif df[col].dtype == 'int64':
                    df[col] = pd.to_numeric(df[col], downcast='integer')
            return df
        
        return optimize_dataframe

class CacheManager:
    """ìºì‹± ê´€ë¦¬ì"""
    
    def __init__(self, max_size_mb: int = 1000, strategy: CacheStrategy = CacheStrategy.ADAPTIVE):
        self.max_size_mb = max_size_mb
        self.strategy = strategy
        self.cache: Dict[str, Any] = {}
        self.access_times: Dict[str, datetime] = {}
        self.access_counts: Dict[str, int] = defaultdict(int)
        self.cache_sizes: Dict[str, int] = {}
        self.total_size_mb = 0
        
        # Redis ì—°ê²° ì‹œë„ (ì„ íƒì )
        self.redis_client = None
        try:
            self.redis_client = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
            self.redis_client.ping()
            logger.info("âœ… Redis ìºì‹œ ì„œë²„ ì—°ê²° ì„±ê³µ")
        except:
            logger.info("â„¹ï¸ Redis ë¯¸ì‚¬ìš© - ë¡œì»¬ ìºì‹œ ì‚¬ìš©")
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        # Redis ìš°ì„  ì‹œë„
        if self.redis_client:
            try:
                data = self.redis_client.get(key)
                if data:
                    self._update_access_stats(key)
                    return pickle.loads(data.encode('latin-1'))
            except Exception as e:
                logger.warning(f"Redis ì¡°íšŒ ì‹¤íŒ¨: {e}")
        
        # ë¡œì»¬ ìºì‹œ ì¡°íšŒ
        if key in self.cache:
            self._update_access_stats(key)
            return self.cache[key]
        
        return None
    
    def set(self, key: str, value: Any, ttl_seconds: Optional[int] = None):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        # ë°ì´í„° í¬ê¸° ê³„ì‚°
        try:
            serialized = pickle.dumps(value)
            size_mb = len(serialized) / (1024**2)
        except:
            size_mb = 1  # ê¸°ë³¸ê°’
        
        # ìºì‹œ ìš©ëŸ‰ í™•ì¸ ë° ì •ë¦¬
        if self.total_size_mb + size_mb > self.max_size_mb:
            self._evict_items(size_mb)
        
        # Redis ì €ì¥ ì‹œë„
        if self.redis_client:
            try:
                if ttl_seconds:
                    self.redis_client.setex(key, ttl_seconds, serialized.decode('latin-1'))
                else:
                    self.redis_client.set(key, serialized.decode('latin-1'))
                return
            except Exception as e:
                logger.warning(f"Redis ì €ì¥ ì‹¤íŒ¨: {e}")
        
        # ë¡œì»¬ ìºì‹œ ì €ì¥
        self.cache[key] = value
        self.cache_sizes[key] = size_mb
        self.total_size_mb += size_mb
        self._update_access_stats(key)
        
        # TTL ì„¤ì •
        if ttl_seconds:
            threading.Timer(ttl_seconds, self._expire_key, args=[key]).start()
    
    def _update_access_stats(self, key: str):
        """ì ‘ê·¼ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.access_times[key] = datetime.now()
        self.access_counts[key] += 1
    
    def _evict_items(self, required_mb: float):
        """ìºì‹œ ì•„ì´í…œ ì œê±°"""
        if self.strategy == CacheStrategy.LRU:
            self._evict_lru(required_mb)
        elif self.strategy == CacheStrategy.LFU:
            self._evict_lfu(required_mb)
        else:  # ADAPTIVE
            self._evict_adaptive(required_mb)
    
    def _evict_lru(self, required_mb: float):
        """LRU ì œê±°"""
        # ì ‘ê·¼ ì‹œê°„ ê¸°ì¤€ ì •ë ¬
        sorted_keys = sorted(self.access_times.items(), key=lambda x: x[1])
        
        freed_mb = 0
        for key, _ in sorted_keys:
            if key in self.cache:
                freed_mb += self.cache_sizes.get(key, 0)
                self._remove_key(key)
                
                if freed_mb >= required_mb:
                    break
    
    def _evict_lfu(self, required_mb: float):
        """LFU ì œê±°"""
        # ì ‘ê·¼ íšŸìˆ˜ ê¸°ì¤€ ì •ë ¬
        sorted_keys = sorted(self.access_counts.items(), key=lambda x: x[1])
        
        freed_mb = 0
        for key, _ in sorted_keys:
            if key in self.cache:
                freed_mb += self.cache_sizes.get(key, 0)
                self._remove_key(key)
                
                if freed_mb >= required_mb:
                    break
    
    def _evict_adaptive(self, required_mb: float):
        """ì ì‘í˜• ì œê±° (í¬ê¸° + ì ‘ê·¼ ë¹ˆë„ ê³ ë ¤)"""
        # í¬ê¸° ëŒ€ë¹„ ì ‘ê·¼ ë¹ˆë„ ì ìˆ˜ ê³„ì‚°
        scores = {}
        for key in self.cache:
            size = self.cache_sizes.get(key, 1)
            access_count = self.access_counts.get(key, 1)
            last_access = self.access_times.get(key, datetime.now())
            
            # ì ìˆ˜ = í¬ê¸° / (ì ‘ê·¼íšŸìˆ˜ * ìµœê·¼ì„±)
            recency = max(1, (datetime.now() - last_access).seconds / 3600)  # ì‹œê°„ ë‹¨ìœ„
            scores[key] = size / (access_count * recency)
        
        # ì ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì œê±° (ì œê±°í•˜ê¸° ì‰¬ìš´ ê²ƒë¶€í„°)
        sorted_keys = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        freed_mb = 0
        for key, _ in sorted_keys:
            freed_mb += self.cache_sizes.get(key, 0)
            self._remove_key(key)
            
            if freed_mb >= required_mb:
                break
    
    def _remove_key(self, key: str):
        """í‚¤ ì œê±°"""
        if key in self.cache:
            self.total_size_mb -= self.cache_sizes.get(key, 0)
            del self.cache[key]
            del self.cache_sizes[key]
            del self.access_times[key]
            del self.access_counts[key]
    
    def _expire_key(self, key: str):
        """í‚¤ ë§Œë£Œ"""
        self._remove_key(key)
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„"""
        return {
            "total_keys": len(self.cache),
            "total_size_mb": self.total_size_mb,
            "max_size_mb": self.max_size_mb,
            "usage_percent": (self.total_size_mb / self.max_size_mb) * 100,
            "strategy": self.strategy.value,
            "redis_available": self.redis_client is not None
        }

class DataChunker:
    """ë°ì´í„° ì²­í‚¹ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, chunk_size_mb: int = 100):
        self.chunk_size_mb = chunk_size_mb
        self.chunk_size_rows = 50000  # ê¸°ë³¸ í–‰ ìˆ˜
    
    def chunk_dataframe(self, df: pd.DataFrame, chunk_size: Optional[int] = None) -> Generator[DataChunk, None, None]:
        """ë°ì´í„°í”„ë ˆì„ ì²­í‚¹"""
        chunk_size = chunk_size or self.chunk_size_rows
        total_rows = len(df)
        
        for i in range(0, total_rows, chunk_size):
            end_idx = min(i + chunk_size, total_rows)
            chunk_df = df.iloc[i:end_idx].copy()
            
            chunk_id = f"chunk_{i}_{end_idx}"
            chunk_size_bytes = chunk_df.memory_usage(deep=True).sum()
            
            yield DataChunk(
                chunk_id=chunk_id,
                data=chunk_df,
                size_bytes=chunk_size_bytes,
                metadata={
                    "start_row": i,
                    "end_row": end_idx,
                    "total_rows": total_rows,
                    "chunk_rows": len(chunk_df)
                }
            )
    
    def chunk_large_file(self, file_path: str, chunk_size: Optional[int] = None) -> Generator[DataChunk, None, None]:
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²­í‚¹"""
        chunk_size = chunk_size or self.chunk_size_rows
        
        try:
            # CSV íŒŒì¼ì¸ ê²½ìš°
            if file_path.endswith('.csv'):
                for i, chunk_df in enumerate(pd.read_csv(file_path, chunksize=chunk_size)):
                    chunk_id = f"file_chunk_{i}"
                    chunk_size_bytes = chunk_df.memory_usage(deep=True).sum()
                    
                    yield DataChunk(
                        chunk_id=chunk_id,
                        data=chunk_df,
                        size_bytes=chunk_size_bytes,
                        metadata={
                            "file_path": file_path,
                            "chunk_index": i,
                            "chunk_rows": len(chunk_df)
                        }
                    )
            
            # Parquet íŒŒì¼ì¸ ê²½ìš°
            elif file_path.endswith('.parquet'):
                df = pd.read_parquet(file_path)
                yield from self.chunk_dataframe(df, chunk_size)
                
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²­í‚¹ ì˜¤ë¥˜: {e}")

class ParallelProcessor:
    """ë³‘ë ¬ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, max_workers: Optional[int] = None):
        self.max_workers = max_workers or multiprocessing.cpu_count()
        self.thread_executor = ThreadPoolExecutor(max_workers=self.max_workers)
        self.process_executor = ProcessPoolExecutor(max_workers=self.max_workers)
        
        # Dask í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹œë„
        self.dask_client = None
        try:
            self.dask_client = Client(processes=False, threads_per_worker=2, n_workers=2)
            logger.info("âœ… Dask ë¶„ì‚° ì²˜ë¦¬ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        except:
            logger.info("â„¹ï¸ Dask ë¯¸ì‚¬ìš© - ê¸°ë³¸ ë³‘ë ¬ ì²˜ë¦¬ ì‚¬ìš©")
    
    async def process_chunks_parallel(self, chunks: List[DataChunk], 
                                    process_func: Callable,
                                    mode: ProcessingMode = ProcessingMode.ADAPTIVE) -> List[Any]:
        """ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬"""
        
        # ì²˜ë¦¬ ëª¨ë“œ ìë™ ì„ íƒ
        if mode == ProcessingMode.ADAPTIVE:
            mode = self._select_optimal_mode(chunks, process_func)
        
        logger.info(f"ğŸ”„ {len(chunks)}ê°œ ì²­í¬ë¥¼ {mode.value} ëª¨ë“œë¡œ ì²˜ë¦¬ ì‹œì‘")
        
        if mode == ProcessingMode.DISTRIBUTED and self.dask_client:
            return await self._process_with_dask(chunks, process_func)
        elif mode == ProcessingMode.PARALLEL_PROCESS:
            return await self._process_with_processes(chunks, process_func)
        elif mode == ProcessingMode.PARALLEL_THREAD:
            return await self._process_with_threads(chunks, process_func)
        else:
            return await self._process_sequential(chunks, process_func)
    
    def _select_optimal_mode(self, chunks: List[DataChunk], process_func: Callable) -> ProcessingMode:
        """ìµœì  ì²˜ë¦¬ ëª¨ë“œ ì„ íƒ"""
        total_size_mb = sum(chunk.size_bytes for chunk in chunks) / (1024**2)
        chunk_count = len(chunks)
        
        # CPU ì§‘ì•½ì  ì‘ì—…ì´ê³  ëŒ€ìš©ëŸ‰ì¸ ê²½ìš° ë¶„ì‚° ì²˜ë¦¬
        if total_size_mb > 1000 and chunk_count > 10 and self.dask_client:
            return ProcessingMode.DISTRIBUTED
        
        # ì¤‘ê°„ ê·œëª¨ë©´ì„œ CPU ì§‘ì•½ì ì¸ ê²½ìš° í”„ë¡œì„¸ìŠ¤ ë³‘ë ¬
        elif total_size_mb > 100 and chunk_count > 4:
            return ProcessingMode.PARALLEL_PROCESS
        
        # ì†Œê·œëª¨ì´ê±°ë‚˜ I/O ì§‘ì•½ì ì¸ ê²½ìš° ìŠ¤ë ˆë“œ ë³‘ë ¬
        elif chunk_count > 2:
            return ProcessingMode.PARALLEL_THREAD
        
        # ì‘ì€ ì‘ì—…ì€ ìˆœì°¨ ì²˜ë¦¬
        else:
            return ProcessingMode.SEQUENTIAL
    
    async def _process_with_dask(self, chunks: List[DataChunk], process_func: Callable) -> List[Any]:
        """Daskë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° ì²˜ë¦¬"""
        if not self.dask_client:
            return await self._process_with_processes(chunks, process_func)
        
        # Dask delayed ì‘ì—…ìœ¼ë¡œ ë³€í™˜
        futures = []
        for chunk in chunks:
            future = self.dask_client.submit(process_func, chunk.data)
            futures.append(future)
        
        # ê²°ê³¼ ìˆ˜ì§‘
        results = []
        for future in as_completed(futures):
            try:
                result = await future
                results.append(result)
            except Exception as e:
                logger.error(f"Dask ì‘ì—… ì‹¤íŒ¨: {e}")
                results.append(None)
        
        return results
    
    async def _process_with_processes(self, chunks: List[DataChunk], process_func: Callable) -> List[Any]:
        """í”„ë¡œì„¸ìŠ¤ í’€ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬"""
        loop = asyncio.get_event_loop()
        
        tasks = []
        for chunk in chunks:
            task = loop.run_in_executor(self.process_executor, process_func, chunk.data)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"í”„ë¡œì„¸ìŠ¤ ì‘ì—… ì‹¤íŒ¨: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_with_threads(self, chunks: List[DataChunk], process_func: Callable) -> List[Any]:
        """ìŠ¤ë ˆë“œ í’€ì„ ì‚¬ìš©í•œ ë³‘ë ¬ ì²˜ë¦¬"""
        loop = asyncio.get_event_loop()
        
        tasks = []
        for chunk in chunks:
            task = loop.run_in_executor(self.thread_executor, process_func, chunk.data)
            tasks.append(task)
        
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"ìŠ¤ë ˆë“œ ì‘ì—… ì‹¤íŒ¨: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _process_sequential(self, chunks: List[DataChunk], process_func: Callable) -> List[Any]:
        """ìˆœì°¨ ì²˜ë¦¬"""
        results = []
        for chunk in chunks:
            try:
                result = process_func(chunk.data)
                results.append(result)
            except Exception as e:
                logger.error(f"ìˆœì°¨ ì‘ì—… ì‹¤íŒ¨: {e}")
                results.append(None)
        
        return results
    
    def shutdown(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        self.thread_executor.shutdown(wait=True)
        self.process_executor.shutdown(wait=True)
        
        if self.dask_client:
            self.dask_client.close()

class BackgroundTaskManager:
    """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ê´€ë¦¬ì"""
    
    def __init__(self, max_concurrent_tasks: int = 10):
        self.max_concurrent_tasks = max_concurrent_tasks
        self.task_queue: queue.PriorityQueue = queue.PriorityQueue()
        self.active_tasks: Dict[str, asyncio.Task] = {}
        self.completed_tasks: Dict[str, Any] = {}
        self.failed_tasks: Dict[str, str] = {}
        
        self.is_running = False
        self.worker_tasks: List[asyncio.Task] = []
    
    async def start(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘"""
        if self.is_running:
            return
        
        self.is_running = True
        
        # ì›Œì»¤ íƒœìŠ¤í¬ ì‹œì‘
        for i in range(self.max_concurrent_tasks):
            worker = asyncio.create_task(self._worker(f"worker_{i}"))
            self.worker_tasks.append(worker)
        
        logger.info(f"ğŸš€ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ê´€ë¦¬ì ì‹œì‘ ({self.max_concurrent_tasks}ê°œ ì›Œì»¤)")
    
    async def stop(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì¤‘ì§€"""
        self.is_running = False
        
        # ì›Œì»¤ íƒœìŠ¤í¬ ì •ë¦¬
        for worker in self.worker_tasks:
            worker.cancel()
        
        await asyncio.gather(*self.worker_tasks, return_exceptions=True)
        self.worker_tasks.clear()
        
        logger.info("â¹ï¸ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ê´€ë¦¬ì ì¤‘ì§€")
    
    def submit_task(self, task: Task) -> str:
        """ì‘ì—… ì œì¶œ"""
        priority_value = task.priority.value
        self.task_queue.put((priority_value, time.time(), task))
        
        logger.info(f"ğŸ“ ì‘ì—… ì œì¶œ: {task.task_id} (ìš°ì„ ìˆœìœ„: {task.priority.value})")
        return task.task_id
    
    async def _worker(self, worker_name: str):
        """ì›Œì»¤ íƒœìŠ¤í¬"""
        logger.info(f"ğŸ‘· ì›Œì»¤ ì‹œì‘: {worker_name}")
        
        while self.is_running:
            try:
                # íì—ì„œ ì‘ì—… ê°€ì ¸ì˜¤ê¸°
                try:
                    priority, timestamp, task = self.task_queue.get(timeout=1.0)
                except queue.Empty:
                    continue
                
                logger.info(f"ğŸ”„ {worker_name}ê°€ ì‘ì—… ì²˜ë¦¬ ì‹œì‘: {task.task_id}")
                
                # ì‘ì—… ì‹¤í–‰
                try:
                    # íƒ€ì„ì•„ì›ƒ ì„¤ì •
                    if task.timeout_seconds:
                        result = await asyncio.wait_for(
                            self._execute_task(task),
                            timeout=task.timeout_seconds
                        )
                    else:
                        result = await self._execute_task(task)
                    
                    self.completed_tasks[task.task_id] = result
                    
                    # ì½œë°± ì‹¤í–‰
                    if task.callback:
                        try:
                            await task.callback(result)
                        except Exception as e:
                            logger.error(f"ì½œë°± ì‹¤í–‰ ì˜¤ë¥˜: {e}")
                    
                    logger.info(f"âœ… {worker_name}ê°€ ì‘ì—… ì™„ë£Œ: {task.task_id}")
                    
                except asyncio.TimeoutError:
                    error_msg = f"ì‘ì—… íƒ€ì„ì•„ì›ƒ: {task.timeout_seconds}ì´ˆ"
                    logger.error(f"â° {worker_name} - {task.task_id}: {error_msg}")
                    self.failed_tasks[task.task_id] = error_msg
                    
                    # ì¬ì‹œë„
                    if task.retry_count < task.max_retries:
                        task.retry_count += 1
                        self.task_queue.put((priority, time.time(), task))
                        logger.info(f"ğŸ”„ ì‘ì—… ì¬ì‹œë„: {task.task_id} ({task.retry_count}/{task.max_retries})")
                
                except Exception as e:
                    error_msg = str(e)
                    logger.error(f"âŒ {worker_name} - {task.task_id}: {error_msg}")
                    self.failed_tasks[task.task_id] = error_msg
                    
                    # ì¬ì‹œë„
                    if task.retry_count < task.max_retries:
                        task.retry_count += 1
                        self.task_queue.put((priority, time.time(), task))
                        logger.info(f"ğŸ”„ ì‘ì—… ì¬ì‹œë„: {task.task_id} ({task.retry_count}/{task.max_retries})")
                
                finally:
                    # í™œì„± ì‘ì—…ì—ì„œ ì œê±°
                    if task.task_id in self.active_tasks:
                        del self.active_tasks[task.task_id]
                
            except Exception as e:
                logger.error(f"ì›Œì»¤ ì˜¤ë¥˜ {worker_name}: {e}")
                await asyncio.sleep(1)
    
    async def _execute_task(self, task: Task) -> Any:
        """ì‘ì—… ì‹¤í–‰"""
        self.active_tasks[task.task_id] = asyncio.current_task()
        
        if asyncio.iscoroutinefunction(task.function):
            return await task.function(*task.args, **task.kwargs)
        else:
            return task.function(*task.args, **task.kwargs)
    
    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """ì‘ì—… ìƒíƒœ ì¡°íšŒ"""
        if task_id in self.active_tasks:
            return {"status": "running", "task_id": task_id}
        elif task_id in self.completed_tasks:
            return {"status": "completed", "task_id": task_id, "result": self.completed_tasks[task_id]}
        elif task_id in self.failed_tasks:
            return {"status": "failed", "task_id": task_id, "error": self.failed_tasks[task_id]}
        else:
            return {"status": "not_found", "task_id": task_id}
    
    def get_queue_status(self) -> Dict[str, Any]:
        """í ìƒíƒœ ì¡°íšŒ"""
        return {
            "queue_size": self.task_queue.qsize(),
            "active_tasks": len(self.active_tasks),
            "completed_tasks": len(self.completed_tasks),
            "failed_tasks": len(self.failed_tasks),
            "workers_running": len(self.worker_tasks),
            "is_running": self.is_running
        }

class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°"""
    
    def __init__(self, monitoring_interval: float = 30.0):
        self.monitoring_interval = monitoring_interval
        self.metrics_history: deque = deque(maxlen=1000)
        self.is_monitoring = False
        self.monitor_task: Optional[asyncio.Task] = None
        
        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            "cpu_usage_percent": 80.0,
            "memory_usage_percent": 85.0,
            "error_rate_percent": 5.0,
            "avg_response_time_ms": 5000.0
        }
        
        # ì•Œë¦¼ ì½œë°±
        self.alert_callbacks: List[Callable] = []
    
    async def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            return
        
        self.is_monitoring = True
        self.monitor_task = asyncio.create_task(self._monitoring_loop())
        logger.info("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
    
    async def stop_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        
        if self.monitor_task:
            self.monitor_task.cancel()
            try:
                await self.monitor_task
            except asyncio.CancelledError:
                pass
        
        logger.info("ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    async def _monitoring_loop(self):
        """ëª¨ë‹ˆí„°ë§ ë£¨í”„"""
        while self.is_monitoring:
            try:
                metrics = self._collect_metrics()
                self.metrics_history.append(metrics)
                
                # ì„ê³„ê°’ í™•ì¸ ë° ì•Œë¦¼
                await self._check_thresholds(metrics)
                
                await asyncio.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                await asyncio.sleep(10)
    
    def _collect_metrics(self) -> PerformanceMetrics:
        """ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk_io = psutil.disk_io_counters()
        network_io = psutil.net_io_counters()
        
        # í”„ë¡œì„¸ìŠ¤ ì •ë³´
        process = psutil.Process()
        connections = len(process.connections())
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ ë©”íŠ¸ë¦­ ìˆ˜ì§‘)
        throughput = 0.0  # ì´ˆë‹¹ ì²˜ë¦¬ëŸ‰
        response_time = 0.0  # í‰ê·  ì‘ë‹µì‹œê°„
        error_rate = 0.0  # ì˜¤ë¥˜ìœ¨
        
        return PerformanceMetrics(
            cpu_usage_percent=cpu_percent,
            memory_usage_mb=memory.used / (1024**2),
            memory_usage_percent=memory.percent,
            disk_io_read_mb=disk_io.read_bytes / (1024**2) if disk_io else 0,
            disk_io_write_mb=disk_io.write_bytes / (1024**2) if disk_io else 0,
            network_io_mb=(network_io.bytes_sent + network_io.bytes_recv) / (1024**2) if network_io else 0,
            active_connections=connections,
            queue_size=0,  # ì‹¤ì œ í í¬ê¸°
            throughput_ops_per_sec=throughput,
            avg_response_time_ms=response_time,
            error_rate_percent=error_rate
        )
    
    async def _check_thresholds(self, metrics: PerformanceMetrics):
        """ì„ê³„ê°’ í™•ì¸"""
        alerts = []
        
        if metrics.cpu_usage_percent > self.thresholds["cpu_usage_percent"]:
            alerts.append(f"CPU ì‚¬ìš©ë¥  ë†’ìŒ: {metrics.cpu_usage_percent:.1f}%")
        
        if metrics.memory_usage_percent > self.thresholds["memory_usage_percent"]:
            alerts.append(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  ë†’ìŒ: {metrics.memory_usage_percent:.1f}%")
        
        if metrics.error_rate_percent > self.thresholds["error_rate_percent"]:
            alerts.append(f"ì˜¤ë¥˜ìœ¨ ë†’ìŒ: {metrics.error_rate_percent:.1f}%")
        
        if metrics.avg_response_time_ms > self.thresholds["avg_response_time_ms"]:
            alerts.append(f"ì‘ë‹µì‹œê°„ ì§€ì—°: {metrics.avg_response_time_ms:.1f}ms")
        
        # ì•Œë¦¼ ë°œì†¡
        for alert in alerts:
            logger.warning(f"ğŸš¨ ì„±ëŠ¥ ê²½ê³ : {alert}")
            
            for callback in self.alert_callbacks:
                try:
                    await callback(alert, metrics)
                except Exception as e:
                    logger.error(f"ì•Œë¦¼ ì½œë°± ì˜¤ë¥˜: {e}")
    
    def add_alert_callback(self, callback: Callable):
        """ì•Œë¦¼ ì½œë°± ì¶”ê°€"""
        self.alert_callbacks.append(callback)
    
    def get_performance_summary(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ìš”ì•½"""
        if not self.metrics_history:
            return {"status": "no_data"}
        
        recent_metrics = list(self.metrics_history)[-10:]  # ìµœê·¼ 10ê°œ
        
        avg_cpu = statistics.mean([m.cpu_usage_percent for m in recent_metrics])
        avg_memory = statistics.mean([m.memory_usage_percent for m in recent_metrics])
        avg_response_time = statistics.mean([m.avg_response_time_ms for m in recent_metrics])
        
        return {
            "monitoring_active": self.is_monitoring,
            "metrics_collected": len(self.metrics_history),
            "avg_cpu_percent": avg_cpu,
            "avg_memory_percent": avg_memory,
            "avg_response_time_ms": avg_response_time,
            "last_update": self.metrics_history[-1].timestamp.isoformat(),
            "system_health": "healthy" if avg_cpu < 70 and avg_memory < 80 else "stressed"
        }

class ScalabilityOptimizer:
    """í™•ì¥ì„± ìµœì í™”ê¸° (í†µí•©)"""
    
    def __init__(self):
        self.memory_manager = MemoryManager()
        self.cache_manager = CacheManager()
        self.data_chunker = DataChunker()
        self.parallel_processor = ParallelProcessor()
        self.task_manager = BackgroundTaskManager()
        self.performance_monitor = PerformanceMonitor()
        
        # ìµœì í™” ì„¤ì •
        self.auto_scaling_enabled = True
        self.optimization_interval = 300  # 5ë¶„ë§ˆë‹¤ ìµœì í™”
        
        # ìµœì í™” íˆìŠ¤í† ë¦¬
        self.optimization_history: deque = deque(maxlen=100)
    
    async def initialize(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        logger.info("âš¡ í™•ì¥ì„± ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        
        # ë°±ê·¸ë¼ìš´ë“œ ì„œë¹„ìŠ¤ ì‹œì‘
        await self.task_manager.start()
        await self.performance_monitor.start_monitoring()
        
        # ì„±ëŠ¥ ì•Œë¦¼ ì½œë°± ë“±ë¡
        self.performance_monitor.add_alert_callback(self._handle_performance_alert)
        
        # ì£¼ê¸°ì  ìµœì í™” ì‹œì‘
        if self.auto_scaling_enabled:
            asyncio.create_task(self._auto_optimization_loop())
        
        logger.info("âœ… í™•ì¥ì„± ìµœì í™” ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_large_dataset(self, data: Union[pd.DataFrame, str], 
                                  process_func: Callable,
                                  chunk_size: Optional[int] = None) -> List[Any]:
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬"""
        logger.info(f"ğŸ”„ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì‹œì‘")
        
        start_time = time.time()
        
        # ë°ì´í„° íƒ€ì…ì— ë”°ë¥¸ ì²­í‚¹
        if isinstance(data, str):  # íŒŒì¼ ê²½ë¡œ
            chunks = list(self.data_chunker.chunk_large_file(data, chunk_size))
        elif isinstance(data, pd.DataFrame):
            chunks = list(self.data_chunker.chunk_dataframe(data, chunk_size))
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…")
        
        logger.info(f"ğŸ“¦ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")
        
        # ë³‘ë ¬ ì²˜ë¦¬
        results = await self.parallel_processor.process_chunks_parallel(chunks, process_func)
        
        # ë©”ëª¨ë¦¬ ìµœì í™”
        self.memory_manager.optimize_memory()
        
        processing_time = time.time() - start_time
        logger.info(f"âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì™„ë£Œ ({processing_time:.2f}ì´ˆ)")
        
        return results
    
    async def optimize_performance(self):
        """ì„±ëŠ¥ ìµœì í™” ì‹¤í–‰"""
        logger.info("ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì‹œì‘...")
        
        optimization_start = time.time()
        actions_taken = []
        
        # 1. ë©”ëª¨ë¦¬ ìµœì í™”
        if self.memory_manager.check_memory_pressure():
            self.memory_manager.optimize_memory()
            actions_taken.append("memory_optimization")
        
        # 2. ìºì‹œ ì •ë¦¬
        cache_stats = self.cache_manager.get_cache_stats()
        if cache_stats["usage_percent"] > 90:
            # ê°•ì œ ìºì‹œ ì •ë¦¬
            self.cache_manager._evict_items(cache_stats["total_size_mb"] * 0.3)
            actions_taken.append("cache_cleanup")
        
        # 3. ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        collected = gc.collect()
        if collected > 0:
            actions_taken.append(f"garbage_collection_{collected}")
        
        # 4. ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í ìƒíƒœ í™•ì¸
        queue_status = self.task_manager.get_queue_status()
        if queue_status["queue_size"] > 100:
            logger.warning("âš ï¸ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í ì ì²´")
            actions_taken.append("queue_backlog_detected")
        
        optimization_time = time.time() - optimization_start
        
        # ìµœì í™” ê¸°ë¡
        optimization_record = {
            "timestamp": datetime.now(),
            "actions_taken": actions_taken,
            "optimization_time": optimization_time,
            "memory_usage": self.memory_manager.get_memory_usage(),
            "cache_stats": cache_stats,
            "queue_status": queue_status
        }
        
        self.optimization_history.append(optimization_record)
        
        logger.info(f"ğŸ”§ ì„±ëŠ¥ ìµœì í™” ì™„ë£Œ ({optimization_time:.2f}ì´ˆ, {len(actions_taken)}ê°œ ì•¡ì…˜)")
        
        return optimization_record
    
    async def _auto_optimization_loop(self):
        """ìë™ ìµœì í™” ë£¨í”„"""
        while self.auto_scaling_enabled:
            try:
                await self.optimize_performance()
                await asyncio.sleep(self.optimization_interval)
            except Exception as e:
                logger.error(f"ìë™ ìµœì í™” ì˜¤ë¥˜: {e}")
                await asyncio.sleep(60)
    
    async def _handle_performance_alert(self, alert: str, metrics: PerformanceMetrics):
        """ì„±ëŠ¥ ì•Œë¦¼ ì²˜ë¦¬"""
        logger.warning(f"ğŸš¨ ì„±ëŠ¥ ê²½ê³  ìˆ˜ì‹ : {alert}")
        
        # ì¦‰ì‹œ ìµœì í™” ì‹¤í–‰
        await self.optimize_performance()
    
    async def shutdown(self):
        """ì‹œìŠ¤í…œ ì¢…ë£Œ"""
        logger.info("â¹ï¸ í™•ì¥ì„± ìµœì í™” ì‹œìŠ¤í…œ ì¢…ë£Œ ì¤‘...")
        
        self.auto_scaling_enabled = False
        
        await self.task_manager.stop()
        await self.performance_monitor.stop_monitoring()
        
        self.parallel_processor.shutdown()
        
        logger.info("âœ… í™•ì¥ì„± ìµœì í™” ì‹œìŠ¤í…œ ì¢…ë£Œ ì™„ë£Œ")
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
        return {
            "memory_status": self.memory_manager.get_memory_usage(),
            "cache_status": self.cache_manager.get_cache_stats(),
            "queue_status": self.task_manager.get_queue_status(),
            "performance_summary": self.performance_monitor.get_performance_summary(),
            "optimization_history_count": len(self.optimization_history),
            "auto_scaling_enabled": self.auto_scaling_enabled,
            "last_optimization": self.optimization_history[-1]["timestamp"].isoformat() if self.optimization_history else None
        }


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
async def test_scalability_optimizer():
    """í™•ì¥ì„± ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    optimizer = ScalabilityOptimizer()
    
    try:
        # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        await optimizer.initialize()
        
        print("âš¡ í™•ì¥ì„± ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        print("\\nğŸ“Š 1. í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±")
        import numpy as np
        
        # ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ë°ì´í„° (10ë§Œ í–‰)
        test_data = pd.DataFrame({
            'id': range(100000),
            'value1': np.random.normal(0, 1, 100000),
            'value2': np.random.exponential(1, 100000),
            'category': np.random.choice(['A', 'B', 'C'], 100000)
        })
        
        print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: {test_data.shape}")
        print(f"   ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {test_data.memory_usage(deep=True).sum() / (1024**2):.1f} MB")
        
        # 2. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
        print("\\nğŸ”„ 2. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
        
        def sample_process_func(chunk_df):
            \"\"\"ìƒ˜í”Œ ì²˜ë¦¬ í•¨ìˆ˜\"\"\"
            # ê°„ë‹¨í•œ í†µê³„ ê³„ì‚°
            return {
                'count': len(chunk_df),
                'mean_value1': chunk_df['value1'].mean(),
                'mean_value2': chunk_df['value2'].mean(),
                'category_counts': chunk_df['category'].value_counts().to_dict()
            }
        
        results = await optimizer.process_large_dataset(
            data=test_data,
            process_func=sample_process_func,
            chunk_size=10000
        )
        
        print(f"   ì²˜ë¦¬ ê²°ê³¼: {len(results)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
        print(f"   ì´ ë ˆì½”ë“œ ìˆ˜: {sum(r['count'] for r in results if r)}")
        
        # 3. ìºì‹œ í…ŒìŠ¤íŠ¸
        print("\\nğŸ’¾ 3. ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
        
        # ìºì‹œì— ë°ì´í„° ì €ì¥
        cache_key = "test_data_summary"
        cache_data = {
            'total_rows': len(test_data),
            'columns': list(test_data.columns),
            'summary': test_data.describe().to_dict()
        }
        
        optimizer.cache_manager.set(cache_key, cache_data)
        
        # ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ
        cached_data = optimizer.cache_manager.get(cache_key)
        cache_hit = cached_data is not None
        
        print(f"   ìºì‹œ ì €ì¥/ì¡°íšŒ: {'âœ… ì„±ê³µ' if cache_hit else 'âŒ ì‹¤íŒ¨'}")
        
        cache_stats = optimizer.cache_manager.get_cache_stats()
        print(f"   ìºì‹œ ì‚¬ìš©ëŸ‰: {cache_stats['usage_percent']:.1f}%")
        
        # 4. ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í…ŒìŠ¤íŠ¸
        print("\\nâš™ï¸ 4. ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… í…ŒìŠ¤íŠ¸")
        
        async def sample_bg_task(data):
            await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
            return f\"ì²˜ë¦¬ ì™„ë£Œ: {len(data)} ë ˆì½”ë“œ\"
        
        # ì‘ì—… ì œì¶œ
        task = Task(
            task_id=\"bg_test_001\",
            function=sample_bg_task,
            args=(test_data.head(1000),),
            kwargs={},
            priority=TaskPriority.NORMAL,
            created_at=datetime.now()
        )
        
        task_id = optimizer.task_manager.submit_task(task)
        print(f\"   ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì œì¶œ: {task_id}\")
        
        # ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
        await asyncio.sleep(2)
        task_status = optimizer.task_manager.get_task_status(task_id)
        print(f\"   ì‘ì—… ìƒíƒœ: {task_status['status']}\")
        
        # 5. ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸
        print(\"\\nğŸ”§ 5. ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸\")
        optimization_result = await optimizer.optimize_performance()
        
        print(f\"   ìµœì í™” ìˆ˜í–‰: {len(optimization_result['actions_taken'])}ê°œ ì•¡ì…˜\")
        print(f\"   ìµœì í™” ì‹œê°„: {optimization_result['optimization_time']:.3f}ì´ˆ\")
        
        # 6. ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ
        print(\"\\nğŸ“Š 6. ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ\")
        system_status = optimizer.get_system_status()
        
        print(f\"   ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : {system_status['memory_status']['system_usage_percent']:.1f}%\")
        print(f\"   ìºì‹œ ì‚¬ìš©ë¥ : {system_status['cache_status']['usage_percent']:.1f}%\")
        print(f\"   ë°±ê·¸ë¼ìš´ë“œ í: {system_status['queue_status']['queue_size']}ê°œ ëŒ€ê¸°\")
        print(f\"   ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§: {'í™œì„±' if system_status['performance_summary']['monitoring_active'] else 'ë¹„í™œì„±'}\")
        print(f\"   ìë™ ìŠ¤ì¼€ì¼ë§: {'í™œì„±' if system_status['auto_scaling_enabled'] else 'ë¹„í™œì„±'}\")
        
        print(\"\\nâœ… í™•ì¥ì„± ìµœì í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!\")
        print(\"   ğŸ”„ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬: ì •ìƒ\")
        print(\"   ğŸ’¾ ìºì‹± ì‹œìŠ¤í…œ: ì •ìƒ\")
        print(\"   âš™ï¸ ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…: ì •ìƒ\")
        print(\"   ğŸ”§ ì„±ëŠ¥ ìµœì í™”: ì •ìƒ\")
        print(\"   ğŸ“Š ëª¨ë‹ˆí„°ë§: ì •ìƒ\")
        
    finally:
        await optimizer.shutdown()

if __name__ == \"__main__\":
    asyncio.run(test_scalability_optimizer()) 