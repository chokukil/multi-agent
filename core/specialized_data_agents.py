"""
Specialized Data Analysis Agents

ë°ì´í„° íƒ€ì…ë³„ ì „ë¬¸í™” ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ
ì •í˜•/ì‹œê³„ì—´/í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ë°ì´í„°ì— íŠ¹í™”ëœ ë¶„ì„ ê¸°ëŠ¥ ì œê³µ

Author: CherryAI Team
Date: 2024-12-30
"""

import logging
import asyncio
import json
import numpy as np
import pandas as pd
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
import re
from abc import ABC, abstractmethod

# Enhanced Tracking System
try:
    from core.enhanced_langfuse_tracer import get_enhanced_tracer
    ENHANCED_TRACKING_AVAILABLE = True
except ImportError:
    ENHANCED_TRACKING_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DataType(Enum):
    """ë°ì´í„° íƒ€ì… ë¶„ë¥˜"""
    STRUCTURED = "structured"       # ì •í˜• ë°ì´í„° (CSV, í…Œì´ë¸”)
    TIME_SERIES = "time_series"     # ì‹œê³„ì—´ ë°ì´í„°
    TEXT = "text"                   # í…ìŠ¤íŠ¸ ë°ì´í„°
    IMAGE = "image"                 # ì´ë¯¸ì§€ ë°ì´í„°
    MIXED = "mixed"                 # í˜¼í•© ë°ì´í„°
    UNKNOWN = "unknown"             # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì…


@dataclass
class DataAnalysisResult:
    """ë°ì´í„° ë¶„ì„ ê²°ê³¼"""
    analysis_type: str
    data_type: DataType
    results: Dict[str, Any]
    insights: List[str]
    recommendations: List[str]
    confidence: float
    metadata: Dict[str, Any] = None
    visualizations: List[Dict] = None


@dataclass 
class DataTypeDetectionResult:
    """ë°ì´í„° íƒ€ì… íƒì§€ ê²°ê³¼"""
    detected_type: DataType
    confidence: float
    reasoning: str
    characteristics: Dict[str, Any]
    recommendations: List[str]


class BaseSpecializedAgent(ABC):
    """ì „ë¬¸í™” ì—ì´ì „íŠ¸ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.enhanced_tracer = None
        
        # Enhanced Tracking ì´ˆê¸°í™”
        if ENHANCED_TRACKING_AVAILABLE:
            try:
                self.enhanced_tracer = get_enhanced_tracer()
                logger.info(f"âœ… {self.__class__.__name__} Enhanced Tracking í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ {self.__class__.__name__} Enhanced Tracking ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    @abstractmethod
    async def analyze(self, data: Any, query: str, context: Optional[Dict] = None) -> DataAnalysisResult:
        """ë°ì´í„° ë¶„ì„ ìˆ˜í–‰"""
        pass
    
    @abstractmethod
    def detect_data_type(self, data: Any) -> DataTypeDetectionResult:
        """ë°ì´í„° íƒ€ì… íƒì§€"""
        pass
    
    @abstractmethod
    def get_capabilities(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ëŠ¥ë ¥ ë°˜í™˜"""
        pass


class StructuredDataAgent(BaseSpecializedAgent):
    """ì •í˜• ë°ì´í„° ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: Optional[Dict] = None):
        super().__init__(config)
        self.data_type = DataType.STRUCTURED
    
    def detect_data_type(self, data: Any) -> DataTypeDetectionResult:
        """ì •í˜• ë°ì´í„° íƒì§€"""
        confidence = 0.0
        characteristics = {}
        reasoning = ""
        
        if isinstance(data, pd.DataFrame):
            confidence = 0.9
            characteristics = {
                "rows": len(data),
                "columns": len(data.columns),
                "column_types": dict(data.dtypes.astype(str)),
                "memory_usage": data.memory_usage(deep=True).sum(),
                "missing_values": data.isnull().sum().sum()
            }
            reasoning = f"pandas DataFrame with {len(data)} rows and {len(data.columns)} columns"
            
        elif isinstance(data, (list, tuple)) and len(data) > 0:
            # ë¦¬ìŠ¤íŠ¸/íŠœí”Œ í˜•íƒœì˜ ì •í˜• ë°ì´í„° ì²´í¬
            first_item = data[0]
            if isinstance(first_item, (dict, list, tuple)):
                confidence = 0.7
                characteristics = {
                    "records": len(data),
                    "structure": type(first_item).__name__,
                    "sample_keys": list(first_item.keys()) if isinstance(first_item, dict) else "list_structure"
                }
                reasoning = f"Structured data as {type(first_item).__name__} with {len(data)} records"
        
        recommendations = []
        if confidence > 0.5:
            recommendations.extend([
                "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA) ìˆ˜í–‰",
                "ê¸°ìˆ í†µê³„ ë¶„ì„",
                "ìƒê´€ê´€ê³„ ë¶„ì„",
                "ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬"
            ])
        
        return DataTypeDetectionResult(
            detected_type=DataType.STRUCTURED if confidence > 0.5 else DataType.UNKNOWN,
            confidence=confidence,
            reasoning=reasoning,
            characteristics=characteristics,
            recommendations=recommendations
        )
    
    async def analyze(self, data: Any, query: str, context: Optional[Dict] = None) -> DataAnalysisResult:
        """ì •í˜• ë°ì´í„° ë¶„ì„"""
        try:
            logger.info(f"ğŸ”„ ì •í˜• ë°ì´í„° ë¶„ì„ ì‹œì‘: {query[:50]}...")
            
            # Enhanced Tracking
            if self.enhanced_tracer:
                self.enhanced_tracer.log_data_operation(
                    "structured_data_analysis",
                    {"query": query, "data_shape": getattr(data, 'shape', 'unknown')},
                    "Analyzing structured data"
                )
            
            # DataFrameìœ¼ë¡œ ë³€í™˜
            if not isinstance(data, pd.DataFrame):
                if isinstance(data, (list, dict)):
                    data = pd.DataFrame(data)
                else:
                    raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤")
            
            results = {}
            insights = []
            recommendations = []
            visualizations = []
            
            # ê¸°ë³¸ ë°ì´í„° ì •ë³´
            results["basic_info"] = {
                "shape": data.shape,
                "columns": list(data.columns),
                "dtypes": dict(data.dtypes.astype(str)),
                "missing_values": data.isnull().sum().to_dict(),
                "memory_usage": data.memory_usage(deep=True).sum()
            }
            
            # ê¸°ë³¸ insights í•­ìƒ ìƒì„±
            insights.append(f"ë°ì´í„°ì…‹ì— {data.shape[0]}ê°œ í–‰, {data.shape[1]}ê°œ ì—´ì´ ìˆìŠµë‹ˆë‹¤")
            
            # ì§ˆì˜ ë¶„ì„ ë° ì ì ˆí•œ ë¶„ì„ ìˆ˜í–‰
            query_lower = query.lower()
            
            if any(keyword in query_lower for keyword in ['ìš”ì•½', 'ê¸°ìˆ í†µê³„', 'summary', 'describe']):
                results["descriptive_stats"] = data.describe().to_dict()
                insights.append("ê¸°ìˆ í†µê³„ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤")
                
            if any(keyword in query_lower for keyword in ['ìƒê´€ê´€ê³„', 'correlation', 'ê´€ê³„']):
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 1:
                    corr_matrix = data[numeric_cols].corr()
                    results["correlation_matrix"] = corr_matrix.to_dict()
                    
                    # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸°
                    strong_corr = []
                    for i in range(len(corr_matrix.columns)):
                        for j in range(i+1, len(corr_matrix.columns)):
                            corr_val = corr_matrix.iloc[i, j]
                            if abs(corr_val) > 0.7:
                                strong_corr.append({
                                    "variables": [corr_matrix.columns[i], corr_matrix.columns[j]],
                                    "correlation": corr_val
                                })
                    
                    if strong_corr:
                        insights.append(f"ê°•í•œ ìƒê´€ê´€ê³„({len(strong_corr)}ê°œ)ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")
                        results["strong_correlations"] = strong_corr
                
            if any(keyword in query_lower for keyword in ['ë¶„í¬', 'distribution', 'íˆìŠ¤í† ê·¸ë¨']):
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    distribution_stats = {}
                    for col in numeric_cols:
                        dist_data = data[col].dropna()
                        distribution_stats[col] = {
                            "mean": float(dist_data.mean()),
                            "std": float(dist_data.std()),
                            "skewness": float(dist_data.skew()),
                            "kurtosis": float(dist_data.kurtosis()),
                            "quartiles": dist_data.quantile([0.25, 0.5, 0.75]).to_dict()
                        }
                    results["distribution_analysis"] = distribution_stats
                    insights.append(f"{len(numeric_cols)}ê°œ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ì˜ ë¶„í¬ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            if any(keyword in query_lower for keyword in ['ì´ìƒê°’', 'outlier', 'íŠ¹ì´ê°’']):
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                outlier_analysis = {}
                
                for col in numeric_cols:
                    Q1 = data[col].quantile(0.25)
                    Q3 = data[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)][col]
                    outlier_analysis[col] = {
                        "count": len(outliers),
                        "percentage": len(outliers) / len(data) * 100,
                        "bounds": {"lower": float(lower_bound), "upper": float(upper_bound)}
                    }
                
                results["outlier_analysis"] = outlier_analysis
                total_outliers = sum(stats["count"] for stats in outlier_analysis.values())
                insights.append(f"ì´ {total_outliers}ê°œì˜ ì´ìƒê°’ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤")
            
            # ê²°ì¸¡ê°’ ë¶„ì„
            missing_analysis = data.isnull().sum()
            if missing_analysis.sum() > 0:
                results["missing_value_analysis"] = missing_analysis.to_dict()
                missing_cols = missing_analysis[missing_analysis > 0]
                insights.append(f"{len(missing_cols)}ê°œ ì—´ì— ê²°ì¸¡ê°’ì´ ìˆìŠµë‹ˆë‹¤")
                recommendations.append("ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „ëµì„ ê³ ë ¤í•˜ì„¸ìš”")
            
            # ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = self._calculate_data_quality_score(data)
            results["data_quality_score"] = quality_score
            
            if quality_score < 0.7:
                recommendations.append("ë°ì´í„° í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
            
            # ì¼ë°˜ì ì¸ ì¶”ì²œì‚¬í•­
            if len(data) > 10000:
                recommendations.append("ëŒ€ìš©ëŸ‰ ë°ì´í„°ì´ë¯€ë¡œ ìƒ˜í”Œë§ì„ ê³ ë ¤í•˜ì„¸ìš”")
            
            if len(data.select_dtypes(include=['object']).columns) > 0:
                recommendations.append("ë²”ì£¼í˜• ë³€ìˆ˜ì— ëŒ€í•œ ì¸ì½”ë”©ì„ ê³ ë ¤í•˜ì„¸ìš”")
                
            return DataAnalysisResult(
                analysis_type="structured_data_analysis",
                data_type=DataType.STRUCTURED,
                results=results,
                insights=insights,
                recommendations=recommendations,
                confidence=0.9,
                metadata={"query": query, "data_shape": data.shape}
            )
            
        except Exception as e:
            logger.error(f"âŒ ì •í˜• ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return DataAnalysisResult(
                analysis_type="structured_data_analysis",
                data_type=DataType.STRUCTURED,
                results={"error": str(e)},
                insights=[f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                recommendations=["ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”"],
                confidence=0.1
            )
    
    def _calculate_data_quality_score(self, data: pd.DataFrame) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1)"""
        scores = []
        
        # ê²°ì¸¡ê°’ ì ìˆ˜ (ê²°ì¸¡ê°’ì´ ì ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
        missing_ratio = data.isnull().sum().sum() / (len(data) * len(data.columns))
        missing_score = max(0, 1 - missing_ratio * 2)  # 50% ì´ìƒ ê²°ì¸¡ì‹œ 0ì 
        scores.append(missing_score)
        
        # ì¤‘ë³µê°’ ì ìˆ˜
        duplicate_ratio = data.duplicated().sum() / len(data)
        duplicate_score = max(0, 1 - duplicate_ratio * 2)
        scores.append(duplicate_score)
        
        # ë°ì´í„° íƒ€ì… ì¼ê´€ì„± ì ìˆ˜
        type_consistency = 1.0  # ê¸°ë³¸ì ìœ¼ë¡œ pandasê°€ íƒ€ì… ì¶”ë¡ ì„ í•˜ë¯€ë¡œ ë†’ì€ ì ìˆ˜
        scores.append(type_consistency)
        
        return np.mean(scores)
    
    def get_capabilities(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ëŠ¥ë ¥ ë°˜í™˜"""
        return {
            "name": "Structured Data Agent",
            "data_type": "structured",
            "capabilities": [
                "ê¸°ìˆ í†µê³„ ë¶„ì„",
                "ìƒê´€ê´€ê³„ ë¶„ì„", 
                "ë¶„í¬ ë¶„ì„",
                "ì´ìƒê°’ íƒì§€",
                "ê²°ì¸¡ê°’ ë¶„ì„",
                "ë°ì´í„° í’ˆì§ˆ í‰ê°€"
            ],
            "supported_formats": ["DataFrame", "CSV", "Excel", "JSON"],
            "analysis_types": ["descriptive", "exploratory", "quality_assessment"]
        }


class TimeSeriesDataAgent(BaseSpecializedAgent):
    """ì‹œê³„ì—´ ë°ì´í„° ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: Optional[Dict] = None):
        super().__init__(config)
        self.data_type = DataType.TIME_SERIES
    
    def detect_data_type(self, data: Any) -> DataTypeDetectionResult:
        """ì‹œê³„ì—´ ë°ì´í„° íƒì§€"""
        confidence = 0.0
        characteristics = {}
        reasoning = ""
        
        if isinstance(data, pd.DataFrame):
            # ë‚ ì§œ/ì‹œê°„ ì»¬ëŸ¼ íƒì§€
            datetime_cols = data.select_dtypes(include=['datetime64']).columns
            date_like_cols = []
            
            # ë¬¸ìì—´ ì»¬ëŸ¼ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì°¾ê¸°
            for col in data.select_dtypes(include=['object']).columns:
                sample_values = data[col].dropna().head(10)
                date_pattern_count = 0
                for val in sample_values:
                    if self._is_date_like(str(val)):
                        date_pattern_count += 1
                
                if date_pattern_count >= 5:  # ìƒ˜í”Œì˜ ì ˆë°˜ ì´ìƒì´ ë‚ ì§œ íŒ¨í„´
                    date_like_cols.append(col)
            
            total_time_cols = len(datetime_cols) + len(date_like_cols)
            
            if total_time_cols > 0:
                confidence = min(0.9, 0.3 + total_time_cols * 0.3)
                characteristics = {
                    "datetime_columns": list(datetime_cols),
                    "date_like_columns": date_like_cols,
                    "total_records": len(data),
                    "potential_time_cols": total_time_cols
                }
                reasoning = f"Found {total_time_cols} time-related columns in DataFrame"
            
            # ì‹œê°„ ìˆœì„œ ë°ì´í„° íŒ¨í„´ ì²´í¬
            if hasattr(data.index, 'dtype') and 'datetime' in str(data.index.dtype):
                confidence = max(confidence, 0.8)
                characteristics["indexed_by_time"] = True
                reasoning += "; Data is indexed by datetime"
        
        recommendations = []
        if confidence > 0.5:
            recommendations.extend([
                "ì‹œê³„ì—´ ë¶„í•´ ë¶„ì„",
                "ì¶”ì„¸ ë° ê³„ì ˆì„± ë¶„ì„",
                "ìê¸°ìƒê´€ ë¶„ì„",
                "ì˜ˆì¸¡ ëª¨ë¸ë§ ê³ ë ¤"
            ])
        
        return DataTypeDetectionResult(
            detected_type=DataType.TIME_SERIES if confidence > 0.5 else DataType.UNKNOWN,
            confidence=confidence,
            reasoning=reasoning,
            characteristics=characteristics,
            recommendations=recommendations
        )
    
    def _is_date_like(self, value: str) -> bool:
        """ë¬¸ìì—´ì´ ë‚ ì§œ í˜•ì‹ì¸ì§€ í™•ì¸"""
        date_patterns = [
            r'\d{4}-\d{2}-\d{2}',           # YYYY-MM-DD
            r'\d{2}/\d{2}/\d{4}',           # MM/DD/YYYY
            r'\d{4}/\d{2}/\d{2}',           # YYYY/MM/DD
            r'\d{2}-\d{2}-\d{4}',           # MM-DD-YYYY
            r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}', # YYYY-MM-DD HH:MM
        ]
        
        return any(re.match(pattern, value) for pattern in date_patterns)
    
    async def analyze(self, data: Any, query: str, context: Optional[Dict] = None) -> DataAnalysisResult:
        """ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„"""
        try:
            logger.info(f"ğŸ”„ ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ì‹œì‘: {query[:50]}...")
            
            # Enhanced Tracking
            if self.enhanced_tracer:
                self.enhanced_tracer.log_data_operation(
                    "time_series_analysis",
                    {"query": query, "data_type": str(type(data))},
                    "Analyzing time series data"
                )
            
            # DataFrameìœ¼ë¡œ ë³€í™˜ ë° ì‹œê°„ ì¸ë±ìŠ¤ ì„¤ì •
            if not isinstance(data, pd.DataFrame):
                data = pd.DataFrame(data)
            
            # ì‹œê°„ ì»¬ëŸ¼ ì°¾ê¸° ë° ì„¤ì •
            time_col = self._find_time_column(data)
            if time_col and not isinstance(data.index, pd.DatetimeIndex):
                data[time_col] = pd.to_datetime(data[time_col])
                data = data.set_index(time_col)
                data = data.sort_index()
            
            results = {}
            insights = []
            recommendations = []
            
            # ê¸°ë³¸ ì‹œê³„ì—´ ì •ë³´
            results["basic_info"] = {
                "start_date": str(data.index.min()) if hasattr(data.index, 'min') else "unknown",
                "end_date": str(data.index.max()) if hasattr(data.index, 'max') else "unknown",
                "frequency": str(data.index.freq) if hasattr(data.index, 'freq') else "irregular",
                "total_periods": len(data),
                "missing_periods": data.isnull().sum().sum()
            }
            
            query_lower = query.lower()
            
            # ì¶”ì„¸ ë¶„ì„
            if any(keyword in query_lower for keyword in ['ì¶”ì„¸', 'trend', 'ê²½í–¥']):
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                trend_analysis = {}
                
                for col in numeric_cols[:3]:  # ì²˜ìŒ 3ê°œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ë¶„ì„
                    series = data[col].dropna()
                    if len(series) > 10:
                        # ë‹¨ìˆœ ì„ í˜• ì¶”ì„¸
                        x = np.arange(len(series))
                        z = np.polyfit(x, series.values, 1)
                        trend_slope = z[0]
                        
                        trend_analysis[col] = {
                            "slope": float(trend_slope),
                            "direction": "ì¦ê°€" if trend_slope > 0 else "ê°ì†Œ" if trend_slope < 0 else "í‰í‰",
                            "magnitude": "ê°•í•¨" if abs(trend_slope) > series.std() else "ì•½í•¨"
                        }
                
                results["trend_analysis"] = trend_analysis
                insights.append(f"{len(trend_analysis)}ê°œ ë³€ìˆ˜ì˜ ì¶”ì„¸ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            # ê³„ì ˆì„± ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)
            if any(keyword in query_lower for keyword in ['ê³„ì ˆì„±', 'seasonal', 'ì£¼ê¸°']):
                if isinstance(data.index, pd.DatetimeIndex):
                    seasonal_analysis = {}
                    numeric_cols = data.select_dtypes(include=[np.number]).columns
                    
                    for col in numeric_cols[:2]:  # ì²˜ìŒ 2ê°œ ì»¬ëŸ¼ë§Œ
                        monthly_avg = data.groupby(data.index.month)[col].mean()
                        seasonal_strength = monthly_avg.std() / monthly_avg.mean() if monthly_avg.mean() != 0 else 0
                        
                        seasonal_analysis[col] = {
                            "seasonal_strength": float(seasonal_strength),
                            "peak_month": int(monthly_avg.idxmax()),
                            "low_month": int(monthly_avg.idxmin()),
                            "has_seasonality": seasonal_strength > 0.1
                        }
                    
                    results["seasonal_analysis"] = seasonal_analysis
                    insights.append("ì›”ë³„ ê³„ì ˆì„± íŒ¨í„´ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            # ë³€ë™ì„± ë¶„ì„
            if any(keyword in query_lower for keyword in ['ë³€ë™ì„±', 'volatility', 'ë³€í™”']):
                volatility_analysis = {}
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                
                for col in numeric_cols[:3]:
                    series = data[col].dropna()
                    if len(series) > 1:
                        # ë¡¤ë§ í‘œì¤€í¸ì°¨ë¡œ ë³€ë™ì„± ì¸¡ì •
                        rolling_std = series.rolling(window=min(30, len(series)//4)).std()
                        
                        volatility_analysis[col] = {
                            "overall_volatility": float(series.std()),
                            "recent_volatility": float(rolling_std.iloc[-1]) if not rolling_std.empty else 0,
                            "volatility_trend": "ì¦ê°€" if rolling_std.iloc[-1] > rolling_std.mean() else "ê°ì†Œ"
                        }
                
                results["volatility_analysis"] = volatility_analysis
                insights.append(f"{len(volatility_analysis)}ê°œ ë³€ìˆ˜ì˜ ë³€ë™ì„±ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            # ìƒê´€ê´€ê³„ ì‹œê³„ì—´ ë¶„ì„
            if any(keyword in query_lower for keyword in ['ìƒê´€ê´€ê³„', 'correlation', 'ê´€ê³„']):
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 1:
                    # ì‹œì°¨ ìƒê´€ê´€ê³„ ë¶„ì„
                    lag_correlation = {}
                    base_col = numeric_cols[0]
                    
                    for col in numeric_cols[1:3]:  # ìµœëŒ€ 2ê°œ ë‹¤ë¥¸ ë³€ìˆ˜ì™€ ë¹„êµ
                        correlations = []
                        for lag in range(0, min(10, len(data)//10)):  # ìµœëŒ€ 10ê°œ ì‹œì°¨
                            if lag == 0:
                                corr = data[base_col].corr(data[col])
                            else:
                                corr = data[base_col].corr(data[col].shift(lag))
                            
                            if not np.isnan(corr):
                                correlations.append({"lag": lag, "correlation": float(corr)})
                        
                        lag_correlation[f"{base_col}_vs_{col}"] = correlations
                    
                    results["lag_correlation"] = lag_correlation
                    insights.append("ì‹œì°¨ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            # ì¼ë°˜ì ì¸ ì¶”ì²œì‚¬í•­
            if len(data) > 1000:
                recommendations.append("ì¥ê¸° ì‹œê³„ì—´ì´ë¯€ë¡œ ì‹œê³„ì—´ ë¶„í•´ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
            
            if isinstance(data.index, pd.DatetimeIndex):
                freq = pd.infer_freq(data.index)
                if freq is None:
                    recommendations.append("ë¶ˆê·œì¹™í•œ ì‹œê°„ ê°„ê²©ì´ë¯€ë¡œ ë¦¬ìƒ˜í”Œë§ì„ ê³ ë ¤í•˜ì„¸ìš”")
            
            recommendations.extend([
                "ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ ì ìš© ê³ ë ¤",
                "ì´ìƒ íŒ¨í„´ íƒì§€ ìˆ˜í–‰",
                "ì‹œê³„ì—´ ë¶„í•´ë¥¼ í†µí•œ ì„±ë¶„ ë¶„ì„"
            ])
            
            return DataAnalysisResult(
                analysis_type="time_series_analysis",
                data_type=DataType.TIME_SERIES,
                results=results,
                insights=insights,
                recommendations=recommendations,
                confidence=0.8,
                metadata={"query": query, "data_periods": len(data)}
            )
            
        except Exception as e:
            logger.error(f"âŒ ì‹œê³„ì—´ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return DataAnalysisResult(
                analysis_type="time_series_analysis",
                data_type=DataType.TIME_SERIES,
                results={"error": str(e)},
                insights=[f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                recommendations=["ì‹œê°„ ì»¬ëŸ¼ í˜•ì‹ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”"],
                confidence=0.1
            )
    
    def _find_time_column(self, data: pd.DataFrame) -> Optional[str]:
        """ì‹œê°„ ì»¬ëŸ¼ ì°¾ê¸°"""
        # datetime íƒ€ì… ì»¬ëŸ¼ ìš°ì„ 
        datetime_cols = data.select_dtypes(include=['datetime64']).columns
        if len(datetime_cols) > 0:
            return datetime_cols[0]
        
        # ì´ë¦„ìœ¼ë¡œ ì¶”ì •
        time_like_names = ['time', 'date', 'datetime', 'timestamp', 'ì‹œê°„', 'ë‚ ì§œ']
        for col in data.columns:
            if any(name in col.lower() for name in time_like_names):
                return col
        
        return None
    
    def get_capabilities(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ëŠ¥ë ¥ ë°˜í™˜"""
        return {
            "name": "Time Series Data Agent",
            "data_type": "time_series",
            "capabilities": [
                "ì¶”ì„¸ ë¶„ì„",
                "ê³„ì ˆì„± ë¶„ì„",
                "ë³€ë™ì„± ë¶„ì„",
                "ì‹œì°¨ ìƒê´€ê´€ê³„ ë¶„ì„",
                "ì‹œê³„ì—´ ë¶„í•´",
                "ì´ìƒ íŒ¨í„´ íƒì§€"
            ],
            "supported_formats": ["DateTime indexed DataFrame", "CSV with time columns"],
            "analysis_types": ["trend", "seasonal", "volatility", "forecasting_prep"]
        }


class TextDataAgent(BaseSpecializedAgent):
    """í…ìŠ¤íŠ¸ ë°ì´í„° ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: Optional[Dict] = None):
        super().__init__(config)
        self.data_type = DataType.TEXT
    
    def detect_data_type(self, data: Any) -> DataTypeDetectionResult:
        """í…ìŠ¤íŠ¸ ë°ì´í„° íƒì§€"""
        confidence = 0.0
        characteristics = {}
        reasoning = ""
        
        if isinstance(data, pd.DataFrame):
            text_cols = data.select_dtypes(include=['object']).columns
            text_analysis = {}
            
            for col in text_cols:
                sample_values = data[col].dropna().head(20)
                avg_length = np.mean([len(str(val)) for val in sample_values])
                
                # í…ìŠ¤íŠ¸ íŠ¹ì„± ë¶„ì„
                if avg_length > 50:  # í‰ê·  50ì ì´ìƒì´ë©´ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼
                    text_analysis[col] = {
                        "avg_length": avg_length,
                        "max_length": max([len(str(val)) for val in sample_values]),
                        "contains_sentences": any('.' in str(val) for val in sample_values)
                    }
            
            if text_analysis:
                confidence = min(0.9, 0.4 + len(text_analysis) * 0.2)
                characteristics = {
                    "text_columns": list(text_analysis.keys()),
                    "text_analysis": text_analysis,
                    "total_records": len(data)
                }
                reasoning = f"Found {len(text_analysis)} text columns with substantial content"
        
        elif isinstance(data, (list, str)):
            if isinstance(data, str):
                # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ìê°€ ìˆìœ¼ë©´ í…ìŠ¤íŠ¸ê°€ ì•„ë‹˜
                image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']
                if any(ext in data.lower() for ext in image_extensions):
                    confidence = 0.1  # ì´ë¯¸ì§€ íŒŒì¼ë¡œ ë³´ì„
                elif len(data) > 50:  # ì¶©ë¶„íˆ ê¸´ í…ìŠ¤íŠ¸ë§Œ ë†’ì€ ì‹ ë¢°ë„
                    confidence = 0.9
                else:
                    confidence = 0.6  # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ë‚®ì€ ì‹ ë¢°ë„
                
                characteristics = {
                    "length": len(data),
                    "word_count": len(data.split()),
                    "contains_sentences": '.' in data
                }
                reasoning = "Single text string detected"
            elif isinstance(data, list) and len(data) > 0:
                first_item = data[0]
                if isinstance(first_item, str) and len(first_item) > 20:
                    confidence = 0.8
                    avg_length = np.mean([len(str(item)) for item in data[:10]])
                    characteristics = {
                        "records": len(data),
                        "avg_length": avg_length,
                        "sample_item": str(first_item)[:100]
                    }
                    reasoning = f"List of {len(data)} text items detected"
        
        recommendations = []
        if confidence > 0.5:
            recommendations.extend([
                "í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ìˆ˜í–‰",
                "ê°ì • ë¶„ì„",
                "í‚¤ì›Œë“œ ì¶”ì¶œ",
                "í† í”½ ëª¨ë¸ë§ ê³ ë ¤"
            ])
        
        return DataTypeDetectionResult(
            detected_type=DataType.TEXT if confidence > 0.5 else DataType.UNKNOWN,
            confidence=confidence,
            reasoning=reasoning,
            characteristics=characteristics,
            recommendations=recommendations
        )
    
    async def analyze(self, data: Any, query: str, context: Optional[Dict] = None) -> DataAnalysisResult:
        """í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„"""
        try:
            logger.info(f"ğŸ”„ í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ì‹œì‘: {query[:50]}...")
            
            # Enhanced Tracking
            if self.enhanced_tracer:
                self.enhanced_tracer.log_data_operation(
                    "text_data_analysis",
                    {"query": query, "data_type": str(type(data))},
                    "Analyzing text data"
                )
            
            # í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
            text_data = self._prepare_text_data(data)
            
            results = {}
            insights = []
            recommendations = []
            
            # ê¸°ë³¸ í…ìŠ¤íŠ¸ í†µê³„
            results["basic_stats"] = self._compute_basic_text_stats(text_data)
            insights.append(f"ì´ {len(text_data)}ê°œì˜ í…ìŠ¤íŠ¸ í•­ëª©ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            query_lower = query.lower()
            
            # í‚¤ì›Œë“œ ë¶„ì„
            if any(keyword in query_lower for keyword in ['í‚¤ì›Œë“œ', 'keyword', 'ë‹¨ì–´', 'ìš©ì–´']):
                keyword_analysis = self._analyze_keywords(text_data)
                results["keyword_analysis"] = keyword_analysis
                insights.append(f"ìƒìœ„ {len(keyword_analysis.get('top_words', []))}ê°œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤")
            
            # ê°ì • ë¶„ì„ (ê°„ë‹¨í•œ ë²„ì „)
            if any(keyword in query_lower for keyword in ['ê°ì •', 'sentiment', 'ê¸ì •', 'ë¶€ì •']):
                sentiment_analysis = self._analyze_sentiment(text_data)
                results["sentiment_analysis"] = sentiment_analysis
                insights.append("ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤")
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„
            if any(keyword in query_lower for keyword in ['ê¸¸ì´', 'length', 'ë¬¸ì¥', 'ë‹¨ì–´ìˆ˜']):
                length_analysis = self._analyze_text_length(text_data)
                results["length_analysis"] = length_analysis
                insights.append("í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„í¬ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            # ì–¸ì–´ íŠ¹ì„± ë¶„ì„
            if any(keyword in query_lower for keyword in ['ì–¸ì–´', 'language', 'íŠ¹ì„±']):
                language_analysis = self._analyze_language_features(text_data)
                results["language_analysis"] = language_analysis
                insights.append("ì–¸ì–´ì  íŠ¹ì„±ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            # ì¼ë°˜ì ì¸ ì¶”ì²œì‚¬í•­
            if len(text_data) > 1000:
                recommendations.append("ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ì´ë¯€ë¡œ ìƒ˜í”Œë§ ë˜ëŠ” ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
            
            recommendations.extend([
                "ì „ë¬¸ì ì¸ NLP ë„êµ¬ í™œìš© ê³ ë ¤",
                "í† í”½ ëª¨ë¸ë§ì„ í†µí•œ ì£¼ì œ ë¶„ì„",
                "í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ë° ì •ì œ ìˆ˜í–‰"
            ])
            
            return DataAnalysisResult(
                analysis_type="text_data_analysis",
                data_type=DataType.TEXT,
                results=results,
                insights=insights,
                recommendations=recommendations,
                confidence=0.8,
                metadata={"query": query, "text_items": len(text_data)}
            )
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return DataAnalysisResult(
                analysis_type="text_data_analysis",
                data_type=DataType.TEXT,
                results={"error": str(e)},
                insights=[f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                recommendations=["í…ìŠ¤íŠ¸ ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”"],
                confidence=0.1
            )
    
    def _prepare_text_data(self, data: Any) -> List[str]:
        """í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        text_data = []
        
        if isinstance(data, str):
            text_data = [data]
        elif isinstance(data, list):
            text_data = [str(item) for item in data]
        elif isinstance(data, pd.DataFrame):
            # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
            text_cols = data.select_dtypes(include=['object']).columns
            for col in text_cols:
                sample_values = data[col].dropna().head(10)
                avg_length = np.mean([len(str(val)) for val in sample_values])
                if avg_length > 20:  # ê¸´ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì„ íƒ
                    text_data.extend(data[col].dropna().astype(str).tolist())
                    break
        
        return text_data
    
    def _compute_basic_text_stats(self, text_data: List[str]) -> Dict[str, Any]:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ í†µê³„ ê³„ì‚°"""
        lengths = [len(text) for text in text_data]
        word_counts = [len(text.split()) for text in text_data]
        
        return {
            "total_texts": len(text_data),
            "avg_char_length": np.mean(lengths),
            "avg_word_count": np.mean(word_counts),
            "max_char_length": max(lengths) if lengths else 0,
            "min_char_length": min(lengths) if lengths else 0,
            "total_characters": sum(lengths),
            "total_words": sum(word_counts)
        }
    
    def _analyze_keywords(self, text_data: List[str]) -> Dict[str, Any]:
        """í‚¤ì›Œë“œ ë¶„ì„"""
        from collections import Counter
        import re
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜)
        all_words = []
        for text in text_data:
            # ë‹¨ìˆœ ë‹¨ì–´ ë¶„ë¦¬ (í•œê¸€/ì˜ë¬¸)
            words = re.findall(r'\b\w+\b', text.lower())
            # ê¸¸ì´ 2 ì´ìƒì¸ ë‹¨ì–´ë§Œ ì„ íƒ
            words = [word for word in words if len(word) >= 2]
            all_words.extend(words)
        
        word_freq = Counter(all_words)
        
        return {
            "total_unique_words": len(word_freq),
            "top_words": [{"word": word, "count": count} for word, count in word_freq.most_common(20)],
            "vocabulary_size": len(word_freq)
        }
    
    def _analyze_sentiment(self, text_data: List[str]) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ ê°ì • ë¶„ì„"""
        # ê¸°ë³¸ì ì¸ ê°ì • í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„
        positive_words = ['ì¢‹ë‹¤', 'í›Œë¥­í•˜ë‹¤', 'ì™„ë²½í•˜ë‹¤', 'ìµœê³ ', 'ë§Œì¡±', 'good', 'great', 'excellent', 'perfect', 'amazing']
        negative_words = ['ë‚˜ì˜ë‹¤', 'ìµœì•…', 'ì‹«ë‹¤', 'ë¶ˆë§Œ', 'í™”ë‚˜ë‹¤', 'bad', 'terrible', 'awful', 'hate', 'angry']
        
        sentiment_scores = []
        for text in text_data:
            text_lower = text.lower()
            positive_count = sum(1 for word in positive_words if word in text_lower)
            negative_count = sum(1 for word in negative_words if word in text_lower)
            
            if positive_count > negative_count:
                sentiment_scores.append(1)  # ê¸ì •
            elif negative_count > positive_count:
                sentiment_scores.append(-1)  # ë¶€ì •
            else:
                sentiment_scores.append(0)  # ì¤‘ë¦½
        
        positive_ratio = sum(1 for s in sentiment_scores if s > 0) / len(sentiment_scores)
        negative_ratio = sum(1 for s in sentiment_scores if s < 0) / len(sentiment_scores)
        neutral_ratio = sum(1 for s in sentiment_scores if s == 0) / len(sentiment_scores)
        
        return {
            "positive_ratio": positive_ratio,
            "negative_ratio": negative_ratio,
            "neutral_ratio": neutral_ratio,
            "overall_sentiment": "ê¸ì •ì " if positive_ratio > negative_ratio else "ë¶€ì •ì " if negative_ratio > positive_ratio else "ì¤‘ë¦½ì "
        }
    
    def _analyze_text_length(self, text_data: List[str]) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„"""
        char_lengths = [len(text) for text in text_data]
        word_lengths = [len(text.split()) for text in text_data]
        
        return {
            "char_length_stats": {
                "mean": np.mean(char_lengths),
                "std": np.std(char_lengths),
                "min": min(char_lengths),
                "max": max(char_lengths),
                "median": np.median(char_lengths)
            },
            "word_length_stats": {
                "mean": np.mean(word_lengths),
                "std": np.std(word_lengths),
                "min": min(word_lengths),
                "max": max(word_lengths),
                "median": np.median(word_lengths)
            }
        }
    
    def _analyze_language_features(self, text_data: List[str]) -> Dict[str, Any]:
        """ì–¸ì–´ì  íŠ¹ì„± ë¶„ì„"""
        # ë¬¸ì¥ ë¶€í˜¸ ì‚¬ìš© ë¹ˆë„
        punctuation_count = 0
        question_count = 0
        exclamation_count = 0
        
        for text in text_data:
            punctuation_count += len(re.findall(r'[.,;:!?]', text))
            question_count += text.count('?')
            exclamation_count += text.count('!')
        
        return {
            "avg_punctuation_per_text": punctuation_count / len(text_data),
            "question_ratio": question_count / len(text_data),
            "exclamation_ratio": exclamation_count / len(text_data),
            "avg_sentence_length": np.mean([len(text.split('.')) for text in text_data])
        }
    
    def get_capabilities(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ëŠ¥ë ¥ ë°˜í™˜"""
        return {
            "name": "Text Data Agent",
            "data_type": "text",
            "capabilities": [
                "í‚¤ì›Œë“œ ì¶”ì¶œ",
                "ê°ì • ë¶„ì„",
                "í…ìŠ¤íŠ¸ ê¸¸ì´ ë¶„ì„",
                "ì–¸ì–´ì  íŠ¹ì„± ë¶„ì„",
                "ê¸°ë³¸ í†µê³„ ë¶„ì„"
            ],
            "supported_formats": ["String", "List of strings", "DataFrame with text columns"],
            "analysis_types": ["keyword", "sentiment", "linguistic", "statistical"]
        }


class ImageDataAgent(BaseSpecializedAgent):
    """ì´ë¯¸ì§€ ë°ì´í„° ì „ë¬¸ ì—ì´ì „íŠ¸"""
    
    def __init__(self, config: Optional[Dict] = None):
        super().__init__(config)
        self.data_type = DataType.IMAGE
    
    def detect_data_type(self, data: Any) -> DataTypeDetectionResult:
        """ì´ë¯¸ì§€ ë°ì´í„° íƒì§€"""
        confidence = 0.0
        characteristics = {}
        reasoning = ""
        
        # íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ íƒì§€
        if isinstance(data, (str, Path)):
            file_path = Path(data)
            image_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']
            
            if file_path.suffix.lower() in image_extensions:
                confidence = 0.9
                characteristics = {
                    "file_path": str(file_path),
                    "file_extension": file_path.suffix.lower(),
                    "file_exists": file_path.exists()
                }
                reasoning = f"Image file detected: {file_path.suffix}"
        
        # DataFrameì—ì„œ ì´ë¯¸ì§€ ê²½ë¡œ ì»¬ëŸ¼ íƒì§€
        elif isinstance(data, pd.DataFrame):
            image_path_cols = []
            for col in data.columns:
                if 'image' in col.lower() or 'img' in col.lower() or 'photo' in col.lower():
                    sample_values = data[col].dropna().head(10)
                    image_files = sum(1 for val in sample_values 
                                    if isinstance(val, str) and any(ext in val.lower() 
                                    for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']))
                    
                    if image_files >= 3:  # ë” ê´€ëŒ€í•œ ê¸°ì¤€
                        image_path_cols.append(col)
            
            if image_path_cols:
                confidence = 0.8
                characteristics = {
                    "image_path_columns": image_path_cols,
                    "total_records": len(data)
                }
                reasoning = f"Found {len(image_path_cols)} columns with image file paths"
        
        # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ ì´ë¯¸ì§€ ê²½ë¡œë“¤
        elif isinstance(data, list) and len(data) > 0:
            first_item = data[0]
            if isinstance(first_item, str):
                image_files = sum(1 for item in data[:10] 
                                if any(ext in item.lower() 
                                for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff']))
                
                if image_files >= 3:  # ë” ê´€ëŒ€í•œ ê¸°ì¤€
                    confidence = 0.7
                    characteristics = {
                        "image_files_count": len(data),
                        "sample_files": data[:5]
                    }
                    reasoning = f"List of {len(data)} image file paths detected"
        
        recommendations = []
        if confidence > 0.5:
            recommendations.extend([
                "ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¶„ì„",
                "ì´ë¯¸ì§€ í¬ê¸° ë° í˜•ì‹ ë¶„ì„",
                "ìƒ‰ìƒ ë¶„í¬ ë¶„ì„",
                "ì»´í“¨í„° ë¹„ì „ ë¶„ì„ ê³ ë ¤"
            ])
        
        return DataTypeDetectionResult(
            detected_type=DataType.IMAGE if confidence > 0.5 else DataType.UNKNOWN,
            confidence=confidence,
            reasoning=reasoning,
            characteristics=characteristics,
            recommendations=recommendations
        )
    
    async def analyze(self, data: Any, query: str, context: Optional[Dict] = None) -> DataAnalysisResult:
        """ì´ë¯¸ì§€ ë°ì´í„° ë¶„ì„"""
        try:
            logger.info(f"ğŸ”„ ì´ë¯¸ì§€ ë°ì´í„° ë¶„ì„ ì‹œì‘: {query[:50]}...")
            
            # Enhanced Tracking
            if self.enhanced_tracer:
                self.enhanced_tracer.log_data_operation(
                    "image_data_analysis",
                    {"query": query, "data_type": str(type(data))},
                    "Analyzing image data"
                )
            
            # ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
            image_paths = self._collect_image_paths(data)
            
            if not image_paths:
                raise ValueError("ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            results = {}
            insights = []
            recommendations = []
            
            # ê¸°ë³¸ ì´ë¯¸ì§€ ì •ë³´ ë¶„ì„
            results["basic_info"] = self._analyze_basic_image_info(image_paths)
            insights.append(f"ì´ {len(image_paths)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            query_lower = query.lower()
            
            # ë©”íƒ€ë°ì´í„° ë¶„ì„
            if any(keyword in query_lower for keyword in ['ë©”íƒ€ë°ì´í„°', 'metadata', 'ì •ë³´', 'exif']):
                metadata_analysis = self._analyze_image_metadata(image_paths)
                results["metadata_analysis"] = metadata_analysis
                insights.append("ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„°ë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            # í¬ê¸° ë° í˜•ì‹ ë¶„ì„
            if any(keyword in query_lower for keyword in ['í¬ê¸°', 'size', 'í•´ìƒë„', 'resolution', 'í˜•ì‹', 'format']):
                size_format_analysis = self._analyze_size_and_format(image_paths)
                results["size_format_analysis"] = size_format_analysis
                insights.append("ì´ë¯¸ì§€ í¬ê¸°ì™€ í˜•ì‹ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤")
            
            # ì¼ë°˜ì ì¸ ì¶”ì²œì‚¬í•­
            if len(image_paths) > 100:
                recommendations.append("ëŒ€ëŸ‰ ì´ë¯¸ì§€ì´ë¯€ë¡œ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”")
            
            recommendations.extend([
                "ì „ë¬¸ ì´ë¯¸ì§€ ë¶„ì„ ë„êµ¬ í™œìš© ê³ ë ¤",
                "ì»´í“¨í„° ë¹„ì „ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©",
                "ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë° ìµœì í™” ìˆ˜í–‰"
            ])
            
            return DataAnalysisResult(
                analysis_type="image_data_analysis",
                data_type=DataType.IMAGE,
                results=results,
                insights=insights,
                recommendations=recommendations,
                confidence=0.7,
                metadata={"query": query, "image_count": len(image_paths)}
            )
            
        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return DataAnalysisResult(
                analysis_type="image_data_analysis",
                data_type=DataType.IMAGE,
                results={"error": str(e)},
                insights=[f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                recommendations=["ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”"],
                confidence=0.1
            )
    
    def _collect_image_paths(self, data: Any) -> List[str]:
        """ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘"""
        image_paths = []
        
        if isinstance(data, (str, Path)):
            if Path(data).exists():
                image_paths = [str(data)]
        elif isinstance(data, list):
            for item in data:
                if isinstance(item, str) and Path(item).exists():
                    image_paths.append(item)
        elif isinstance(data, pd.DataFrame):
            for col in data.columns:
                if 'image' in col.lower() or 'img' in col.lower():
                    paths = data[col].dropna().tolist()
                    valid_paths = [path for path in paths if isinstance(path, str) and Path(path).exists()]
                    image_paths.extend(valid_paths)
                    break
        
        return image_paths
    
    def _analyze_basic_image_info(self, image_paths: List[str]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì´ë¯¸ì§€ ì •ë³´ ë¶„ì„"""
        file_sizes = []
        file_extensions = []
        
        for path in image_paths:
            try:
                file_path = Path(path)
                if file_path.exists():
                    file_sizes.append(file_path.stat().st_size)
                    file_extensions.append(file_path.suffix.lower())
            except Exception:
                continue
        
        from collections import Counter
        extension_counts = Counter(file_extensions)
        
        return {
            "total_images": len(image_paths),
            "valid_files": len(file_sizes),
            "total_size_bytes": sum(file_sizes),
            "avg_size_bytes": np.mean(file_sizes) if file_sizes else 0,
            "extension_distribution": dict(extension_counts),
            "size_range": {
                "min": min(file_sizes) if file_sizes else 0,
                "max": max(file_sizes) if file_sizes else 0
            }
        }
    
    def _analyze_image_metadata(self, image_paths: List[str]) -> Dict[str, Any]:
        """ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¶„ì„ (ê¸°ë³¸ ì •ë³´)"""
        # ê¸°ë³¸ì ì¸ íŒŒì¼ ì •ë³´ë§Œ ì œê³µ (EXIFëŠ” ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”)
        metadata_info = {
            "analyzed_images": 0,
            "creation_dates": [],
            "file_formats": [],
            "estimated_dimensions": "ë¶„ì„ì„ ìœ„í•´ PIL ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤"
        }
        
        for path in image_paths[:10]:  # ì²˜ìŒ 10ê°œë§Œ ë¶„ì„
            try:
                file_path = Path(path)
                if file_path.exists():
                    stat = file_path.stat()
                    metadata_info["creation_dates"].append(stat.st_mtime)
                    metadata_info["file_formats"].append(file_path.suffix.lower())
                    metadata_info["analyzed_images"] += 1
            except Exception:
                continue
        
        return metadata_info
    
    def _analyze_size_and_format(self, image_paths: List[str]) -> Dict[str, Any]:
        """í¬ê¸° ë° í˜•ì‹ ë¶„ì„"""
        format_analysis = {
            "supported_formats": ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff'],
            "found_formats": [],
            "format_distribution": {},
            "size_analysis": "ì´ë¯¸ì§€ í¬ê¸° ë¶„ì„ì„ ìœ„í•´ PIL ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤"
        }
        
        from collections import Counter
        formats = [Path(path).suffix.lower() for path in image_paths]
        format_counts = Counter(formats)
        
        format_analysis["found_formats"] = list(set(formats))
        format_analysis["format_distribution"] = dict(format_counts)
        
        return format_analysis
    
    def get_capabilities(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ëŠ¥ë ¥ ë°˜í™˜"""
        return {
            "name": "Image Data Agent",
            "data_type": "image",
            "capabilities": [
                "ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ë¶„ì„",
                "íŒŒì¼ í¬ê¸° ë° í˜•ì‹ ë¶„ì„",
                "ê¸°ë³¸ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ",
                "ì´ë¯¸ì§€ ê²½ë¡œ ê´€ë¦¬"
            ],
            "supported_formats": ["JPG", "JPEG", "PNG", "GIF", "BMP", "TIFF"],
            "analysis_types": ["metadata", "format", "basic_stats"],
            "limitations": ["ê³ ê¸‰ ì´ë¯¸ì§€ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” OpenCV, PIL ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤"]
        }


class DataTypeDetector:
    """ë°ì´í„° íƒ€ì… ìë™ íƒì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.agents = {
            DataType.STRUCTURED: StructuredDataAgent(),
            DataType.TIME_SERIES: TimeSeriesDataAgent(),
            DataType.TEXT: TextDataAgent(),
            DataType.IMAGE: ImageDataAgent()
        }
    
    def detect_data_type(self, data: Any) -> DataTypeDetectionResult:
        """ë°ì´í„° íƒ€ì… ìë™ íƒì§€"""
        detection_results = []
        
        for data_type, agent in self.agents.items():
            result = agent.detect_data_type(data)
            detection_results.append((data_type, result))
        
        # ê°€ì¥ ë†’ì€ ì‹ ë¢°ë„ì˜ ê²°ê³¼ ì„ íƒ
        best_result = max(detection_results, key=lambda x: x[1].confidence)
        
        if best_result[1].confidence > 0.5:
            return best_result[1]
        else:
            return DataTypeDetectionResult(
                detected_type=DataType.UNKNOWN,
                confidence=0.0,
                reasoning="ë°ì´í„° íƒ€ì…ì„ í™•ì‹¤í•˜ê²Œ íƒì§€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                characteristics={},
                recommendations=["ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•˜ê³  ì ì ˆí•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”"]
            )
    
    async def analyze_with_best_agent(self, data: Any, query: str, context: Optional[Dict] = None) -> DataAnalysisResult:
        """ìµœì ì˜ ì—ì´ì „íŠ¸ë¡œ ë°ì´í„° ë¶„ì„"""
        detection_result = self.detect_data_type(data)
        
        if detection_result.detected_type == DataType.UNKNOWN:
            # ê¸°ë³¸ì ìœ¼ë¡œ ì •í˜• ë°ì´í„° ì—ì´ì „íŠ¸ ì‚¬ìš©
            agent = self.agents[DataType.STRUCTURED]
        else:
            agent = self.agents[detection_result.detected_type]
        
        return await agent.analyze(data, query, context)


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
_detector_instance = None


def get_data_type_detector() -> DataTypeDetector:
    """ë°ì´í„° íƒ€ì… íƒì§€ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _detector_instance
    if _detector_instance is None:
        _detector_instance = DataTypeDetector()
    return _detector_instance


# ê°œë³„ ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ í•¨ìˆ˜ë“¤
def get_structured_agent(config: Optional[Dict] = None) -> StructuredDataAgent:
    """ì •í˜• ë°ì´í„° ì—ì´ì „íŠ¸ ë°˜í™˜"""
    return StructuredDataAgent(config)


def get_time_series_agent(config: Optional[Dict] = None) -> TimeSeriesDataAgent:
    """ì‹œê³„ì—´ ë°ì´í„° ì—ì´ì „íŠ¸ ë°˜í™˜"""
    return TimeSeriesDataAgent(config)


def get_text_agent(config: Optional[Dict] = None) -> TextDataAgent:
    """í…ìŠ¤íŠ¸ ë°ì´í„° ì—ì´ì „íŠ¸ ë°˜í™˜"""
    return TextDataAgent(config)


def get_image_agent(config: Optional[Dict] = None) -> ImageDataAgent:
    """ì´ë¯¸ì§€ ë°ì´í„° ì—ì´ì „íŠ¸ ë°˜í™˜"""
    return ImageDataAgent(config)


# CLI í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_specialized_agents():
    """ì „ë¬¸í™” ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸"""
    print("ğŸ”„ ì „ë¬¸í™” ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    structured_data = pd.DataFrame({
        'name': ['Alice', 'Bob', 'Charlie'],
        'age': [25, 30, 35],
        'salary': [50000, 60000, 70000]
    })
    
    text_data = [
        "ì´ê²ƒì€ ê¸ì •ì ì¸ ë¦¬ë·°ì…ë‹ˆë‹¤. ì •ë§ ì¢‹ì•˜ì–´ìš”!",
        "ë³„ë¡œì˜€ìŠµë‹ˆë‹¤. ë‹¤ì‹œëŠ” ì‚¬ìš©í•˜ì§€ ì•Šì„ ê²ƒ ê°™ì•„ìš”.",
        "ë³´í†µì´ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ­ì €ëŸ­ ê´œì°®ì•˜ì–´ìš”."
    ]
    
    # ë°ì´í„° íƒ€ì… íƒì§€ í…ŒìŠ¤íŠ¸
    detector = get_data_type_detector()
    
    print("ğŸ“Š ì •í˜• ë°ì´í„° íƒì§€:")
    structured_detection = detector.detect_data_type(structured_data)
    print(f"  íƒì§€ ê²°ê³¼: {structured_detection.detected_type.value}")
    print(f"  ì‹ ë¢°ë„: {structured_detection.confidence:.2f}")
    print(f"  ì´ìœ : {structured_detection.reasoning}\n")
    
    print("ğŸ“ í…ìŠ¤íŠ¸ ë°ì´í„° íƒì§€:")
    text_detection = detector.detect_data_type(text_data)
    print(f"  íƒì§€ ê²°ê³¼: {text_detection.detected_type.value}")
    print(f"  ì‹ ë¢°ë„: {text_detection.confidence:.2f}")
    print(f"  ì´ìœ : {text_detection.reasoning}\n")
    
    # ë¶„ì„ í…ŒìŠ¤íŠ¸
    print("ğŸ” ì •í˜• ë°ì´í„° ë¶„ì„:")
    structured_result = await detector.analyze_with_best_agent(
        structured_data, 
        "ë°ì´í„° ìš”ì•½ê³¼ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
    )
    print(f"  ë¶„ì„ ê²°ê³¼: {len(structured_result.insights)}ê°œ ì¸ì‚¬ì´íŠ¸")
    for insight in structured_result.insights:
        print(f"    - {insight}")
    print()
    
    print("ğŸ“ í…ìŠ¤íŠ¸ ë°ì´í„° ë¶„ì„:")
    text_result = await detector.analyze_with_best_agent(
        text_data,
        "ê°ì • ë¶„ì„ê³¼ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”"
    )
    print(f"  ë¶„ì„ ê²°ê³¼: {len(text_result.insights)}ê°œ ì¸ì‚¬ì´íŠ¸")
    for insight in text_result.insights:
        print(f"    - {insight}")
    
    print("\nâœ… ì „ë¬¸í™” ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(test_specialized_agents()) 