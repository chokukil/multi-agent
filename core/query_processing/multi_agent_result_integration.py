"""
Multi-Agent Result Integration

ì´ ëª¨ë“ˆì€ A2A Agent Execution Orchestratorì—ì„œ ìˆ˜ì§‘ëœ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ 
í†µí•©í•˜ê³  ë¶„ì„í•˜ì—¬ ì „ì²´ë¡ ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•© ë° ì •ê·œí™”
- êµì°¨ ê²€ì¦ ë° ì¼ê´€ì„± ë¶„ì„
- ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë° íŒ¨í„´ ë°œê²¬
- ê²°ê³¼ í’ˆì§ˆ í‰ê°€
- ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±
"""

import asyncio
import json
import logging
from typing import Dict, List, Optional, Any, Union, Tuple
from dataclasses import dataclass, field
from enum import Enum
import time
from datetime import datetime

from core.llm_factory import create_llm_instance
from .a2a_agent_execution_orchestrator import ExecutionResult, ExecutionStatus

logger = logging.getLogger(__name__)


class IntegrationStrategy(Enum):
    """í†µí•© ì „ëµ"""
    SEQUENTIAL = "sequential"      # ìˆœì°¨ í†µí•©
    HIERARCHICAL = "hierarchical"  # ê³„ì¸µì  í†µí•©
    CONSENSUS = "consensus"        # í•©ì˜ ê¸°ë°˜ í†µí•©
    WEIGHTED = "weighted"          # ê°€ì¤‘ì¹˜ ê¸°ë°˜ í†µí•©


class ResultType(Enum):
    """ê²°ê³¼ íƒ€ì…"""
    DATA_ANALYSIS = "data_analysis"
    VISUALIZATION = "visualization"
    STATISTICAL = "statistical"
    PREDICTION = "prediction"
    INSIGHT = "insight"
    RECOMMENDATION = "recommendation"
    REPORT = "report"


class QualityMetric(Enum):
    """í’ˆì§ˆ ì§€í‘œ"""
    COMPLETENESS = "completeness"
    CONSISTENCY = "consistency"
    ACCURACY = "accuracy"
    RELEVANCE = "relevance"
    CLARITY = "clarity"
    ACTIONABILITY = "actionability"


@dataclass
class AgentResult:
    """ì—ì´ì „íŠ¸ ê²°ê³¼"""
    agent_name: str
    agent_type: str
    result_type: ResultType
    content: Dict[str, Any]
    confidence: float
    execution_time: float
    timestamp: datetime
    metadata: Dict[str, Any] = field(default_factory=dict)
    quality_scores: Dict[QualityMetric, float] = field(default_factory=dict)


@dataclass
class CrossValidationResult:
    """êµì°¨ ê²€ì¦ ê²°ê³¼"""
    consistency_score: float
    conflicting_findings: List[Dict[str, Any]]
    supporting_evidence: List[Dict[str, Any]]
    validation_notes: str
    confidence_adjustment: float


@dataclass
class IntegratedInsight:
    """í†µí•© ì¸ì‚¬ì´íŠ¸"""
    insight_type: str
    content: str
    confidence: float
    supporting_agents: List[str]
    evidence_strength: float
    actionable_items: List[str]
    priority: int


@dataclass
class IntegrationResult:
    """í†µí•© ê²°ê³¼"""
    integration_id: str
    strategy: IntegrationStrategy
    agent_results: List[AgentResult]
    cross_validation: CrossValidationResult
    integrated_insights: List[IntegratedInsight]
    quality_assessment: Dict[QualityMetric, float]
    synthesis_report: str
    recommendations: List[str]
    confidence_score: float
    integration_time: float
    metadata: Dict[str, Any] = field(default_factory=dict)


class MultiAgentResultIntegrator:
    """ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•©ê¸°"""
    
    def __init__(self):
        self.llm = create_llm_instance()
        self.integration_history: List[IntegrationResult] = []
        logger.info("MultiAgentResultIntegrator initialized")
    
    async def integrate_results(
        self,
        execution_result: ExecutionResult,
        integration_strategy: IntegrationStrategy = IntegrationStrategy.HIERARCHICAL,
        context: Optional[Dict[str, Any]] = None
    ) -> IntegrationResult:
        """
        ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•©
        
        Args:
            execution_result: A2A ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼
            integration_strategy: í†µí•© ì „ëµ
            context: í†µí•© ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            IntegrationResult: í†µí•© ê²°ê³¼
        """
        start_time = time.time()
        integration_id = f"integration_{int(start_time)}"
        
        logger.info(f"ğŸ”„ Starting result integration: {integration_id}")
        
        try:
            # 1. ì—ì´ì „íŠ¸ ê²°ê³¼ ì •ê·œí™”
            normalized_results = await self._normalize_agent_results(execution_result)
            
            # 2. êµì°¨ ê²€ì¦ ìˆ˜í–‰
            cross_validation = await self._perform_cross_validation(normalized_results)
            
            # 3. ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
            insights = await self._extract_integrated_insights(
                normalized_results, cross_validation, integration_strategy
            )
            
            # 4. í’ˆì§ˆ í‰ê°€
            quality_assessment = await self._assess_integration_quality(
                normalized_results, insights, cross_validation
            )
            
            # 5. ì¢…í•© ë³´ê³ ì„œ ìƒì„±
            synthesis_report = await self._generate_synthesis_report(
                normalized_results, insights, cross_validation, quality_assessment
            )
            
            # 6. ì¶”ì²œì‚¬í•­ ìƒì„±
            recommendations = await self._generate_recommendations(
                insights, quality_assessment, execution_result
            )
            
            # 7. ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            confidence_score = self._calculate_overall_confidence(
                normalized_results, cross_validation, quality_assessment
            )
            
            integration_time = time.time() - start_time
            
            result = IntegrationResult(
                integration_id=integration_id,
                strategy=integration_strategy,
                agent_results=normalized_results,
                cross_validation=cross_validation,
                integrated_insights=insights,
                quality_assessment=quality_assessment,
                synthesis_report=synthesis_report,
                recommendations=recommendations,
                confidence_score=confidence_score,
                integration_time=integration_time,
                metadata=context or {}
            )
            
            self.integration_history.append(result)
            
            logger.info(f"âœ… Integration completed: {integration_id} ({integration_time:.2f}s)")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Integration failed: {integration_id} - {e}")
            raise
    
    async def _normalize_agent_results(self, execution_result: ExecutionResult) -> List[AgentResult]:
        """ì—ì´ì „íŠ¸ ê²°ê³¼ ì •ê·œí™”"""
        normalized_results = []
        
        for task_result in execution_result.task_results:
            if task_result.get("status") == "completed" and task_result.get("result"):
                try:
                    # ê²°ê³¼ íƒ€ì… ì¶”ë¡ 
                    result_type = self._infer_result_type(task_result)
                    
                    # ì—ì´ì „íŠ¸ ê²°ê³¼ ê°ì²´ ìƒì„±
                    agent_result = AgentResult(
                        agent_name=task_result["agent_name"],
                        agent_type=task_result["agent_type"],
                        result_type=result_type,
                        content=task_result["result"],
                        confidence=self._extract_confidence(task_result),
                        execution_time=task_result.get("execution_time", 0),
                        timestamp=datetime.now(),
                        metadata=task_result
                    )
                    
                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    agent_result.quality_scores = await self._calculate_quality_scores(agent_result)
                    
                    normalized_results.append(agent_result)
                    
                except Exception as e:
                    logger.warning(f"Failed to normalize result from {task_result.get('agent_name')}: {e}")
        
        logger.info(f"ğŸ“Š Normalized {len(normalized_results)} agent results")
        return normalized_results
    
    def _infer_result_type(self, task_result: Dict[str, Any]) -> ResultType:
        """ê²°ê³¼ íƒ€ì… ì¶”ë¡ """
        agent_type = task_result.get("agent_type", "").lower()
        
        if "visualization" in agent_type:
            return ResultType.VISUALIZATION
        elif "eda" in agent_type or "analysis" in agent_type:
            return ResultType.DATA_ANALYSIS
        elif "ml" in agent_type or "model" in agent_type:
            return ResultType.PREDICTION
        elif "cleaning" in agent_type:
            return ResultType.DATA_ANALYSIS
        else:
            return ResultType.INSIGHT
    
    def _extract_confidence(self, task_result: Dict[str, Any]) -> float:
        """ì‹ ë¢°ë„ ì¶”ì¶œ"""
        result = task_result.get("result", {})
        
        # ë‹¤ì–‘í•œ ì‹ ë¢°ë„ í•„ë“œ í™•ì¸
        for field in ["confidence", "confidence_score", "reliability", "certainty"]:
            if field in result:
                return float(result[field])
        
        # ê¸°ë³¸ ì‹ ë¢°ë„ (ì„±ê³µ ì‹œ 0.8)
        return 0.8 if task_result.get("status") == "completed" else 0.0
    
    async def _calculate_quality_scores(self, agent_result: AgentResult) -> Dict[QualityMetric, float]:
        """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        prompt = f"""ë‹¤ìŒ ì—ì´ì „íŠ¸ ê²°ê³¼ì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”:

**ì—ì´ì „íŠ¸ ì •ë³´**:
- ì´ë¦„: {agent_result.agent_name}
- íƒ€ì…: {agent_result.agent_type}
- ê²°ê³¼ íƒ€ì…: {agent_result.result_type.value}
- ì‹ ë¢°ë„: {agent_result.confidence:.2f}

**ê²°ê³¼ ë‚´ìš©**:
{json.dumps(agent_result.content, ensure_ascii=False, indent=2)}

**í’ˆì§ˆ í‰ê°€ ê¸°ì¤€**:
1. completeness (ì™„ì„±ë„): ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ì™„ì „í•œê°€?
2. consistency (ì¼ê´€ì„±): ë‚´ë¶€ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ìˆëŠ”ê°€?
3. accuracy (ì •í™•ì„±): ê²°ê³¼ê°€ ì •í™•í•´ ë³´ì´ëŠ”ê°€?
4. relevance (ê´€ë ¨ì„±): ìš”ì²­ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ê°€?
5. clarity (ëª…í™•ì„±): ê²°ê³¼ê°€ ëª…í™•í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
6. actionability (ì‹¤í–‰ê°€ëŠ¥ì„±): ì‹¤ì œë¡œ í™œìš© ê°€ëŠ¥í•œê°€?

ê° í’ˆì§ˆ ì§€í‘œë¥¼ 0.0-1.0 ì‚¬ì´ì˜ ì ìˆ˜ë¡œ í‰ê°€í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "completeness": 0.8,
  "consistency": 0.9,
  "accuracy": 0.7,
  "relevance": 0.8,
  "clarity": 0.9,
  "actionability": 0.6
}}"""

        try:
            response = await self.llm.ainvoke(prompt)
            content = response.content.strip()
            
            # JSON ì¶”ì¶œ
            if content.startswith('{') and content.endswith('}'):
                scores_dict = json.loads(content)
            else:
                import re
                json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
                if json_match:
                    scores_dict = json.loads(json_match.group(1))
                else:
                    scores_dict = {}
            
            # QualityMetric enumìœ¼ë¡œ ë³€í™˜
            quality_scores = {}
            for metric in QualityMetric:
                score = scores_dict.get(metric.value, 0.5)  # ê¸°ë³¸ê°’ 0.5
                quality_scores[metric] = max(0.0, min(1.0, float(score)))
            
            return quality_scores
            
        except Exception as e:
            logger.warning(f"Quality score calculation failed: {e}")
            # ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜
            return {metric: 0.5 for metric in QualityMetric}
    
    async def _perform_cross_validation(self, agent_results: List[AgentResult]) -> CrossValidationResult:
        """êµì°¨ ê²€ì¦ ìˆ˜í–‰"""
        
        if len(agent_results) < 2:
            return CrossValidationResult(
                consistency_score=1.0,
                conflicting_findings=[],
                supporting_evidence=[],
                validation_notes="ë‹¨ì¼ ì—ì´ì „íŠ¸ ê²°ê³¼ë¡œ êµì°¨ ê²€ì¦ ë¶ˆê°€",
                confidence_adjustment=0.0
            )
        
        # ê²°ê³¼ ê°„ ì¼ê´€ì„± ë¶„ì„
        consistency_prompt = f"""ë‹¤ìŒ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ê²°ê³¼ë“¤ì˜ ì¼ê´€ì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:

**ì—ì´ì „íŠ¸ ê²°ê³¼ë“¤**:
{json.dumps([{
    'agent': r.agent_name,
    'type': r.agent_type,
    'content': r.content,
    'confidence': r.confidence
} for r in agent_results], ensure_ascii=False, indent=2)}

**ë¶„ì„ ìš”êµ¬ì‚¬í•­**:
1. ê²°ê³¼ ê°„ ì¼ê´€ì„± í‰ê°€ (0.0-1.0)
2. ì¶©ëŒí•˜ëŠ” ë°œê²¬ì‚¬í•­ ì‹ë³„
3. ì„œë¡œ ì§€ì§€í•˜ëŠ” ì¦ê±° ì‹ë³„
4. êµì°¨ ê²€ì¦ ë…¸íŠ¸ ì‘ì„±
5. ì‹ ë¢°ë„ ì¡°ì • ê¶Œê³  (-0.2 ~ +0.2)

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "consistency_score": 0.8,
  "conflicting_findings": [
    {{"description": "ì—ì´ì „íŠ¸ AëŠ” Xë¼ê³  í–ˆì§€ë§Œ ì—ì´ì „íŠ¸ BëŠ” Yë¼ê³  í•¨", "severity": "medium"}}
  ],
  "supporting_evidence": [
    {{"description": "ì—ì´ì „íŠ¸ Aì™€ B ëª¨ë‘ Zë¥¼ í™•ì¸í•¨", "strength": "high"}}
  ],
  "validation_notes": "ì „ë°˜ì ìœ¼ë¡œ ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼...",
  "confidence_adjustment": 0.1
}}"""

        try:
            response = await self.llm.ainvoke(consistency_prompt)
            content = response.content.strip()
            
            # JSON ì¶”ì¶œ
            if content.startswith('{') and content.endswith('}'):
                validation_dict = json.loads(content)
            else:
                import re
                json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
                if json_match:
                    validation_dict = json.loads(json_match.group(1))
                else:
                    validation_dict = {}
            
            return CrossValidationResult(
                consistency_score=validation_dict.get("consistency_score", 0.5),
                conflicting_findings=validation_dict.get("conflicting_findings", []),
                supporting_evidence=validation_dict.get("supporting_evidence", []),
                validation_notes=validation_dict.get("validation_notes", "êµì°¨ ê²€ì¦ ì™„ë£Œ"),
                confidence_adjustment=validation_dict.get("confidence_adjustment", 0.0)
            )
            
        except Exception as e:
            logger.warning(f"Cross validation failed: {e}")
            return CrossValidationResult(
                consistency_score=0.5,
                conflicting_findings=[],
                supporting_evidence=[],
                validation_notes=f"êµì°¨ ê²€ì¦ ì˜¤ë¥˜: {str(e)}",
                confidence_adjustment=0.0
            )
    
    async def _extract_integrated_insights(
        self,
        agent_results: List[AgentResult],
        cross_validation: CrossValidationResult,
        strategy: IntegrationStrategy
    ) -> List[IntegratedInsight]:
        """í†µí•© ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        
        insights_prompt = f"""ë‹¤ìŒ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í†µí•© ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”:

**ì—ì´ì „íŠ¸ ê²°ê³¼ë“¤**:
{json.dumps([{
    'agent': r.agent_name,
    'type': r.agent_type,
    'content': r.content,
    'confidence': r.confidence,
    'quality_scores': {k.value: v for k, v in r.quality_scores.items()}
} for r in agent_results], ensure_ascii=False, indent=2)}

**êµì°¨ ê²€ì¦ ê²°ê³¼**:
- ì¼ê´€ì„± ì ìˆ˜: {cross_validation.consistency_score:.2f}
- ì§€ì§€ ì¦ê±°: {len(cross_validation.supporting_evidence)}ê°œ
- ì¶©ëŒ ë°œê²¬: {len(cross_validation.conflicting_findings)}ê°œ

**í†µí•© ì „ëµ**: {strategy.value}

**ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ìš”êµ¬ì‚¬í•­**:
1. ê° ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
2. ì¸ì‚¬ì´íŠ¸ë³„ ì‹ ë¢°ë„ í‰ê°€
3. ì‹¤í–‰ ê°€ëŠ¥í•œ í•­ëª© ì‹ë³„
4. ìš°ì„ ìˆœìœ„ ì„¤ì •

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
  "insights": [
    {{
      "insight_type": "data_quality",
      "content": "ë°ì´í„° í’ˆì§ˆì´ ì „ë°˜ì ìœ¼ë¡œ ì–‘í˜¸í•˜ë©°...",
      "confidence": 0.9,
      "supporting_agents": ["DataCleaningAgent", "EDAAgent"],
      "evidence_strength": 0.8,
      "actionable_items": ["ë°ì´í„° ì •ë¦¬ í”„ë¡œì„¸ìŠ¤ ìœ ì§€", "í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ê°•í™”"],
      "priority": 1
    }}
  ]
}}"""

        try:
            response = await self.llm.ainvoke(insights_prompt)
            content = response.content.strip()
            
            # JSON ì¶”ì¶œ
            if content.startswith('{') and content.endswith('}'):
                insights_dict = json.loads(content)
            else:
                import re
                json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
                if json_match:
                    insights_dict = json.loads(json_match.group(1))
                else:
                    insights_dict = {"insights": []}
            
            # IntegratedInsight ê°ì²´ë¡œ ë³€í™˜
            insights = []
            for insight_data in insights_dict.get("insights", []):
                insight = IntegratedInsight(
                    insight_type=insight_data.get("insight_type", "general"),
                    content=insight_data.get("content", ""),
                    confidence=insight_data.get("confidence", 0.5),
                    supporting_agents=insight_data.get("supporting_agents", []),
                    evidence_strength=insight_data.get("evidence_strength", 0.5),
                    actionable_items=insight_data.get("actionable_items", []),
                    priority=insight_data.get("priority", 5)
                )
                insights.append(insight)
            
            # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
            insights.sort(key=lambda x: x.priority)
            
            logger.info(f"ğŸ“ Extracted {len(insights)} integrated insights")
            return insights
            
        except Exception as e:
            logger.warning(f"Insight extraction failed: {e}")
            return []
    
    async def _assess_integration_quality(
        self,
        agent_results: List[AgentResult],
        insights: List[IntegratedInsight],
        cross_validation: CrossValidationResult
    ) -> Dict[QualityMetric, float]:
        """í†µí•© í’ˆì§ˆ í‰ê°€"""
        
        quality_assessment = {}
        
        # ì™„ì„±ë„: ì—ì´ì „íŠ¸ ê²°ê³¼ ìˆ˜ì™€ ì¸ì‚¬ì´íŠ¸ ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í‰ê°€
        completeness = min(1.0, len(agent_results) / 3.0) * 0.5 + min(1.0, len(insights) / 3.0) * 0.5
        quality_assessment[QualityMetric.COMPLETENESS] = completeness
        
        # ì¼ê´€ì„±: êµì°¨ ê²€ì¦ ê²°ê³¼ í™œìš©
        quality_assessment[QualityMetric.CONSISTENCY] = cross_validation.consistency_score
        
        # ì •í™•ì„±: ì—ì´ì „íŠ¸ ê²°ê³¼ë“¤ì˜ í‰ê·  ì‹ ë¢°ë„
        if agent_results:
            accuracy = sum(r.confidence for r in agent_results) / len(agent_results)
            quality_assessment[QualityMetric.ACCURACY] = accuracy
        else:
            quality_assessment[QualityMetric.ACCURACY] = 0.0
        
        # ê´€ë ¨ì„±: ì—ì´ì „íŠ¸ ê²°ê³¼ë“¤ì˜ ê´€ë ¨ì„± ì ìˆ˜ í‰ê· 
        relevance_scores = []
        for result in agent_results:
            if QualityMetric.RELEVANCE in result.quality_scores:
                relevance_scores.append(result.quality_scores[QualityMetric.RELEVANCE])
        
        if relevance_scores:
            quality_assessment[QualityMetric.RELEVANCE] = sum(relevance_scores) / len(relevance_scores)
        else:
            quality_assessment[QualityMetric.RELEVANCE] = 0.5
        
        # ëª…í™•ì„±: ì¸ì‚¬ì´íŠ¸ì˜ ëª…í™•ì„± í‰ê°€
        if insights:
            clarity = sum(i.confidence for i in insights) / len(insights)
            quality_assessment[QualityMetric.CLARITY] = clarity
        else:
            quality_assessment[QualityMetric.CLARITY] = 0.0
        
        # ì‹¤í–‰ê°€ëŠ¥ì„±: ì‹¤í–‰ ê°€ëŠ¥í•œ í•­ëª©ì˜ ë¹„ìœ¨
        total_actionable = sum(len(i.actionable_items) for i in insights)
        actionability = min(1.0, total_actionable / max(1, len(insights) * 2))
        quality_assessment[QualityMetric.ACTIONABILITY] = actionability
        
        return quality_assessment
    
    async def _generate_synthesis_report(
        self,
        agent_results: List[AgentResult],
        insights: List[IntegratedInsight],
        cross_validation: CrossValidationResult,
        quality_assessment: Dict[QualityMetric, float]
    ) -> str:
        """ì¢…í•© ë³´ê³ ì„œ ìƒì„±"""
        
        report_prompt = f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

**ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ ìš”ì•½**:
- ì´ {len(agent_results)}ê°œ ì—ì´ì „íŠ¸ ê²°ê³¼
- í‰ê·  ì‹ ë¢°ë„: {sum(r.confidence for r in agent_results) / len(agent_results):.2f}
- ì£¼ìš” ê²°ê³¼ íƒ€ì…: {', '.join(set(r.result_type.value for r in agent_results))}

**í†µí•© ì¸ì‚¬ì´íŠ¸**: {len(insights)}ê°œ
{chr(10).join(f"- {i.insight_type}: {i.content[:100]}..." for i in insights[:5])}

**êµì°¨ ê²€ì¦ ê²°ê³¼**:
- ì¼ê´€ì„±: {cross_validation.consistency_score:.2f}
- ì§€ì§€ ì¦ê±°: {len(cross_validation.supporting_evidence)}ê°œ
- ì¶©ëŒ ë°œê²¬: {len(cross_validation.conflicting_findings)}ê°œ

**í’ˆì§ˆ í‰ê°€**:
{chr(10).join(f"- {k.value}: {v:.2f}" for k, v in quality_assessment.items())}

**ë³´ê³ ì„œ êµ¬ì„± ìš”êµ¬ì‚¬í•­**:
1. ì‹¤í–‰ ê°œìš” (3-4ë¬¸ì¥)
2. ì£¼ìš” ë°œê²¬ì‚¬í•­ (5-8ê°œ í•­ëª©)
3. í’ˆì§ˆ ë° ì‹ ë¢°ë„ í‰ê°€
4. ì œí•œì‚¬í•­ ë° ê³ ë ¤ì‚¬í•­
5. ê²°ë¡  ë° ë‹¤ìŒ ë‹¨ê³„

ì²´ê³„ì ì´ê³  ì „ë¬¸ì ì¸ í•œêµ­ì–´ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”."""

        try:
            response = await self.llm.ainvoke(report_prompt)
            return response.content.strip()
        except Exception as e:
            logger.warning(f"Report generation failed: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì˜¤ë¥˜: {str(e)}"
    
    async def _generate_recommendations(
        self,
        insights: List[IntegratedInsight],
        quality_assessment: Dict[QualityMetric, float],
        execution_result: ExecutionResult
    ) -> List[str]:
        """ì¶”ì²œì‚¬í•­ ìƒì„±"""
        
        recommendations_prompt = f"""ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ì¶”ì²œì‚¬í•­ì„ ì œì•ˆí•´ì£¼ì„¸ìš”:

**í†µí•© ì¸ì‚¬ì´íŠ¸**:
{json.dumps([{
    'type': i.insight_type,
    'content': i.content,
    'confidence': i.confidence,
    'actionable_items': i.actionable_items,
    'priority': i.priority
} for i in insights], ensure_ascii=False, indent=2)}

**í’ˆì§ˆ í‰ê°€**:
{json.dumps({k.value: v for k, v in quality_assessment.items()}, ensure_ascii=False, indent=2)}

**ì‹¤í–‰ ê²°ê³¼**:
- ì´ ì‹¤í–‰ ì‹œê°„: {execution_result.execution_time:.2f}ì´ˆ
- ì™„ë£Œ íƒœìŠ¤í¬: {execution_result.completed_tasks}/{execution_result.total_tasks}
- ì „ì²´ ì‹ ë¢°ë„: {execution_result.confidence_score:.2f}

**ì¶”ì²œì‚¬í•­ ìœ í˜•**:
1. ì¦‰ì‹œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì¹˜
2. ë‹¨ê¸° ê°œì„  ë°©ì•ˆ (1-2ì£¼)
3. ì¤‘ê¸° ì „ëµ (1-3ê°œì›”)
4. ì¥ê¸° ê³„íš (3ê°œì›” ì´ìƒ)
5. ë¦¬ìŠ¤í¬ ê´€ë¦¬ ë°©ì•ˆ

êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¶”ì²œì‚¬í•­ì„ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”."""

        try:
            response = await self.llm.ainvoke(recommendations_prompt)
            content = response.content.strip()
            
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ íŒŒì‹±
            recommendations = []
            lines = content.split('\n')
            for line in lines:
                line = line.strip()
                if line and (line.startswith('-') or line.startswith('â€¢') or line.startswith('*')):
                    recommendations.append(line[1:].strip())
                elif line and any(char.isdigit() for char in line[:3]):
                    # ë²ˆí˜¸ê°€ ìˆëŠ” í•­ëª©
                    recommendations.append(line.split('.', 1)[-1].strip())
            
            return recommendations if recommendations else [content]
            
        except Exception as e:
            logger.warning(f"Recommendations generation failed: {e}")
            return [f"ì¶”ì²œì‚¬í•­ ìƒì„± ì˜¤ë¥˜: {str(e)}"]
    
    def _calculate_overall_confidence(
        self,
        agent_results: List[AgentResult],
        cross_validation: CrossValidationResult,
        quality_assessment: Dict[QualityMetric, float]
    ) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°"""
        
        if not agent_results:
            return 0.0
        
        # ê¸°ë³¸ ì‹ ë¢°ë„: ì—ì´ì „íŠ¸ ê²°ê³¼ë“¤ì˜ í‰ê·  ì‹ ë¢°ë„
        base_confidence = sum(r.confidence for r in agent_results) / len(agent_results)
        
        # êµì°¨ ê²€ì¦ ì¡°ì •
        consistency_adjustment = (cross_validation.consistency_score - 0.5) * 0.2
        validation_adjustment = cross_validation.confidence_adjustment
        
        # í’ˆì§ˆ í‰ê°€ ì¡°ì •
        quality_score = sum(quality_assessment.values()) / len(quality_assessment)
        quality_adjustment = (quality_score - 0.5) * 0.1
        
        # ì—ì´ì „íŠ¸ ìˆ˜ ë³´ì • (ë” ë§ì€ ì—ì´ì „íŠ¸ = ë” ë†’ì€ ì‹ ë¢°ë„)
        agent_count_bonus = min(0.1, len(agent_results) * 0.02)
        
        final_confidence = base_confidence + consistency_adjustment + validation_adjustment + quality_adjustment + agent_count_bonus
        
        return max(0.0, min(1.0, final_confidence))
    
    async def get_integration_history(self) -> List[IntegrationResult]:
        """í†µí•© ì´ë ¥ ì¡°íšŒ"""
        return self.integration_history.copy()
    
    async def get_integration_summary(self, integration_id: str) -> Optional[Dict[str, Any]]:
        """í†µí•© ìš”ì•½ ì¡°íšŒ"""
        for result in self.integration_history:
            if result.integration_id == integration_id:
                return {
                    "integration_id": result.integration_id,
                    "strategy": result.strategy.value,
                    "agent_count": len(result.agent_results),
                    "insight_count": len(result.integrated_insights),
                    "confidence_score": result.confidence_score,
                    "integration_time": result.integration_time,
                    "quality_scores": {k.value: v for k, v in result.quality_assessment.items()}
                }
        return None 