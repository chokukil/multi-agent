"""
Orchestration Engine - A2A ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ í•µì‹¬ ì—”ì§„

Phase 1, 2, 4, 5 í†µí•© êµ¬í˜„:
- LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ê³„íš ìƒì„± (Phase 5) - Rule ê¸°ë°˜ ì œê±°, ì™„ì „ LLM ì˜ì¡´
- ì‹¤ì‹œê°„ ì‹¤í–‰ ë° ëª¨ë‹ˆí„°ë§ (Phase 1)
- ë©€í‹°ëª¨ë‹¬ ì•„í‹°íŒ©íŠ¸ ì²˜ë¦¬ (Phase 2)
- ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œë„ ë¡œì§ (Phase 4)
"""

import asyncio
import httpx
import json
import time
from typing import Dict, List, Optional, Any
import logging

from .a2a_task_executor import task_executor, ExecutionPlan

logger = logging.getLogger(__name__)

class OrchestrationEngine:
    """A2A ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì—”ì§„ - ì™„ì „ LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ê³„íš"""
    
    def __init__(self):
        self.orchestrator_url = "http://localhost:8100"
        self.client_timeout = httpx.Timeout(30.0, connect=10.0)
        
        # Phase 5: ì§€ëŠ¥í˜• ê³„íš ìƒì„±ê¸° í†µí•© (LLM ê¸°ë°˜)
        from .intelligent_planner import intelligent_planner
        self.intelligent_planner = intelligent_planner
        
        # Phase 4: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í†µí•©
        try:
            from .performance_monitor import performance_monitor
            self.performance_monitor = performance_monitor
        except ImportError:
            self.performance_monitor = None
    
    async def process_query_with_orchestration(
        self, 
        prompt: str, 
        available_agents: Dict,
        data_context: Optional[Dict] = None,
        progress_callback=None
    ) -> Dict[str, Any]:
        """
        ì¿¼ë¦¬ë¥¼ ì™„ì „ LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ìœ¼ë¡œ ì²˜ë¦¬
        
        Args:
            prompt: ì‚¬ìš©ì ìš”ì²­
            available_agents: ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ ì •ë³´
            data_context: ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
            progress_callback: ì§„í–‰ ìƒí™© ì½œë°±
            
        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        try:
            # Phase 4: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œì‘
            orchestration_id = f"orch_{int(time.time())}"
            
            # 1ë‹¨ê³„: Phase 5 ì™„ì „ LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ê³„íš ìƒì„±
            if progress_callback:
                progress_callback("ğŸ§  LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ê³„íš ìƒì„± ì¤‘...")
            
            # LLMì˜ ëŠ¥ë ¥ì„ ìµœëŒ€í•œ í™œìš©í•œ ê³„íš ìƒì„± (Rule ê¸°ë°˜ ì™„ì „ ì œê±°)
            plan_result = await self.intelligent_planner.generate_context_aware_plan(
                user_query=prompt,
                data_context=data_context,
                available_agents=available_agents,
                execution_history=getattr(self, '_execution_history', [])
            )
            
            if plan_result.get("error"):
                logger.warning(f"âš ï¸ ì§€ëŠ¥í˜• ê³„íš ìƒì„± ì‹¤íŒ¨: {plan_result['error']}")
                # LLM ê¸°ë°˜ ì§ì ‘ ê³„íš ìƒì„±ìœ¼ë¡œ í´ë°± (Rule ê¸°ë°˜ ì•„ë‹˜)
                plan_result = await self._generate_llm_fallback_plan(prompt, available_agents, data_context)
                
                if plan_result.get("error"):
                    return {
                        "status": "failed",
                        "error": f"ëª¨ë“  ê³„íš ìƒì„± ë°©ë²• ì‹¤íŒ¨: {plan_result['error']}",
                        "stage": "planning"
                    }
            
            logger.info(f"âœ… LLM ì§€ëŠ¥í˜• ê³„íš ìƒì„± ì„±ê³µ (ì‹ ë¢°ë„: {plan_result.get('confidence_score', 'N/A')})")
            
            # 2ë‹¨ê³„: ì‹¤í–‰ ê³„íš ê°ì²´ ìƒì„±
            execution_plan = ExecutionPlan(
                objective=plan_result.get("objective", prompt),
                reasoning=plan_result.get("reasoning", ""),
                steps=plan_result.get("steps", []),
                selected_agents=plan_result.get("selected_agents", [])
            )
            
            # 3ë‹¨ê³„: ì‹¤ì œ ì‹¤í–‰ (Phase 1, 4 í†µí•©)
            if progress_callback:
                progress_callback("ğŸš€ LLM ê³„íš ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì‹¤í–‰ ì‹œì‘...")
            
            execution_result = await task_executor.execute_orchestration_plan(
                execution_plan,
                data_context=data_context,
                progress_callback=progress_callback
            )
            
            # 4ë‹¨ê³„: Phase 5 ì‹¤í–‰ ì´ë ¥ í•™ìŠµ (LLMì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡)
            self._update_execution_history({
                "prompt": prompt,
                "plan": plan_result,
                "execution_result": execution_result,
                "timestamp": time.time(),
                "success": execution_result.get("status") == "completed"
            })
            
            # 5ë‹¨ê³„: ê²°ê³¼ í†µí•© (Phase 2 ì•„í‹°íŒ©íŠ¸ ì²˜ë¦¬ í¬í•¨)
            final_result = {
                **execution_result,
                "plan": plan_result,
                "query": prompt,
                "orchestration_id": orchestration_id,
                "planning_method": "intelligent_llm",
                "llm_confidence": plan_result.get("confidence_score"),
                "llm_reasoning": plan_result.get("reasoning"),
                "adaptation_notes": plan_result.get("adaptation_notes")
            }
            
            # Phase 4: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
            if self.performance_monitor:
                self.performance_monitor._add_metric(
                    "orchestration_total_time", 
                    execution_result.get("execution_time", 0), 
                    "seconds"
                )
                self.performance_monitor._add_metric(
                    "llm_plan_confidence", 
                    plan_result.get("confidence_score", 0), 
                    "score"
                )
            
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            
            # Phase 4: ì—ëŸ¬ ë©”íŠ¸ë¦­ ê¸°ë¡
            if self.performance_monitor:
                self.performance_monitor._add_metric("orchestration_error", 1, "count")
            
            return {
                "status": "failed",
                "error": str(e),
                "stage": "execution"
            }
    
    async def _generate_llm_fallback_plan(
        self, 
        prompt: str, 
        available_agents: Dict, 
        data_context: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """
        LLM ê¸°ë°˜ í´ë°± ê³„íš ìƒì„± (Rule ê¸°ë°˜ ì•„ë‹˜, ë” ê°„ë‹¨í•œ LLM í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
        """
        try:
            # ë” ê°„ë‹¨í•˜ê³  ì§ì ‘ì ì¸ LLM í”„ë¡¬í”„íŠ¸
            fallback_prompt = self._create_simple_llm_prompt(prompt, available_agents, data_context)
            
            # LLMì—ê²Œ ì§ì ‘ ìš”ì²­
            llm_response = await self._query_llm_directly(fallback_prompt)
            
            # ì‘ë‹µ íŒŒì‹±
            parsed_plan = self._parse_simple_llm_response(llm_response, available_agents)
            
            return parsed_plan
            
        except Exception as e:
            logger.error(f"âŒ LLM í´ë°± ê³„íš ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "error": f"LLM í´ë°± ì‹¤íŒ¨: {str(e)}"
            }
    
    def _create_simple_llm_prompt(
        self, 
        prompt: str, 
        available_agents: Dict, 
        data_context: Optional[Dict] = None
    ) -> str:
        """ê°„ë‹¨í•œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„± (í´ë°±ìš©)"""
        
        agent_list = "\n".join([
            f"- {name}: {info.get('description', 'AI ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì—ì´ì „íŠ¸')}"
            for name, info in available_agents.items()
            if info.get('status') == 'available'
        ])
        
        data_info = ""
        if data_context:
            data_info = f"\në°ì´í„° ì •ë³´: {data_context.get('dataset_info', 'Unknown')}"
            if data_context.get('columns'):
                data_info += f"\nì£¼ìš” ì»¬ëŸ¼: {', '.join(data_context['columns'][:5])}"
        
        return f"""
ì‚¬ìš©ì ìš”ì²­: {prompt}{data_info}

ì‚¬ìš© ê°€ëŠ¥í•œ AI ì—ì´ì „íŠ¸:
{agent_list}

ìœ„ ì—ì´ì „íŠ¸ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ì ìš”ì²­ì„ ì²˜ë¦¬í•  ë‹¨ê³„ë³„ ê³„íšì„ JSON í˜•ì‹ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

{{
    "objective": "ëª©í‘œ",
    "reasoning": "ì´ìœ ", 
    "steps": [
        {{"step_number": 1, "agent_name": "ì •í™•í•œ_ì—ì´ì „íŠ¸_ì´ë¦„", "task_description": "ì‘ì—…_ì„¤ëª…"}}
    ],
    "selected_agents": ["ì—ì´ì „íŠ¸_ëª©ë¡"],
    "confidence_score": 0.8
}}
"""
    
    async def _query_llm_directly(self, prompt: str) -> str:
        """LLMì—ê²Œ ì§ì ‘ ì¿¼ë¦¬"""
        try:
            message_id = f"fallback_plan_{int(time.time())}"
            payload = {
                "jsonrpc": "2.0",
                "method": "message/send",
                "params": {
                    "message": {
                        "messageId": message_id,
                        "role": "user",
                        "parts": [
                            {
                                "type": "text",
                                "text": prompt
                            }
                        ]
                    }
                },
                "id": 1
            }
            
            async with httpx.AsyncClient(timeout=self.client_timeout) as client:
                response = await client.post(
                    self.orchestrator_url,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    if "result" in result and "parts" in result["result"]:
                        for part in result["result"]["parts"]:
                            if part.get("type") == "text":
                                return part.get("text", "")
                
                raise Exception(f"LLM ì‘ë‹µ ì˜¤ë¥˜: HTTP {response.status_code}")
                
        except Exception as e:
            logger.error(f"âŒ LLM ì§ì ‘ ì¿¼ë¦¬ ì‹¤íŒ¨: {e}")
            raise e
    
    def _parse_simple_llm_response(self, llm_response: str, available_agents: Dict) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ LLM ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ
            json_start = llm_response.find('{')
            json_end = llm_response.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = llm_response[json_start:json_end]
                plan_data = json.loads(json_str)
                
                # ê¸°ë³¸ ê²€ì¦ë§Œ (LLMì„ ì‹ ë¢°)
                if 'steps' not in plan_data:
                    plan_data['steps'] = []
                if 'selected_agents' not in plan_data:
                    plan_data['selected_agents'] = []
                
                # ì—ì´ì „íŠ¸ ìœ íš¨ì„±ë§Œ ê°„ë‹¨íˆ ì²´í¬
                valid_steps = []
                for step in plan_data.get('steps', []):
                    agent_name = step.get('agent_name')
                    if agent_name in available_agents:
                        valid_steps.append(step)
                    else:
                        # ì²« ë²ˆì§¸ ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë¡œ ëŒ€ì²´
                        available_names = [
                            name for name, info in available_agents.items()
                            if info.get('status') == 'available'
                        ]
                        if available_names:
                            step_copy = step.copy()
                            step_copy['agent_name'] = available_names[0]
                            valid_steps.append(step_copy)
                
                plan_data['steps'] = valid_steps
                plan_data['selected_agents'] = [step['agent_name'] for step in valid_steps]
                
                return plan_data
            
            raise ValueError("JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
        except Exception as e:
            logger.error(f"âŒ ê°„ë‹¨í•œ LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            raise Exception(f"LLM ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")
    
    def _update_execution_history(self, execution_data: Dict):
        """Phase 5: ì‹¤í–‰ ì´ë ¥ ì—…ë°ì´íŠ¸ (LLM í•™ìŠµìš©)"""
        if not hasattr(self, '_execution_history'):
            self._execution_history = []
        
        self._execution_history.append(execution_data)
        
        # ìµœê·¼ 30ê°œë§Œ ìœ ì§€ (LLMì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì ì ˆí•œ ì–‘)
        self._execution_history = self._execution_history[-30:]
        
        # ì§€ëŠ¥í˜• ê³„íš ìƒì„±ê¸°ì—ë„ ì´ë ¥ ì „ë‹¬ (LLM í•™ìŠµ ê°•í™”)
        self.intelligent_planner._update_execution_history([execution_data])

# ê¸€ë¡œë²Œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤
orchestration_engine = OrchestrationEngine()
