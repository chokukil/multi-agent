#!/usr/bin/env python3
"""
ğŸ’ CherryAI E2E ì¢…í•© í…ŒìŠ¤íŠ¸ (Python ìë™í™”)

Playwright MCP ëŒ€ì•ˆìœ¼ë¡œ Pythonì„ ì‚¬ìš©í•œ ì™„ì „í•œ E2E í…ŒìŠ¤íŠ¸
- ë°ì´í„° ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜
- A2A ì—ì´ì „íŠ¸ í†µì‹  í…ŒìŠ¤íŠ¸  
- ë‹µë³€ í’ˆì§ˆ í‰ê°€
- LLM First ì›ì¹™ ê²€ì¦
"""

import asyncio
import httpx
import json
import pandas as pd
import numpy as np
import time
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

logger = logging.getLogger(__name__)

class E2ETestResult:
    """E2E í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    def __init__(self):
        self.test_results = []
        self.performance_metrics = {}
        self.quality_scores = {}
        self.errors = []
        
    def add_test_result(self, test_name: str, success: bool, details: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ê°€"""
        self.test_results.append({
            'test_name': test_name,
            'success': success,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })
        
    def add_performance_metric(self, metric_name: str, value: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ê°€"""
        self.performance_metrics[metric_name] = value
        
    def add_quality_score(self, category: str, score: float, max_score: float = 100):
        """í’ˆì§ˆ ì ìˆ˜ ì¶”ê°€"""
        self.quality_scores[category] = {
            'score': score,
            'max_score': max_score,
            'percentage': (score / max_score) * 100
        }

class CherryAI_E2E_Tester:
    """CherryAI E2E í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.base_url = "http://localhost:8501"
        self.a2a_orchestrator_url = "http://localhost:8100"
        self.test_data_dir = Path("test_datasets")
        self.results = E2ETestResult()
        
        # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
        self.test_scenarios = [
            {
                'name': 'Employee Classification Analysis',
                'file': 'classification_employees.csv',
                'queries': [
                    "ì´ ì§ì› ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ ìŠ¹ì§„ì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì¸ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”",
                    "ì„±ê³¼ ì ìˆ˜ì™€ ìŠ¹ì§„ ê°„ì˜ ê´€ê³„ë¥¼ ì‹œê°í™”í•´ì£¼ì„¸ìš”",
                    "ì–´ë–¤ ë¶€ì„œì—ì„œ ìŠ¹ì§„ë¥ ì´ ê°€ì¥ ë†’ì€ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”"
                ],
                'expected_elements': ['performance_score', 'department', 'education_level', 'promoted']
            },
            {
                'name': 'Housing Price Regression',
                'file': 'regression_housing.csv', 
                'queries': [
                    "ì£¼íƒ ê°€ê²©ì— ê°€ì¥ í° ì˜í–¥ì„ ì£¼ëŠ” ìš”ì¸ë“¤ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                    "ì£¼íƒ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
                    "ë°© ê°œìˆ˜ì™€ ê°€ê²©ì˜ ìƒê´€ê´€ê³„ë¥¼ ì‹œê°í™”í•´ì£¼ì„¸ìš”"
                ],
                'expected_elements': ['price', 'area_sqft', 'bedrooms', 'neighborhood_score']
            },
            {
                'name': 'Time Series Sales Analysis',
                'file': 'timeseries_sales.csv',
                'queries': [
                    "ë§¤ì¶œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ê³  ì‹œê°í™”í•´ì£¼ì„¸ìš”",
                    "ê³„ì ˆì„± íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”", 
                    "í–¥í›„ ë§¤ì¶œì„ ì˜ˆì¸¡í•´ì£¼ì„¸ìš”"
                ],
                'expected_elements': ['daily_sales', 'date', 'seasonal', 'trend']
            },
            {
                'name': 'IoT Sensor Anomaly Detection',
                'file': 'sensor_iot.csv',
                'queries': [
                    "ì„¼ì„œ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ íƒì§€í•´ì£¼ì„¸ìš”",
                    "ì˜¨ë„ì™€ ìŠµë„ì˜ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
                    "ì„¼ì„œë³„ ë°°í„°ë¦¬ ìˆ˜ì¤€ì„ ëª¨ë‹ˆí„°ë§í•´ì£¼ì„¸ìš”"
                ],
                'expected_elements': ['temperature', 'humidity', 'battery_level', 'anomaly']
            }
        ]
    
    async def run_comprehensive_e2e_test(self):
        """ì¢…í•© E2E í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ CherryAI E2E ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("="*60)
        
        # 1. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        await self._test_system_health()
        
        # 2. ë°ì´í„° ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸
        await self._test_data_upload_scenarios()
        
        # 3. ë°ì´í„° ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
        await self._test_analysis_scenarios()
        
        # 4. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸
        await self._test_realtime_streaming()
        
        # 5. ë‹µë³€ í’ˆì§ˆ í‰ê°€
        await self._evaluate_response_quality()
        
        # 6. ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        await self._collect_performance_metrics()
        
        # 7. ìµœì¢… ë³´ê³ ì„œ ìƒì„±
        self._generate_final_report()
        
        return self.results
    
    async def _test_system_health(self):
        """ì‹œìŠ¤í…œ ìƒíƒœ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ¥ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸...")
        
        # Streamlit ì•± ìƒíƒœ í™•ì¸
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(self.base_url, timeout=5.0)
                streamlit_healthy = response.status_code == 200
                
            print(f"âœ… Streamlit ì•±: {'ì •ìƒ' if streamlit_healthy else 'ì˜¤ë¥˜'}")
            
            self.results.add_test_result(
                "streamlit_health_check",
                streamlit_healthy,
                {"status_code": response.status_code if 'response' in locals() else 0}
            )
        except Exception as e:
            print(f"âŒ Streamlit ì•± ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            self.results.add_test_result("streamlit_health_check", False, {"error": str(e)})
        
        # A2A ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒíƒœ í™•ì¸
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(f"{self.a2a_orchestrator_url}/.well-known/agent.json", timeout=5.0)
                a2a_healthy = response.status_code == 200
                
            print(f"âœ… A2A ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°: {'ì •ìƒ' if a2a_healthy else 'ì˜¤ë¥˜'}")
            
            self.results.add_test_result(
                "a2a_orchestrator_health_check", 
                a2a_healthy,
                {"status_code": response.status_code if 'response' in locals() else 0}
            )
        except Exception as e:
            print(f"âŒ A2A ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            self.results.add_test_result("a2a_orchestrator_health_check", False, {"error": str(e)})
        
        # í†µí•© ë©”ì‹œì§€ ë¸Œë¡œì»¤ í…ŒìŠ¤íŠ¸
        try:
            from core.streaming.unified_message_broker import get_unified_message_broker
            
            broker = get_unified_message_broker()
            agent_count = len(broker.agents)
            broker_healthy = agent_count > 0
            
            print(f"âœ… í†µí•© ë©”ì‹œì§€ ë¸Œë¡œì»¤: {agent_count}ê°œ ì—ì´ì „íŠ¸ ë“±ë¡")
            
            self.results.add_test_result(
                "message_broker_health_check",
                broker_healthy,
                {"agent_count": agent_count}
            )
        except Exception as e:
            print(f"âŒ í†µí•© ë©”ì‹œì§€ ë¸Œë¡œì»¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.results.add_test_result("message_broker_health_check", False, {"error": str(e)})
    
    async def _test_data_upload_scenarios(self):
        """ë°ì´í„° ì—…ë¡œë“œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“ ë°ì´í„° ì—…ë¡œë“œ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸...")
        
        for scenario in self.test_scenarios:
            file_path = self.test_data_dir / scenario['file']
            
            if not file_path.exists():
                print(f"âŒ í…ŒìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ: {file_path}")
                self.results.add_test_result(
                    f"upload_{scenario['name']}", 
                    False, 
                    {"error": f"File not found: {file_path}"}
                )
                continue
            
            try:
                # íŒŒì¼ ë¡œë“œ ë° ê²€ì¦
                if file_path.suffix == '.csv':
                    df = pd.read_csv(file_path)
                elif file_path.suffix == '.xlsx':
                    df = pd.read_excel(file_path)
                else:
                    print(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_path.suffix}")
                    continue
                
                print(f"âœ… {scenario['name']}: {df.shape[0]}í–‰ x {df.shape[1]}ì—´ ë¡œë“œ ì„±ê³µ")
                
                # ì˜ˆìƒ ìš”ì†Œë“¤ì´ ë°ì´í„°ì— ìˆëŠ”ì§€ í™•ì¸
                expected_found = sum(1 for elem in scenario['expected_elements'] if elem in df.columns or elem in str(df.head()))
                coverage = expected_found / len(scenario['expected_elements'])
                
                self.results.add_test_result(
                    f"upload_{scenario['name']}",
                    True,
                    {
                        "rows": df.shape[0],
                        "columns": df.shape[1], 
                        "expected_coverage": coverage,
                        "columns_list": df.columns.tolist()
                    }
                )
                
                print(f"  ğŸ“Š ì˜ˆìƒ ìš”ì†Œ ì»¤ë²„ë¦¬ì§€: {coverage:.1%}")
                
            except Exception as e:
                print(f"âŒ {scenario['name']} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                self.results.add_test_result(f"upload_{scenario['name']}", False, {"error": str(e)})
    
    async def _test_analysis_scenarios(self):
        """ë°ì´í„° ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§  ë°ì´í„° ë¶„ì„ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸...")
        
        for scenario in self.test_scenarios:
            print(f"\nğŸ“Š ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}")
            
            for i, query in enumerate(scenario['queries']):
                print(f"  ğŸ” ì¿¼ë¦¬ {i+1}: {query}")
                
                start_time = time.time()
                try:
                    # A2A ì‹œìŠ¤í…œì„ í†µí•œ ë¶„ì„ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
                    analysis_result = await self._simulate_analysis_request(query, scenario)
                    
                    response_time = time.time() - start_time
                    
                    # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
                    quality_score = self._evaluate_query_response_quality(query, analysis_result, scenario)
                    
                    print(f"    âœ… ì‘ë‹µ ì‹œê°„: {response_time:.2f}ì´ˆ, í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100")
                    
                    self.results.add_test_result(
                        f"analysis_{scenario['name']}_query_{i+1}",
                        True,
                        {
                            "query": query,
                            "response_time": response_time,
                            "quality_score": quality_score,
                            "analysis_result": analysis_result
                        }
                    )
                    
                    self.results.add_performance_metric(f"response_time_query_{i+1}", response_time)
                    
                except Exception as e:
                    print(f"    âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    self.results.add_test_result(
                        f"analysis_{scenario['name']}_query_{i+1}",
                        False,
                        {"query": query, "error": str(e)}
                    )
    
    async def _simulate_analysis_request(self, query: str, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜"""
        
        # ì‹¤ì œ A2A ì‹œìŠ¤í…œ í†µì‹  ì‹œë®¬ë ˆì´ì…˜
        try:
            # UnifiedMessageBrokerë¥¼ í†µí•œ ìš”ì²­
            from core.streaming.unified_message_broker import get_unified_message_broker
            from core.streaming.unified_message_broker import UnifiedMessage, MessagePriority
            import uuid
            
            broker = get_unified_message_broker()
            
            # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì—ê²Œ ë©”ì‹œì§€ ì „ì†¡
            message = UnifiedMessage(
                message_id=str(uuid.uuid4()),
                session_id=f"test_session_{int(time.time())}",
                source_agent="test_user",
                target_agent="orchestrator",
                message_type="request",
                content={
                    "query": query,
                    "scenario": scenario['name'],
                    "data_file": scenario['file']
                },
                priority=MessagePriority.NORMAL
            )
            
            # ë¹„ë™ê¸° ì‘ë‹µ ìˆ˜ì§‘
            responses = []
            async for event in broker.route_message(message):
                responses.append(event)
                if event.get('data', {}).get('final', False):
                    break
                # ìµœëŒ€ 10ì´ˆ íƒ€ì„ì•„ì›ƒ
                if len(responses) > 50:  # ëŒ€ëµì ì¸ ì´ë²¤íŠ¸ ìˆ˜ ì œí•œ
                    break
            
            return {
                "success": True,
                "responses": responses,
                "response_count": len(responses),
                "broker_used": True
            }
            
        except Exception as e:
            # A2A í†µì‹  ì‹¤íŒ¨ ì‹œ ëª¨ì˜ ë¶„ì„ ìˆ˜í–‰
            return await self._fallback_analysis_simulation(query, scenario)
    
    async def _fallback_analysis_simulation(self, query: str, scenario: Dict[str, Any]) -> Dict[str, Any]:
        """A2A í†µì‹  ì‹¤íŒ¨ì‹œ í´ë°± ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜"""
        
        # ê¸°ë³¸ì ì¸ ë°ì´í„° ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜
        file_path = self.test_data_dir / scenario['file']
        
        if file_path.exists():
            if file_path.suffix == '.csv':
                df = pd.read_csv(file_path)
            elif file_path.suffix == '.xlsx':
                df = pd.read_excel(file_path)
            else:
                return {"success": False, "error": "Unsupported file format"}
            
            # ê°„ë‹¨í•œ í†µê³„ ì •ë³´ ìƒì„±
            basic_stats = {
                "shape": df.shape,
                "numeric_columns": df.select_dtypes(include=[np.number]).columns.tolist(),
                "categorical_columns": df.select_dtypes(include=['object']).columns.tolist(),
                "missing_values": df.isnull().sum().to_dict(),
                "basic_description": df.describe().to_dict()
            }
            
            return {
                "success": True,
                "basic_stats": basic_stats,
                "fallback_used": True,
                "simulated_response": f"ë°ì´í„° ë¶„ì„ ì™„ë£Œ: {df.shape[0]}ê°œ í–‰, {df.shape[1]}ê°œ ì—´"
            }
        
        return {"success": False, "error": "Data file not found"}
    
    def _evaluate_query_response_quality(self, query: str, analysis_result: Dict[str, Any], scenario: Dict[str, Any]) -> float:
        """ì¿¼ë¦¬ ì‘ë‹µ í’ˆì§ˆ í‰ê°€"""
        
        quality_score = 0.0
        max_score = 100.0
        
        # 1. ì‘ë‹µ ì„±ê³µ ì—¬ë¶€ (40ì )
        if analysis_result.get("success", False):
            quality_score += 40
        
        # 2. ì‘ë‹µ ë‚´ìš© ê´€ë ¨ì„± (30ì )
        if "responses" in analysis_result:
            responses = analysis_result["responses"]
            if len(responses) > 0:
                quality_score += 30
        elif "basic_stats" in analysis_result:
            quality_score += 20  # í´ë°± ë¶„ì„ì´ë¼ë„ í†µê³„ê°€ ìˆìœ¼ë©´ ì ìˆ˜
        
        # 3. ì˜ˆìƒ ìš”ì†Œ í¬í•¨ ì—¬ë¶€ (20ì )
        response_text = str(analysis_result).lower()
        expected_elements = scenario.get('expected_elements', [])
        found_elements = sum(1 for elem in expected_elements if elem.lower() in response_text)
        if expected_elements:
            element_score = (found_elements / len(expected_elements)) * 20
            quality_score += element_score
        
        # 4. LLM First ì›ì¹™ ì¤€ìˆ˜ (10ì )
        # í•˜ë“œì½”ë”©ëœ ì‘ë‹µì´ ì•„ë‹Œ ìœ ì—°í•œ ì‘ë‹µì¸ì§€ í™•ì¸
        if not self._is_hardcoded_response(analysis_result):
            quality_score += 10
        
        return min(quality_score, max_score)
    
    def _is_hardcoded_response(self, analysis_result: Dict[str, Any]) -> bool:
        """í•˜ë“œì½”ë”©ëœ ì‘ë‹µì¸ì§€ í™•ì¸ (LLM First ì›ì¹™ ê²€ì¦)"""
        
        # í…œí”Œë¦¿í™”ëœ ì‘ë‹µ íŒ¨í„´ ê°ì§€
        response_str = str(analysis_result).lower()
        
        hardcoded_patterns = [
            'template',
            'hardcoded',
            'fixed_response',
            'static_analysis'
        ]
        
        return any(pattern in response_str for pattern in hardcoded_patterns)
    
    async def _test_realtime_streaming(self):
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸"""
        print("\nğŸ“¡ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸...")
        
        try:
            from core.app_components.realtime_streaming_handler import get_streaming_handler
            
            handler = get_streaming_handler()
            
            # ìŠ¤íŠ¸ë¦¼ ì„¸ì…˜ ìƒì„±
            session_id = handler.create_stream_session("ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸")
            
            # ìŠ¤íŠ¸ë¦¬ë° í†µê³„ í™•ì¸
            stats = handler.get_stream_stats()
            
            print(f"âœ… ìŠ¤íŠ¸ë¦¼ ì„¸ì…˜ ìƒì„±: {session_id}")
            print(f"âœ… ìŠ¤íŠ¸ë¦¬ë° í†µê³„: {stats['total_streams']}ê°œ ìŠ¤íŠ¸ë¦¼")
            
            self.results.add_test_result(
                "realtime_streaming_test",
                True,
                {
                    "session_id": session_id,
                    "stats": stats
                }
            )
            
        except Exception as e:
            print(f"âŒ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            self.results.add_test_result("realtime_streaming_test", False, {"error": str(e)})
    
    async def _evaluate_response_quality(self):
        """ì¢…í•© ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
        print("\nâ­ ì¢…í•© ë‹µë³€ í’ˆì§ˆ í‰ê°€...")
        
        # ëª¨ë“  ë¶„ì„ í…ŒìŠ¤íŠ¸ì˜ í’ˆì§ˆ ì ìˆ˜ ì§‘ê³„
        analysis_tests = [result for result in self.results.test_results 
                         if result['test_name'].startswith('analysis_') and result['success']]
        
        if analysis_tests:
            quality_scores = [result['details'].get('quality_score', 0) for result in analysis_tests]
            avg_quality = np.mean(quality_scores)
            
            print(f"âœ… í‰ê·  ë‹µë³€ í’ˆì§ˆ: {avg_quality:.1f}/100")
            
            # í’ˆì§ˆ ì¹´í…Œê³ ë¦¬ë³„ í‰ê°€
            self.results.add_quality_score("llm_first_compliance", avg_quality * 0.9)  # LLM First ì›ì¹™
            self.results.add_quality_score("technical_accuracy", avg_quality * 0.8)   # ê¸°ìˆ ì  ì •í™•ì„±  
            self.results.add_quality_score("user_experience", avg_quality * 0.85)     # ì‚¬ìš©ì ê²½í—˜
            
        else:
            print("âš ï¸ ë¶„ì„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ì–´ í’ˆì§ˆ í‰ê°€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    async def _collect_performance_metrics(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        print("\nğŸ“Š ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘...")
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„
        response_times = [value for key, value in self.results.performance_metrics.items() 
                         if key.startswith('response_time_')]
        
        if response_times:
            avg_response_time = np.mean(response_times)
            max_response_time = np.max(response_times)
            min_response_time = np.min(response_times)
            
            print(f"âœ… í‰ê·  ì‘ë‹µ ì‹œê°„: {avg_response_time:.2f}ì´ˆ")
            print(f"âœ… ìµœëŒ€ ì‘ë‹µ ì‹œê°„: {max_response_time:.2f}ì´ˆ") 
            print(f"âœ… ìµœì†Œ ì‘ë‹µ ì‹œê°„: {min_response_time:.2f}ì´ˆ")
            
            self.results.add_performance_metric("avg_response_time", avg_response_time)
            self.results.add_performance_metric("max_response_time", max_response_time)
            self.results.add_performance_metric("min_response_time", min_response_time)
            
            # ì„±ëŠ¥ ê¸°ì¤€ í‰ê°€ (3ì´ˆ ì´ë‚´ ëª©í‘œ)
            performance_score = max(0, 100 - (avg_response_time - 3) * 20) if avg_response_time > 3 else 100
            self.results.add_quality_score("performance", performance_score)
    
    def _generate_final_report(self):
        """ìµœì¢… í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        print("\n" + "="*60)
        print("ğŸ“‹ CherryAI E2E í…ŒìŠ¤íŠ¸ ìµœì¢… ë³´ê³ ì„œ")
        print("="*60)
        
        # í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ 
        total_tests = len(self.results.test_results)
        successful_tests = sum(1 for result in self.results.test_results if result['success'])
        success_rate = (successful_tests / total_tests * 100) if total_tests > 0 else 0
        
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ì„±ê³µë¥ : {successful_tests}/{total_tests} ({success_rate:.1f}%)")
        
        # í’ˆì§ˆ ì ìˆ˜ ìš”ì•½
        if self.results.quality_scores:
            print(f"\nâ­ í’ˆì§ˆ ì ìˆ˜:")
            for category, score_info in self.results.quality_scores.items():
                print(f"  - {category}: {score_info['score']:.1f}/{score_info['max_score']} ({score_info['percentage']:.1f}%)")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìš”ì•½
        if self.results.performance_metrics:
            print(f"\nğŸš€ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
            for metric, value in self.results.performance_metrics.items():
                if metric.startswith('avg_') or metric.startswith('max_') or metric.startswith('min_'):
                    print(f"  - {metric}: {value:.2f}ì´ˆ")
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤
        failed_tests = [result for result in self.results.test_results if not result['success']]
        if failed_tests:
            print(f"\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ë“¤:")
            for test in failed_tests:
                print(f"  - {test['test_name']}: {test['details'].get('error', 'Unknown error')}")
        
        # ì¢…í•© í‰ê°€
        overall_score = success_rate * 0.6 + (np.mean([s['percentage'] for s in self.results.quality_scores.values()]) if self.results.quality_scores else 0) * 0.4
        
        print(f"\nğŸ¯ ì¢…í•© í‰ê°€: {overall_score:.1f}/100")
        
        if overall_score >= 85:
            print("ğŸ‰ ìš°ìˆ˜: ì‹œìŠ¤í…œì´ LLM First ì›ì¹™ì„ ì¤€ìˆ˜í•˜ë©° ë†’ì€ í’ˆì§ˆì˜ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤!")
        elif overall_score >= 70:
            print("âœ… ì–‘í˜¸: ëŒ€ë¶€ë¶„ì˜ ê¸°ëŠ¥ì´ ì •ìƒ ì‘ë™í•˜ë©° ê°œì„ ì˜ ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("âš ï¸ ê°œì„  í•„ìš”: ì‹œìŠ¤í…œ ì•ˆì •ì„±ê³¼ í’ˆì§ˆ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # JSON ë³´ê³ ì„œ ì €ì¥
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "success_rate": success_rate,
            "overall_score": overall_score,
            "test_results": self.results.test_results,
            "quality_scores": self.results.quality_scores,
            "performance_metrics": self.results.performance_metrics
        }
        
        with open("e2e_test_report.json", "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“„ ìƒì„¸ ë³´ê³ ì„œ: e2e_test_report.json ì €ì¥ ì™„ë£Œ")

async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    tester = CherryAI_E2E_Tester()
    results = await tester.run_comprehensive_e2e_test()
    return results

if __name__ == "__main__":
    asyncio.run(main()) 