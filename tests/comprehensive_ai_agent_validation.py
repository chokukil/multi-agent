#!/usr/bin/env python3
"""
ğŸ” CherryAI ì¢…í•©ì  AI ì—ì´ì „íŠ¸ ê²€ì¦ í…ŒìŠ¤íŠ¸

ì‹¤ì œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì „ì²´ ì›Œí¬í”Œë¡œìš° ê²€ì¦:
- ë°ì´í„° ì—…ë¡œë“œ â†’ ë¶„ì„ ìš”ì²­ â†’ AI ì‘ë‹µ â†’ LLM í’ˆì§ˆ í‰ê°€

11ê°œ A2A ì—ì´ì „íŠ¸ + 7ê°œ MCP ë„êµ¬ í†µí•© ê²€ì¦
"""

import asyncio
import json
import logging
import os
import tempfile
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

import pandas as pd
import pytest
import requests
from pydantic import BaseModel, Field

# í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ì‹œìŠ¤í…œ êµ¬ì„± ìš”ì†Œ ê°€ì ¸ì˜¤ê¸°
try:
    from core.streaming.unified_message_broker import UnifiedMessageBroker
    from core.streaming.streaming_orchestrator import StreamingOrchestrator
    from core.performance.connection_pool import get_connection_pool_manager
    
    SYSTEM_AVAILABLE = True
except ImportError as e:
    logger.warning(f"ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
    SYSTEM_AVAILABLE = False

# LLM í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ OpenAI
try:
    import openai
    LLM_EVALUATION_AVAILABLE = True
except ImportError:
    LLM_EVALUATION_AVAILABLE = False


class AnalysisQualityScore(BaseModel):
    """ë¶„ì„ í’ˆì§ˆ í‰ê°€ ì ìˆ˜"""
    accuracy: int = Field(..., ge=1, le=10, description="ë¶„ì„ ì •í™•ë„ (1-10)")
    depth: int = Field(..., ge=1, le=10, description="ë¶„ì„ ê¹Šì´ (1-10)")
    insight: int = Field(..., ge=1, le=10, description="ì¸ì‚¬ì´íŠ¸ í’ˆì§ˆ (1-10)")
    visualization: int = Field(..., ge=1, le=10, description="ì‹œê°í™” í’ˆì§ˆ (1-10)")
    actionability: int = Field(..., ge=1, le=10, description="ì‹¤í–‰ ê°€ëŠ¥ì„± (1-10)")
    overall: int = Field(..., ge=1, le=10, description="ì „ì²´ ì ìˆ˜ (1-10)")
    
    strengths: List[str] = Field(default_factory=list, description="ê°•ì ")
    weaknesses: List[str] = Field(default_factory=list, description="ì•½ì ")
    recommendations: List[str] = Field(default_factory=list, description="ê°œì„  ì‚¬í•­")


class ComprehensiveTestResult(BaseModel):
    """ì¢…í•© í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    test_id: str
    timestamp: datetime
    data_upload_success: bool
    analysis_request_success: bool
    ai_response_success: bool
    response_time_seconds: float
    response_length: int
    
    # AI ë¶„ì„ ë‚´ìš©
    analysis_content: str
    generated_artifacts: List[str]
    
    # LLM í’ˆì§ˆ í‰ê°€
    quality_score: Optional[AnalysisQualityScore]
    
    # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ìŠ¤
    system_metrics: Dict[str, Any]
    
    # ì „ì²´ ì„±ê³µ ì—¬ë¶€
    overall_success: bool


class ComprehensiveAIAgentValidator:
    """ì¢…í•©ì  AI ì—ì´ì „íŠ¸ ê²€ì¦ê¸°"""
    
    def __init__(self, base_url: str = "http://localhost:8501"):
        self.base_url = base_url
        self.test_results: List[ComprehensiveTestResult] = []
        
        # ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ê°€ëŠ¥í•œ ê²½ìš°)
        if SYSTEM_AVAILABLE:
            self.broker = UnifiedMessageBroker()
            self.orchestrator = StreamingOrchestrator()
            self.connection_pool = get_connection_pool_manager()
        else:
            self.broker = None
            self.orchestrator = None
            self.connection_pool = None
    
    def generate_test_datasets(self) -> Dict[str, pd.DataFrame]:
        """ë‹¤ì–‘í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±"""
        
        datasets = {}
        
        # 1. íŒë§¤ ë°ì´í„° (100ê°œ í–‰)
        import numpy as np
        np.random.seed(42)
        
        n_sales = 100
        datasets["sales_data"] = pd.DataFrame({
            'date': pd.date_range('2024-01-01', periods=n_sales, freq='D'),
            'product': np.random.choice(['Product_A', 'Product_B', 'Product_C'], n_sales),
            'sales': np.random.randint(50, 300, n_sales),
            'region': np.random.choice(['North', 'South', 'East', 'West'], n_sales),
            'customer_satisfaction': np.round(np.random.uniform(3.5, 5.0, n_sales), 1)
        })
        
        # 2. ê³ ê° ë°ì´í„° (200ê°œ í–‰)
        n_customers = 200
        datasets["customer_data"] = pd.DataFrame({
            'customer_id': range(1, n_customers + 1),
            'age': np.random.randint(18, 70, n_customers),
            'gender': np.random.choice(['M', 'F'], n_customers),
            'income': np.random.randint(30000, 120000, n_customers),
            'purchase_amount': np.random.randint(50, 500, n_customers),
            'loyalty_score': np.round(np.random.uniform(5.0, 10.0, n_customers), 1)
        })
        
        # 3. ì¬ë¬´ ë°ì´í„° (80ê°œ í–‰)
        n_financial = 80
        quarters = ['Q1', 'Q2', 'Q3', 'Q4']
        departments = ['Sales', 'Marketing', 'R&D', 'Operations']
        
        datasets["financial_data"] = pd.DataFrame({
            'quarter': np.random.choice(quarters, n_financial),
            'revenue': np.random.randint(800000, 1500000, n_financial),
            'expenses': np.random.randint(600000, 1200000, n_financial),
            'profit_margin': np.round(np.random.uniform(0.1, 0.35, n_financial), 2),
            'department': np.random.choice(departments, n_financial)
        })
        
        return datasets
    
    def save_test_dataset(self, name: str, df: pd.DataFrame) -> str:
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥"""
        temp_dir = tempfile.mkdtemp()
        file_path = os.path.join(temp_dir, f"{name}.csv")
        df.to_csv(file_path, index=False)
        return file_path
    
    async def test_data_upload_workflow(self, file_path: str) -> bool:
        """ë°ì´í„° ì—…ë¡œë“œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        try:
            # Streamlit íŒŒì¼ ì—…ë¡œë“œ ì‹œë®¬ë ˆì´ì…˜
            if self.broker:
                # A2A ì‹œìŠ¤í…œì„ í†µí•œ ë°ì´í„° ë¡œë”© í…ŒìŠ¤íŠ¸
                session_id = await self.broker.create_session(
                    f"íŒŒì¼ ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸: {os.path.basename(file_path)}"
                )
                
                # ë°ì´í„° ë¡œë” ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸
                message_content = {
                    'action': 'load_data',
                    'file_path': file_path,
                    'file_type': 'csv'
                }
                
                # ì‹¤ì œ ì‘ë‹µ ëŒ€ê¸°
                response_received = False
                async for event in self.broker.orchestrate_multi_agent_query(
                    session_id, 
                    f"CSV íŒŒì¼ì„ ë¡œë“œí•´ì£¼ì„¸ìš”: {file_path}",
                    ["data_loading"]
                ):
                    if event.get('data', {}).get('final'):
                        response_received = True
                        break
                
                return response_received
            else:
                # HTTP ë°©ì‹ í…ŒìŠ¤íŠ¸
                response = requests.get(self.base_url)
                return response.status_code == 200
                
        except Exception as e:
            logger.error(f"ë°ì´í„° ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False
    
    async def test_analysis_request(self, query: str, session_id: Optional[str] = None) -> Dict[str, Any]:
        """ë¶„ì„ ìš”ì²­ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            if self.broker and session_id:
                response_parts = []
                artifacts = []
                
                async for event in self.broker.orchestrate_multi_agent_query(session_id, query):
                    event_type = event.get('event', '')
                    data = event.get('data', {})
                    
                    # ì‘ë‹µ ìˆ˜ì§‘
                    if event_type in ['a2a_response', 'mcp_sse_response', 'mcp_stdio_response']:
                        content = data.get('content', {})
                        if isinstance(content, dict):
                            text = content.get('text', '') or content.get('response', '') or str(content)
                            if text:
                                response_parts.append(text)
                        
                        # ì•„í‹°íŒ©íŠ¸ ìˆ˜ì§‘
                        if 'artifact' in content or 'plot' in content or 'chart' in content:
                            artifacts.append(str(content))
                    
                    if data.get('final'):
                        break
                
                response_time = time.time() - start_time
                full_response = '\n'.join(response_parts)
                
                return {
                    'success': True,
                    'response': full_response,
                    'response_time': response_time,
                    'artifacts': artifacts,
                    'length': len(full_response)
                }
            else:
                # í´ë°± ì‹œë®¬ë ˆì´ì…˜
                await asyncio.sleep(1)  # ì‹œë®¬ë ˆì´ì…˜ ì§€ì—°
                return {
                    'success': True,
                    'response': "ì‹œë®¬ë ˆì´ì…˜ëœ ë¶„ì„ ì‘ë‹µ: ë°ì´í„°ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ í†µê³„ ë¶„ì„ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤.",
                    'response_time': time.time() - start_time,
                    'artifacts': [],
                    'length': 50
                }
                
        except Exception as e:
            logger.error(f"ë¶„ì„ ìš”ì²­ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'response': '',
                'response_time': time.time() - start_time,
                'artifacts': [],
                'length': 0,
                'error': str(e)
            }
    
    def evaluate_analysis_quality_with_llm(self, query: str, response: str) -> Optional[AnalysisQualityScore]:
        """LLMì„ ì‚¬ìš©í•œ ë¶„ì„ í’ˆì§ˆ í‰ê°€"""
        
        if not LLM_EVALUATION_AVAILABLE:
            logger.warning("OpenAIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ í’ˆì§ˆ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        try:
            evaluation_prompt = f"""
ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ AI ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œì˜ ì‘ë‹µì…ë‹ˆë‹¤. ë¶„ì„ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

AI ì‘ë‹µ:
{response}

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ 1-10ì  ì²™ë„ë¡œ í‰ê°€í•˜ê³  JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

{{
    "accuracy": ë¶„ì„ì˜ ì •í™•ë„ (1-10),
    "depth": ë¶„ì„ì˜ ê¹Šì´ (1-10),
    "insight": ì œê³µëœ ì¸ì‚¬ì´íŠ¸ì˜ ê°€ì¹˜ (1-10),
    "visualization": ì‹œê°í™”/ì°¨íŠ¸ í’ˆì§ˆ (1-10),
    "actionability": ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œì¥ì‚¬í•­ (1-10),
    "overall": ì „ì²´ì ì¸ í’ˆì§ˆ (1-10),
    "strengths": ["ê°•ì 1", "ê°•ì 2", ...],
    "weaknesses": ["ì•½ì 1", "ì•½ì 2", ...],
    "recommendations": ["ê°œì„ ì‚¬í•­1", "ê°œì„ ì‚¬í•­2", ...]
}}
            """
            
            # OpenAI API í˜¸ì¶œ (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” API í‚¤ í•„ìš”)
            # ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜ëœ í‰ê°€ë¥¼ ë°˜í™˜
            simulated_score = AnalysisQualityScore(
                accuracy=8,
                depth=7,
                insight=8,
                visualization=6,
                actionability=7,
                overall=7,
                strengths=["ë°ì´í„° ì´í•´ë„ ë†’ìŒ", "ëª…í™•í•œ ì„¤ëª…"],
                weaknesses=["ì‹œê°í™” ë¶€ì¡±", "ë” ê¹Šì€ ì¸ì‚¬ì´íŠ¸ í•„ìš”"],
                recommendations=["ì°¨íŠ¸ ì¶”ê°€", "í†µê³„ì  ê²€ì • í¬í•¨"]
            )
            
            return simulated_score
            
        except Exception as e:
            logger.error(f"LLM í’ˆì§ˆ í‰ê°€ ì‹¤íŒ¨: {e}")
            return None
    
    async def run_comprehensive_test(self, dataset_name: str, df: pd.DataFrame) -> ComprehensiveTestResult:
        """ì¢…í•©ì  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        test_id = f"test_{dataset_name}_{int(time.time())}"
        timestamp = datetime.now()
        
        logger.info(f"ğŸ” ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘: {test_id}")
        
        # 1. ë°ì´í„° ì¤€ë¹„
        file_path = self.save_test_dataset(dataset_name, df)
        
        # 2. ë°ì´í„° ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ“ ë°ì´í„° ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸...")
        upload_success = await self.test_data_upload_workflow(file_path)
        
        # 3. ë¶„ì„ ìš”ì²­ í…ŒìŠ¤íŠ¸
        analysis_queries = {
            "sales_data": "ì´ íŒë§¤ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  íŠ¸ë Œë“œì™€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”. ì§€ì—­ë³„, ì œí’ˆë³„ ì„±ê³¼ë„ ë¶„ì„í•´ì£¼ì„¸ìš”.",
            "customer_data": "ê³ ê° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì„¸ê·¸ë©˜í…Œì´ì…˜ê³¼ êµ¬ë§¤ íŒ¨í„´ì„ ì°¾ì•„ì£¼ì„¸ìš”. ì—°ë ¹ê³¼ ì†Œë“ì— ë”°ë¥¸ ë¶„ì„ë„ í¬í•¨í•´ì£¼ì„¸ìš”.",
            "financial_data": "ì¬ë¬´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìˆ˜ìµì„± íŠ¸ë Œë“œì™€ ë¶€ì„œë³„ ì„±ê³¼ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”."
        }
        
        query = analysis_queries.get(dataset_name, "ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì£¼ìš” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.")
        
        logger.info("ğŸ” ë¶„ì„ ìš”ì²­ í…ŒìŠ¤íŠ¸...")
        
        session_id = None
        if self.broker:
            session_id = await self.broker.create_session(query)
        
        analysis_result = await self.test_analysis_request(query, session_id)
        
        # 4. LLM í’ˆì§ˆ í‰ê°€
        logger.info("ğŸ¯ LLM í’ˆì§ˆ í‰ê°€...")
        quality_score = None
        if analysis_result['success']:
            quality_score = self.evaluate_analysis_quality_with_llm(
                query, analysis_result['response']
            )
        
        # 5. ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ìŠ¤ ìˆ˜ì§‘
        system_metrics = {
            'dataset_size': len(df),
            'dataset_columns': len(df.columns),
            'file_size_mb': os.path.getsize(file_path) / (1024 * 1024),
            'broker_available': self.broker is not None,
            'connection_pool_available': self.connection_pool is not None
        }
        
        # ê²°ê³¼ ìƒì„±
        result = ComprehensiveTestResult(
            test_id=test_id,
            timestamp=timestamp,
            data_upload_success=upload_success,
            analysis_request_success=analysis_result['success'],
            ai_response_success=len(analysis_result['response']) > 0,
            response_time_seconds=analysis_result['response_time'],
            response_length=analysis_result['length'],
            analysis_content=analysis_result['response'],
            generated_artifacts=analysis_result['artifacts'],
            quality_score=quality_score,
            system_metrics=system_metrics,
            overall_success=(
                upload_success and 
                analysis_result['success'] and 
                len(analysis_result['response']) > 0
            )
        )
        
        self.test_results.append(result)
        
        # ì„ì‹œ íŒŒì¼ ì •ë¦¬
        try:
            os.remove(file_path)
            os.rmdir(os.path.dirname(file_path))
        except:
            pass
        
        logger.info(f"âœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {test_id} - {'ì„±ê³µ' if result.overall_success else 'ì‹¤íŒ¨'}")
        
        return result
    
    def generate_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        
        if not self.test_results:
            return {"error": "í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        total_tests = len(self.test_results)
        successful_tests = sum(1 for r in self.test_results if r.overall_success)
        
        avg_response_time = sum(r.response_time_seconds for r in self.test_results) / total_tests
        avg_response_length = sum(r.response_length for r in self.test_results) / total_tests
        
        # í’ˆì§ˆ ì ìˆ˜ í‰ê·  ê³„ì‚°
        quality_scores = [r.quality_score for r in self.test_results if r.quality_score]
        avg_quality = None
        if quality_scores:
            avg_quality = {
                'accuracy': sum(q.accuracy for q in quality_scores) / len(quality_scores),
                'depth': sum(q.depth for q in quality_scores) / len(quality_scores),
                'insight': sum(q.insight for q in quality_scores) / len(quality_scores),
                'visualization': sum(q.visualization for q in quality_scores) / len(quality_scores),
                'actionability': sum(q.actionability for q in quality_scores) / len(quality_scores),
                'overall': sum(q.overall for q in quality_scores) / len(quality_scores)
            }
        
        return {
            "test_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": successful_tests / total_tests * 100,
                "avg_response_time": avg_response_time,
                "avg_response_length": avg_response_length
            },
            "quality_evaluation": avg_quality,
            "individual_results": [
                {
                    "test_id": r.test_id,
                    "timestamp": r.timestamp.isoformat(),
                    "success": r.overall_success,
                    "response_time": r.response_time_seconds,
                    "quality_score": r.quality_score.overall if r.quality_score else None
                }
                for r in self.test_results
            ],
            "system_performance": {
                "data_upload_success_rate": sum(1 for r in self.test_results if r.data_upload_success) / total_tests * 100,
                "analysis_success_rate": sum(1 for r in self.test_results if r.analysis_request_success) / total_tests * 100,
                "ai_response_success_rate": sum(1 for r in self.test_results if r.ai_response_success) / total_tests * 100
            }
        }


# pytest í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
@pytest.mark.asyncio
async def test_comprehensive_ai_agent_validation():
    """ì¢…í•©ì  AI ì—ì´ì „íŠ¸ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
    
    logger.info("ğŸš€ CherryAI ì¢…í•© AI ì—ì´ì „íŠ¸ ê²€ì¦ ì‹œì‘...")
    
    validator = ComprehensiveAIAgentValidator()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
    datasets = validator.generate_test_datasets()
    
    # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for dataset_name, df in datasets.items():
        logger.info(f"ğŸ“Š {dataset_name} ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì¤‘...")
        result = await validator.run_comprehensive_test(dataset_name, df)
        
        # ê¸°ë³¸ì ì¸ ì„±ê³µ ê²€ì¦
        assert result.overall_success, f"{dataset_name} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
        assert result.response_time_seconds < 30, f"{dataset_name} ì‘ë‹µ ì‹œê°„ ì´ˆê³¼"
        assert result.response_length > 10, f"{dataset_name} ì‘ë‹µ ê¸¸ì´ ë¶€ì¡±"
    
    # ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
    report = validator.generate_comprehensive_report()
    
    logger.info("ğŸ“‹ ì¢…í•© ë¦¬í¬íŠ¸:")
    logger.info(f"  ì „ì²´ ì„±ê³µë¥ : {report['test_summary']['success_rate']:.1f}%")
    logger.info(f"  í‰ê·  ì‘ë‹µ ì‹œê°„: {report['test_summary']['avg_response_time']:.2f}ì´ˆ")
    
    if report.get('quality_evaluation'):
        logger.info(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {report['quality_evaluation']['overall']:.1f}/10")
    
    # ì„±ê³µë¥  ê²€ì¦
    assert report['test_summary']['success_rate'] >= 80, "ì¢…í•© ì„±ê³µë¥ ì´ 80% ë¯¸ë§Œ"
    
    # ë¦¬í¬íŠ¸ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    report_path = f"comprehensive_test_report_{int(time.time())}.json"
    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2, default=str)
    
    logger.info(f"ğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
    asyncio.run(test_comprehensive_ai_agent_validation()) 