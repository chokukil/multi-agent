#!/usr/bin/env python3
"""
Collaboration Rules Engine í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸

Context Engineering ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ë¥¼ ìœ„í•œ Collaboration Rules Engine ê¸°ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸

Test Coverage:
- Rule Registry í…ŒìŠ¤íŠ¸
- Pattern Learning Engine í…ŒìŠ¤íŠ¸  
- Conflict Resolution System í…ŒìŠ¤íŠ¸
- Workflow Optimizer í…ŒìŠ¤íŠ¸
- Collaboration Rules Engine í†µí•© í…ŒìŠ¤íŠ¸
"""

import pytest
import asyncio
import json
import tempfile
from datetime import datetime, timedelta
from unittest.mock import AsyncMock, MagicMock, patch
import statistics

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'a2a_ds_servers', 'context_engineering'))

# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ì„í¬íŠ¸
from collaboration_rules_engine import (
    CollaborationRulesEngine,
    RuleRegistry,
    PatternLearningEngine,
    ConflictResolutionSystem,
    WorkflowOptimizer,
    CollaborationRule,
    CollaborationEvent,
    WorkflowPattern,
    ConflictSituation,
    RuleType,
    CollaborationStatus,
    ConflictType,
    get_collaboration_rules_engine,
    initialize_collaboration_rules_engine
)

class TestRuleRegistry:
    """Rule Registry í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def temp_registry_path(self):
        """ì„ì‹œ ë ˆì§€ìŠ¤íŠ¸ë¦¬ íŒŒì¼ ê²½ë¡œ"""
        with tempfile.NamedTemporaryFile(delete=False, suffix='.json') as f:
            yield f.name
        try:
            os.unlink(f.name)
        except FileNotFoundError:
            pass
    
    @pytest.fixture
    def rule_registry(self, temp_registry_path):
        """Rule Registry í”½ìŠ¤ì²˜"""
        return RuleRegistry(temp_registry_path)
    
    def test_rule_registry_initialization(self, rule_registry):
        """Rule Registry ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        assert rule_registry.registry_path is not None
        assert isinstance(rule_registry.rules, dict)
        assert isinstance(rule_registry.rule_templates, dict)
        assert len(rule_registry.rule_templates) > 0
        
        # ê¸°ë³¸ í…œí”Œë¦¿ í™•ì¸
        assert "sequential_data_processing" in rule_registry.rule_templates
        assert "parallel_analysis" in rule_registry.rule_templates
        assert "resource_priority" in rule_registry.rule_templates
    
    @pytest.mark.asyncio
    async def test_load_rules_create_default(self, rule_registry):
        """ê¸°ë³¸ ê·œì¹™ ìƒì„± í…ŒìŠ¤íŠ¸"""
        rules = await rule_registry.load_rules()
        
        assert len(rules) > 0
        
        # ê° í…œí”Œë¦¿ì— ëŒ€ì‘í•˜ëŠ” ê·œì¹™ í™•ì¸
        template_count = len(rule_registry.rule_templates)
        assert len(rules) == template_count
        
        # ê·œì¹™ êµ¬ì¡° í™•ì¸
        for rule in rules.values():
            assert isinstance(rule, CollaborationRule)
            assert rule.rule_id is not None
            assert isinstance(rule.rule_type, RuleType)
            assert rule.name != ""
            assert isinstance(rule.conditions, dict)
            assert isinstance(rule.actions, list)
            assert rule.priority > 0
            assert rule.scope in ["global", "domain", "session"]
    
    @pytest.mark.asyncio
    async def test_save_and_load_rules(self, rule_registry):
        """ê·œì¹™ ì €ì¥ ë° ë¡œë“œ í…ŒìŠ¤íŠ¸"""
        await rule_registry.load_rules()
        original_count = len(rule_registry.rules)
        
        # ì €ì¥
        await rule_registry.save_rules()
        
        # ìƒˆë¡œìš´ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¡œë“œ
        new_registry = RuleRegistry(rule_registry.registry_path)
        loaded_rules = await new_registry.load_rules()
        
        assert len(loaded_rules) == original_count
        
        # ë°ì´í„° ì¼ì¹˜ í™•ì¸
        for rule_id, original_rule in rule_registry.rules.items():
            loaded_rule = loaded_rules[rule_id]
            assert loaded_rule.rule_id == original_rule.rule_id
            assert loaded_rule.rule_type == original_rule.rule_type
            assert loaded_rule.name == original_rule.name
    
    @pytest.mark.asyncio
    async def test_get_applicable_rules(self, rule_registry):
        """ì ìš© ê°€ëŠ¥í•œ ê·œì¹™ ì¡°íšŒ í…ŒìŠ¤íŠ¸"""
        await rule_registry.load_rules()
        
        # í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸
        context = {
            "task_type": "data_processing",
            "data_size": "large",
            "complexity": "high"
        }
        agents = ["data_loader", "data_cleaning"]
        
        applicable_rules = await rule_registry.get_applicable_rules(context, agents)
        
        assert len(applicable_rules) > 0
        
        # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
        for i in range(1, len(applicable_rules)):
            assert applicable_rules[i-1].priority >= applicable_rules[i].priority
        
        # ì¡°ê±´ ë§¤ì¹­ í™•ì¸
        for rule in applicable_rules:
            assert rule.is_active is True
    
    def test_matches_conditions(self, rule_registry):
        """ì¡°ê±´ ë§¤ì¹­ í…ŒìŠ¤íŠ¸"""
        # ì •í™•í•œ ë§¤ì¹­
        conditions = {"task_type": "analysis", "urgency": "high"}
        context = {"task_type": "analysis", "urgency": "high", "extra": "value"}
        
        assert rule_registry._matches_conditions(conditions, context) is True
        
        # ë¶ˆì¼ì¹˜
        context_mismatch = {"task_type": "processing", "urgency": "high"}
        assert rule_registry._matches_conditions(conditions, context_mismatch) is False
        
        # ìˆ«ì ì¡°ê±´ í…ŒìŠ¤íŠ¸
        numeric_conditions = {"agents_available": ">=3", "priority": ">5"}
        numeric_context = {"agents_available": 4, "priority": 7}
        
        assert rule_registry._matches_conditions(numeric_conditions, numeric_context) is True
        
        numeric_context_fail = {"agents_available": 2, "priority": 7}
        assert rule_registry._matches_conditions(numeric_conditions, numeric_context_fail) is False
    
    @pytest.mark.asyncio
    async def test_update_rule_performance(self, rule_registry):
        """ê·œì¹™ ì„±ëŠ¥ ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸"""
        await rule_registry.load_rules()
        
        rule_id = list(rule_registry.rules.keys())[0]
        original_rule = rule_registry.rules[rule_id]
        original_usage = original_rule.usage_count
        original_success_rate = original_rule.success_rate
        
        # ì„±ê³µì ì¸ ì‚¬ìš© ì—…ë°ì´íŠ¸
        await rule_registry.update_rule_performance(rule_id, success=True)
        
        updated_rule = rule_registry.rules[rule_id]
        assert updated_rule.usage_count == original_usage + 1
        assert updated_rule.success_rate >= original_success_rate

class TestCollaborationEvent:
    """CollaborationEvent ë°ì´í„° êµ¬ì¡° í…ŒìŠ¤íŠ¸"""
    
    def test_collaboration_event_creation(self):
        """CollaborationEvent ìƒì„± í…ŒìŠ¤íŠ¸"""
        event = CollaborationEvent(
            event_id="test_event",
            event_type="analysis",
            timestamp=datetime.now(),
            agents_involved=["agent1", "agent2"],
            event_data={"key": "value"},
            duration=5.0,
            success=True,
            performance_metrics={"accuracy": 0.95}
        )
        
        assert event.event_id == "test_event"
        assert event.event_type == "analysis"
        assert len(event.agents_involved) == 2
        assert event.success is True
        assert event.performance_metrics["accuracy"] == 0.95

class TestPatternLearningEngine:
    """Pattern Learning Engine í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    async def pattern_learning_setup(self):
        """Pattern Learning Engine ì„¤ì •"""
        registry = RuleRegistry("test_registry.json")
        await registry.load_rules()
        learning_engine = PatternLearningEngine(registry)
        return learning_engine, registry
    
    @pytest.mark.asyncio
    async def test_record_collaboration_event(self, pattern_learning_setup):
        """í˜‘ì—… ì´ë²¤íŠ¸ ê¸°ë¡ í…ŒìŠ¤íŠ¸"""
        learning_engine, registry = await pattern_learning_setup
        
        event = CollaborationEvent(
            event_id="test_event_1",
            event_type="data_analysis",
            timestamp=datetime.now(),
            agents_involved=["pandas_hub", "eda_tools"],
            event_data={"task": "statistical_analysis"},
            duration=3.0,
            success=True,
            performance_metrics={"accuracy": 0.9}
        )
        
        await learning_engine.record_collaboration_event(event)
        
        assert len(learning_engine.collaboration_history) == 1
        assert learning_engine.collaboration_history[0] == event
    
    def test_extract_sequence_patterns(self, pattern_learning_setup):
        """ì‹œí€€ìŠ¤ íŒ¨í„´ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        learning_engine, registry = asyncio.run(pattern_learning_setup)
        
        # í…ŒìŠ¤íŠ¸ ì´ë²¤íŠ¸ë“¤ ìƒì„±
        events = [
            CollaborationEvent(
                event_id=f"event_{i}",
                event_type="analysis",
                timestamp=datetime.now(),
                agents_involved=["agent1", "agent2"] if i % 2 == 0 else ["agent2", "agent3"],
                event_data={},
                duration=1.0,
                success=True,
                performance_metrics={}
            )
            for i in range(5)
        ]
        
        patterns = learning_engine._extract_sequence_patterns(events)
        
        assert len(patterns) > 0
        
        for pattern in patterns.values():
            assert isinstance(pattern, WorkflowPattern)
            assert pattern.usage_frequency > 0
            assert len(pattern.agent_sequence) <= 4
    
    def test_identify_successful_patterns(self, pattern_learning_setup):
        """ì„±ê³µì ì¸ íŒ¨í„´ ì‹ë³„ í…ŒìŠ¤íŠ¸"""
        learning_engine, registry = asyncio.run(pattern_learning_setup)
        
        # ì„±ê³µì ì¸ ì´ë²¤íŠ¸ë“¤ë¡œ íˆìŠ¤í† ë¦¬ êµ¬ì„±
        for i in range(10):
            event = CollaborationEvent(
                event_id=f"event_{i}",
                event_type="analysis",
                timestamp=datetime.now(),
                agents_involved=["agent1", "agent2"],
                event_data={},
                duration=2.0,
                success=i < 8,  # 80% ì„±ê³µë¥ 
                performance_metrics={"score": 0.8 + i * 0.02}
            )
            learning_engine.collaboration_history.append(event)
        
        # íŒ¨í„´ ìƒì„±
        patterns = {"test_pattern": WorkflowPattern(
            pattern_id="test_pattern",
            pattern_name="Test Pattern",
            agent_sequence=["agent1", "agent2"],
            typical_duration=0.0,
            success_rate=0.0,
            usage_frequency=5,
            performance_score=0.0,
            context_requirements={},
            learned_rules=[],
            last_updated=datetime.now()
        )}
        
        successful_patterns = learning_engine._identify_successful_patterns(patterns)
        
        assert len(successful_patterns) > 0
        for pattern in successful_patterns:
            assert pattern.success_rate > 0.6
            assert pattern.usage_frequency >= 3

class TestConflictResolutionSystem:
    """Conflict Resolution System í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    async def conflict_resolution_setup(self):
        """Conflict Resolution System ì„¤ì •"""
        registry = RuleRegistry("test_registry.json")
        await registry.load_rules()
        conflict_system = ConflictResolutionSystem(registry)
        return conflict_system, registry
    
    @pytest.mark.asyncio
    async def test_detect_conflict_resource_contention(self, conflict_resolution_setup):
        """ë¦¬ì†ŒìŠ¤ ê²½í•© ì¶©ëŒ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        conflict_system, registry = await conflict_resolution_setup
        
        context = {"resource_contention": True}
        agents = ["agent1", "agent2"]
        
        conflict = await conflict_system.detect_conflict(context, agents)
        
        assert conflict is not None
        assert isinstance(conflict, ConflictSituation)
        assert conflict.conflict_type == ConflictType.RESOURCE_CONTENTION
        assert conflict.involved_agents == agents
        assert conflict.conflict_id in conflict_system.active_conflicts
    
    @pytest.mark.asyncio
    async def test_detect_conflict_priority_conflict(self, conflict_resolution_setup):
        """ìš°ì„ ìˆœìœ„ ì¶©ëŒ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        conflict_system, registry = await conflict_resolution_setup
        
        context = {"priority_conflict": True}
        agents = ["agent1", "agent2", "agent3"]
        
        conflict = await conflict_system.detect_conflict(context, agents)
        
        assert conflict is not None
        assert conflict.conflict_type == ConflictType.PRIORITY_CONFLICT
        assert len(conflict.involved_agents) == 3
    
    @pytest.mark.asyncio
    async def test_resolve_conflict_success(self, conflict_resolution_setup):
        """ì¶©ëŒ í•´ê²° ì„±ê³µ í…ŒìŠ¤íŠ¸"""
        conflict_system, registry = await conflict_resolution_setup
        
        # ì¶©ëŒ ìƒì„±
        conflict = await conflict_system._create_conflict(
            ConflictType.RESOURCE_CONTENTION,
            ["agent1", "agent2"],
            "Test conflict"
        )
        
        # ì¶©ëŒ í•´ê²°
        success = await conflict_system.resolve_conflict(conflict)
        
        assert success is True
        assert conflict.resolved is True
        assert conflict.success is True
        assert conflict.resolution_time is not None
        assert conflict.resolution_strategy != ""
        assert len(conflict.resolution_actions) > 0
    
    @pytest.mark.asyncio
    async def test_get_conflict_analytics(self, conflict_resolution_setup):
        """ì¶©ëŒ ë¶„ì„ ì •ë³´ í…ŒìŠ¤íŠ¸"""
        conflict_system, registry = await conflict_resolution_setup
        
        # í…ŒìŠ¤íŠ¸ìš© ì¶©ëŒ ì´ë ¥ ìƒì„±
        for i in range(5):
            conflict = ConflictSituation(
                conflict_id=f"conflict_{i}",
                conflict_type=ConflictType.RESOURCE_CONTENTION,
                involved_agents=["agent1", "agent2"],
                description=f"Test conflict {i}",
                detected_at=datetime.now(),
                resolution_strategy="test_strategy",
                resolution_actions=[],
                resolution_time=1.0 + i * 0.5,
                resolved=True,
                success=i < 4  # 80% ì„±ê³µë¥ 
            )
            conflict_system.conflict_history.append(conflict)
        
        analytics = await conflict_system.get_conflict_analytics()
        
        assert analytics["total_conflicts"] == 5
        assert analytics["resolved_conflicts"] == 5
        assert analytics["success_rate"] == 0.8
        assert analytics["average_resolution_time"] > 0

class TestWorkflowOptimizer:
    """Workflow Optimizer í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    async def workflow_optimizer_setup(self):
        """Workflow Optimizer ì„¤ì •"""
        registry = RuleRegistry("test_registry.json")
        await registry.load_rules()
        pattern_learning = PatternLearningEngine(registry)
        optimizer = WorkflowOptimizer(registry, pattern_learning)
        return optimizer, registry
    
    @pytest.mark.asyncio
    async def test_optimize_workflow(self, workflow_optimizer_setup):
        """ì›Œí¬í”Œë¡œìš° ìµœì í™” í…ŒìŠ¤íŠ¸"""
        optimizer, registry = await workflow_optimizer_setup
        
        context = {
            "complexity": "high",
            "estimated_duration": 10.0,
            "resource_usage": "high",
            "agent_count": 4
        }
        agents = ["agent1", "agent2", "agent3", "agent4"]
        
        optimized_workflow = await optimizer.optimize_workflow(context, agents)
        
        assert "parallel_execution" in optimized_workflow or "load_balancing" in optimized_workflow
        assert len(optimizer.optimization_history) > 0
        
        # ìµœì í™” ê¸°ë¡ í™•ì¸
        optimization_record = optimizer.optimization_history[-1]
        assert "original_workflow" in optimization_record
        assert "optimized_workflow" in optimization_record
        assert "optimization_opportunities" in optimization_record
    
    def test_calculate_parallel_potential(self, workflow_optimizer_setup):
        """ë³‘ë ¬ ì²˜ë¦¬ ì ì¬ë ¥ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        optimizer, registry = asyncio.run(workflow_optimizer_setup)
        
        # ë‹¨ì¼ ì—ì´ì „íŠ¸
        potential_single = optimizer._calculate_parallel_potential(["agent1"])
        assert potential_single == 0.0
        
        # ì†Œìˆ˜ ì—ì´ì „íŠ¸
        potential_few = optimizer._calculate_parallel_potential(["agent1", "agent2"])
        assert potential_few == 0.6
        
        # ë‹¤ìˆ˜ ì—ì´ì „íŠ¸
        potential_many = optimizer._calculate_parallel_potential(["agent1", "agent2", "agent3", "agent4"])
        assert potential_many == 0.8
    
    def test_identify_bottlenecks(self, workflow_optimizer_setup):
        """ë³‘ëª© ì§€ì  ì‹ë³„ í…ŒìŠ¤íŠ¸"""
        optimizer, registry = asyncio.run(workflow_optimizer_setup)
        
        # ê³ ë³µì¡ë„ + ê³ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰
        context_bottleneck = {
            "complexity": "high",
            "resource_usage": "high"
        }
        agents = ["agent1", "agent2", "agent3", "agent4", "agent5", "agent6"]
        
        bottlenecks = optimizer._identify_bottlenecks(context_bottleneck, agents)
        
        assert "high_complexity_processing" in bottlenecks
        assert "resource_contention" in bottlenecks
        assert "coordination_overhead" in bottlenecks

class TestCollaborationRulesEngine:
    """Collaboration Rules Engine í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def temp_rules_path(self):
        """ì„ì‹œ ê·œì¹™ íŒŒì¼ ê²½ë¡œ"""
        with tempfile.NamedTemporaryFile(delete=False, suffix='.json') as f:
            yield f.name
        try:
            os.unlink(f.name)
        except FileNotFoundError:
            pass
    
    @pytest.fixture
    def rules_engine(self, temp_rules_path):
        """Collaboration Rules Engine í”½ìŠ¤ì²˜"""
        return CollaborationRulesEngine(temp_rules_path)
    
    @pytest.mark.asyncio
    async def test_rules_engine_initialization(self, rules_engine):
        """Rules Engine ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        result = await rules_engine.initialize()
        
        assert "total_rules" in result
        assert "rule_types" in result
        assert result["initialization_status"] == "completed"
        assert result["total_rules"] > 0
        assert len(result["features"]) > 0
    
    @pytest.mark.asyncio
    async def test_start_collaboration(self, rules_engine):
        """í˜‘ì—… ì‹œì‘ í…ŒìŠ¤íŠ¸"""
        await rules_engine.initialize()
        
        context = {
            "task_type": "data_analysis",
            "complexity": "medium",
            "estimated_duration": 5.0
        }
        agents = ["pandas_hub", "eda_tools"]
        
        result = await rules_engine.start_collaboration("test_collab_1", context, agents)
        
        assert "error" not in result
        assert result["collaboration_id"] == "test_collab_1"
        assert result["status"] == CollaborationStatus.ACTIVE.value
        assert result["agents"] == agents
        assert result["applied_rules"] >= 0
        assert "collaboration_features" in result
        
        # í™œì„± í˜‘ì—… ëª©ë¡ í™•ì¸
        assert "test_collab_1" in rules_engine.active_collaborations
    
    @pytest.mark.asyncio
    async def test_update_collaboration_progress(self, rules_engine):
        """í˜‘ì—… ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸ í…ŒìŠ¤íŠ¸"""
        await rules_engine.initialize()
        
        # í˜‘ì—… ì‹œì‘
        context = {"task_type": "analysis"}
        await rules_engine.start_collaboration("test_collab_2", context, ["agent1"])
        
        # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
        progress_data = {
            "event_type": "analysis_complete",
            "agents": ["agent1"],
            "duration": 2.0,
            "success": True,
            "performance_metrics": {"accuracy": 0.95}
        }
        
        result = await rules_engine.update_collaboration_progress("test_collab_2", progress_data)
        
        assert result["collaboration_id"] == "test_collab_2"
        assert result["event_recorded"] is True
        assert result["total_events"] == 1
        assert result["learning_active"] is True
        
        # í˜‘ì—… ì„¸ì…˜ì— ì´ë²¤íŠ¸ê°€ ê¸°ë¡ë˜ì—ˆëŠ”ì§€ í™•ì¸
        collaboration = rules_engine.active_collaborations["test_collab_2"]
        assert "events" in collaboration
        assert len(collaboration["events"]) == 1
    
    @pytest.mark.asyncio
    async def test_complete_collaboration(self, rules_engine):
        """í˜‘ì—… ì™„ë£Œ í…ŒìŠ¤íŠ¸"""
        await rules_engine.initialize()
        
        # í˜‘ì—… ì‹œì‘
        context = {"task_type": "analysis"}
        await rules_engine.start_collaboration("test_collab_3", context, ["agent1"])
        
        # í˜‘ì—… ì™„ë£Œ
        final_metrics = {"final_accuracy": 0.92, "total_time": 3.5}
        result = await rules_engine.complete_collaboration("test_collab_3", success=True, final_metrics=final_metrics)
        
        assert result["collaboration_id"] == "test_collab_3"
        assert result["status"] == CollaborationStatus.COMPLETED.value
        assert result["success"] is True
        assert result["total_duration"] > 0
        assert result["performance_learned"] is True
        
        # í™œì„± í˜‘ì—… ëª©ë¡ì—ì„œ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert "test_collab_3" not in rules_engine.active_collaborations
    
    @pytest.mark.asyncio
    async def test_collaboration_with_conflict(self, rules_engine):
        """ì¶©ëŒì´ ìˆëŠ” í˜‘ì—… í…ŒìŠ¤íŠ¸"""
        await rules_engine.initialize()
        
        # ì¶©ëŒ ìƒí™© ì»¨í…ìŠ¤íŠ¸
        context = {
            "resource_contention": True,
            "task_type": "analysis"
        }
        agents = ["agent1", "agent2"]
        
        result = await rules_engine.start_collaboration("test_collab_conflict", context, agents)
        
        # ì¶©ëŒì´ ê°ì§€ë˜ê³  í•´ê²°ë˜ì—ˆëŠ”ì§€ í™•ì¸
        assert result["conflict_detected"] is True
        assert result["conflict_resolved"] is True
        assert result["status"] == CollaborationStatus.ACTIVE.value
    
    @pytest.mark.asyncio
    async def test_get_collaboration_analytics(self, rules_engine):
        """í˜‘ì—… ë¶„ì„ ì •ë³´ í…ŒìŠ¤íŠ¸"""
        await rules_engine.initialize()
        
        # ëª‡ ê°œì˜ í˜‘ì—… ì‹¤í–‰
        for i in range(3):
            context = {"task_type": f"analysis_{i}"}
            await rules_engine.start_collaboration(f"test_collab_analytics_{i}", context, ["agent1"])
            await rules_engine.complete_collaboration(f"test_collab_analytics_{i}", success=i < 2)
        
        analytics = await rules_engine.get_collaboration_analytics()
        
        assert "rules_summary" in analytics
        assert "collaboration_summary" in analytics
        assert "conflict_analytics" in analytics
        assert "performance_metrics" in analytics
        
        # ê·œì¹™ ìš”ì•½ í™•ì¸
        rules_summary = analytics["rules_summary"]
        assert rules_summary["total_rules"] > 0
        assert rules_summary["active_rules"] > 0

class TestGlobalFunctions:
    """ì „ì—­ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
    
    def test_get_collaboration_rules_engine_singleton(self):
        """Collaboration Rules Engine ì‹±ê¸€í†¤ íŒ¨í„´ í…ŒìŠ¤íŠ¸"""
        engine1 = get_collaboration_rules_engine()
        engine2 = get_collaboration_rules_engine()
        
        # ê°™ì€ ì¸ìŠ¤í„´ìŠ¤ì—¬ì•¼ í•¨
        assert engine1 is engine2
    
    @pytest.mark.asyncio
    async def test_initialize_collaboration_rules_engine_global(self):
        """ì „ì—­ Collaboration Rules Engine ì´ˆê¸°í™” í•¨ìˆ˜ í…ŒìŠ¤íŠ¸"""
        result = await initialize_collaboration_rules_engine()
        
        assert "total_rules" in result
        assert "initialization_status" in result
        assert result["initialization_status"] == "completed"

class TestEnums:
    """ì—´ê±°í˜• í…ŒìŠ¤íŠ¸"""
    
    def test_rule_type_enum(self):
        """RuleType ì—´ê±°í˜• í…ŒìŠ¤íŠ¸"""
        assert RuleType.WORKFLOW.value == "workflow"
        assert RuleType.PRIORITY.value == "priority"
        assert RuleType.RESOURCE.value == "resource"
        assert RuleType.DEPENDENCY.value == "dependency"
        assert RuleType.CONFLICT_RESOLUTION.value == "conflict"
        assert RuleType.PERFORMANCE.value == "performance"
        assert RuleType.COMMUNICATION.value == "communication"
        assert RuleType.QUALITY.value == "quality"
    
    def test_collaboration_status_enum(self):
        """CollaborationStatus ì—´ê±°í˜• í…ŒìŠ¤íŠ¸"""
        assert CollaborationStatus.PENDING.value == "pending"
        assert CollaborationStatus.ACTIVE.value == "active"
        assert CollaborationStatus.COMPLETED.value == "completed"
        assert CollaborationStatus.FAILED.value == "failed"
        assert CollaborationStatus.CONFLICT.value == "conflict"
    
    def test_conflict_type_enum(self):
        """ConflictType ì—´ê±°í˜• í…ŒìŠ¤íŠ¸"""
        assert ConflictType.RESOURCE_CONTENTION.value == "resource_contention"
        assert ConflictType.PRIORITY_CONFLICT.value == "priority_conflict"
        assert ConflictType.DEPENDENCY_CYCLE.value == "dependency_cycle"
        assert ConflictType.COMMUNICATION_BREAKDOWN.value == "communication_breakdown"
        assert ConflictType.PERFORMANCE_DEGRADATION.value == "performance_degradation"
        assert ConflictType.QUALITY_MISMATCH.value == "quality_mismatch"

class TestPerformanceTracking:
    """ì„±ëŠ¥ ì¶”ì  í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    async def engine_with_history(self):
        """ì´ë ¥ì´ ìˆëŠ” Engine ì„¤ì •"""
        with tempfile.NamedTemporaryFile(delete=False, suffix='.json') as f:
            engine = CollaborationRulesEngine(f.name)
            await engine.initialize()
            
            # ì—¬ëŸ¬ í˜‘ì—… ì‹œë®¬ë ˆì´ì…˜
            for i in range(5):
                collab_id = f"perf_test_collab_{i}"
                context = {"task_type": "analysis", "complexity": "medium"}
                agents = ["agent1", "agent2"]
                
                await engine.start_collaboration(collab_id, context, agents)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                progress_data = {
                    "event_type": "progress",
                    "duration": 1.0 + i * 0.5,
                    "success": True,
                    "performance_metrics": {"score": 0.8 + i * 0.05}
                }
                await engine.update_collaboration_progress(collab_id, progress_data)
                
                # ì™„ë£Œ
                await engine.complete_collaboration(collab_id, success=i < 4)
            
            yield engine
            
            # ì •ë¦¬
            try:
                os.unlink(f.name)
            except FileNotFoundError:
                pass
    
    @pytest.mark.asyncio
    async def test_performance_metrics_tracking(self, engine_with_history):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        engine = await engine_with_history
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ í™•ì¸
        assert len(engine.performance_metrics) > 0
        
        # íŒ¨í„´ í•™ìŠµ í™•ì¸
        assert len(engine.pattern_learning.collaboration_history) > 0
        
        # ë¶„ì„ ì •ë³´ í™•ì¸
        analytics = await engine.get_collaboration_analytics()
        assert analytics["rules_summary"]["total_rules"] > 0
        assert analytics["optimization_history"] >= 0

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    import subprocess
    import sys
    
    print("ğŸ¤ Collaboration Rules Engine í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
    
    # pytest ì‹¤í–‰
    result = subprocess.run([
        sys.executable, "-m", "pytest", __file__, "-v", "--tb=short"
    ], capture_output=True, text=True)
    
    print(result.stdout)
    if result.stderr:
        print("STDERR:", result.stderr)
    
    print(f"\nğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {'âœ… ì„±ê³µ' if result.returncode == 0 else 'âŒ ì‹¤íŒ¨'}")
    
    if result.returncode == 0:
        print("ğŸ¤ Collaboration Rules Engineì´ ëª¨ë“  í…ŒìŠ¤íŠ¸ë¥¼ í†µê³¼í–ˆìŠµë‹ˆë‹¤!")
        print("âœ¨ Context Engineering ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤.")
    else:
        print("âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.") 