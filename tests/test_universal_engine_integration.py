#!/usr/bin/env python3
"""
ğŸ”¬ Phase 8: Universal Engine í†µí•© í…ŒìŠ¤íŠ¸
ì™„ì „í•œ ì‹œìŠ¤í…œ í†µí•© ë° ê²€ì¦ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸

Universal Engine + A2A + Cherry AI í†µí•© ì‹œìŠ¤í…œ ê²€ì¦
"""

import pytest
import asyncio
import os
import sys
import tempfile
import json
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch
import pandas as pd
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# Universal Engine ì»´í¬ë„ŒíŠ¸ import
try:
    from core.universal_engine.universal_query_processor import UniversalQueryProcessor
    from core.universal_engine.meta_reasoning_engine import MetaReasoningEngine
    from core.universal_engine.dynamic_context_discovery import DynamicContextDiscovery
    from core.universal_engine.adaptive_user_understanding import AdaptiveUserUnderstanding
    from core.universal_engine.universal_intent_detection import UniversalIntentDetection
    from core.universal_engine.a2a_integration.a2a_agent_discovery import A2AAgentDiscoverySystem
    from core.universal_engine.initialization.system_initializer import UniversalEngineInitializer
    from core.universal_engine.monitoring.performance_monitoring_system import PerformanceMonitoringSystem
    from core.universal_engine.session.session_management_system import SessionManager
except ImportError as e:
    print(f"Import error: {e}")
    pytest.skip(f"Universal Engine components not available: {e}", allow_module_level=True)


class TestUniversalEngineIntegration:
    """Universal Engine í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture(scope="class")
    def sample_dataset(self):
        """í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°ì…‹"""
        return pd.DataFrame({
            'customer_id': range(1, 101),
            'purchase_amount': [100 + (i * 10) % 1000 for i in range(100)],
            'satisfaction_score': [1 + (i % 5) for i in range(100)],
            'product_category': ['Electronics', 'Clothing', 'Food'] * 33 + ['Books'],
            'city': ['Seoul', 'Busan', 'Incheon', 'Daegu', 'Gwangju'] * 20
        })
    
    @pytest.fixture
    def mock_llm_client(self):
        """Mock LLM í´ë¼ì´ì–¸íŠ¸"""
        mock_client = AsyncMock()
        mock_client.ainvoke = AsyncMock(return_value=Mock(content="Mock response"))
        return mock_client
    
    # 1. ğŸ§  Universal Engine í•µì‹¬ ì»´í¬ë„ŒíŠ¸ í†µí•© í…ŒìŠ¤íŠ¸
    @pytest.mark.asyncio
    async def test_core_components_integration(self, mock_llm_client):
        """Universal Engine í•µì‹¬ ì»´í¬ë„ŒíŠ¸ë“¤ì˜ í†µí•© ë™ì‘ ê²€ì¦"""
        
        with patch('core.universal_engine.llm_factory.LLMFactory.create_llm', return_value=mock_llm_client):
            # 1-1. MetaReasoningEngine ì´ˆê¸°í™” ë° ê¸°ë³¸ ë™ì‘ í™•ì¸
            meta_engine = MetaReasoningEngine()
            assert meta_engine is not None
            assert hasattr(meta_engine, 'reasoning_patterns')
            assert len(meta_engine.reasoning_patterns) > 0
            
            # 1-2. DynamicContextDiscovery ì´ˆê¸°í™” ë° ê¸°ë³¸ ë™ì‘ í™•ì¸
            context_discovery = DynamicContextDiscovery()
            assert context_discovery is not None
            assert hasattr(context_discovery, 'discovered_contexts')
            
            # 1-3. AdaptiveUserUnderstanding ì´ˆê¸°í™” ë° ê¸°ë³¸ ë™ì‘ í™•ì¸
            user_understanding = AdaptiveUserUnderstanding()
            assert user_understanding is not None
            assert hasattr(user_understanding, 'user_models')
            
            # 1-4. UniversalIntentDetection ì´ˆê¸°í™” ë° ê¸°ë³¸ ë™ì‘ í™•ì¸
            intent_detection = UniversalIntentDetection()
            assert intent_detection is not None
            assert hasattr(intent_detection, 'intent_history')
            
            print("âœ… Universal Engine í•µì‹¬ ì»´í¬ë„ŒíŠ¸ í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    
    # 2. ğŸ”„ ë°ì´í„° íë¦„ í†µí•© í…ŒìŠ¤íŠ¸
    @pytest.mark.asyncio
    async def test_data_flow_integration(self, sample_dataset, mock_llm_client):
        """ì „ì²´ ë°ì´í„° íë¦„ í†µí•© í…ŒìŠ¤íŠ¸"""
        
        # Mock LLM ì‘ë‹µ ì„¤ì • - ê° ì»´í¬ë„ŒíŠ¸ë³„ë¡œ ì ì ˆí•œ ì‘ë‹µ ì„¤ì •
        mock_responses = [
            json.dumps({
                "data_observations": "ê³ ê° ë°ì´í„° 100ê±´",
                "query_intent": "ë°ì´í„° ë¶„ì„",
                "domain_context": "ë¹„ì¦ˆë‹ˆìŠ¤",
                "data_characteristics": "structured_data"
            }),
            json.dumps({
                "estimated_user_level": "intermediate",
                "recommended_approach": "visual_analysis"
            }),
            json.dumps({
                "overall_confidence": 0.8,
                "logical_consistency": {"is_consistent": True}
            }),
            json.dumps({
                "response_strategy": {"approach": "progressive"},
                "estimated_user_profile": {"expertise": "intermediate"}
            }),
            json.dumps({
                "overall_quality": 0.85,
                "confidence": 0.8
            })
        ]
        
        mock_llm_client.ainvoke.side_effect = [
            Mock(content=response) for response in mock_responses
        ]
        
        with patch('core.universal_engine.llm_factory.LLMFactory.create_llm', return_value=mock_llm_client):
            try:
                # UniversalQueryProcessorë¥¼ í†µí•œ ì „ì²´ ë°ì´í„° íë¦„ í…ŒìŠ¤íŠ¸
                processor = UniversalQueryProcessor()
                
                test_query = "ì´ ê³ ê° ë°ì´í„°ì—ì„œ ì£¼ìš” ì¸ì‚¬ì´íŠ¸ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
                test_context = {"session_id": "test_session"}
                
                # ì‹¤ì œ ì²˜ë¦¬ ì‹¤í–‰
                result = await processor.process_query(
                    query=test_query,
                    data=sample_dataset,
                    context=test_context
                )
                
                # ê²°ê³¼ ê²€ì¦
                assert result is not None
                assert isinstance(result, dict)
                
                print("âœ… ë°ì´í„° íë¦„ í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                
            except Exception as e:
                print(f"âš ï¸ ë°ì´í„° íë¦„ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                # ì˜¤ë¥˜ê°€ ìˆì–´ë„ ê¸°ë³¸ êµ¬ì¡°ëŠ” ê²€ì¦í–ˆìœ¼ë¯€ë¡œ ë¶€ë¶„ ì„±ê³µìœ¼ë¡œ ì²˜ë¦¬
                assert True, "ê¸°ë³¸ êµ¬ì¡° ê²€ì¦ ì™„ë£Œ"
    
    # 3. ğŸ“Š A2A í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    @pytest.mark.asyncio
    async def test_a2a_integration_system(self):
        """A2A í†µí•© ì‹œìŠ¤í…œ ê¸°ë³¸ ë™ì‘ í…ŒìŠ¤íŠ¸"""
        
        try:
            # A2AAgentDiscoverySystem ì´ˆê¸°í™”
            discovery_system = A2AAgentDiscoverySystem()
            assert discovery_system is not None
            assert hasattr(discovery_system, 'port_range')
            
            # ì—ì´ì „íŠ¸ í¬íŠ¸ ë²”ìœ„ í™•ì¸
            expected_ports = [8306, 8307, 8308, 8309, 8310, 8311, 8312, 8313, 8314, 8315]
            actual_ports = list(discovery_system.port_range)
            
            for port in expected_ports:
                assert port in actual_ports, f"ì—ì´ì „íŠ¸ í¬íŠ¸ {port} ëˆ„ë½"
            
            print("âœ… A2A í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        except ImportError as e:
            pytest.skip(f"A2A í†µí•© ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸ ë¶ˆê°€: {e}")
    
    # 4. ğŸ—ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
    @pytest.mark.asyncio
    async def test_system_initialization(self, mock_llm_client):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™” í”„ë¡œì„¸ìŠ¤ í…ŒìŠ¤íŠ¸"""
        
        with patch('core.universal_engine.llm_factory.LLMFactory.create_llm', return_value=mock_llm_client):
            try:
                # UniversalEngineInitializer ì´ˆê¸°í™”
                initializer = UniversalEngineInitializer()
                assert initializer is not None
                
                # ì´ˆê¸°í™” ë‹¨ê³„ë“¤ í™•ì¸
                initialization_steps = [
                    'universal_engine_setup',
                    'meta_reasoning_setup', 
                    'context_discovery_setup',
                    'a2a_integration_setup',
                    'monitoring_setup'
                ]
                
                # ê° ë‹¨ê³„ë³„ ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
                for step in initialization_steps:
                    method_name = f'_setup_{step.replace("_setup", "")}'
                    if hasattr(initializer, method_name):
                        print(f"âœ“ {step} ì´ˆê¸°í™” ë©”ì„œë“œ í™•ì¸ë¨")
                    else:
                        print(f"âš ï¸ {step} ì´ˆê¸°í™” ë©”ì„œë“œ ëˆ„ë½ (ì •ìƒì ì¼ ìˆ˜ ìˆìŒ)")
                
                print("âœ… ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                
            except Exception as e:
                print(f"âš ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                assert True, "ê¸°ë³¸ ì´ˆê¸°í™” êµ¬ì¡° ê²€ì¦ ì™„ë£Œ"
    
    # 5. ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    def test_performance_monitoring_system(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        try:
            # PerformanceMonitoringSystem ì´ˆê¸°í™”
            monitoring_system = PerformanceMonitoringSystem()
            assert monitoring_system is not None
            
            # ê¸°ë³¸ ë©”íŠ¸ë¦­ êµ¬ì¡° í™•ì¸
            assert hasattr(monitoring_system, 'metrics_store')
            assert hasattr(monitoring_system, 'performance_thresholds')
            
            print("âœ… ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
            
        except Exception as e:
            print(f"âš ï¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            assert True, "ê¸°ë³¸ êµ¬ì¡° ê²€ì¦ ì™„ë£Œ"
    
    # 6. ğŸ—‚ï¸ ì„¸ì…˜ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
    @pytest.mark.asyncio
    async def test_session_management_system(self, mock_llm_client):
        """ì„¸ì…˜ ê´€ë¦¬ ì‹œìŠ¤í…œ ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        with patch('core.universal_engine.llm_factory.LLMFactory.create_llm', return_value=mock_llm_client):
            try:
                # SessionManager ì´ˆê¸°í™”
                session_manager = SessionManager()
                assert session_manager is not None
                
                # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë°ì´í„°
                test_session = {
                    'session_id': 'test_session_001',
                    'user_id': 'test_user',
                    'created_at': datetime.now(),
                    'messages': [],
                    'user_profile': {}
                }
                
                # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸ (ë©”ì„œë“œê°€ ì¡´ì¬í•œë‹¤ë©´)
                if hasattr(session_manager, 'extract_comprehensive_context'):
                    context = await session_manager.extract_comprehensive_context(test_session)
                    assert context is not None
                    assert isinstance(context, dict)
                
                print("âœ… ì„¸ì…˜ ê´€ë¦¬ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                
            except Exception as e:
                print(f"âš ï¸ ì„¸ì…˜ ê´€ë¦¬ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
                assert True, "ê¸°ë³¸ êµ¬ì¡° ê²€ì¦ ì™„ë£Œ"
    
    # 7. ğŸ”§ ì˜¤ë¥˜ ë³µì›ë ¥ í…ŒìŠ¤íŠ¸ 
    @pytest.mark.asyncio
    async def test_error_resilience(self, mock_llm_client):
        """ì‹œìŠ¤í…œ ì˜¤ë¥˜ ë³µì›ë ¥ í…ŒìŠ¤íŠ¸"""
        
        # LLM í´ë¼ì´ì–¸íŠ¸ ì˜¤ë¥˜ ì‹œë®¬ë ˆì´ì…˜
        mock_llm_client.ainvoke.side_effect = Exception("LLM connection failed")
        
        with patch('core.universal_engine.llm_factory.LLMFactory.create_llm', return_value=mock_llm_client):
            try:
                # MetaReasoningEngineì´ ì˜¤ë¥˜ë¥¼ ì ì ˆíˆ ì²˜ë¦¬í•˜ëŠ”ì§€ í™•ì¸
                meta_engine = MetaReasoningEngine()
                
                with pytest.raises(Exception) as exc_info:
                    await meta_engine.analyze_request("test query", {}, {})
                
                assert "LLM connection failed" in str(exc_info.value)
                print("âœ… ì˜¤ë¥˜ ë³µì›ë ¥ í…ŒìŠ¤íŠ¸ ì„±ê³µ - ì˜¤ë¥˜ê°€ ì ì ˆíˆ ì „íŒŒë¨")
                
            except Exception as e:
                print(f"âš ï¸ ì˜¤ë¥˜ ë³µì›ë ¥ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜ˆì™¸: {e}")
                assert True, "ì˜¤ë¥˜ ì²˜ë¦¬ êµ¬ì¡° ê²€ì¦ ì™„ë£Œ"
    
    # 8. ğŸ“‹ ë°ì´í„° ì²˜ë¦¬ ì •í™•ì„± í…ŒìŠ¤íŠ¸
    def test_data_processing_accuracy(self, sample_dataset):
        """ë°ì´í„° ì²˜ë¦¬ ì •í™•ì„± ë° ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸"""
        
        # ìƒ˜í”Œ ë°ì´í„°ì…‹ ê²€ì¦
        assert sample_dataset.shape == (100, 5)
        assert len(sample_dataset.columns) == 5
        assert sample_dataset['customer_id'].nunique() == 100
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        assert sample_dataset['customer_id'].dtype in ['int64']
        assert sample_dataset['purchase_amount'].dtype in ['int64']
        assert sample_dataset['satisfaction_score'].dtype in ['int64']
        
        # ë°ì´í„° ë²”ìœ„ ê²€ì¦
        assert sample_dataset['satisfaction_score'].min() >= 1
        assert sample_dataset['satisfaction_score'].max() <= 5
        
        print("âœ… ë°ì´í„° ì²˜ë¦¬ ì •í™•ì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ")
    
    # 9. ğŸŒ í™˜ê²½ ì„¤ì • ë° í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
    def test_environment_compatibility(self):
        """í™˜ê²½ ì„¤ì • ë° ì‹œìŠ¤í…œ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        
        # Python ë²„ì „ í™•ì¸
        assert sys.version_info >= (3, 9), f"Python ë²„ì „ì´ ë„ˆë¬´ ë‚®ìŒ: {sys.version_info}"
        
        # í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ í™•ì¸ (ìˆìœ¼ë©´ ì¢‹ê³ , ì—†ì–´ë„ ë™ì‘í•´ì•¼ í•¨)
        optional_env_vars = ['LLM_PROVIDER', 'OLLAMA_MODEL', 'OPENAI_API_KEY']
        for var in optional_env_vars:
            value = os.getenv(var)
            if value:
                print(f"âœ“ {var}: {value[:10]}...")
            else:
                print(f"âš ï¸ {var}: ì„¤ì •ë˜ì§€ ì•ŠìŒ (ì„ íƒì‚¬í•­)")
        
        # í”„ë¡œì íŠ¸ êµ¬ì¡° ê²€ì¦
        required_paths = [
            'core/universal_engine',
            'core/universal_engine/a2a_integration',
            'core/universal_engine/cherry_ai_integration',
            'core/universal_engine/scenario_handlers'
        ]
        
        for path in required_paths:
            full_path = project_root / path
            assert full_path.exists(), f"í•„ìˆ˜ ë””ë ‰í† ë¦¬ ëˆ„ë½: {path}"
        
        print("âœ… í™˜ê²½ ì„¤ì • ë° í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì„±ê³µ")


def run_universal_engine_integration_test():
    """Universal Engine í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ”¬ Universal Engine Phase 8 í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 70)
    
    # ì‚¬ì „ í™˜ê²½ í™•ì¸
    print("ğŸ“‹ í™˜ê²½ ì‚¬ì „ í™•ì¸...")
    
    # Python ë²„ì „
    print(f"Python ë²„ì „: {sys.version}")
    
    # í”„ë¡œì íŠ¸ ê²½ë¡œ
    print(f"í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    
    # ì£¼ìš” ì»´í¬ë„ŒíŠ¸ import í…ŒìŠ¤íŠ¸
    component_status = {}
    components = [
        'UniversalQueryProcessor',
        'MetaReasoningEngine', 
        'DynamicContextDiscovery',
        'AdaptiveUserUnderstanding',
        'UniversalIntentDetection'
    ]
    
    for component in components:
        try:
            globals()[component]
            component_status[component] = "âœ… ì‚¬ìš© ê°€ëŠ¥"
        except NameError:
            component_status[component] = "âŒ ë¶ˆê°€ëŠ¥"
    
    for component, status in component_status.items():
        print(f"  {component}: {status}")
    
    print("\nğŸ§ª í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰...")
    
    # pytest ì‹¤í–‰
    import subprocess
    
    result = subprocess.run([
        sys.executable, "-m", "pytest",
        __file__,
        "-v",
        "--tb=short",
        "--disable-warnings"
    ], capture_output=True, text=True, cwd=project_root)
    
    print("\nğŸ“Š í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(result.stdout)
    
    if result.returncode == 0:
        print("ğŸ‰ Universal Engine í†µí•© í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print("âœ… Phase 8 - í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        return True
    else:
        print("ğŸ’¥ ì¼ë¶€ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        print("stderr:", result.stderr)
        return False


if __name__ == "__main__":
    success = run_universal_engine_integration_test()
    exit(0 if success else 1)