#!/usr/bin/env python3
"""
ğŸ’ Qwen3-4b-fast ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • E2E í…ŒìŠ¤íŠ¸
LLM First ì›ì¹™ì— ë”°ë¥¸ í•˜ë“œì½”ë”© ì—†ëŠ” ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸

ëª©í‘œ:
- 1ë¶„ ì´ë‚´ ì²˜ë¦¬ (ëª©í‘œ)
- ìµœëŒ€ 2ë¶„ ì´ë‚´ ì²˜ë¦¬ (ì œí•œ)
- LLMì˜ ëŠ¥ë ¥ì„ ìµœëŒ€í•œ í™œìš©í•œ ë™ì  ìµœì í™”
"""

import asyncio
import json
import time
import os
import sys
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import logging

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from core.universal_engine.llm_factory import LLMFactory
from core.universal_engine.universal_query_processor import UniversalQueryProcessor
from core.universal_engine.meta_reasoning_engine import MetaReasoningEngine
from core.universal_engine.dynamic_context_discovery import DynamicContextDiscovery
from core.universal_engine.adaptive_user_understanding import AdaptiveUserUnderstanding
from core.universal_engine.universal_intent_detection import UniversalIntentDetection

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    test_id: str
    timestamp: str
    model_name: str
    total_execution_time: float
    component_times: Dict[str, float]
    quality_score: float
    response_length: int
    success: bool
    optimization_method: str
    meets_time_target: bool
    meets_quality_threshold: bool

class Qwen3FastPerformanceTester:
    """Qwen3-4b-fast ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.test_id = f"qwen3_4b_fast_performance_{int(time.time())}"
        self.results = []
        self.llm_factory = LLMFactory()
        
    async def initialize_components(self) -> Dict[str, Any]:
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ë° ì„±ëŠ¥ ì¸¡ì •"""
        start_time = time.time()
        
        try:
            # LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± (ë™ê¸° í•¨ìˆ˜)
            llm_start = time.time()
            llm_client = self.llm_factory.create_llm_client()
            llm_time = time.time() - llm_start
            
            # Universal Query Processor ì´ˆê¸°í™”
            uqp_start = time.time()
            uqp = UniversalQueryProcessor()
            await uqp.initialize()
            uqp_time = time.time() - uqp_start
            
            # Meta Reasoning Engine ì´ˆê¸°í™”
            mre_start = time.time()
            mre = MetaReasoningEngine()
            mre_time = time.time() - mre_start
            
            # Dynamic Context Discovery ì´ˆê¸°í™”
            dcd_start = time.time()
            dcd = DynamicContextDiscovery()
            dcd_time = time.time() - dcd_start
            
            # Adaptive User Understanding ì´ˆê¸°í™”
            auu_start = time.time()
            auu = AdaptiveUserUnderstanding()
            auu_time = time.time() - auu_start
            
            # Universal Intent Detection ì´ˆê¸°í™”
            uid_start = time.time()
            uid = UniversalIntentDetection()
            uid_time = time.time() - uid_start
            
            total_init_time = time.time() - start_time
            
            return {
                "success": True,
                "llm_time": llm_time,
                "uqp_time": uqp_time,
                "mre_time": mre_time,
                "dcd_time": dcd_time,
                "auu_time": auu_time,
                "uid_time": uid_time,
                "total_init_time": total_init_time,
                "components": {
                    "llm_client": llm_client,
                    "uqp": uqp,
                    "mre": mre,
                    "dcd": dcd,
                    "auu": auu,
                    "uid": uid
                }
            }
            
        except Exception as e:
            logger.error(f"ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def test_simple_query_performance(self, components: Dict[str, Any]) -> PerformanceMetrics:
        """ê°„ë‹¨í•œ ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ (LLMì´ ë™ì ìœ¼ë¡œ ì²˜ë¦¬)
            test_query = "ë°ì´í„° ë¶„ì„ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
            
            # 1ë‹¨ê³„: ì‚¬ìš©ì ì˜ë„ ë¶„ì„
            intent_start = time.time()
            intent_result = await components["uid"].analyze_semantic_space(test_query)
            intent_time = time.time() - intent_start
            
            # 2ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ë°œê²¬
            context_start = time.time()
            context_result = await components["dcd"].analyze_data_characteristics(test_query)
            context_time = time.time() - context_start
            
            # 3ë‹¨ê³„: ì‚¬ìš©ì ìˆ˜ì¤€ ì¶”ì •
            user_start = time.time()
            user_level = await components["auu"].estimate_user_level(test_query, [])
            user_time = time.time() - user_start
            
            # 4ë‹¨ê³„: ë©”íƒ€ ì¶”ë¡ 
            meta_start = time.time()
            meta_result = await components["mre"].perform_meta_reasoning(test_query, {})
            meta_time = time.time() - meta_start
            
            # 5ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„±
            response_start = time.time()
            final_response = await components["uqp"].process_query(test_query, {}, {})
            response_time = time.time() - response_start
            
            total_time = time.time() - start_time
            
            # í’ˆì§ˆ í‰ê°€ (LLM ê¸°ë°˜)
            quality_start = time.time()
            quality_score = await components["mre"].assess_analysis_quality(final_response)
            quality_time = time.time() - quality_start
            
            return PerformanceMetrics(
                test_id=self.test_id,
                timestamp=datetime.now().isoformat(),
                model_name=os.getenv("OLLAMA_MODEL", "qwen3-4b-fast"),
                total_execution_time=total_time,
                component_times={
                    "intent_analysis": intent_time,
                    "context_discovery": context_time,
                    "user_level_estimation": user_time,
                    "meta_reasoning": meta_time,
                    "response_generation": response_time,
                    "quality_assessment": quality_time
                },
                quality_score=quality_score,
                response_length=len(str(final_response)),
                success=True,
                optimization_method="llm_first_dynamic",
                meets_time_target=total_time <= 60,  # 1ë¶„ ëª©í‘œ
                meets_quality_threshold=quality_score >= 0.7
            )
            
        except Exception as e:
            logger.error(f"ê°„ë‹¨í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return PerformanceMetrics(
                test_id=self.test_id,
                timestamp=datetime.now().isoformat(),
                model_name=os.getenv("OLLAMA_MODEL", "qwen3-4b-fast"),
                total_execution_time=time.time() - start_time,
                component_times={},
                quality_score=0.0,
                response_length=0,
                success=False,
                optimization_method="failed",
                meets_time_target=False,
                meets_quality_threshold=False
            )
    
    async def test_complex_query_performance(self, components: Dict[str, Any]) -> PerformanceMetrics:
        """ë³µì¡í•œ ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            # ë³µì¡í•œ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            test_query = """
            ë°˜ë„ì²´ ì œì¡° ê³µì •ì—ì„œ í’ˆì§ˆ ê´€ë¦¬ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. 
            ì›¨ì´í¼ ê²€ì‚¬ ë°ì´í„°, ê³µì • íŒŒë¼ë¯¸í„°, ë¶ˆëŸ‰ë¥  ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.
            ì–´ë–¤ ë¶„ì„ ë°©ë²•ì„ ì‚¬ìš©í•´ì•¼ í•˜ë©°, ì–´ë–¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆì„ê¹Œìš”?
            """
            
            # 1ë‹¨ê³„: ì˜ë¯¸ ê³µê°„ ë¶„ì„
            intent_start = time.time()
            intent_result = await components["uid"].analyze_semantic_space(test_query)
            intent_time = time.time() - intent_start
            
            # 2ë‹¨ê³„: ë„ë©”ì¸ ê°ì§€
            domain_start = time.time()
            domain_result = await components["dcd"].detect_domain({}, test_query)
            domain_time = time.time() - domain_start
            
            # 3ë‹¨ê³„: ì‚¬ìš©ì ìˆ˜ì¤€ ì ì‘
            adapt_start = time.time()
            adapted_response = await components["auu"].adapt_response(test_query, "expert")
            adapt_time = time.time() - adapt_start
            
            # 4ë‹¨ê³„: ë©”íƒ€ ì¶”ë¡  (4ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤)
            meta_start = time.time()
            meta_result = await components["mre"].perform_meta_reasoning(test_query, {})
            meta_time = time.time() - meta_start
            
            # 5ë‹¨ê³„: ìµœì¢… ì‘ë‹µ
            response_start = time.time()
            final_response = await components["uqp"].process_query(test_query, {}, {})
            response_time = time.time() - response_start
            
            total_time = time.time() - start_time
            
            # í’ˆì§ˆ í‰ê°€
            quality_start = time.time()
            quality_score = await components["mre"].assess_analysis_quality(final_response)
            quality_time = time.time() - quality_start
            
            return PerformanceMetrics(
                test_id=self.test_id,
                timestamp=datetime.now().isoformat(),
                model_name=os.getenv("OLLAMA_MODEL", "qwen3-4b-fast"),
                total_execution_time=total_time,
                component_times={
                    "intent_analysis": intent_time,
                    "domain_detection": domain_time,
                    "response_adaptation": adapt_time,
                    "meta_reasoning": meta_time,
                    "response_generation": response_time,
                    "quality_assessment": quality_time
                },
                quality_score=quality_score,
                response_length=len(str(final_response)),
                success=True,
                optimization_method="llm_first_advanced",
                meets_time_target=total_time <= 120,  # 2ë¶„ ì œí•œ
                meets_quality_threshold=quality_score >= 0.8
            )
            
        except Exception as e:
            logger.error(f"ë³µì¡í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return PerformanceMetrics(
                test_id=self.test_id,
                timestamp=datetime.now().isoformat(),
                model_name=os.getenv("OLLAMA_MODEL", "qwen3-4b-fast"),
                total_execution_time=time.time() - start_time,
                component_times={},
                quality_score=0.0,
                response_length=0,
                success=False,
                optimization_method="failed",
                meets_time_target=False,
                meets_quality_threshold=False
            )
    
    async def test_e2e_scenario(self, components: Dict[str, Any]) -> PerformanceMetrics:
        """End-to-End ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        start_time = time.time()
        
        try:
            # E2E ì‹œë‚˜ë¦¬ì˜¤: ë°ì´í„° ë¶„ì„ ìš”ì²­ â†’ ì²˜ë¦¬ â†’ ê²°ê³¼
            scenario_query = """
            ê³ ê° ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ ë§ˆì¼€íŒ… ì „ëµì„ ìˆ˜ë¦½í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.
            ê³ ê°ì˜ êµ¬ë§¤ ì´ë ¥, ì¸êµ¬í†µê³„í•™ì  ì •ë³´, ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸ íŒ¨í„´ì´ ìˆìŠµë‹ˆë‹¤.
            ì–´ë–¤ ë¶„ì„ì„ ìˆ˜í–‰í•´ì•¼ í•˜ë©°, ì–´ë–¤ ë§ˆì¼€íŒ… ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•  ìˆ˜ ìˆì„ê¹Œìš”?
            """
            
            # ì „ì²´ E2E í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
            e2e_start = time.time()
            
            # 1. ì‚¬ìš©ì ë¶„ì„
            user_analysis_start = time.time()
            user_intent = await components["uid"].analyze_semantic_space(scenario_query)
            user_level = await components["auu"].estimate_user_level(scenario_query, [])
            user_analysis_time = time.time() - user_analysis_start
            
            # 2. ì»¨í…ìŠ¤íŠ¸ ë°œê²¬
            context_start = time.time()
            data_characteristics = await components["dcd"].analyze_data_characteristics(scenario_query)
            domain_context = await components["dcd"].detect_domain({}, scenario_query)
            context_time = time.time() - context_start
            
            # 3. ë©”íƒ€ ì¶”ë¡ 
            meta_start = time.time()
            meta_analysis = await components["mre"].perform_meta_reasoning(scenario_query, {})
            quality_assessment = await components["mre"].assess_analysis_quality(meta_analysis)
            meta_time = time.time() - meta_start
            
            # 4. ìµœì¢… ì‘ë‹µ ìƒì„±
            response_start = time.time()
            final_response = await components["uqp"].process_query(scenario_query, {}, {})
            response_time = time.time() - response_start
            
            total_e2e_time = time.time() - e2e_start
            total_time = time.time() - start_time
            
            return PerformanceMetrics(
                test_id=self.test_id,
                timestamp=datetime.now().isoformat(),
                model_name=os.getenv("OLLAMA_MODEL", "qwen3-4b-fast"),
                total_execution_time=total_time,
                component_times={
                    "user_analysis": user_analysis_time,
                    "context_discovery": context_time,
                    "meta_reasoning": meta_time,
                    "final_response": response_time,
                    "total_e2e": total_e2e_time
                },
                quality_score=quality_assessment,
                response_length=len(str(final_response)),
                success=True,
                optimization_method="llm_first_e2e",
                meets_time_target=total_time <= 120,  # 2ë¶„ ì œí•œ
                meets_quality_threshold=quality_assessment >= 0.75
            )
            
        except Exception as e:
            logger.error(f"E2E ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return PerformanceMetrics(
                test_id=self.test_id,
                timestamp=datetime.now().isoformat(),
                model_name=os.getenv("OLLAMA_MODEL", "qwen3-4b-fast"),
                total_execution_time=time.time() - start_time,
                component_times={},
                quality_score=0.0,
                response_length=0,
                success=False,
                optimization_method="failed",
                meets_time_target=False,
                meets_quality_threshold=False
            )
    
    async def run_performance_test_suite(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ’ Qwen3-4b-fast ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        # 1. ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        init_result = await self.initialize_components()
        if not init_result["success"]:
            return {"error": "ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨", "details": init_result}
        
        components = init_result["components"]
        init_time = init_result["total_init_time"]
        
        logger.info(f"âœ… ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ì†Œìš”ì‹œê°„: {init_time:.2f}ì´ˆ)")
        
        # 2. ê°ì¢… í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_results = []
        
        # ê°„ë‹¨í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ” ê°„ë‹¨í•œ ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        simple_result = await self.test_simple_query_performance(components)
        test_results.append(("simple_query", simple_result))
        
        # ë³µì¡í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ” ë³µì¡í•œ ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        complex_result = await self.test_complex_query_performance(components)
        test_results.append(("complex_query", complex_result))
        
        # E2E ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ” E2E ì‹œë‚˜ë¦¬ì˜¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        e2e_result = await self.test_e2e_scenario(components)
        test_results.append(("e2e_scenario", e2e_result))
        
        # 3. ê²°ê³¼ ë¶„ì„
        total_tests = len(test_results)
        successful_tests = sum(1 for _, result in test_results if result.success)
        time_target_met = sum(1 for _, result in test_results if result.meets_time_target)
        quality_target_met = sum(1 for _, result in test_results if result.meets_quality_threshold)
        
        avg_execution_time = sum(result.total_execution_time for _, result in test_results) / total_tests
        avg_quality_score = sum(result.quality_score for _, result in test_results) / total_tests
        
        # 4. ìµœì¢… ê²°ê³¼ ìƒì„±
        final_results = {
            "test_id": self.test_id,
            "timestamp": datetime.now().isoformat(),
            "model_name": os.getenv("OLLAMA_MODEL", "qwen2.5:3b-instruct"),
            "initialization_time": init_time,
            "test_summary": {
                "total_tests": total_tests,
                "successful_tests": successful_tests,
                "success_rate": successful_tests / total_tests,
                "time_target_met": time_target_met,
                "quality_target_met": quality_target_met,
                "avg_execution_time": avg_execution_time,
                "avg_quality_score": avg_quality_score
            },
            "detailed_results": {
                test_name: asdict(result) for test_name, result in test_results
            },
            "performance_assessment": {
                "meets_1min_target": avg_execution_time <= 60,
                "meets_2min_limit": avg_execution_time <= 120,
                "overall_success": successful_tests == total_tests,
                "recommendation": self._generate_recommendation(avg_execution_time, avg_quality_score)
            }
        }
        
        # 5. ê²°ê³¼ ì €ì¥
        output_file = f"qwen25_3b_performance_results_{int(time.time())}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ - ê²°ê³¼ ì €ì¥: {output_file}")
        logger.info(f"ğŸ“Š í‰ê·  ì‹¤í–‰ì‹œê°„: {avg_execution_time:.2f}ì´ˆ")
        logger.info(f"ğŸ“Š í‰ê·  í’ˆì§ˆì ìˆ˜: {avg_quality_score:.2f}")
        logger.info(f"ğŸ¯ 1ë¶„ ëª©í‘œ ë‹¬ì„±: {avg_execution_time <= 60}")
        logger.info(f"ğŸ¯ 2ë¶„ ì œí•œ ì¤€ìˆ˜: {avg_execution_time <= 120}")
        
        return final_results
    
    def _generate_recommendation(self, avg_time: float, avg_quality: float) -> str:
        """ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        if avg_time <= 60 and avg_quality >= 0.7:
            return "âœ… ìµœì  ì„±ëŠ¥: qwen3-4b-fast ëª¨ë¸ì´ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."
        elif avg_time <= 120 and avg_quality >= 0.6:
            return "âš ï¸ ì–‘í˜¸í•œ ì„±ëŠ¥: ì‹œê°„ ì œí•œ ë‚´ì—ì„œ ì ì ˆí•œ í’ˆì§ˆì„ ì œê³µí•©ë‹ˆë‹¤."
        elif avg_time > 120:
            return "âŒ ì„±ëŠ¥ ê°œì„  í•„ìš”: 2ë¶„ ì œí•œì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤."
        else:
            return "âŒ í’ˆì§ˆ ê°œì„  í•„ìš”: í’ˆì§ˆ ì ìˆ˜ê°€ ë‚®ìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤."


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    tester = Qwen3FastPerformanceTester()
    results = await tester.run_performance_test_suite()
    
    if "error" in results:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {results['error']}")
        return
    
    print("\n" + "="*60)
    print("ğŸ’ Qwen3-4b-fast ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š í‰ê·  ì‹¤í–‰ì‹œê°„: {results['test_summary']['avg_execution_time']:.2f}ì´ˆ")
    print(f"ğŸ“Š í‰ê·  í’ˆì§ˆì ìˆ˜: {results['test_summary']['avg_quality_score']:.2f}")
    print(f"ğŸ¯ 1ë¶„ ëª©í‘œ ë‹¬ì„±: {results['performance_assessment']['meets_1min_target']}")
    print(f"ğŸ¯ 2ë¶„ ì œí•œ ì¤€ìˆ˜: {results['performance_assessment']['meets_2min_limit']}")
    print(f"ğŸ’¡ ê¶Œì¥ì‚¬í•­: {results['performance_assessment']['recommendation']}")
    print("="*60)


if __name__ == "__main__":
    asyncio.run(main()) 