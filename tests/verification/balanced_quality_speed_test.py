#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‚öñÔ∏è Í∑†Ìòï Ïû°Ìûå ÌíàÏßà-ÏÜçÎèÑ ÏµúÏ†ÅÌôî ÌÖåÏä§Ìä∏
ÏÜçÎèÑÏôÄ ÌíàÏßàÏùò ÏµúÏ†Å Í∑†ÌòïÏ†êÏùÑ Ï∞æÎäî Ï¢ÖÌï©Ï†Å Í≤ÄÏ¶ù

ÌÖåÏä§Ìä∏ Ï†ÑÎûµ:
1. Îã§ÏñëÌïú ÌíàÏßà ÏàòÏ§ÄÎ≥Ñ ÏÑ±Îä• Ï∏°Ï†ï
2. ÌîÑÎ°¨ÌîÑÌä∏ Î≥µÏû°ÎèÑÎ≥Ñ ÏµúÏ†ÅÌôî Ìö®Í≥º Î∂ÑÏÑù
3. ÌíàÏßà Î©îÌä∏Î¶≠Í≥º ÏùëÎãµ ÏãúÍ∞ÑÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù
4. Ï†ÅÏùëÏ†Å ÏµúÏ†ÅÌôî Ìö®Í≥º Í≤ÄÏ¶ù
5. Ïã§Ïö©Ï†Å E2E ÏãúÎÇòÎ¶¨Ïò§ ÌÖåÏä§Ìä∏
"""

import asyncio
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import json
import statistics

# Universal Engine Components
from core.universal_engine.llm_factory import LLMFactory
from core.universal_engine.universal_query_processor import UniversalQueryProcessor
from core.universal_engine.meta_reasoning_engine import MetaReasoningEngine
from core.universal_engine.adaptive_user_understanding import AdaptiveUserUnderstanding
from core.universal_engine.optimizations.balanced_performance_optimizer import (
    BalancedPerformanceOptimizer,
    BalancedConfig,
    QualityLevel
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BalancedQualitySpeedTest:
    """Í∑†Ìòï Ïû°Ìûå ÌíàÏßà-ÏÜçÎèÑ ÌÖåÏä§Ìä∏"""
    
    def __init__(self):
        """Ï¥àÍ∏∞Ìôî"""
        # Í∑†Ìòï Ïû°Ìûå ÏÑ§Ï†ï
        self.balanced_config = BalancedConfig(
            target_quality_level=QualityLevel.BALANCED,
            max_response_time=10.0,
            min_quality_threshold=0.7,
            adaptive_compression=True,
            quality_monitoring=True,
            fallback_on_timeout=True,
            preserve_technical_content=True
        )
        
        self.optimizer = BalancedPerformanceOptimizer(self.balanced_config)
        
        self.results = {
            "test_id": f"balanced_quality_speed_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "approach": "Balanced Quality-Speed Optimization",
            "test_categories": {
                "quality_level_tests": {},
                "complexity_tests": {},
                "e2e_scenarios": {},
                "adaptive_tests": {}
            },
            "performance_analysis": {},
            "quality_analysis": {},
            "optimization_effectiveness": {},
            "overall_status": "pending"
        }
        
        # Îã§ÏñëÌïú Î≥µÏû°ÎèÑÏùò ÌÖåÏä§Ìä∏ ÏãúÎÇòÎ¶¨Ïò§
        self.test_scenarios = {
            "simple_queries": [
                {
                    "name": "basic_definition",
                    "prompt": "What is artificial intelligence?",
                    "expected_quality": 0.7,
                    "max_time": 6.0,
                    "category": "simple"
                },
                {
                    "name": "simple_comparison",
                    "prompt": "What's the difference between AI and ML?",
                    "expected_quality": 0.7,
                    "max_time": 6.0,
                    "category": "simple"
                }
            ],
            "technical_queries": [
                {
                    "name": "algorithm_explanation",
                    "prompt": "Explain the backpropagation algorithm in neural networks and its mathematical foundation",
                    "expected_quality": 0.8,
                    "max_time": 10.0,
                    "category": "technical"
                },
                {
                    "name": "architecture_design",
                    "prompt": "Design a scalable microservices architecture for a real-time data processing system with fault tolerance",
                    "expected_quality": 0.8,
                    "max_time": 12.0,
                    "category": "technical"
                }
            ],
            "complex_queries": [
                {
                    "name": "multi_step_analysis",
                    "prompt": "Analyze the trade-offs between different machine learning model selection strategies, including cross-validation, information criteria, and Bayesian approaches. Provide practical implementation recommendations.",
                    "expected_quality": 0.85,
                    "max_time": 15.0,
                    "category": "complex"
                }
            ]
        }
        
        logger.info("BalancedQualitySpeedTest initialized with comprehensive test scenarios")
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """Ï¢ÖÌï©Ï†Å Í∑†Ìòï ÌÖåÏä§Ìä∏ Ïã§Ìñâ"""
        print("‚öñÔ∏è Starting Balanced Quality-Speed Optimization Test...")
        print(f"   Target: Quality {self.balanced_config.min_quality_threshold:.1f}+ within {self.balanced_config.max_response_time}s")
        
        try:
            # 1. ÌíàÏßà ÏàòÏ§ÄÎ≥Ñ ÌÖåÏä§Ìä∏
            await self._test_quality_levels()
            
            # 2. Î≥µÏû°ÎèÑÎ≥Ñ ÏµúÏ†ÅÌôî Ìö®Í≥º ÌÖåÏä§Ìä∏
            await self._test_complexity_optimization()
            
            # 3. Ïã§Ï†ú E2E ÏãúÎÇòÎ¶¨Ïò§ ÌÖåÏä§Ìä∏
            await self._test_e2e_scenarios()
            
            # 4. Ï†ÅÏùëÏ†Å ÏµúÏ†ÅÌôî Ìö®Í≥º ÌÖåÏä§Ìä∏
            await self._test_adaptive_optimization()
            
            # 5. Í≤∞Í≥º Î∂ÑÏÑù
            await self._analyze_comprehensive_results()
            
            # 6. Í≤∞Í≥º Ï†ÄÏû•
            await self._save_results()
            
            return self.results
            
        except Exception as e:
            logger.error(f"Comprehensive test failed: {e}")
            self.results["error"] = str(e)
            self.results["overall_status"] = "error"
            return self.results
    
    async def _test_quality_levels(self):
        """ÌíàÏßà ÏàòÏ§ÄÎ≥Ñ ÌÖåÏä§Ìä∏"""
        print("\nüéØ Testing different quality levels...")
        
        test_prompt = "Explain the concept of machine learning and its applications"
        quality_levels = [QualityLevel.MINIMUM, QualityLevel.BALANCED, QualityLevel.HIGH, QualityLevel.PREMIUM]
        
        for quality_level in quality_levels:
            print(f"  üîç Testing {quality_level.value} quality level...")
            
            try:
                start_time = time.time()
                llm_client = LLMFactory.create_llm()
                
                result = await self.optimizer.optimize_with_quality_balance(
                    llm_client, 
                    test_prompt,
                    target_quality=quality_level
                )
                
                execution_time = time.time() - start_time
                
                # Í≤∞Í≥º Î∂ÑÏÑù
                quality_metrics = result.get("quality_metrics", {})
                performance_metrics = result.get("performance_metrics", {})
                
                self.results["test_categories"]["quality_level_tests"][quality_level.value] = {
                    "execution_time": execution_time,
                    "success": result.get("success", False),
                    "quality_score": quality_metrics.get("overall_quality", 0),
                    "response_length": quality_metrics.get("response_length", 0),
                    "compression_ratio": performance_metrics.get("compression_ratio", 1.0),
                    "optimization_method": result.get("optimization_method", "unknown"),
                    "meets_quality_threshold": quality_metrics.get("overall_quality", 0) >= self.balanced_config.min_quality_threshold
                }
                
                if result.get("success", False):
                    quality_score = quality_metrics.get("overall_quality", 0)
                    print(f"    ‚úÖ SUCCESS: {execution_time:.3f}s, Quality: {quality_score:.2f}")
                    print(f"       Response: {quality_metrics.get('response_length', 0)} chars")
                else:
                    print(f"    ‚ùå FAILED: {execution_time:.3f}s")
                
            except Exception as e:
                execution_time = time.time() - start_time
                self.results["test_categories"]["quality_level_tests"][quality_level.value] = {
                    "execution_time": execution_time,
                    "success": False,
                    "error": str(e)
                }
                print(f"    ‚ùå ERROR: {execution_time:.3f}s - {e}")
    
    async def _test_complexity_optimization(self):
        """Î≥µÏû°ÎèÑÎ≥Ñ ÏµúÏ†ÅÌôî Ìö®Í≥º ÌÖåÏä§Ìä∏"""
        print("\nüß† Testing complexity-based optimization...")
        
        for category, scenarios in self.test_scenarios.items():
            print(f"  üìÇ Testing {category}...")
            
            category_results = []
            
            for scenario in scenarios:
                scenario_name = scenario["name"]
                print(f"    üîç {scenario_name}...")
                
                try:
                    start_time = time.time()
                    llm_client = LLMFactory.create_llm()
                    
                    result = await asyncio.wait_for(
                        self.optimizer.optimize_with_quality_balance(
                            llm_client, 
                            scenario["prompt"],
                            target_quality=QualityLevel.BALANCED
                        ),
                        timeout=scenario["max_time"]
                    )
                    
                    execution_time = time.time() - start_time
                    
                    # Í∏∞ÎåÄÏπò ÎåÄÎπÑ ÏÑ±Îä• ÌèâÍ∞Ä
                    quality_metrics = result.get("quality_metrics", {})
                    actual_quality = quality_metrics.get("overall_quality", 0)
                    expected_quality = scenario["expected_quality"]
                    
                    meets_expectations = (
                        execution_time <= scenario["max_time"] and
                        actual_quality >= expected_quality
                    )
                    
                    scenario_result = {
                        "scenario": scenario_name,
                        "execution_time": execution_time,
                        "expected_time": scenario["max_time"],
                        "actual_quality": actual_quality,
                        "expected_quality": expected_quality,
                        "meets_expectations": meets_expectations,
                        "success": result.get("success", False),
                        "optimization_method": result.get("optimization_method", "unknown"),
                        "response_length": quality_metrics.get("response_length", 0)
                    }
                    
                    category_results.append(scenario_result)
                    
                    status = "‚úÖ" if meets_expectations else "‚ö†Ô∏è"
                    print(f"      {status} {execution_time:.3f}s, Quality: {actual_quality:.2f} (expected: {expected_quality:.2f})")
                    
                except asyncio.TimeoutError:
                    execution_time = time.time() - start_time
                    scenario_result = {
                        "scenario": scenario_name,
                        "execution_time": execution_time,
                        "expected_time": scenario["max_time"],
                        "meets_expectations": False,
                        "success": False,
                        "timeout": True
                    }
                    category_results.append(scenario_result)
                    print(f"      ‚è∞ TIMEOUT after {execution_time:.3f}s")
                    
                except Exception as e:
                    execution_time = time.time() - start_time
                    scenario_result = {
                        "scenario": scenario_name,
                        "execution_time": execution_time,
                        "meets_expectations": False,
                        "success": False,
                        "error": str(e)
                    }
                    category_results.append(scenario_result)
                    print(f"      ‚ùå ERROR: {e}")
            
            self.results["test_categories"]["complexity_tests"][category] = category_results
    
    async def _test_e2e_scenarios(self):
        """Ïã§Ï†ú E2E ÏãúÎÇòÎ¶¨Ïò§ ÌÖåÏä§Ìä∏"""
        print("\nüåê Testing real E2E scenarios...")
        
        e2e_scenarios = [
            {
                "name": "user_query_processing",
                "query": "How can I optimize my machine learning model for production deployment?",
                "context": "Production optimization question",
                "target_quality": QualityLevel.HIGH
            },
            {
                "name": "technical_troubleshooting", 
                "query": "My neural network is overfitting. What techniques can I use to improve generalization?",
                "context": "Technical problem-solving",
                "target_quality": QualityLevel.HIGH
            }
        ]
        
        e2e_results = []
        
        for scenario in e2e_scenarios:
            scenario_name = scenario["name"]
            print(f"  üîç E2E Scenario: {scenario_name}...")
            
            try:
                # Ï†ÑÏ≤¥ E2E ÌååÏù¥ÌîÑÎùºÏù∏ ÏãúÎÆ¨Î†àÏù¥ÏÖò
                e2e_start = time.time()
                
                # 1. ÏÇ¨Ïö©Ïûê Ïù¥Ìï¥ Î∂ÑÏÑù
                user_understanding = AdaptiveUserUnderstanding()
                user_analysis_start = time.time()
                user_analysis = await user_understanding.analyze_user_expertise(
                    scenario["query"], []
                )
                user_analysis_time = time.time() - user_analysis_start
                
                # 2. Î©îÌÉÄ Ï∂îÎ°†
                meta_reasoning = MetaReasoningEngine()
                meta_start = time.time()
                meta_result = await meta_reasoning.analyze_request(
                    scenario["query"],
                    {"context": scenario["context"]},
                    {"user_analysis": user_analysis}
                )
                meta_time = time.time() - meta_start
                
                # 3. ÏµúÏ†ÅÌôîÎêú ÏµúÏ¢Ö ÏùëÎãµ ÏÉùÏÑ±
                final_start = time.time()
                llm_client = LLMFactory.create_llm()
                
                final_result = await self.optimizer.optimize_with_quality_balance(
                    llm_client,
                    f"Based on user analysis and meta reasoning, provide a comprehensive response to: {scenario['query']}",
                    context={"user_analysis": user_analysis, "meta_reasoning": meta_result},
                    target_quality=scenario["target_quality"]
                )
                final_time = time.time() - final_start
                
                total_e2e_time = time.time() - e2e_start
                
                # E2E Í≤∞Í≥º Î∂ÑÏÑù
                quality_metrics = final_result.get("quality_metrics", {})
                
                e2e_result = {
                    "scenario": scenario_name,
                    "total_e2e_time": total_e2e_time,
                    "component_times": {
                        "user_analysis": user_analysis_time,
                        "meta_reasoning": meta_time,
                        "final_response": final_time
                    },
                    "final_quality": quality_metrics.get("overall_quality", 0),
                    "final_response_length": quality_metrics.get("response_length", 0),
                    "success": final_result.get("success", False),
                    "optimization_effective": total_e2e_time <= 15.0,  # 15Ï¥à Î™©Ìëú
                    "quality_achieved": quality_metrics.get("overall_quality", 0) >= 0.8
                }
                
                e2e_results.append(e2e_result)
                
                if e2e_result["optimization_effective"] and e2e_result["quality_achieved"]:
                    print(f"    ‚úÖ E2E SUCCESS: {total_e2e_time:.3f}s, Quality: {quality_metrics.get('overall_quality', 0):.2f}")
                else:
                    print(f"    ‚ö†Ô∏è E2E PARTIAL: {total_e2e_time:.3f}s, Quality: {quality_metrics.get('overall_quality', 0):.2f}")
                
                print(f"       Components: User({user_analysis_time:.1f}s) + Meta({meta_time:.1f}s) + Final({final_time:.1f}s)")
                
            except Exception as e:
                total_time = time.time() - e2e_start
                e2e_result = {
                    "scenario": scenario_name,
                    "total_e2e_time": total_time,
                    "success": False,
                    "error": str(e)
                }
                e2e_results.append(e2e_result)
                print(f"    ‚ùå E2E ERROR: {total_time:.3f}s - {e}")
        
        self.results["test_categories"]["e2e_scenarios"] = e2e_results
    
    async def _test_adaptive_optimization(self):
        """Ï†ÅÏùëÏ†Å ÏµúÏ†ÅÌôî Ìö®Í≥º ÌÖåÏä§Ìä∏"""
        print("\nüîÑ Testing adaptive optimization...")
        
        # ÎèôÏùºÌïú ÌîÑÎ°¨ÌîÑÌä∏Î•º Ïó¨Îü¨ Î≤à Ïã§ÌñâÌïòÏó¨ Ï†ÅÏùë Ìö®Í≥º Ï∏°Ï†ï
        test_prompt = "Explain the principles of distributed computing and scalability challenges"
        iterations = 5
        
        adaptive_results = []
        llm_client = LLMFactory.create_llm()
        
        for i in range(iterations):
            print(f"  üîÑ Adaptive iteration {i+1}/{iterations}...")
            
            try:
                start_time = time.time()
                
                result = await self.optimizer.optimize_with_quality_balance(
                    llm_client,
                    test_prompt,
                    target_quality=QualityLevel.BALANCED
                )
                
                execution_time = time.time() - start_time
                quality_metrics = result.get("quality_metrics", {})
                
                # ÏòµÌã∞ÎßàÏù¥Ï†Ä ÏÉÅÌÉú Ï∫°Ï≤ò
                optimizer_summary = self.optimizer.get_optimization_summary()
                
                iteration_result = {
                    "iteration": i + 1,
                    "execution_time": execution_time,
                    "quality_score": quality_metrics.get("overall_quality", 0),
                    "optimization_method": result.get("optimization_method", "unknown"),
                    "adaptive_settings": optimizer_summary["current_settings"].copy(),
                    "cache_size": optimizer_summary["cache_size"],
                    "success": result.get("success", False)
                }
                
                adaptive_results.append(iteration_result)
                
                if result.get("success", False):
                    print(f"    ‚úÖ Iteration {i+1}: {execution_time:.3f}s, Quality: {quality_metrics.get('overall_quality', 0):.2f}")
                else:
                    print(f"    ‚ùå Iteration {i+1}: {execution_time:.3f}s")
                
                # ÏûëÏùÄ ÏßÄÏó∞ÏúºÎ°ú Ï†ÅÏùë Ìö®Í≥º ÌôïÏù∏
                await asyncio.sleep(0.5)
                
            except Exception as e:
                iteration_result = {
                    "iteration": i + 1,
                    "success": False,
                    "error": str(e)
                }
                adaptive_results.append(iteration_result)
                print(f"    ‚ùå Iteration {i+1}: ERROR - {e}")
        
        self.results["test_categories"]["adaptive_tests"] = adaptive_results
        
        # Ï†ÅÏùë Ìö®Í≥º Î∂ÑÏÑù
        successful_iterations = [r for r in adaptive_results if r.get("success", False)]
        if len(successful_iterations) >= 2:
            first_time = successful_iterations[0]["execution_time"]
            last_time = successful_iterations[-1]["execution_time"]
            improvement = (first_time - last_time) / first_time * 100
            
            print(f"    üìà Adaptive improvement: {improvement:+.1f}% time reduction")
    
    async def _analyze_comprehensive_results(self):
        """Ï¢ÖÌï©Ï†Å Í≤∞Í≥º Î∂ÑÏÑù"""
        print("\nüìä Analyzing comprehensive results...")
        
        # ÌíàÏßà ÏàòÏ§ÄÎ≥Ñ Î∂ÑÏÑù
        quality_analysis = {}
        if "quality_level_tests" in self.results["test_categories"]:
            quality_tests = self.results["test_categories"]["quality_level_tests"]
            
            for level, data in quality_tests.items():
                if data.get("success", False):
                    quality_analysis[level] = {
                        "avg_time": data["execution_time"],
                        "quality_score": data["quality_score"],
                        "efficiency_ratio": data["quality_score"] / data["execution_time"] if data["execution_time"] > 0 else 0
                    }
        
        self.results["quality_analysis"] = quality_analysis
        
        # Î≥µÏû°ÎèÑÎ≥Ñ ÏÑ±Îä• Î∂ÑÏÑù
        performance_analysis = {}
        if "complexity_tests" in self.results["test_categories"]:
            complexity_tests = self.results["test_categories"]["complexity_tests"]
            
            for category, scenarios in complexity_tests.items():
                successful_scenarios = [s for s in scenarios if s.get("success", False)]
                if successful_scenarios:
                    avg_time = statistics.mean([s["execution_time"] for s in successful_scenarios])
                    avg_quality = statistics.mean([s.get("actual_quality", 0) for s in successful_scenarios])
                    success_rate = len(successful_scenarios) / len(scenarios)
                    
                    performance_analysis[category] = {
                        "avg_time": avg_time,
                        "avg_quality": avg_quality,
                        "success_rate": success_rate,
                        "meets_expectations_rate": sum(1 for s in successful_scenarios if s.get("meets_expectations", False)) / len(successful_scenarios)
                    }
        
        self.results["performance_analysis"] = performance_analysis
        
        # E2E Ìö®Í≥ºÏÑ± Î∂ÑÏÑù
        e2e_analysis = {}
        if "e2e_scenarios" in self.results["test_categories"]:
            e2e_scenarios = self.results["test_categories"]["e2e_scenarios"]
            successful_e2e = [s for s in e2e_scenarios if s.get("success", False)]
            
            if successful_e2e:
                e2e_analysis = {
                    "avg_total_time": statistics.mean([s["total_e2e_time"] for s in successful_e2e]),
                    "avg_quality": statistics.mean([s.get("final_quality", 0) for s in successful_e2e]),
                    "optimization_effectiveness": sum(1 for s in successful_e2e if s.get("optimization_effective", False)) / len(successful_e2e),
                    "quality_achievement_rate": sum(1 for s in successful_e2e if s.get("quality_achieved", False)) / len(successful_e2e)
                }
        
        self.results["optimization_effectiveness"] = e2e_analysis
        
        # Ï†ÑÏ≤¥ ÌèâÍ∞Ä
        overall_scores = []
        
        # ÌíàÏßà Ï†êÏàò
        if quality_analysis:
            quality_scores = [data["quality_score"] for data in quality_analysis.values()]
            if quality_scores:
                overall_scores.append(statistics.mean(quality_scores))
        
        # ÏÑ±Îä• Ï†êÏàò
        if performance_analysis:
            success_rates = [data["success_rate"] for data in performance_analysis.values()]
            if success_rates:
                overall_scores.append(statistics.mean(success_rates))
        
        # E2E Ï†êÏàò
        if e2e_analysis:
            e2e_score = (e2e_analysis.get("optimization_effectiveness", 0) + e2e_analysis.get("quality_achievement_rate", 0)) / 2
            overall_scores.append(e2e_score)
        
        # Ï†ÑÏ≤¥ ÏÉÅÌÉú Í≤∞Ï†ï
        if overall_scores:
            overall_score = statistics.mean(overall_scores)
            
            if overall_score >= 0.9:
                self.results["overall_status"] = "excellent"
            elif overall_score >= 0.8:
                self.results["overall_status"] = "good"
            elif overall_score >= 0.7:
                self.results["overall_status"] = "acceptable"
            else:
                self.results["overall_status"] = "needs_improvement"
                
            self.results["overall_score"] = overall_score
        else:
            self.results["overall_status"] = "insufficient_data"
        
        print(f"  üìà Overall Status: {self.results['overall_status'].upper()}")
        if "overall_score" in self.results:
            print(f"  üìà Overall Score: {self.results['overall_score']:.2f}")
        
        # ÌíàÏßà-ÏÜçÎèÑ Í∑†Ìòï ÌèâÍ∞Ä
        if quality_analysis and performance_analysis:
            print(f"\n  ‚öñÔ∏è Quality-Speed Balance Analysis:")
            for level, data in quality_analysis.items():
                efficiency = data["efficiency_ratio"]
                print(f"     {level}: {efficiency:.3f} quality/second")
    
    async def _save_results(self):
        """Í≤∞Í≥º Ï†ÄÏû•"""
        output_dir = Path("tests/verification")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"balanced_quality_speed_results_{int(time.time())}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Results saved to: {output_file}")
    
    def print_summary(self):
        """Í≤∞Í≥º ÏöîÏïΩ Ï∂úÎ†•"""
        print("\n" + "="*80)
        print("‚öñÔ∏è Balanced Quality-Speed Optimization Test Summary")
        print("="*80)
        
        print(f"üéØ Overall Status: {self.results['overall_status'].upper()}")
        if "overall_score" in self.results:
            print(f"üìä Overall Score: {self.results['overall_score']:.2f}/1.00")
        
        # ÌíàÏßà ÏàòÏ§ÄÎ≥Ñ Í≤∞Í≥º
        if "quality_analysis" in self.results and self.results["quality_analysis"]:
            print(f"\nüìà Quality Level Performance:")
            for level, data in self.results["quality_analysis"].items():
                efficiency = data["efficiency_ratio"]
                print(f"   {level.upper()}: {data['avg_time']:.2f}s, Quality: {data['quality_score']:.2f}, Efficiency: {efficiency:.3f}")
        
        # Î≥µÏû°ÎèÑÎ≥Ñ Í≤∞Í≥º
        if "performance_analysis" in self.results and self.results["performance_analysis"]:
            print(f"\nüß† Complexity-based Performance:")
            for category, data in self.results["performance_analysis"].items():
                print(f"   {category}: {data['avg_time']:.2f}s, Success: {data['success_rate']*100:.1f}%, Quality: {data['avg_quality']:.2f}")
        
        # E2E Ìö®Í≥ºÏÑ±
        if "optimization_effectiveness" in self.results and self.results["optimization_effectiveness"]:
            e2e = self.results["optimization_effectiveness"]
            print(f"\nüåê E2E Optimization Effectiveness:")
            print(f"   Average E2E Time: {e2e.get('avg_total_time', 0):.2f}s")
            print(f"   Quality Achievement: {e2e.get('quality_achievement_rate', 0)*100:.1f}%")
            print(f"   Optimization Effectiveness: {e2e.get('optimization_effectiveness', 0)*100:.1f}%")
        
        # ÏµúÏ¢Ö ÌèâÍ∞Ä
        print(f"\nüéØ Quality-Speed Balance Assessment:")
        if self.results["overall_status"] == "excellent":
            print("   ‚úÖ EXCELLENT: Optimal balance between quality and speed achieved")
            print("   üöÄ Ready for production deployment with real-time capabilities")
        elif self.results["overall_status"] == "good":
            print("   ‚ö° GOOD: Strong balance with minor optimization opportunities")
            print("   ‚úÖ Suitable for most production scenarios")
        elif self.results["overall_status"] == "acceptable":
            print("   üìà ACCEPTABLE: Reasonable balance with room for improvement")
            print("   ‚ö†Ô∏è May require further tuning for optimal performance")
        else:
            print("   ‚ö†Ô∏è NEEDS IMPROVEMENT: Significant optimization required")
            print("   üîß Consider adjusting quality thresholds or optimization strategies")


async def main():
    """Î©îÏù∏ Ïã§Ìñâ"""
    test = BalancedQualitySpeedTest()
    
    try:
        results = await test.run_comprehensive_test()
        test.print_summary()
        
        return results
        
    except Exception as e:
        print(f"\n‚ùå Balanced quality-speed test failed: {e}")
        logger.error(f"Test error: {e}")


if __name__ == "__main__":
    asyncio.run(main())