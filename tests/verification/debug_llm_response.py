#!/usr/bin/env python3
"""
LLM ì‘ë‹µ êµ¬ì¡° ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import os
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))

from core.universal_engine.llm_factory import LLMFactory

async def debug_llm_response():
    """LLM ì‘ë‹µ êµ¬ì¡° ë””ë²„ê¹…"""
    print("ğŸ” LLM ì‘ë‹µ êµ¬ì¡° ë””ë²„ê¹… ì‹œì‘")
    
    try:
        # LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        llm_client = LLMFactory.create_llm_client()
        print(f"âœ… LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì™„ë£Œ: {type(llm_client)}")
        
        # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸
        prompt = "ì•ˆë…•í•˜ì„¸ìš”"
        print(f"ğŸ“ í”„ë¡¬í”„íŠ¸: {prompt}")
        
        # LLM í˜¸ì¶œ
        print("ğŸ”„ LLM í˜¸ì¶œ ì¤‘...")
        response = await llm_client.agenerate([prompt])
        
        # ì‘ë‹µ êµ¬ì¡° ë¶„ì„
        print(f"ğŸ“Š ì‘ë‹µ íƒ€ì…: {type(response)}")
        print(f"ğŸ“Š ì‘ë‹µ ì†ì„±ë“¤: {dir(response)}")
        
        if hasattr(response, 'generations'):
            print(f"ğŸ“Š generations ì†ì„±: {response.generations}")
            if response.generations:
                print(f"ğŸ“Š ì²« ë²ˆì§¸ generation: {response.generations[0]}")
                if response.generations[0]:
                    print(f"ğŸ“Š ì²« ë²ˆì§¸ generationì˜ íƒ€ì…: {type(response.generations[0][0])}")
                    print(f"ğŸ“Š ì²« ë²ˆì§¸ generationì˜ ì†ì„±ë“¤: {dir(response.generations[0][0])}")
        
        if hasattr(response, 'content'):
            print(f"ğŸ“Š content ì†ì„±: {response.content}")
        
        if hasattr(response, 'text'):
            print(f"ğŸ“Š text ì†ì„±: {response.text}")
        
        # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
        print(f"ğŸ“Š str(response): {str(response)}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

async def main():
    await debug_llm_response()

if __name__ == "__main__":
    asyncio.run(main()) 