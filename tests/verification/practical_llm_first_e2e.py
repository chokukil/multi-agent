#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üéØ Ïã§Ïö©Ï†Å LLM First E2E Í≤ÄÏ¶ù
LLM First ÏõêÏπôÏùÑ Ïú†ÏßÄÌïòÎ©¥ÏÑú ÌòÑÏã§Ï†Å ÏÑ±Îä• Ï†úÏïΩÏùÑ Í≥†Î†§Ìïú Í≤ÄÏ¶ù ÏãúÏä§ÌÖú

ÌïµÏã¨ Ï†ÑÎûµ:
1. LLM First ÏïÑÌÇ§ÌÖçÏ≤ò Í≤ÄÏ¶ù - Ïã§Ï†ú Ïª¥Ìè¨ÎÑåÌä∏ ÎèôÏûë ÌôïÏù∏
2. ÏÉòÌîåÎßÅ Í∏∞Î∞ò ÌÖåÏä§Ìä∏ - ÌïµÏã¨ Í≤ΩÎ°úÎßå LLMÏúºÎ°ú Í≤ÄÏ¶ù
3. Íµ¨Ï°∞Ï†Å Í≤ÄÏ¶ù - LLM First ÏõêÏπô Ï§ÄÏàò Ïó¨Î∂Ä Í≤ÄÏ¶ù
4. Ïã§Ïö©Ï†Å ÏÑ±Îä• - Ïã§Ï†ú Ïö¥ÏòÅ Í∞ÄÎä•Ìïú ÏàòÏ§ÄÏùò Í≤ÄÏ¶ù
"""

import asyncio
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import json

# Universal Engine Components
from core.universal_engine.llm_factory import LLMFactory
from core.universal_engine.universal_query_processor import UniversalQueryProcessor
from core.universal_engine.meta_reasoning_engine import MetaReasoningEngine
from core.universal_engine.adaptive_user_understanding import AdaptiveUserUnderstanding

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PracticalLLMFirstE2E:
    """Ïã§Ïö©Ï†Å LLM First E2E Í≤ÄÏ¶ùÍ∏∞"""
    
    def __init__(self):
        """Ï¥àÍ∏∞Ìôî"""
        self.results = {
            "test_id": f"practical_llm_first_e2e_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "approach": "Practical LLM-First Verification (Architecture + Sampling)",
            "verification_areas": {
                "llm_first_architecture": {"tested": False, "passed": False},
                "zero_hardcoding_compliance": {"tested": False, "passed": False},
                "component_integration": {"tested": False, "passed": False},
                "llm_based_processing": {"tested": False, "passed": False},
                "dynamic_response_capability": {"tested": False, "passed": False}
            },
            "sample_scenarios": {
                "tested": 0,
                "passed": 0,
                "failed": 0,
                "results": {}
            },
            "performance_metrics": {},
            "overall_score": 0.0,
            "overall_status": "pending"
        }
        
        logger.info("PracticalLLMFirstE2E initialized")
    
    async def run_practical_verification(self) -> Dict[str, Any]:
        """Ïã§Ïö©Ï†Å Í≤ÄÏ¶ù Ïã§Ìñâ"""
        print("üéØ Starting Practical LLM-First E2E Verification...")
        print("   Strategy: Architecture Verification + Sample LLM Testing")
        
        try:
            # 1. LLM First ÏïÑÌÇ§ÌÖçÏ≤ò Í≤ÄÏ¶ù
            await self._verify_llm_first_architecture()
            
            # 2. Zero-Hardcoding Ïª¥ÌîåÎùºÏù¥Ïñ∏Ïä§ Í≤ÄÏ¶ù
            await self._verify_zero_hardcoding_compliance()
            
            # 3. Ïª¥Ìè¨ÎÑåÌä∏ ÌÜµÌï© Í≤ÄÏ¶ù
            await self._verify_component_integration()
            
            # 4. LLM Í∏∞Î∞ò Ï≤òÎ¶¨ Í≤ÄÏ¶ù (ÏÉòÌîåÎßÅ)
            await self._verify_llm_based_processing()
            
            # 5. ÎèôÏ†Å ÏùëÎãµ Îä•Î†• Í≤ÄÏ¶ù
            await self._verify_dynamic_response_capability()
            
            # 6. ÏµúÏ¢Ö ÌèâÍ∞Ä
            self._calculate_final_score()
            
            # 7. Í≤∞Í≥º Ï†ÄÏû•
            await self._save_results()
            
            return self.results
            
        except Exception as e:
            logger.error(f"Practical verification failed: {e}")
            self.results["error"] = str(e)
            self.results["overall_status"] = "error"
            return self.results
    
    async def _verify_llm_first_architecture(self):
        """LLM First ÏïÑÌÇ§ÌÖçÏ≤ò Í≤ÄÏ¶ù"""
        print("\\nüèóÔ∏è 1. Verifying LLM-First Architecture...")
        
        verification = self.results["verification_areas"]["llm_first_architecture"]
        verification["tested"] = True
        
        try:
            architecture_checks = []
            
            # 1.1 LLM Factory Í≤ÄÏ¶ù
            start_time = time.time()
            llm_client = LLMFactory.create_llm()
            init_time = time.time() - start_time
            
            architecture_checks.append({
                "component": "LLMFactory",
                "check": "LLM client creation",
                "result": "passed",
                "time": init_time,
                "details": f"Successfully created {type(llm_client).__name__}"
            })
            print(f"   ‚úÖ LLM Factory: {init_time:.3f}s")
            
            # 1.2 Universal Engine Ïª¥Ìè¨ÎÑåÌä∏ ÌôïÏù∏
            components = [
                ("UniversalQueryProcessor", UniversalQueryProcessor),
                ("MetaReasoningEngine", MetaReasoningEngine),
                ("AdaptiveUserUnderstanding", AdaptiveUserUnderstanding)
            ]
            
            for name, component_class in components:
                start_time = time.time()
                component = component_class()
                init_time = time.time() - start_time
                
                # LLM ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Î≥¥Ïú† ÌôïÏù∏
                has_llm = hasattr(component, 'llm_client') and component.llm_client is not None
                
                architecture_checks.append({
                    "component": name,
                    "check": "LLM client integration",
                    "result": "passed" if has_llm else "failed",
                    "time": init_time,
                    "details": f"LLM client: {'Yes' if has_llm else 'No'}"
                })
                
                status = "‚úÖ" if has_llm else "‚ùå"
                print(f"   {status} {name}: {init_time:.3f}s (LLM: {'Yes' if has_llm else 'No'})")
            
            # Î™®Îì† Ï≤¥ÌÅ¨Í∞Ä ÌÜµÍ≥ºÌñàÎäîÏßÄ ÌôïÏù∏
            all_passed = all(check["result"] == "passed" for check in architecture_checks)
            verification["passed"] = all_passed
            verification["details"] = architecture_checks
            
            if all_passed:
                print("   üéØ LLM-First Architecture: VERIFIED")
            else:
                print("   ‚ö†Ô∏è LLM-First Architecture: ISSUES FOUND")
                
        except Exception as e:
            verification["passed"] = False
            verification["error"] = str(e)
            print(f"   ‚ùå Architecture verification failed: {e}")
    
    async def _verify_zero_hardcoding_compliance(self):
        """Zero-Hardcoding Ïª¥ÌîåÎùºÏù¥Ïñ∏Ïä§ Í≤ÄÏ¶ù"""
        print("\\nüö´ 2. Verifying Zero-Hardcoding Compliance...")
        
        verification = self.results["verification_areas"]["zero_hardcoding_compliance"]
        verification["tested"] = True
        
        try:
            # ÏµúÍ∑º ÌïòÎìúÏΩîÎî© Ïª¥ÌîåÎùºÏù¥Ïñ∏Ïä§ Í≤∞Í≥º ÌôïÏù∏
            compliance_files = list(Path("tests/verification").glob("hardcoding_compliance_results_*.json"))
            
            if compliance_files:
                # Í∞ÄÏû• ÏµúÍ∑º ÌååÏùº ÏÑ†ÌÉù
                latest_file = max(compliance_files, key=lambda f: f.stat().st_mtime)
                
                with open(latest_file, 'r') as f:
                    compliance_data = json.load(f)
                
                compliance_score = compliance_data.get("compliance_score", 0)
                violations_found = compliance_data.get("total_violations_found", 999)
                
                # 95% Ïù¥ÏÉÅÏù¥Î©¥ ÌÜµÍ≥º
                verification["passed"] = compliance_score >= 95.0
                verification["details"] = {
                    "compliance_score": compliance_score,
                    "violations_found": violations_found,
                    "source_file": str(latest_file),
                    "timestamp": compliance_data.get("timestamp", "unknown")
                }
                
                status = "‚úÖ" if verification["passed"] else "‚ö†Ô∏è"
                print(f"   {status} Compliance Score: {compliance_score:.1f}%")
                print(f"   üìã Violations Found: {violations_found}")
                
                if verification["passed"]:
                    print("   üéØ Zero-Hardcoding Compliance: VERIFIED")
                else:
                    print("   ‚ö†Ô∏è Zero-Hardcoding Compliance: NEEDS IMPROVEMENT")
            else:
                verification["passed"] = False
                verification["details"] = {"error": "No compliance test results found"}
                print("   ‚ùå No compliance test results found")
                
        except Exception as e:
            verification["passed"] = False
            verification["error"] = str(e)
            print(f"   ‚ùå Compliance verification failed: {e}")
    
    async def _verify_component_integration(self):
        """Ïª¥Ìè¨ÎÑåÌä∏ ÌÜµÌï© Í≤ÄÏ¶ù"""
        print("\\nüîó 3. Verifying Component Integration...")
        
        verification = self.results["verification_areas"]["component_integration"]
        verification["tested"] = True
        
        try:
            integration_checks = []
            
            # UniversalQueryProcessor Ï¥àÍ∏∞Ìôî ÌÖåÏä§Ìä∏
            start_time = time.time()
            query_processor = UniversalQueryProcessor()
            
            # initialize Î©îÏÑúÎìú Ïã§Ìñâ
            init_result = await asyncio.wait_for(
                query_processor.initialize(),
                timeout=10.0  # 10Ï¥à Ï†úÌïú
            )
            
            init_time = time.time() - start_time
            
            integration_checks.append({
                "component": "UniversalQueryProcessor",
                "check": "initialization",
                "result": "passed" if init_result.get("overall_status") == "ready" else "partial",
                "time": init_time,
                "details": init_result.get("overall_status", "unknown")
            })
            
            status = "‚úÖ" if init_result.get("overall_status") == "ready" else "‚ö°"
            print(f"   {status} UniversalQueryProcessor.initialize(): {init_time:.3f}s")
            print(f"      Status: {init_result.get('overall_status', 'unknown')}")
            
            # get_status Î©îÏÑúÎìú ÌÖåÏä§Ìä∏
            start_time = time.time()
            status_result = await asyncio.wait_for(
                query_processor.get_status(),
                timeout=5.0
            )
            status_time = time.time() - start_time
            
            integration_checks.append({
                "component": "UniversalQueryProcessor",
                "check": "status_check",
                "result": "passed",
                "time": status_time,
                "details": status_result.get("overall_status", "unknown")
            })
            
            print(f"   ‚úÖ UniversalQueryProcessor.get_status(): {status_time:.3f}s")
            
            # ÌÜµÌï© ÌèâÍ∞Ä
            verification["passed"] = len(integration_checks) > 0
            verification["details"] = integration_checks
            
            print("   üéØ Component Integration: VERIFIED")
            
        except asyncio.TimeoutError:
            elapsed = time.time() - start_time
            verification["passed"] = False
            verification["error"] = f"Integration test timeout after {elapsed:.3f}s"
            print(f"   ‚è∞ Integration test timeout after {elapsed:.3f}s")
            
        except Exception as e:
            verification["passed"] = False
            verification["error"] = str(e)
            print(f"   ‚ùå Integration verification failed: {e}")
    
    async def _verify_llm_based_processing(self):
        """LLM Í∏∞Î∞ò Ï≤òÎ¶¨ Í≤ÄÏ¶ù (ÏÉòÌîåÎßÅ)"""
        print("\\nü§ñ 4. Verifying LLM-Based Processing (Sampling)...")
        
        verification = self.results["verification_areas"]["llm_based_processing"]
        verification["tested"] = True
        
        try:
            processing_samples = []
            
            # ÏÉòÌîå 1: ÏÇ¨Ïö©Ïûê Î∂ÑÏÑù Ï≤òÎ¶¨
            print("   üîç Sample 1: User Analysis...")
            start_time = time.time()
            
            user_understanding = AdaptiveUserUnderstanding()
            user_result = await asyncio.wait_for(
                user_understanding.analyze_user_expertise("What is machine learning?", []),
                timeout=15.0
            )
            
            sample1_time = time.time() - start_time
            
            processing_samples.append({
                "sample": "user_analysis",
                "query": "What is machine learning?",
                "result": "passed",
                "time": sample1_time,
                "llm_based": True,
                "details": f"Generated {len(str(user_result))} chars of analysis"
            })
            
            print(f"      ‚úÖ User Analysis: {sample1_time:.3f}s")
            print(f"         Generated analysis: {len(str(user_result))} characters")
            
            # ÏÉòÌîå 2: Î©îÌÉÄ Ï∂îÎ°† Ï≤òÎ¶¨
            print("   üîç Sample 2: Meta Reasoning...")
            start_time = time.time()
            
            meta_reasoning = MetaReasoningEngine()
            meta_result = await asyncio.wait_for(
                meta_reasoning.analyze_request(
                    "Simple test query",
                    {"test": "data"},
                    {"context": "verification"}
                ),
                timeout=15.0
            )
            
            sample2_time = time.time() - start_time
            
            processing_samples.append({
                "sample": "meta_reasoning",
                "query": "Simple test query",
                "result": "passed",
                "time": sample2_time,
                "llm_based": True,
                "details": f"Generated {len(str(meta_result))} chars of reasoning"
            })
            
            print(f"      ‚úÖ Meta Reasoning: {sample2_time:.3f}s")
            print(f"         Generated reasoning: {len(str(meta_result))} characters")
            
            # Í≤ÄÏ¶ù ÏôÑÎ£å
            verification["passed"] = len(processing_samples) > 0
            verification["details"] = processing_samples
            
            avg_time = sum(s["time"] for s in processing_samples) / len(processing_samples)
            print(f"   üéØ LLM-Based Processing: VERIFIED (avg: {avg_time:.3f}s)")
            
        except asyncio.TimeoutError:
            verification["passed"] = False
            verification["error"] = "LLM processing timeout"
            print("   ‚è∞ LLM processing timeout")
            
        except Exception as e:
            verification["passed"] = False
            verification["error"] = str(e)
            print(f"   ‚ùå LLM processing verification failed: {e}")
    
    async def _verify_dynamic_response_capability(self):
        """ÎèôÏ†Å ÏùëÎãµ Îä•Î†• Í≤ÄÏ¶ù"""
        print("\\n‚ö° 5. Verifying Dynamic Response Capability...")
        
        verification = self.results["verification_areas"]["dynamic_response_capability"]
        verification["tested"] = True
        
        try:
            # Í∞ÑÎã®Ìïú ÎèôÏ†Å ÏùëÎãµ ÌÖåÏä§Ìä∏
            start_time = time.time()
            
            llm_client = LLMFactory.create_llm()
            
            # ÎèôÏ†Å ÌîÑÎ°¨ÌîÑÌä∏ ÏÉùÏÑ± Î∞è Ïã§Ìñâ
            dynamic_prompt = f"""
            This is a dynamic test at {datetime.now().isoformat()}.
            Respond with a brief confirmation that you can process dynamic requests.
            """
            
            response = await asyncio.wait_for(
                llm_client.ainvoke(dynamic_prompt),
                timeout=10.0
            )
            
            response_time = time.time() - start_time
            response_content = response.content if hasattr(response, 'content') else str(response)
            
            # ÏùëÎãµÏù¥ ÎèôÏ†Å ÏöîÏ≤≠ÏùÑ Ïù¥Ìï¥ÌñàÎäîÏßÄ ÌôïÏù∏ (Í∞ÑÎã®Ìïú ÌÇ§ÏõåÎìú Ï≤¥ÌÅ¨)
            dynamic_indicators = ["dynamic", "test", "confirm", "process", "request"]
            found_indicators = sum(1 for indicator in dynamic_indicators 
                                 if indicator.lower() in response_content.lower())
            
            is_dynamic = found_indicators >= 2  # 2Í∞ú Ïù¥ÏÉÅ ÌÇ§ÏõåÎìú Ìè¨Ìï®Ïãú ÎèôÏ†Å ÏùëÎãµÏúºÎ°ú ÌåêÎã®
            
            verification["passed"] = is_dynamic
            verification["details"] = {
                "response_time": response_time,
                "response_length": len(response_content),
                "dynamic_indicators_found": found_indicators,
                "response_preview": response_content[:100] + "..." if len(response_content) > 100 else response_content
            }
            
            status = "‚úÖ" if is_dynamic else "‚ö†Ô∏è"
            print(f"   {status} Dynamic Response Test: {response_time:.3f}s")
            print(f"      Dynamic indicators found: {found_indicators}/{len(dynamic_indicators)}")
            print(f"      Response preview: {response_content[:80]}...")
            
            if verification["passed"]:
                print("   üéØ Dynamic Response Capability: VERIFIED")
            else:
                print("   ‚ö†Ô∏è Dynamic Response Capability: LIMITED")
                
        except asyncio.TimeoutError:
            verification["passed"] = False
            verification["error"] = "Dynamic response timeout"
            print("   ‚è∞ Dynamic response timeout")
            
        except Exception as e:
            verification["passed"] = False
            verification["error"] = str(e)
            print(f"   ‚ùå Dynamic response verification failed: {e}")
    
    def _calculate_final_score(self):
        """ÏµúÏ¢Ö Ï†êÏàò Í≥ÑÏÇ∞"""
        print("\\nüìä Calculating Final Score...")
        
        verification_areas = self.results["verification_areas"]
        
        # Í∞Å ÏòÅÏó≠Ïùò Í∞ÄÏ§ëÏπò
        weights = {
            "llm_first_architecture": 0.25,      # 25% - Í∏∞Î≥∏ ÏïÑÌÇ§ÌÖçÏ≤ò
            "zero_hardcoding_compliance": 0.25,  # 25% - ÌïòÎìúÏΩîÎî© Ï§ÄÏàò
            "component_integration": 0.20,       # 20% - Ïª¥Ìè¨ÎÑåÌä∏ ÌÜµÌï©
            "llm_based_processing": 0.20,        # 20% - LLM Ï≤òÎ¶¨
            "dynamic_response_capability": 0.10  # 10% - ÎèôÏ†Å ÏùëÎãµ
        }
        
        total_score = 0.0
        
        for area, weight in weights.items():
            if verification_areas[area]["tested"]:
                area_score = 1.0 if verification_areas[area]["passed"] else 0.0
                total_score += area_score * weight
                
                status = "‚úÖ" if verification_areas[area]["passed"] else "‚ùå"
                print(f"   {status} {area}: {area_score * weight:.2f}/{weight:.2f}")
        
        self.results["overall_score"] = total_score
        
        # Ï†ÑÏ≤¥ ÏÉÅÌÉú Í≤∞Ï†ï
        if total_score >= 0.9:
            self.results["overall_status"] = "excellent"
        elif total_score >= 0.8:
            self.results["overall_status"] = "good"
        elif total_score >= 0.7:
            self.results["overall_status"] = "acceptable"
        else:
            self.results["overall_status"] = "needs_improvement"
        
        print(f"\\nüéØ Final Score: {total_score:.2f}/1.00 ({total_score*100:.1f}%)")
        print(f"üèÜ Overall Status: {self.results['overall_status'].upper()}")
    
    async def _save_results(self):
        """Í≤∞Í≥º Ï†ÄÏû•"""
        output_dir = Path("tests/verification")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"practical_llm_first_e2e_results_{int(time.time())}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\\nüíæ Results saved to: {output_file}")
    
    def print_final_summary(self):
        """ÏµúÏ¢Ö ÏöîÏïΩ Ï∂úÎ†•"""
        print("\\n" + "="*60)
        print("üéØ Practical LLM-First E2E Verification Summary")
        print("="*60)
        
        print(f"üéØ Overall Score: {self.results['overall_score']:.2f}/1.00 ({self.results['overall_score']*100:.1f}%)")
        print(f"üèÜ Overall Status: {self.results['overall_status'].upper()}")
        print(f"üöÄ Approach: {self.results['approach']}")
        
        print(f"\\nüìã Verification Areas:")
        for area, data in self.results["verification_areas"].items():
            if data["tested"]:
                status = "‚úÖ PASS" if data["passed"] else "‚ùå FAIL"
                print(f"   {status} {area}")
            else:
                print(f"   ‚è∏Ô∏è SKIP {area}")
        
        print(f"\\nüéØ LLM-First Compliance Status:")
        if self.results["overall_score"] >= 0.8:
            print("   ‚úÖ System demonstrates strong LLM-First architecture")
            print("   ‚úÖ Ready for production deployment")
        elif self.results["overall_score"] >= 0.7:
            print("   ‚ö° System shows good LLM-First principles")
            print("   ‚ö° Minor improvements recommended")
        else:
            print("   ‚ö†Ô∏è System needs LLM-First architecture improvements")
            print("   ‚ö†Ô∏è Significant issues need to be addressed")


async def main():
    """Î©îÏù∏ Ïã§Ìñâ"""
    verification = PracticalLLMFirstE2E()
    
    try:
        results = await verification.run_practical_verification()
        verification.print_final_summary()
        
        return results
        
    except Exception as e:
        print(f"\\n‚ùå Practical verification failed: {e}")
        logger.error(f"Verification error: {e}")


if __name__ == "__main__":
    asyncio.run(main())