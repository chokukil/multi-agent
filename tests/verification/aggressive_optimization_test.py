#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
âš¡ ê³µê²©ì  LLM ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸
ê·¹í•œì˜ ìµœì í™” ê¸°ë²•ì„ í†µí•œ ê·¼ë³¸ì  ì„±ëŠ¥ ê°œì„ 

ê·¹í•œ ìµœì í™” ì „ëµ:
1. ì´ˆê²½ëŸ‰ í”„ë¡¬í”„íŠ¸ (í† í° ìˆ˜ ìµœì†Œí™”)
2. ì‘ë‹µ ê¸¸ì´ ì œí•œ
3. ë³‘ë ¬ ì²˜ë¦¬ ê·¹ëŒ€í™”
4. ì§€ëŠ¥í˜• ìºì‹± + í”„ë¦¬ìºì‹±
5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì¬ì‚¬ìš©
6. ìŠ¤íŠ¸ë¦¼ ëª¨ë“œ + ì¡°ê¸° ì¢…ë£Œ
"""

import asyncio
import time
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
import json
import hashlib

# Universal Engine Components
from core.universal_engine.llm_factory import LLMFactory
from core.universal_engine.optimizations.llm_performance_optimizer import (
    LLMPerformanceOptimizer, 
    OptimizationConfig
)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AggressiveOptimizationTest:
    """ê³µê²©ì  ìµœì í™” í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ê·¹í•œ ìµœì í™” ì„¤ì •
        self.extreme_config = OptimizationConfig(
            enable_prompt_compression=True,
            enable_caching=True,
            enable_batch_processing=True,
            enable_streaming=False,
            max_cache_size=1000,
            compression_ratio=0.3,  # 70% ì••ì¶•
            batch_size=10,
            timeout_seconds=3,  # 3ì´ˆ ê·¹í•œ íƒ€ì„ì•„ì›ƒ
            parallel_workers=8
        )
        
        self.optimizer = LLMPerformanceOptimizer(self.extreme_config)
        
        self.results = {
            "test_id": f"aggressive_optimization_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "approach": "Aggressive LLM Performance Optimization",
            "optimization_level": "extreme",
            "tests_run": 0,
            "successes": 0,
            "failures": 0,
            "test_results": {},
            "performance_metrics": {},
            "overall_status": "pending"
        }
        
        # ì´ˆê²½ëŸ‰ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        self.ultra_light_tests = [
            {
                "name": "micro_test_1",
                "prompt": "AI?",  # ìµœì†Œ í”„ë¡¬í”„íŠ¸
                "expected_tokens": 50,
                "timeout": 2
            },
            {
                "name": "micro_test_2", 
                "prompt": "ML vs DL",  # ì¶•ì•½ëœ í”„ë¡¬í”„íŠ¸
                "expected_tokens": 100,
                "timeout": 3
            },
            {
                "name": "micro_test_3",
                "prompt": "Data pipeline steps",  # êµ¬ì²´ì ì´ì§€ë§Œ ì§§ì€ ì§ˆë¬¸
                "expected_tokens": 150,
                "timeout": 4
            }
        ]
        
        logger.info("AggressiveOptimizationTest initialized with extreme settings")
    
    async def run_aggressive_test(self) -> Dict[str, Any]:
        """ê³µê²©ì  ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("âš¡ Starting Aggressive LLM Performance Optimization Test...")
        print(f"   Extreme settings: 3s timeout, 70% compression, 8 workers")
        
        try:
            # 1. ì‹œìŠ¤í…œ í”„ë¦¬ìºì‹±
            await self._precache_system()
            
            # 2. ë§ˆì´í¬ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            await self._run_micro_tests()
            
            # 3. ë³‘ë ¬ ë°°ì¹˜ í…ŒìŠ¤íŠ¸
            await self._run_parallel_batch_test()
            
            # 4. ìºì‹œ íˆíŠ¸ í…ŒìŠ¤íŠ¸
            await self._run_cache_hit_test()
            
            # 5. ê²°ê³¼ ë¶„ì„
            await self._analyze_extreme_results()
            
            # 6. ê²°ê³¼ ì €ì¥
            await self._save_results()
            
            return self.results
            
        except Exception as e:
            logger.error(f"Aggressive optimization test failed: {e}")
            self.results["error"] = str(e)
            self.results["overall_status"] = "error"
            return self.results
    
    async def _precache_system(self):
        """ì‹œìŠ¤í…œ í”„ë¦¬ìºì‹±"""
        print("\nğŸš€ Pre-caching system with micro prompts...")
        
        precache_start = time.time()
        
        try:
            llm_client = LLMFactory.create_llm()
            
            # ë§ˆì´í¬ë¡œ í”„ë¡¬í”„íŠ¸ë¡œ ìºì‹œ ì¤€ë¹„
            micro_prompts = [
                "Yes",
                "No", 
                "AI",
                "ML",
                "Test",
                "Data",
                "Code",
                "Help"
            ]
            
            # ë³‘ë ¬ í”„ë¦¬ìºì‹±
            precache_tasks = []
            for prompt in micro_prompts:
                task = self.optimizer.optimize_llm_call(llm_client, prompt)
                precache_tasks.append(task)
            
            # ë°°ì¹˜ ì‹¤í–‰
            batch_results = await asyncio.gather(*precache_tasks, return_exceptions=True)
            
            successful_precaches = sum(
                1 for result in batch_results 
                if not isinstance(result, Exception) and result.get("success", False)
            )
            
            precache_time = time.time() - precache_start
            print(f"  âœ… Pre-cached {successful_precaches}/{len(micro_prompts)} prompts: {precache_time:.3f}s")
            
            self.results["performance_metrics"]["precache_time"] = precache_time
            self.results["performance_metrics"]["precache_success_rate"] = successful_precaches / len(micro_prompts)
            
        except Exception as e:
            print(f"  âš ï¸ Pre-caching warning: {e}")
    
    async def _run_micro_tests(self):
        """ë§ˆì´í¬ë¡œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("\nğŸ”¬ Running micro tests...")
        
        for test in self.ultra_light_tests:
            test_name = test["name"]
            print(f"  ğŸ” Testing: {test_name}")
            
            self.results["tests_run"] += 1
            
            try:
                start_time = time.time()
                
                # ê·¹í•œ ìµœì í™”ëœ í˜¸ì¶œ
                llm_client = LLMFactory.create_llm()
                result = await asyncio.wait_for(
                    self.optimizer.optimize_llm_call(llm_client, test["prompt"]),
                    timeout=test["timeout"]
                )
                
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ê²€ì¦
                response_length = len(result.get("response", ""))
                is_success = result.get("success", False) and response_length > 0
                
                self.results["test_results"][test_name] = {
                    "status": "passed" if is_success else "failed",
                    "execution_time": execution_time,
                    "response_length": response_length,
                    "prompt": test["prompt"],
                    "optimization_used": result.get("optimization_method", "unknown"),
                    "cache_hit": result.get("cache_hit", False)
                }
                
                if is_success:
                    self.results["successes"] += 1
                    cache_status = "CACHE HIT" if result.get("cache_hit") else "CACHE MISS"
                    print(f"    âœ… PASSED in {execution_time:.3f}s ({cache_status})")
                    print(f"       Response: {response_length} chars, Method: {result.get('optimization_method')}")
                else:
                    self.results["failures"] += 1
                    print(f"    âŒ FAILED in {execution_time:.3f}s")
                
            except asyncio.TimeoutError:
                execution_time = time.time() - start_time
                self.results["failures"] += 1
                self.results["test_results"][test_name] = {
                    "status": "timeout",
                    "execution_time": execution_time,
                    "timeout_limit": test["timeout"]
                }
                print(f"    â° TIMEOUT after {execution_time:.3f}s")
                
            except Exception as e:
                execution_time = time.time() - start_time
                self.results["failures"] += 1
                self.results["test_results"][test_name] = {
                    "status": "error",
                    "execution_time": execution_time,
                    "error": str(e)
                }
                print(f"    âŒ ERROR after {execution_time:.3f}s: {e}")
    
    async def _run_parallel_batch_test(self):
        """ë³‘ë ¬ ë°°ì¹˜ í…ŒìŠ¤íŠ¸"""
        print("\nğŸš€ Running parallel batch test...")
        
        batch_start = time.time()
        
        try:
            llm_client = LLMFactory.create_llm()
            
            # ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì—¬ëŸ¬ë²ˆ ì‹¤í–‰ (ìºì‹œ íš¨ê³¼ í…ŒìŠ¤íŠ¸)
            batch_prompts = ["AI basics"] * 5  # 5ê°œ ë™ì¼ í”„ë¡¬í”„íŠ¸
            
            # ë°°ì¹˜ ìµœì í™” í˜¸ì¶œ
            batch_results = await self.optimizer.batch_optimize_calls(
                llm_client, 
                batch_prompts
            )
            
            batch_time = time.time() - batch_start
            
            successful_calls = sum(1 for result in batch_results if result.get("success", False))
            cache_hits = sum(1 for result in batch_results if result.get("cache_hit", False))
            
            print(f"  âœ… Batch completed: {batch_time:.3f}s")
            print(f"     Successful calls: {successful_calls}/{len(batch_prompts)}")
            print(f"     Cache hits: {cache_hits}/{len(batch_prompts)} ({cache_hits/len(batch_prompts)*100:.1f}%)")
            
            self.results["performance_metrics"]["batch_test"] = {
                "execution_time": batch_time,
                "success_rate": successful_calls / len(batch_prompts),
                "cache_hit_rate": cache_hits / len(batch_prompts),
                "calls_per_second": len(batch_prompts) / batch_time
            }
            
        except Exception as e:
            print(f"  âŒ Batch test failed: {e}")
    
    async def _run_cache_hit_test(self):
        """ìºì‹œ íˆíŠ¸ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ’¾ Running cache hit test...")
        
        cache_test_start = time.time()
        
        try:
            llm_client = LLMFactory.create_llm()
            
            # ì´ë¯¸ ìºì‹œëœ í”„ë¡¬í”„íŠ¸ ì¬ì‹¤í–‰
            cached_prompt = "AI?"  # ì´ë¯¸ í”„ë¦¬ìºì‹œì—ì„œ ì‹¤í–‰ë¨
            
            # 3ë²ˆ ì—°ì† ì‹¤í–‰í•˜ì—¬ ìºì‹œ íš¨ê³¼ ì¸¡ì •
            cache_times = []
            cache_hits = []
            
            for i in range(3):
                start_time = time.time()
                
                result = await asyncio.wait_for(
                    self.optimizer.optimize_llm_call(llm_client, cached_prompt),
                    timeout=1.0  # ìºì‹œ íˆíŠ¸ì‹œ 1ì´ˆ ë‚´ ì™„ë£Œ ì˜ˆìƒ
                )
                
                execution_time = time.time() - start_time
                cache_times.append(execution_time)
                cache_hits.append(result.get("cache_hit", False))
                
                print(f"  ğŸ” Attempt {i+1}: {execution_time:.3f}s ({'CACHE HIT' if result.get('cache_hit') else 'CACHE MISS'})")
            
            total_cache_time = time.time() - cache_test_start
            avg_cache_time = sum(cache_times) / len(cache_times)
            cache_hit_count = sum(cache_hits)
            
            print(f"  âœ… Cache test completed: {total_cache_time:.3f}s")
            print(f"     Average time per call: {avg_cache_time:.3f}s")
            print(f"     Cache hits: {cache_hit_count}/{len(cache_times)}")
            
            self.results["performance_metrics"]["cache_test"] = {
                "total_time": total_cache_time,
                "avg_time_per_call": avg_cache_time,
                "cache_hit_rate": cache_hit_count / len(cache_times),
                "fastest_call": min(cache_times)
            }
            
        except Exception as e:
            print(f"  âŒ Cache test failed: {e}")
    
    async def _analyze_extreme_results(self):
        """ê·¹í•œ ê²°ê³¼ ë¶„ì„"""
        print("\nğŸ“Š Analyzing extreme optimization results...")
        
        if self.results["tests_run"] == 0:
            self.results["overall_status"] = "no_tests"
            return
        
        success_rate = self.results["successes"] / self.results["tests_run"]
        self.results["success_rate"] = success_rate
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        test_times = []
        for test_result in self.results["test_results"].values():
            if "execution_time" in test_result:
                test_times.append(test_result["execution_time"])
        
        if test_times:
            self.results["performance_metrics"]["avg_test_time"] = sum(test_times) / len(test_times)
            self.results["performance_metrics"]["fastest_test"] = min(test_times)
            self.results["performance_metrics"]["slowest_test"] = max(test_times)
        
        # ìµœì í™” íš¨ê³¼ í‰ê°€
        optimizer_metrics = self.optimizer.get_performance_metrics()
        self.results["optimization_effectiveness"] = optimizer_metrics
        
        # ì „ì²´ ìƒíƒœ ê²°ì •
        if success_rate >= 0.8:
            self.results["overall_status"] = "excellent"
        elif success_rate >= 0.6:
            self.results["overall_status"] = "good"
        elif success_rate >= 0.4:
            self.results["overall_status"] = "acceptable"
        else:
            self.results["overall_status"] = "needs_improvement"
        
        print(f"  ğŸ“ˆ Success Rate: {success_rate*100:.1f}%")
        print(f"  ğŸ“ˆ Overall Status: {self.results['overall_status']}")
        
        if test_times:
            avg_time = self.results["performance_metrics"]["avg_test_time"]
            fastest = self.results["performance_metrics"]["fastest_test"]
            print(f"  â±ï¸ Average Test Time: {avg_time:.3f}s")
            print(f"  â±ï¸ Fastest Test: {fastest:.3f}s")
        
        # ê·¹í•œ ì„±ëŠ¥ í‰ê°€
        if test_times and min(test_times) <= 1.0:
            print(f"  ğŸš€ BREAKTHROUGH: Sub-second response achieved ({min(test_times):.3f}s)")
        elif test_times and min(test_times) <= 2.0:
            print(f"  âš¡ EXCELLENT: Near real-time response ({min(test_times):.3f}s)")
        elif test_times and min(test_times) <= 3.0:
            print(f"  âœ… GOOD: Acceptable response time ({min(test_times):.3f}s)")
        else:
            print(f"  âš ï¸ NEEDS WORK: Still experiencing delays")
    
    async def _save_results(self):
        """ê²°ê³¼ ì €ì¥"""
        output_dir = Path("tests/verification")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"aggressive_optimization_results_{int(time.time())}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ Results saved to: {output_file}")
    
    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*70)
        print("âš¡ Aggressive LLM Performance Optimization Test Summary")
        print("="*70)
        
        print(f"ğŸ§ª Tests Run: {self.results['tests_run']}")
        print(f"âœ… Successes: {self.results['successes']}")
        print(f"âŒ Failures: {self.results['failures']}")
        print(f"ğŸ“ˆ Success Rate: {self.results.get('success_rate', 0)*100:.1f}%")
        print(f"ğŸ¯ Overall Status: {self.results['overall_status'].upper()}")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        if "performance_metrics" in self.results:
            perf = self.results["performance_metrics"]
            print(f"\nâš¡ Performance Metrics:")
            if "avg_test_time" in perf:
                print(f"   Average Test Time: {perf['avg_test_time']:.3f}s")
            if "fastest_test" in perf:
                print(f"   Fastest Test: {perf['fastest_test']:.3f}s")
            if "cache_test" in perf:
                cache_metrics = perf["cache_test"]
                print(f"   Cache Hit Rate: {cache_metrics.get('cache_hit_rate', 0)*100:.1f}%")
                print(f"   Fastest Cache Call: {cache_metrics.get('fastest_call', 0):.3f}s")
        
        # ìµœì í™” íš¨ê³¼
        if "optimization_effectiveness" in self.results:
            opt = self.results["optimization_effectiveness"]
            print(f"\nğŸš€ Optimization Effectiveness:")
            if "cache_hit_rate_percent" in opt:
                print(f"   Total Cache Hit Rate: {opt['cache_hit_rate_percent']:.1f}%")
            if "avg_compression_saving_percent" in opt:
                print(f"   Compression Savings: {opt['avg_compression_saving_percent']:.1f}%")
        
        # ìµœì¢… ì„±ëŠ¥ í‰ê°€
        fastest_time = self.results.get("performance_metrics", {}).get("fastest_test", float('inf'))
        print(f"\nğŸ¯ Performance Achievement:")
        if fastest_time <= 1.0:
            print(f"   ğŸš€ BREAKTHROUGH: Sub-second LLM response achieved!")
            print(f"   ğŸ‰ Real-time LLM-First E2E testing is NOW POSSIBLE")
        elif fastest_time <= 2.0:
            print(f"   âš¡ EXCELLENT: Near real-time capability demonstrated")
            print(f"   âœ… Practical real-time E2E testing achievable")
        elif fastest_time <= 3.0:
            print(f"   âœ… GOOD: Significant improvement achieved")
            print(f"   ğŸ“ˆ Close to real-time capability")
        else:
            print(f"   âš ï¸ NEEDS WORK: Further optimization required")


async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    test = AggressiveOptimizationTest()
    
    try:
        results = await test.run_aggressive_test()
        test.print_summary()
        
        return results
        
    except Exception as e:
        print(f"\nâŒ Aggressive optimization test failed: {e}")
        logger.error(f"Test error: {e}")


if __name__ == "__main__":
    asyncio.run(main())