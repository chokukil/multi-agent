#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¯ LLM-First E2E Scenario Verification
ì™„ì „í•œ LLM ê¸°ë°˜ End-to-End ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦ ì‹œìŠ¤í…œ

í•µì‹¬ ì›ì¹™:
- Zero Rule-based hardcoding
- 100% LLM-based decision making
- Dynamic response generation
- Adaptive verification logic
"""

import asyncio
import logging
import json
import time
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path

# Universal Engine Components (ì‹¤ì œ LLM ê¸°ë°˜ êµ¬í˜„)
from core.universal_engine.llm_factory import LLMFactory
from core.universal_engine.universal_query_processor import UniversalQueryProcessor
from core.universal_engine.meta_reasoning_engine import MetaReasoningEngine
from core.universal_engine.adaptive_user_understanding import AdaptiveUserUnderstanding

logger = logging.getLogger(__name__)

class LLMFirstE2EVerification:
    """
    ì™„ì „í•œ LLM ê¸°ë°˜ E2E ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦ ì‹œìŠ¤í…œ
    
    íŠ¹ì§•:
    - Zero hardcoding: ëª¨ë“  ì‘ë‹µê³¼ ê²€ì¦ì´ LLM ê¸°ë°˜
    - Dynamic evaluation: ì‹¤ì‹œê°„ LLM ê¸°ë°˜ í‰ê°€
    - Adaptive scenarios: LLMì´ ì‹œë‚˜ë¦¬ì˜¤ í•´ì„ ë° ì‘ë‹µ
    - Meta verification: LLMì´ ìì²´ ê²€ì¦ ìˆ˜í–‰
    """
    
    def __init__(self):
        """LLM First E2E ê²€ì¦ê¸° ì´ˆê¸°í™”"""
        # LLM ê¸°ë°˜ Universal Engine ì»´í¬ë„ŒíŠ¸ë“¤
        self.llm_client = LLMFactory.create_llm()
        self.query_processor = UniversalQueryProcessor()
        self.meta_reasoning = MetaReasoningEngine() 
        self.user_understanding = AdaptiveUserUnderstanding()
        
        # ê²€ì¦ ê²°ê³¼ ì €ì¥
        self.verification_results = {
            "test_id": f"llm_first_e2e_{int(time.time())}",
            "timestamp": datetime.now().isoformat(),
            "approach": "100% LLM-First, Zero-Hardcoding",
            "scenarios_tested": 0,
            "scenarios_passed": 0,
            "scenarios_failed": 0,
            "scenario_results": {},
            "llm_evaluation_metrics": {},
            "overall_status": "pending"
        }
        
        # ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜ (LLMì´ í•´ì„í•˜ê³  ì²˜ë¦¬)
        self.test_scenarios = {
            "beginner_scenarios": [
                {
                    "name": "complete_beginner_data_exploration",
                    "query": "I have no idea what this data means. Can you help me?",
                    "context": "First-time user with no data analysis experience",
                    "data": {"type": "sample_dataset", "complexity": "simple"},
                    "success_criteria": "Beginner-friendly explanation with step-by-step guidance"
                },
                {
                    "name": "basic_terminology_explanation", 
                    "query": "What is an average? Why did these numbers come out this way?",
                    "context": "User asking for basic statistical concept explanation",
                    "data": {"type": "numerical_data", "concept": "average"},
                    "success_criteria": "Clear, simple explanation with intuitive examples"
                }
            ],
            "expert_scenarios": [
                {
                    "name": "process_capability_analysis",
                    "query": "Process capability index is 1.2, need to reach 1.33 target. Which process parameters to adjust?",
                    "context": "Industrial expert requiring technical analysis",
                    "data": {"type": "process_data", "domain": "manufacturing"},
                    "success_criteria": "Technical analysis with specific parameter recommendations"
                },
                {
                    "name": "advanced_statistical_analysis",
                    "query": "Multivariate regression R-squared is 0.85 but residual analysis shows suspected heteroscedasticity.",
                    "context": "Statistics expert requiring advanced diagnostic analysis",
                    "data": {"type": "regression_data", "issue": "heteroscedasticity"},
                    "success_criteria": "Expert-level statistical diagnosis with academic rigor"
                }
            ],
            "ambiguous_scenarios": [
                {
                    "name": "vague_anomaly_detection",
                    "query": "Something seems wrong. This looks different from usual.",
                    "context": "Unclear problem with vague description",
                    "data": {"type": "time_series", "anomaly": "unknown"},
                    "success_criteria": "Clarifying questions and systematic exploration approach"
                },
                {
                    "name": "unclear_performance_issue",
                    "query": "The results look weird. Is this correct?",
                    "context": "User questioning results without specific details",
                    "data": {"type": "analysis_results", "concern": "validation"},
                    "success_criteria": "Targeted questions to identify specific concerns"
                }
            ],
            "integrated_scenarios": [
                {
                    "name": "full_system_integration_test",
                    "query": "Find the most important insights from this data",
                    "context": "Comprehensive analysis requiring full system integration",
                    "data": {"type": "complex_dataset", "analysis": "comprehensive"},
                    "success_criteria": "Integrated meta-reasoning with adaptive user understanding"
                }
            ]
        }
        
        logger.info("LLM-First E2E Verification system initialized")
    
    async def run_full_verification(self) -> Dict[str, Any]:
        """
        ì™„ì „í•œ LLM ê¸°ë°˜ E2E ê²€ì¦ ì‹¤í–‰
        """
        logger.info("Starting LLM-First E2E scenario verification...")
        
        try:
            # 1. Universal Engine ì´ˆê¸°í™”
            await self._initialize_universal_engine()
            
            # 2. ê° ì‹œë‚˜ë¦¬ì˜¤ ì¹´í…Œê³ ë¦¬ ì‹¤í–‰
            await self._test_beginner_scenarios()
            await self._test_expert_scenarios() 
            await self._test_ambiguous_scenarios()
            await self._test_integrated_scenarios()
            
            # 3. LLM ê¸°ë°˜ ì „ì²´ í‰ê°€
            await self._perform_llm_meta_evaluation()
            
            # 4. ìµœì¢… ê²°ê³¼ ê³„ì‚°
            self._calculate_final_results()
            
            # 5. ê²°ê³¼ ì €ì¥
            await self._save_verification_results()
            
            return self.verification_results
            
        except Exception as e:
            logger.error(f"E2E verification failed: {e}")
            self.verification_results["error"] = str(e)
            self.verification_results["overall_status"] = "error"
            return self.verification_results
    
    async def _initialize_universal_engine(self):
        """Universal Engine ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        try:
            # UniversalQueryProcessorì˜ initialize ë©”ì„œë“œ ì‚¬ìš©
            init_result = await self.query_processor.initialize()
            logger.info(f"Universal Engine initialization result: {init_result['overall_status']}")
        except Exception as e:
            logger.error(f"Failed to initialize Universal Engine: {e}")
            raise
    
    async def _test_beginner_scenarios(self):
        """ì´ˆì‹¬ì ì‹œë‚˜ë¦¬ì˜¤ LLM ê¸°ë°˜ í…ŒìŠ¤íŠ¸"""
        logger.info("Testing beginner scenarios with LLM...")
        
        for scenario in self.test_scenarios["beginner_scenarios"]:
            scenario_name = f"beginner_{scenario['name']}"
            self.verification_results["scenarios_tested"] += 1
            
            try:
                # LLM ê¸°ë°˜ ì‹¤ì œ ì²˜ë¦¬
                result = await self._execute_llm_scenario(scenario)
                
                # LLM ê¸°ë°˜ í‰ê°€
                evaluation = await self._evaluate_scenario_with_llm(result, scenario)
                
                self.verification_results["scenario_results"][scenario_name] = {
                    "status": "passed" if evaluation["success"] else "failed",
                    "scenario": scenario,
                    "llm_response": result,
                    "llm_evaluation": evaluation,
                    "approach": "100% LLM-based processing and evaluation"
                }
                
                if evaluation["success"]:
                    self.verification_results["scenarios_passed"] += 1
                    logger.info(f"{scenario_name}: PASSED (LLM evaluation)")
                else:
                    self.verification_results["scenarios_failed"] += 1
                    logger.warning(f"{scenario_name}: FAILED (LLM evaluation)")
                    
            except Exception as e:
                self.verification_results["scenarios_failed"] += 1
                self.verification_results["scenario_results"][scenario_name] = {
                    "status": "error",
                    "scenario": scenario,
                    "error": str(e)
                }
                logger.error(f"{scenario_name}: ERROR - {e}")
    
    async def _test_expert_scenarios(self):
        """ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ LLM ê¸°ë°˜ í…ŒìŠ¤íŠ¸"""
        logger.info("Testing expert scenarios with LLM...")
        
        for scenario in self.test_scenarios["expert_scenarios"]:
            scenario_name = f"expert_{scenario['name']}"
            self.verification_results["scenarios_tested"] += 1
            
            try:
                # LLM ê¸°ë°˜ ì‹¤ì œ ì²˜ë¦¬
                result = await self._execute_llm_scenario(scenario)
                
                # LLM ê¸°ë°˜ í‰ê°€
                evaluation = await self._evaluate_scenario_with_llm(result, scenario)
                
                self.verification_results["scenario_results"][scenario_name] = {
                    "status": "passed" if evaluation["success"] else "failed",
                    "scenario": scenario,
                    "llm_response": result,
                    "llm_evaluation": evaluation,
                    "approach": "100% LLM-based processing and evaluation"
                }
                
                if evaluation["success"]:
                    self.verification_results["scenarios_passed"] += 1
                    logger.info(f"{scenario_name}: PASSED (LLM evaluation)")
                else:
                    self.verification_results["scenarios_failed"] += 1
                    logger.warning(f"{scenario_name}: FAILED (LLM evaluation)")
                    
            except Exception as e:
                self.verification_results["scenarios_failed"] += 1
                self.verification_results["scenario_results"][scenario_name] = {
                    "status": "error",
                    "scenario": scenario,
                    "error": str(e)
                }
                logger.error(f"{scenario_name}: ERROR - {e}")
    
    async def _test_ambiguous_scenarios(self):
        """ëª¨í˜¸í•œ ì‹œë‚˜ë¦¬ì˜¤ LLM ê¸°ë°˜ í…ŒìŠ¤íŠ¸"""
        logger.info("Testing ambiguous scenarios with LLM...")
        
        for scenario in self.test_scenarios["ambiguous_scenarios"]:
            scenario_name = f"ambiguous_{scenario['name']}"
            self.verification_results["scenarios_tested"] += 1
            
            try:
                # LLM ê¸°ë°˜ ì‹¤ì œ ì²˜ë¦¬
                result = await self._execute_llm_scenario(scenario)
                
                # LLM ê¸°ë°˜ í‰ê°€
                evaluation = await self._evaluate_scenario_with_llm(result, scenario)
                
                self.verification_results["scenario_results"][scenario_name] = {
                    "status": "passed" if evaluation["success"] else "failed",
                    "scenario": scenario,
                    "llm_response": result,
                    "llm_evaluation": evaluation,
                    "approach": "100% LLM-based processing and evaluation"
                }
                
                if evaluation["success"]:
                    self.verification_results["scenarios_passed"] += 1
                    logger.info(f"{scenario_name}: PASSED (LLM evaluation)")
                else:
                    self.verification_results["scenarios_failed"] += 1
                    logger.warning(f"{scenario_name}: FAILED (LLM evaluation)")
                    
            except Exception as e:
                self.verification_results["scenarios_failed"] += 1
                self.verification_results["scenario_results"][scenario_name] = {
                    "status": "error",
                    "scenario": scenario,
                    "error": str(e)
                }
                logger.error(f"{scenario_name}: ERROR - {e}")
    
    async def _test_integrated_scenarios(self):
        """í†µí•© ì‹œë‚˜ë¦¬ì˜¤ LLM ê¸°ë°˜ í…ŒìŠ¤íŠ¸"""
        logger.info("Testing integrated scenarios with LLM...")
        
        for scenario in self.test_scenarios["integrated_scenarios"]:
            scenario_name = f"integrated_{scenario['name']}"
            self.verification_results["scenarios_tested"] += 1
            
            try:
                # LLM ê¸°ë°˜ ì‹¤ì œ ì²˜ë¦¬ (ì „ì²´ ì‹œìŠ¤í…œ í†µí•©)
                result = await self._execute_integrated_llm_scenario(scenario)
                
                # LLM ê¸°ë°˜ í†µí•© í‰ê°€
                evaluation = await self._evaluate_integration_with_llm(result, scenario)
                
                self.verification_results["scenario_results"][scenario_name] = {
                    "status": "passed" if evaluation["success"] else "failed",
                    "scenario": scenario,
                    "llm_response": result,
                    "llm_evaluation": evaluation,
                    "integration_metrics": evaluation.get("integration_metrics", {}),
                    "approach": "Full Universal Engine integration with LLM evaluation"
                }
                
                if evaluation["success"]:
                    self.verification_results["scenarios_passed"] += 1
                    logger.info(f"{scenario_name}: PASSED (LLM integration evaluation)")
                else:
                    self.verification_results["scenarios_failed"] += 1
                    logger.warning(f"{scenario_name}: FAILED (LLM integration evaluation)")
                    
            except Exception as e:
                self.verification_results["scenarios_failed"] += 1
                self.verification_results["scenario_results"][scenario_name] = {
                    "status": "error",
                    "scenario": scenario,
                    "error": str(e)
                }
                logger.error(f"{scenario_name}: ERROR - {e}")
    
    async def _execute_llm_scenario(self, scenario: Dict) -> Dict[str, Any]:
        """
        LLM ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ (Zero hardcoding)
        """
        try:
            # 1. ì‚¬ìš©ì ì´í•´ ë¶„ì„
            user_analysis = await self.user_understanding.analyze_user_expertise(
                query=scenario["query"],
                interaction_history=[]
            )
            
            # 2. ë©”íƒ€ ì¶”ë¡  ìˆ˜í–‰ (ì‹¤ì œ êµ¬í˜„ëœ ë©”ì„œë“œ ì‚¬ìš©)
            meta_reasoning_result = await self.meta_reasoning.analyze_request(
                query=scenario["query"],
                data=scenario["data"],
                context={
                    "scenario_context": scenario["context"],
                    "user_analysis": user_analysis
                }
            )
            
            # 3. ì¿¼ë¦¬ ì²˜ë¦¬ (ì‹¤ì œ êµ¬í˜„ëœ ë©”ì„œë“œ ì‚¬ìš©)
            query_result = await self.query_processor.process_query(
                query=scenario["query"],
                data=scenario["data"],
                context={
                    "user_analysis": user_analysis,
                    "meta_reasoning": meta_reasoning_result
                }
            )
            
            return {
                "scenario_id": scenario["name"],
                "user_analysis": user_analysis,
                "meta_reasoning": meta_reasoning_result,
                "query_processing": query_result,
                "processing_approach": "100% LLM-based, Zero hardcoding",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            return {
                "scenario_id": scenario["name"],
                "error": str(e),
                "processing_approach": "LLM-based processing failed",
                "timestamp": datetime.now().isoformat()
            }
    
    async def _execute_integrated_llm_scenario(self, scenario: Dict) -> Dict[str, Any]:
        """
        í†µí•© ì‹œìŠ¤í…œ LLM ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
        """
        try:
            # ì „ì²´ Universal Engine íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì‹¤ì œ êµ¬í˜„ëœ ë©”ì„œë“œ ì‚¬ìš©)
            integrated_result = await self.query_processor.process_query(
                query=scenario["query"],
                data=scenario["data"],
                context={"integrated_test": True, "scenario_context": scenario["context"]}
            )
            
            return {
                "scenario_id": scenario["name"],
                "integrated_processing": integrated_result,
                "system_integration": "Full Universal Engine pipeline",
                "processing_approach": "Complete LLM-First architecture",
                "timestamp": datetime.now().isoformat()
            }
            
        except Exception as e:
            # í†µí•© ì‹œìŠ¤í…œ ì‹¤í–‰ ì‹¤íŒ¨ì‹œ ê°œë³„ ì»´í¬ë„ŒíŠ¸ë¡œ ì‹œë®¬ë ˆì´ì…˜
            return await self._execute_llm_scenario(scenario)
    
    async def _evaluate_scenario_with_llm(self, result: Dict, scenario: Dict) -> Dict[str, Any]:
        """
        LLM ê¸°ë°˜ ì‹œë‚˜ë¦¬ì˜¤ í‰ê°€ (Zero rule-based evaluation)
        """
        evaluation_prompt = f"""
        ë‹¤ìŒ ì‹œë‚˜ë¦¬ì˜¤ì˜ LLM ì‘ë‹µì„ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        ì‹œë‚˜ë¦¬ì˜¤:
        - ì´ë¦„: {scenario['name']}
        - ì¿¼ë¦¬: {scenario['query']}
        - ì»¨í…ìŠ¤íŠ¸: {scenario['context']}
        - ì„±ê³µ ê¸°ì¤€: {scenario['success_criteria']}
        
        LLM ì‘ë‹µ:
        {json.dumps(result, indent=2)}
        
        ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
        1. ì„±ê³µ ê¸°ì¤€ ì¶©ì¡±ë„ (0-1.0)
        2. ì‘ë‹µì˜ ì ì ˆì„± (ì‚¬ìš©ì ë ˆë²¨ì— ë§ëŠ”ì§€)
        3. ì •í™•ì„±ê³¼ ìœ ìš©ì„±
        4. LLM-First ì›ì¹™ ì¤€ìˆ˜ë„
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "success": true/false,
            "success_criteria_score": 0.0-1.0,
            "appropriateness_score": 0.0-1.0,
            "accuracy_score": 0.0-1.0,
            "llm_first_compliance": 0.0-1.0,
            "overall_score": 0.0-1.0,
            "strengths": ["strength1", "strength2"],
            "weaknesses": ["weakness1", "weakness2"],
            "reasoning": "detailed evaluation reasoning"
        }}
        """
        
        try:
            llm_evaluation_response = await self.llm_client.ainvoke(evaluation_prompt)
            evaluation_result = json.loads(llm_evaluation_response.content)
            
            # LLM í‰ê°€ ê²°ê³¼ ê²€ì¦
            evaluation_result["evaluation_method"] = "100% LLM-based evaluation"
            evaluation_result["evaluator"] = "LLM meta-evaluation"
            
            return evaluation_result
            
        except Exception as e:
            # LLM í‰ê°€ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì‹¤íŒ¨ ì²˜ë¦¬
            return {
                "success": False,
                "overall_score": 0.0,
                "error": f"LLM evaluation failed: {e}",
                "evaluation_method": "LLM evaluation failed"
            }
    
    async def _evaluate_integration_with_llm(self, result: Dict, scenario: Dict) -> Dict[str, Any]:
        """
        LLM ê¸°ë°˜ í†µí•© ì‹œìŠ¤í…œ í‰ê°€
        """
        integration_evaluation_prompt = f"""
        ë‹¤ìŒ í†µí•© ì‹œìŠ¤í…œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        ì‹œë‚˜ë¦¬ì˜¤: {scenario['name']}
        ì¿¼ë¦¬: {scenario['query']}
        í†µí•© ê²°ê³¼: {json.dumps(result, indent=2)}
        
        ë‹¤ìŒ í†µí•© íŠ¹ì„±ì„ í‰ê°€í•´ì£¼ì„¸ìš”:
        1. Meta-reasoning í†µí•©ë„
        2. Context discovery íš¨ê³¼ì„±
        3. User understanding ì ì‘ì„±
        4. A2A integration ë™ì‘ì„±
        5. ì „ì²´ ì‹œìŠ¤í…œ cohesion
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "success": true/false,
            "integration_metrics": {{
                "meta_reasoning_integration": 0.0-1.0,
                "context_discovery_effectiveness": 0.0-1.0,
                "user_adaptation": 0.0-1.0,
                "a2a_integration": 0.0-1.0,
                "system_cohesion": 0.0-1.0
            }},
            "overall_integration_score": 0.0-1.0,
            "integration_strengths": [],
            "integration_weaknesses": [],
            "reasoning": "detailed integration evaluation"
        }}
        """
        
        try:
            llm_integration_response = await self.llm_client.ainvoke(integration_evaluation_prompt)
            integration_result = json.loads(llm_integration_response.content)
            
            integration_result["evaluation_method"] = "LLM-based integration evaluation"
            return integration_result
            
        except Exception as e:
            return {
                "success": False,
                "overall_integration_score": 0.0,
                "error": f"LLM integration evaluation failed: {e}",
                "evaluation_method": "LLM integration evaluation failed"
            }
    
    async def _perform_llm_meta_evaluation(self):
        """
        LLM ê¸°ë°˜ ì „ì²´ E2E í…ŒìŠ¤íŠ¸ ë©”íƒ€ í‰ê°€
        """
        meta_evaluation_prompt = f"""
        ì „ì²´ E2E í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë©”íƒ€ í‰ê°€í•´ì£¼ì„¸ìš”:
        
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½:
        - ì „ì²´ ì‹œë‚˜ë¦¬ì˜¤: {self.verification_results['scenarios_tested']}ê°œ
        - ì„±ê³µ: {self.verification_results['scenarios_passed']}ê°œ
        - ì‹¤íŒ¨: {self.verification_results['scenarios_failed']}ê°œ
        
        ê°œë³„ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼:
        {json.dumps(self.verification_results['scenario_results'], indent=2)}
        
        ë‹¤ìŒ ê´€ì ì—ì„œ ì¢…í•© í‰ê°€í•´ì£¼ì„¸ìš”:
        1. LLM-First ì›ì¹™ ì¤€ìˆ˜ë„
        2. Zero-Hardcoding ë‹¬ì„±ë„
        3. ì‹œìŠ¤í…œ í†µí•© ì™„ì„±ë„
        4. ì‹¤ìš©ì„± ë° íš¨ìš©ì„±
        5. ì „ì²´ í’ˆì§ˆ ìˆ˜ì¤€
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "llm_first_compliance": 0.0-1.0,
            "zero_hardcoding_achievement": 0.0-1.0,
            "system_integration_completeness": 0.0-1.0,
            "practical_utility": 0.0-1.0,
            "overall_quality": 0.0-1.0,
            "meta_assessment": "ì¢…í•© í‰ê°€",
            "recommendations": ["ê°œì„ ì‚¬í•­1", "ê°œì„ ì‚¬í•­2"],
            "system_readiness": "production_ready/needs_improvement/not_ready"
        }}
        """
        
        try:
            meta_response = await self.llm_client.ainvoke(meta_evaluation_prompt)
            meta_result = json.loads(meta_response.content)
            
            self.verification_results["llm_evaluation_metrics"] = meta_result
            
        except Exception as e:
            self.verification_results["llm_evaluation_metrics"] = {
                "error": f"LLM meta evaluation failed: {e}",
                "meta_evaluation_method": "LLM meta evaluation failed"
            }
    
    def _calculate_final_results(self):
        """ìµœì¢… ê²°ê³¼ ê³„ì‚°"""
        if self.verification_results["scenarios_tested"] == 0:
            self.verification_results["overall_status"] = "no_tests"
            self.verification_results["success_rate"] = 0.0
            return
        
        success_rate = (self.verification_results["scenarios_passed"] / 
                       self.verification_results["scenarios_tested"]) * 100
        
        self.verification_results["success_rate"] = success_rate
        
        # LLM ë©”íƒ€ í‰ê°€ ê¸°ë°˜ ì „ì²´ ìƒíƒœ ê²°ì •
        llm_metrics = self.verification_results.get("llm_evaluation_metrics", {})
        system_readiness = llm_metrics.get("system_readiness", "unknown")
        
        if success_rate == 100.0 and system_readiness == "production_ready":
            self.verification_results["overall_status"] = "excellent"
        elif success_rate >= 85.0 and system_readiness in ["production_ready", "needs_improvement"]:
            self.verification_results["overall_status"] = "good"
        elif success_rate >= 70.0:
            self.verification_results["overall_status"] = "acceptable"
        else:
            self.verification_results["overall_status"] = "needs_improvement"
    
    async def _save_verification_results(self):
        """ê²€ì¦ ê²°ê³¼ ì €ì¥"""
        output_dir = Path("tests/verification")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_file = output_dir / f"llm_first_e2e_results_{int(time.time())}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.verification_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"LLM-First E2E verification results saved to: {output_file}")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    verification = LLMFirstE2EVerification()
    
    try:
        results = await verification.run_full_verification()
        
        print("\\nLLM-First E2E Scenario Verification")
        print("=" * 60)
        print(f"\\nE2E Scenario Results Summary:")
        print(f"Overall Status: {results['overall_status']}")
        print(f"Scenarios Tested: {results['scenarios_tested']}")
        print(f"Scenarios Passed: {results['scenarios_passed']}")
        print(f"Scenarios Failed: {results['scenarios_failed']}")
        print(f"Success Rate: {results.get('success_rate', 0):.1f}%")
        print(f"Approach: {results['approach']}")
        
        # LLM ë©”íƒ€ í‰ê°€ ê²°ê³¼ ì¶œë ¥
        llm_metrics = results.get("llm_evaluation_metrics", {})
        if llm_metrics and "error" not in llm_metrics:
            print(f"\\nLLM Meta-Evaluation Metrics:")
            print(f"LLM-First Compliance: {llm_metrics.get('llm_first_compliance', 0):.1%}")
            print(f"Zero-Hardcoding Achievement: {llm_metrics.get('zero_hardcoding_achievement', 0):.1%}")
            print(f"System Integration: {llm_metrics.get('system_integration_completeness', 0):.1%}")
            print(f"Overall Quality: {llm_metrics.get('overall_quality', 0):.1%}")
            print(f"System Readiness: {llm_metrics.get('system_readiness', 'unknown')}")
        
        # ìƒíƒœë³„ ë©”ì‹œì§€
        if results["overall_status"] == "excellent":
            print("\\nğŸ‰ Excellent! All scenarios work perfectly with LLM-First approach!")
        elif results["overall_status"] == "good":
            print("\\nâœ… Good! Most scenarios work well with LLM-First approach!")
        elif results["overall_status"] == "acceptable":
            print("\\nâš ï¸ Acceptable, but LLM-First implementation needs improvement.")
        else:
            print("\\nâŒ Needs significant improvements in LLM-First implementation.")
        
        # ì‹¤íŒ¨í•œ ì‹œë‚˜ë¦¬ì˜¤ ì¶œë ¥
        failed_scenarios = [name for name, result in results["scenario_results"].items() 
                          if result["status"] == "failed"]
        if failed_scenarios:
            print(f"\\nFailed scenarios ({len(failed_scenarios)}):")
            for scenario in failed_scenarios:
                print(f"   - {scenario}")
        
    except Exception as e:
        print(f"\\nLLM-First E2E verification failed: {e}")
        logger.error(f"E2E verification error: {e}")


if __name__ == "__main__":
    asyncio.run(main())