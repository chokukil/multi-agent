"""
ì„±ëŠ¥ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸

Task 4.2.2: ì„±ëŠ¥ ë° ë¶€í•˜ í…ŒìŠ¤íŠ¸ - ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸ ë° ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰/ì‘ë‹µ ì‹œê°„ ë²¤ì¹˜ë§ˆí¬

í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:
1. ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸
3. ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸
4. ì•„í‹°íŒ©íŠ¸ ë Œë”ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
5. ê²°ê³¼ í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
6. ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
7. ë„¤íŠ¸ì›Œí¬ ë¶€í•˜ í…ŒìŠ¤íŠ¸
8. í™•ì¥ì„± í…ŒìŠ¤íŠ¸
9. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
10. ì„±ëŠ¥ íšŒê·€ í…ŒìŠ¤íŠ¸
"""

import unittest
import time
import threading
import psutil
import pandas as pd
import numpy as np
import json
import concurrent.futures
from datetime import datetime, timedelta
from typing import Dict, List, Any, Tuple
import sys
import os
from unittest.mock import patch, MagicMock
import gc
import memory_profiler

# ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '../..'))

from modules.artifacts.a2a_artifact_extractor import A2AArtifactExtractor
from modules.ui.real_time_artifact_renderer import RealTimeArtifactRenderer
from modules.integration.agent_result_collector import AgentResultCollector
from modules.integration.result_integrator import MultiAgentResultIntegrator
from modules.performance.performance_optimizer import PerformanceOptimizer
from modules.scalability.scalability_manager import ScalabilityManager

class PerformanceTestBase(unittest.TestCase):
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.start_time = time.time()
        self.initial_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        # ì„±ëŠ¥ ê¸°ì¤€ê°’ (ì¡°ì • ê°€ëŠ¥)
        self.PERFORMANCE_THRESHOLDS = {
            'max_response_time': 3.0,  # 3ì´ˆ
            'max_memory_usage': 500,   # 500MB
            'min_throughput': 10,      # 10 requests/second
            'max_error_rate': 0.05     # 5%
        }
    
    def tearDown(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        end_time = time.time()
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        
        execution_time = end_time - self.start_time
        memory_usage = final_memory - self.initial_memory
        
        print(f"\nğŸ“Š Performance Metrics:")
        print(f"   â±ï¸  Execution Time: {execution_time:.3f}s")
        print(f"   ğŸ§  Memory Usage: {memory_usage:.2f}MB")
        print(f"   ğŸ’¾ Final Memory: {final_memory:.2f}MB")
    
    def measure_performance(self, func, *args, **kwargs) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì¸¡ì • í—¬í¼ í•¨ìˆ˜"""
        
        # ì´ˆê¸° ìƒíƒœ ì¸¡ì •
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        start_cpu = psutil.cpu_percent()
        
        # í•¨ìˆ˜ ì‹¤í–‰
        try:
            result = func(*args, **kwargs)
            success = True
            error = None
        except Exception as e:
            result = None
            success = False
            error = str(e)
        
        # ìµœì¢… ìƒíƒœ ì¸¡ì •
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        end_cpu = psutil.cpu_percent()
        
        return {
            'result': result,
            'success': success,
            'error': error,
            'execution_time': end_time - start_time,
            'memory_usage': end_memory - start_memory,
            'peak_memory': end_memory,
            'cpu_usage': end_cpu - start_cpu,
            'timestamp': datetime.now()
        }

class TestLargeDatasetProcessing(PerformanceTestBase):
    """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        super().setUp()
        self.extractor = A2AArtifactExtractor()
        self.renderer = RealTimeArtifactRenderer()
    
    def _generate_large_dataset(self, rows: int, columns: int) -> Dict[str, Any]:
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±"""
        
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´
        
        # ì»¬ëŸ¼ëª… ìƒì„±
        column_names = [f"column_{i}" for i in range(columns)]
        
        # ë°ì´í„° ìƒì„± (ë‹¤ì–‘í•œ íƒ€ì… í¬í•¨)
        data = []
        for i in range(rows):
            row = []
            for j in range(columns):
                if j % 4 == 0:  # ì •ìˆ˜
                    row.append(np.random.randint(1, 1000))
                elif j % 4 == 1:  # ì‹¤ìˆ˜
                    row.append(round(np.random.random() * 100, 2))
                elif j % 4 == 2:  # ë¬¸ìì—´
                    row.append(f"value_{i}_{j}")
                else:  # ë¶ˆë¦°
                    row.append(bool(np.random.randint(0, 2)))
            data.append(row)
        
        return {
            "columns": column_names,
            "data": data,
            "index": list(range(rows))
        }
    
    def test_small_dataset_processing(self):
        """ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (1K rows, 10 columns)"""
        
        dataset = self._generate_large_dataset(1000, 10)
        
        def process_dataset():
            a2a_response = {
                "agent_id": "performance_test_agent",
                "status": "completed",
                "artifacts": [{
                    "type": "dataframe",
                    "title": "Small Dataset",
                    "data": dataset,
                    "metadata": {"rows": 1000, "columns": 10}
                }]
            }
            return self.extractor.extract_artifacts(a2a_response)
        
        # ì„±ëŠ¥ ì¸¡ì •
        metrics = self.measure_performance(process_dataset)
        
        # ê²€ì¦
        self.assertTrue(metrics['success'])
        self.assertIsNotNone(metrics['result'])
        self.assertEqual(len(metrics['result']), 1)
        
        # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
        self.assertLess(metrics['execution_time'], 1.0)  # 1ì´ˆ ì´ë‚´
        self.assertLess(metrics['memory_usage'], 50)     # 50MB ì´ë‚´
        
        print(f"Small Dataset Processing: {metrics['execution_time']:.3f}s, {metrics['memory_usage']:.2f}MB")
    
    def test_medium_dataset_processing(self):
        """ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (10K rows, 20 columns)"""
        
        dataset = self._generate_large_dataset(10000, 20)
        
        def process_dataset():
            a2a_response = {
                "agent_id": "performance_test_agent",
                "status": "completed", 
                "artifacts": [{
                    "type": "dataframe",
                    "title": "Medium Dataset",
                    "data": dataset,
                    "metadata": {"rows": 10000, "columns": 20}
                }]
            }
            return self.extractor.extract_artifacts(a2a_response)
        
        # ì„±ëŠ¥ ì¸¡ì •
        metrics = self.measure_performance(process_dataset)
        
        # ê²€ì¦
        self.assertTrue(metrics['success'])
        self.assertIsNotNone(metrics['result'])
        
        # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
        self.assertLess(metrics['execution_time'], 2.0)  # 2ì´ˆ ì´ë‚´
        self.assertLess(metrics['memory_usage'], 100)    # 100MB ì´ë‚´
        
        print(f"Medium Dataset Processing: {metrics['execution_time']:.3f}s, {metrics['memory_usage']:.2f}MB")
    
    def test_large_dataset_processing(self):
        """ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ (100K rows, 50 columns)"""
        
        dataset = self._generate_large_dataset(100000, 50)
        
        def process_dataset():
            a2a_response = {
                "agent_id": "performance_test_agent",
                "status": "completed",
                "artifacts": [{
                    "type": "dataframe", 
                    "title": "Large Dataset",
                    "data": dataset,
                    "metadata": {"rows": 100000, "columns": 50}
                }]
            }
            return self.extractor.extract_artifacts(a2a_response)
        
        # ì„±ëŠ¥ ì¸¡ì •
        metrics = self.measure_performance(process_dataset)
        
        # ê²€ì¦
        self.assertTrue(metrics['success'])
        self.assertIsNotNone(metrics['result'])
        
        # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸ (ëŒ€ìš©ëŸ‰ì´ë¯€ë¡œ ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        self.assertLess(metrics['execution_time'], 5.0)  # 5ì´ˆ ì´ë‚´
        self.assertLess(metrics['memory_usage'], 300)    # 300MB ì´ë‚´
        
        print(f"Large Dataset Processing: {metrics['execution_time']:.3f}s, {metrics['memory_usage']:.2f}MB")
    
    @patch('streamlit.dataframe')
    def test_large_dataset_rendering_performance(self, mock_dataframe):
        """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ë Œë”ë§ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        # ëŒ€ìš©ëŸ‰ ì°¨íŠ¸ ë°ì´í„° ìƒì„± (10K points)
        large_chart_data = {
            "data": [{
                "x": list(range(10000)),
                "y": np.random.randn(10000).tolist(),
                "type": "scatter",
                "mode": "markers"
            }],
            "layout": {
                "title": "Large Chart (10K points)",
                "xaxis": {"title": "X"},
                "yaxis": {"title": "Y"}
            }
        }
        
        from modules.artifacts.a2a_artifact_extractor import ArtifactInfo, ArtifactType
        
        large_chart_artifact = ArtifactInfo(
            artifact_id="large_chart",
            type=ArtifactType.PLOTLY_CHART,
            title="Large Chart",
            data=large_chart_data,
            agent_id="test_agent",
            created_at=datetime.now(),
            metadata={"data_points": 10000}
        )
        
        # ë Œë”ë§ ì„±ëŠ¥ ì¸¡ì •
        def render_large_chart():
            return self.renderer.render_artifact(large_chart_artifact)
        
        with patch('streamlit.plotly_chart'):
            metrics = self.measure_performance(render_large_chart)
        
        # ê²€ì¦
        self.assertTrue(metrics['success'])
        self.assertLess(metrics['execution_time'], 2.0)  # 2ì´ˆ ì´ë‚´
        
        print(f"Large Chart Rendering: {metrics['execution_time']:.3f}s")

class TestConcurrentUserLoad(PerformanceTestBase):
    """ë™ì‹œ ì‚¬ìš©ì ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        super().setUp()
        self.scalability_manager = ScalabilityManager()
        self.collector = AgentResultCollector()
        self.integrator = MultiAgentResultIntegrator()
    
    def _simulate_user_session(self, user_id: int, session_duration: float = 1.0) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì„¸ì…˜ ì‹œë®¬ë ˆì´ì…˜"""
        
        session_start = time.time()
        
        try:
            # ì„¸ì…˜ ì‹œì‘
            session_info = self.scalability_manager.session_manager.create_session(
                user_id=f"user_{user_id}",
                metadata={"test_session": True}
            )
            
            # ì‘ì—… ì‹œë®¬ë ˆì´ì…˜ (ë°ì´í„° ì²˜ë¦¬)
            time.sleep(session_duration * 0.1)  # ì‹¤ì œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
            
            # ê²°ê³¼ ìƒì„±
            from modules.integration.agent_result_collector import AgentResult
            
            test_result = AgentResult(
                agent_id=f"agent_{user_id}",
                agent_type="test",
                status="completed",
                start_time=datetime.now(),
                end_time=datetime.now(),
                data={"user_id": user_id, "processed": True},
                artifacts=[],
                metadata={"session_id": session_info.session_id}
            )
            
            # ê²°ê³¼ ìˆ˜ì§‘
            self.collector.add_result(test_result)
            
            # ì„¸ì…˜ ì¢…ë£Œ
            self.scalability_manager.session_manager.end_session(session_info.session_id)
            
            session_end = time.time()
            
            return {
                "user_id": user_id,
                "session_id": session_info.session_id,
                "duration": session_end - session_start,
                "success": True,
                "error": None
            }
            
        except Exception as e:
            return {
                "user_id": user_id,
                "session_id": None,
                "duration": time.time() - session_start,
                "success": False,
                "error": str(e)
            }
    
    def test_low_concurrency_load(self):
        """ì €ë¶€í•˜ ë™ì‹œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ (5 users)"""
        
        num_users = 5
        session_duration = 0.5  # 0.5ì´ˆ
        
        # ë™ì‹œ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_users) as executor:
            futures = [
                executor.submit(self._simulate_user_session, i, session_duration)
                for i in range(num_users)
            ]
            
            # ê²°ê³¼ ìˆ˜ì§‘
            results = []
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                results.append(result)
        
        # ì„±ê³µë¥  í™•ì¸
        success_count = sum(1 for r in results if r['success'])
        success_rate = success_count / len(results)
        
        # í‰ê·  ì‘ë‹µ ì‹œê°„
        avg_duration = sum(r['duration'] for r in results) / len(results)
        
        # ê²€ì¦
        self.assertGreatterEqual(success_rate, 0.95)  # 95% ì„±ê³µë¥ 
        self.assertLess(avg_duration, 1.0)  # í‰ê·  1ì´ˆ ì´ë‚´
        
        print(f"Low Concurrency Load: {success_count}/{num_users} users, {avg_duration:.3f}s avg")
    
    def test_medium_concurrency_load(self):
        """ì¤‘ê°„ ë¶€í•˜ ë™ì‹œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ (20 users)"""
        
        num_users = 20
        session_duration = 1.0  # 1ì´ˆ
        
        start_time = time.time()
        
        # ë™ì‹œ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_users) as executor:
            futures = [
                executor.submit(self._simulate_user_session, i, session_duration)
                for i in range(num_users)
            ]
            
            results = []
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                results.append(result)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        success_count = sum(1 for r in results if r['success'])
        success_rate = success_count / len(results)
        throughput = success_count / total_time
        avg_duration = sum(r['duration'] for r in results if r['success']) / max(success_count, 1)
        
        # ê²€ì¦
        self.assertGreaterEqual(success_rate, 0.90)  # 90% ì„±ê³µë¥ 
        self.assertGreater(throughput, 5)  # ìµœì†Œ 5 requests/second
        self.assertLess(avg_duration, 2.0)  # í‰ê·  2ì´ˆ ì´ë‚´
        
        print(f"Medium Concurrency Load: {success_count}/{num_users} users, {throughput:.1f} req/s, {avg_duration:.3f}s avg")
    
    def test_high_concurrency_load(self):
        """ê³ ë¶€í•˜ ë™ì‹œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ (50 users)"""
        
        num_users = 50
        session_duration = 1.5  # 1.5ì´ˆ
        
        start_time = time.time()
        
        # ì„œí‚· ë¸Œë ˆì´ì»¤ ë° ë¶€í•˜ ë¶„ì‚° í™œì„±í™”
        self.scalability_manager.enable_circuit_breaker()
        self.scalability_manager.enable_load_balancing()
        
        # ë™ì‹œ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
        with concurrent.futures.ThreadPoolExecutor(max_workers=num_users) as executor:
            futures = [
                executor.submit(self._simulate_user_session, i, session_duration)
                for i in range(num_users)
            ]
            
            results = []
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                results.append(result)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê³„ì‚°
        success_count = sum(1 for r in results if r['success'])
        success_rate = success_count / len(results)
        throughput = success_count / total_time
        avg_duration = sum(r['duration'] for r in results if r['success']) / max(success_count, 1)
        
        # ì—ëŸ¬ ë¶„ì„
        errors = [r['error'] for r in results if not r['success']]
        error_rate = len(errors) / len(results)
        
        # ê²€ì¦ (ê³ ë¶€í•˜ì—ì„œëŠ” ë” ê´€ëŒ€í•œ ê¸°ì¤€)
        self.assertGreaterEqual(success_rate, 0.80)  # 80% ì„±ê³µë¥ 
        self.assertLessEqual(error_rate, 0.20)  # 20% ì´í•˜ ì—ëŸ¬ìœ¨
        self.assertLess(avg_duration, 3.0)  # í‰ê·  3ì´ˆ ì´ë‚´
        
        print(f"High Concurrency Load: {success_count}/{num_users} users, {throughput:.1f} req/s, {error_rate:.2%} errors")

class TestMemoryUsageBenchmark(PerformanceTestBase):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        super().setUp()
        self.performance_optimizer = PerformanceOptimizer()
        
        # ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì‹œì‘
        self.performance_optimizer.start_monitoring()
    
    def tearDown(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        super().tearDown()
        self.performance_optimizer.stop_monitoring()
    
    @memory_profiler.profile
    def test_memory_usage_with_caching(self):
        """ìºì‹± ì‚¬ìš© ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        
        # ëŒ€ëŸ‰ì˜ ë°ì´í„° ìƒì„± ë° ìºì‹±
        large_data_items = []
        
        for i in range(100):
            data = {
                "id": i,
                "data": list(range(1000)),  # 1K integers
                "metadata": {"processed": True, "timestamp": datetime.now().isoformat()}
            }
            
            # ìºì‹œì— ì €ì¥
            cache_key = f"test_data_{i}"
            self.performance_optimizer.cache_artifact(cache_key, data, ttl=300, priority=1)
            large_data_items.append(cache_key)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_stats = self.performance_optimizer.get_performance_summary()
        
        # ìºì‹œ íˆíŠ¸ í…ŒìŠ¤íŠ¸
        cache_hits = 0
        for key in large_data_items[:50]:  # ì²« 50ê°œ í•­ëª© ì¡°íšŒ
            cached_data = self.performance_optimizer.get_cached_artifact(key)
            if cached_data is not None:
                cache_hits += 1
        
        # ê²€ì¦
        self.assertGreater(cache_hits, 40)  # 80% ì´ìƒ ìºì‹œ íˆíŠ¸
        self.assertLess(memory_stats['avg_memory_usage'], 80)  # 80% ì´í•˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
        
        print(f"Caching Memory Test: {cache_hits}/50 cache hits, {memory_stats['avg_memory_usage']:.1f}% memory usage")
    
    def test_memory_leak_detection(self):
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # ë°˜ë³µì ì¸ ë°ì´í„° ì²˜ë¦¬
        for iteration in range(10):
            # ëŒ€ëŸ‰ ë°ì´í„° ìƒì„±
            large_dataset = pd.DataFrame({
                f'col_{i}': np.random.randn(10000) 
                for i in range(20)
            })
            
            # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            processed_data = large_dataset.describe()
            
            # ëª…ì‹œì  ì •ë¦¬
            del large_dataset
            del processed_data
            
            # ì£¼ê¸°ì  ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
            if iteration % 3 == 0:
                gc.collect()
            
            # ë©”ëª¨ë¦¬ ì²´í¬
            current_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_growth = current_memory - initial_memory
            
            # ë©”ëª¨ë¦¬ ì¦ê°€ëŸ‰ ì²´í¬ (ë°˜ë³µë‹¹ 10MB ì´í•˜)
            self.assertLess(memory_growth, 10 * (iteration + 1))
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì²´í¬
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        total_growth = final_memory - initial_memory
        
        # ì „ì²´ ì¦ê°€ëŸ‰ 50MB ì´í•˜
        self.assertLess(total_growth, 50)
        
        print(f"Memory Leak Test: {total_growth:.2f}MB growth over 10 iterations")
    
    def test_garbage_collection_efficiency(self):
        """ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸"""
        
        # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
        gc.collect()
        initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # ëŒ€ëŸ‰ ê°ì²´ ìƒì„±
        large_objects = []
        for i in range(1000):
            obj = {
                'id': i,
                'data': list(range(1000)),
                'nested': {'values': list(range(100))}
            }
            large_objects.append(obj)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        peak_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # ê°ì²´ í•´ì œ
        large_objects.clear()
        del large_objects
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
        collected = gc.collect()
        
        # ìµœì¢… ë©”ëª¨ë¦¬ ì¸¡ì •
        final_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        # ë©”ëª¨ë¦¬ íšŒìˆ˜ìœ¨ ê³„ì‚°
        memory_used = peak_memory - initial_memory
        memory_freed = peak_memory - final_memory
        recovery_rate = memory_freed / memory_used if memory_used > 0 else 0
        
        # ê²€ì¦: ìµœì†Œ 70% ë©”ëª¨ë¦¬ íšŒìˆ˜
        self.assertGreater(recovery_rate, 0.7)
        self.assertGreater(collected, 0)  # ê°€ë¹„ì§€ ê°ì²´ ìˆ˜ì§‘ë¨
        
        print(f"GC Efficiency Test: {recovery_rate:.1%} memory recovered, {collected} objects collected")

class TestStressTest(PerformanceTestBase):
    """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
    
    def test_system_under_extreme_load(self):
        """ê·¹í•œ ë¶€í•˜ ìƒí™© í…ŒìŠ¤íŠ¸"""
        
        # ë™ì‹œ ì‘ì—… ìˆ˜
        concurrent_tasks = 100
        task_duration = 2.0
        
        def stress_task(task_id: int) -> Dict[str, Any]:
            """ìŠ¤íŠ¸ë ˆìŠ¤ íƒœìŠ¤í¬"""
            start_time = time.time()
            
            try:
                # CPU ì§‘ì•½ì  ì‘ì—…
                data = np.random.randn(1000, 100)
                result = np.dot(data, data.T)
                
                # ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì‘ì—…
                large_list = [i for i in range(10000)]
                processed = [x * 2 for x in large_list]
                
                # I/O ì‹œë®¬ë ˆì´ì…˜
                time.sleep(0.01)
                
                return {
                    'task_id': task_id,
                    'success': True,
                    'duration': time.time() - start_time,
                    'result_size': len(processed)
                }
                
            except Exception as e:
                return {
                    'task_id': task_id,
                    'success': False,
                    'duration': time.time() - start_time,
                    'error': str(e)
                }
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent_tasks) as executor:
            futures = [
                executor.submit(stress_task, i)
                for i in range(concurrent_tasks)
            ]
            
            results = []
            for future in concurrent.futures.as_completed(futures):
                result = future.result()
                results.append(result)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # ê²°ê³¼ ë¶„ì„
        successful_tasks = [r for r in results if r['success']]
        failed_tasks = [r for r in results if not r['success']]
        
        success_rate = len(successful_tasks) / len(results)
        avg_duration = sum(r['duration'] for r in successful_tasks) / len(successful_tasks) if successful_tasks else 0
        throughput = len(successful_tasks) / total_time
        
        # ê²€ì¦: ê·¹í•œ ìƒí™©ì—ì„œë„ ìµœì†Œ ê¸°ëŠ¥ ìœ ì§€
        self.assertGreater(success_rate, 0.5)  # 50% ì´ìƒ ì„±ê³µ
        self.assertLess(len(failed_tasks), concurrent_tasks * 0.5)  # 50% ì´í•˜ ì‹¤íŒ¨
        
        print(f"Stress Test: {len(successful_tasks)}/{concurrent_tasks} tasks succeeded")
        print(f"Success Rate: {success_rate:.1%}, Throughput: {throughput:.1f} tasks/s")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
        cpu_usage = psutil.cpu_percent()
        memory_usage = psutil.virtual_memory().percent
        
        print(f"System State: CPU {cpu_usage:.1f}%, Memory {memory_usage:.1f}%")
    
    def test_sustained_load_over_time(self):
        """ì§€ì†ì  ë¶€í•˜ í…ŒìŠ¤íŠ¸"""
        
        duration_minutes = 1  # 1ë¶„ê°„ í…ŒìŠ¤íŠ¸
        test_duration = duration_minutes * 60
        request_interval = 0.1  # 100ms ê°„ê²©
        
        start_time = time.time()
        results = []
        
        while time.time() - start_time < test_duration:
            # ìš”ì²­ ì‹œë®¬ë ˆì´ì…˜
            request_start = time.time()
            
            try:
                # ê°„ë‹¨í•œ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                data = [i ** 2 for i in range(100)]
                processed = sum(data)
                
                success = True
                error = None
                
            except Exception as e:
                success = False
                error = str(e)
                processed = None
            
            request_end = time.time()
            
            results.append({
                'timestamp': request_start,
                'duration': request_end - request_start,
                'success': success,
                'error': error
            })
            
            # ë‹¤ìŒ ìš”ì²­ê¹Œì§€ ëŒ€ê¸°
            time.sleep(max(0, request_interval - (request_end - request_start)))
        
        # ê²°ê³¼ ë¶„ì„
        total_requests = len(results)
        successful_requests = sum(1 for r in results if r['success'])
        success_rate = successful_requests / total_requests
        
        avg_response_time = sum(r['duration'] for r in results if r['success']) / successful_requests
        throughput = successful_requests / test_duration
        
        # ê²€ì¦
        self.assertGreater(success_rate, 0.95)  # 95% ì„±ê³µë¥ 
        self.assertLess(avg_response_time, 0.05)  # 50ms ì´í•˜ ì‘ë‹µì‹œê°„
        self.assertGreater(throughput, 8)  # ìµœì†Œ 8 requests/second
        
        print(f"Sustained Load Test ({duration_minutes}min):")
        print(f"  Success Rate: {success_rate:.1%}")
        print(f"  Avg Response Time: {avg_response_time*1000:.1f}ms")
        print(f"  Throughput: {throughput:.1f} req/s")

if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    unittest.main(verbosity=2)