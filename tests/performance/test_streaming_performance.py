#!/usr/bin/env python3
"""
CherryAI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

ë¬¸ì„œ ìš”êµ¬ì‚¬í•­:
- ì‘ë‹µ ì‹œê°„: ì²« ì‘ë‹µ < 2ì´ˆ
- ìŠ¤íŠ¸ë¦¬ë° ì§€ì—°: < 100ms
- ë™ì‹œ ì‚¬ìš©ì: ìµœëŒ€ 50ëª…  
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: < 2GB
- CPU ì‚¬ìš©ë¥ : í‰ê·  < 70%
"""

import asyncio
import time
import psutil
import httpx
import pytest
import concurrent.futures
from typing import List, Dict, Any
import statistics
import json

class StreamingPerformanceBenchmark:
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬"""
    
    def __init__(self):
        self.base_url = "http://localhost:8501"
        self.results = {
            "response_times": [],
            "streaming_delays": [],
            "concurrent_users": 0,
            "memory_usage_mb": 0,
            "cpu_usage_percent": 0,
            "errors": []
        }
    
    def measure_system_resources(self) -> Dict[str, float]:
        """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¸¡ì •"""
        try:
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (MB)
            memory = psutil.virtual_memory()
            memory_used_mb = (memory.used / 1024 / 1024)
            
            # CPU ì‚¬ìš©ë¥  (%)
            cpu_percent = psutil.cpu_percent(interval=1)
            
            return {
                "memory_mb": memory_used_mb,
                "cpu_percent": cpu_percent,
                "memory_available_mb": (memory.available / 1024 / 1024)
            }
        except Exception as e:
            return {"error": str(e)}
    
    async def test_response_time(self) -> float:
        """ì²« ì‘ë‹µ ì‹œê°„ ì¸¡ì • (< 2ì´ˆ ëª©í‘œ)"""
        start_time = time.time()
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(self.base_url)
                
                if response.status_code == 200:
                    response_time = time.time() - start_time
                    self.results["response_times"].append(response_time)
                    return response_time
                else:
                    raise Exception(f"HTTP {response.status_code}")
                    
        except Exception as e:
            self.results["errors"].append(f"Response time test: {str(e)}")
            return float('inf')
    
    async def test_streaming_delay(self) -> float:
        """ìŠ¤íŠ¸ë¦¬ë° ì§€ì—° ì‹œê°„ ì¸¡ì • (< 100ms ëª©í‘œ)"""
        # ì‹¤ì œ A2A ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì— ìš”ì²­ ì „ì†¡
        orchestrator_url = "http://localhost:8100"
        
        try:
            start_time = time.time()
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                # A2A ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° health check
                response = await client.get(f"{orchestrator_url}/.well-known/agent.json")
                
                if response.status_code == 200:
                    delay = (time.time() - start_time) * 1000  # ms
                    self.results["streaming_delays"].append(delay)
                    return delay
                else:
                    raise Exception(f"A2A HTTP {response.status_code}")
                    
        except Exception as e:
            self.results["errors"].append(f"Streaming delay test: {str(e)}")
            return float('inf')
    
    async def test_concurrent_users(self, user_count: int = 10) -> Dict[str, Any]:
        """ë™ì‹œ ì‚¬ìš©ì ì²˜ë¦¬ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸš€ Testing {user_count} concurrent users...")
        
        async def simulate_user():
            """ë‹¨ì¼ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜"""
            try:
                async with httpx.AsyncClient(timeout=15.0) as client:
                    start = time.time()
                    response = await client.get(self.base_url)
                    duration = time.time() - start
                    
                    return {
                        "success": response.status_code == 200,
                        "duration": duration,
                        "status_code": response.status_code
                    }
            except Exception as e:
                return {
                    "success": False,
                    "duration": float('inf'),
                    "error": str(e)
                }
        
        # ë™ì‹œ ì‚¬ìš©ì ì‹œë®¬ë ˆì´ì…˜
        start_time = time.time()
        
        tasks = [simulate_user() for _ in range(user_count)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # ê²°ê³¼ ë¶„ì„
        successful = [r for r in results if isinstance(r, dict) and r.get("success")]
        failed = [r for r in results if not (isinstance(r, dict) and r.get("success"))]
        
        if successful:
            avg_duration = statistics.mean([r["duration"] for r in successful])
            max_duration = max([r["duration"] for r in successful])
        else:
            avg_duration = float('inf')
            max_duration = float('inf')
        
        return {
            "total_users": user_count,
            "successful": len(successful),
            "failed": len(failed),
            "success_rate": len(successful) / user_count * 100,
            "avg_response_time": avg_duration,
            "max_response_time": max_duration,
            "total_test_time": total_time
        }
    
    async def run_full_benchmark(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        print("ğŸ¯ CherryAI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        benchmark_results = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "system_info": {},
            "response_time_test": {},
            "streaming_delay_test": {},
            "concurrent_users_test": {},
            "resource_usage": {},
            "performance_summary": {}
        }
        
        # 1. ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì´ˆê¸° ì¸¡ì •
        print("ğŸ“Š ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¸¡ì • ì¤‘...")
        initial_resources = self.measure_system_resources()
        benchmark_results["system_info"] = initial_resources
        
        # 2. ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸ (5íšŒ ì¸¡ì •)
        print("â±ï¸ ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸ ì¤‘...")
        response_times = []
        for i in range(5):
            rt = await self.test_response_time()
            response_times.append(rt)
            print(f"  ì‘ë‹µ ì‹œê°„ {i+1}: {rt:.3f}ì´ˆ")
        
        avg_response_time = statistics.mean([rt for rt in response_times if rt != float('inf')])
        benchmark_results["response_time_test"] = {
            "measurements": response_times,
            "average": avg_response_time,
            "target": 2.0,
            "passed": avg_response_time < 2.0
        }
        
        # 3. ìŠ¤íŠ¸ë¦¬ë° ì§€ì—° í…ŒìŠ¤íŠ¸ (5íšŒ ì¸¡ì •)
        print("ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì—° í…ŒìŠ¤íŠ¸ ì¤‘...")
        streaming_delays = []
        for i in range(5):
            delay = await self.test_streaming_delay()
            streaming_delays.append(delay)
            print(f"  ìŠ¤íŠ¸ë¦¬ë° ì§€ì—° {i+1}: {delay:.1f}ms")
        
        avg_streaming_delay = statistics.mean([d for d in streaming_delays if d != float('inf')])
        benchmark_results["streaming_delay_test"] = {
            "measurements": streaming_delays,
            "average": avg_streaming_delay,
            "target": 100.0,
            "passed": avg_streaming_delay < 100.0
        }
        
        # 4. ë™ì‹œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ (10ëª…, 20ëª… ë‹¨ê³„ì  ì¦ê°€)
        print("ğŸ‘¥ ë™ì‹œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ ì¤‘...")
        concurrent_results = {}
        for user_count in [5, 10, 15]:
            result = await self.test_concurrent_users(user_count)
            concurrent_results[f"{user_count}_users"] = result
            print(f"  {user_count}ëª… ë™ì‹œ ì‚¬ìš©ì: {result['success_rate']:.1f}% ì„±ê³µë¥ ")
            
            # ë¶€í•˜ê°€ ë„ˆë¬´ í¬ë©´ ì¤‘ë‹¨
            if result['success_rate'] < 80:
                print(f"  âš ï¸ ì„±ê³µë¥ ì´ 80% ë¯¸ë§Œì´ë¯€ë¡œ ë” í° ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì¤‘ë‹¨")
                break
        
        benchmark_results["concurrent_users_test"] = concurrent_results
        
        # 5. ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¸¡ì •
        print("ğŸ“ˆ ìµœì¢… ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì¤‘...")
        final_resources = self.measure_system_resources()
        benchmark_results["resource_usage"] = {
            "initial": initial_resources,
            "final": final_resources,
            "memory_target_mb": 2048,  # 2GB
            "cpu_target_percent": 70,
            "memory_passed": final_resources.get("memory_mb", 0) < 2048,
            "cpu_passed": final_resources.get("cpu_percent", 0) < 70
        }
        
        # 6. ì„±ëŠ¥ ìš”ì•½
        overall_passed = all([
            benchmark_results["response_time_test"]["passed"],
            benchmark_results["streaming_delay_test"]["passed"],
            benchmark_results["resource_usage"]["memory_passed"],
            benchmark_results["resource_usage"]["cpu_passed"]
        ])
        
        benchmark_results["performance_summary"] = {
            "overall_passed": overall_passed,
            "response_time_score": "âœ… PASS" if benchmark_results["response_time_test"]["passed"] else "âŒ FAIL",
            "streaming_delay_score": "âœ… PASS" if benchmark_results["streaming_delay_test"]["passed"] else "âŒ FAIL", 
            "memory_score": "âœ… PASS" if benchmark_results["resource_usage"]["memory_passed"] else "âŒ FAIL",
            "cpu_score": "âœ… PASS" if benchmark_results["resource_usage"]["cpu_passed"] else "âŒ FAIL",
            "total_errors": len(self.results["errors"])
        }
        
        return benchmark_results


# Pytest í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤
@pytest.mark.asyncio
async def test_response_time_benchmark():
    """ì‘ë‹µ ì‹œê°„ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    benchmark = StreamingPerformanceBenchmark()
    response_time = await benchmark.test_response_time()
    
    assert response_time < 2.0, f"ì‘ë‹µ ì‹œê°„ì´ ëª©í‘œì¹˜(2ì´ˆ)ë¥¼ ì´ˆê³¼í•¨: {response_time:.3f}ì´ˆ"
    print(f"âœ… ì‘ë‹µ ì‹œê°„ í…ŒìŠ¤íŠ¸ í†µê³¼: {response_time:.3f}ì´ˆ")


@pytest.mark.asyncio 
async def test_streaming_delay_benchmark():
    """ìŠ¤íŠ¸ë¦¬ë° ì§€ì—° ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    benchmark = StreamingPerformanceBenchmark()
    delay = await benchmark.test_streaming_delay()
    
    assert delay < 100.0, f"ìŠ¤íŠ¸ë¦¬ë° ì§€ì—°ì´ ëª©í‘œì¹˜(100ms)ë¥¼ ì´ˆê³¼í•¨: {delay:.1f}ms"
    print(f"âœ… ìŠ¤íŠ¸ë¦¬ë° ì§€ì—° í…ŒìŠ¤íŠ¸ í†µê³¼: {delay:.1f}ms")


@pytest.mark.asyncio
async def test_concurrent_users_benchmark():
    """ë™ì‹œ ì‚¬ìš©ì ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    benchmark = StreamingPerformanceBenchmark()
    result = await benchmark.test_concurrent_users(10)
    
    assert result["success_rate"] >= 90, f"ë™ì‹œ ì‚¬ìš©ì ì„±ê³µë¥ ì´ 90% ë¯¸ë§Œ: {result['success_rate']:.1f}%"
    print(f"âœ… ë™ì‹œ ì‚¬ìš©ì í…ŒìŠ¤íŠ¸ í†µê³¼: {result['success_rate']:.1f}% ì„±ê³µë¥ ")


@pytest.mark.asyncio
async def test_full_performance_benchmark():
    """ì „ì²´ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
    benchmark = StreamingPerformanceBenchmark()
    results = await benchmark.run_full_benchmark()
    
    # ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
    with open("performance_benchmark_results.json", "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print("\n" + "="*60)
    print("ğŸ¯ CherryAI ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼")
    print("="*60)
    print(f"ğŸ“Š ì „ì²´ ê²°ê³¼: {'âœ… PASS' if results['performance_summary']['overall_passed'] else 'âŒ FAIL'}")
    print(f"â±ï¸ ì‘ë‹µ ì‹œê°„: {results['performance_summary']['response_time_score']} ({results['response_time_test']['average']:.3f}ì´ˆ)")
    print(f"ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì—°: {results['performance_summary']['streaming_delay_score']} ({results['streaming_delay_test']['average']:.1f}ms)")
    print(f"ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {results['performance_summary']['memory_score']} ({results['resource_usage']['final'].get('memory_mb', 0):.1f}MB)")
    print(f"âš¡ CPU ì‚¬ìš©ë¥ : {results['performance_summary']['cpu_score']} ({results['resource_usage']['final'].get('cpu_percent', 0):.1f}%)")
    print(f"âŒ ì´ ì˜¤ë¥˜ ìˆ˜: {results['performance_summary']['total_errors']}")
    print("="*60)
    
    assert results["performance_summary"]["overall_passed"], "ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í•¨"


if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    async def main():
        benchmark = StreamingPerformanceBenchmark()
        results = await benchmark.run_full_benchmark()
        
        with open("performance_benchmark_results.json", "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print("âœ… ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ! ê²°ê³¼ëŠ” performance_benchmark_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    asyncio.run(main()) 