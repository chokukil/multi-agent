"""
A2A ìµœì í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
Phase 2.5: ëª¨ë“  Phase 2 ìµœì í™” íš¨ê³¼ ê²€ì¦

í…ŒìŠ¤íŠ¸ ë²”ìœ„:
- A2A ë¸Œë¡œì»¤ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ê²€ì¦
- ì—°ê²° í’€ ìµœì í™” íš¨ê³¼ ì¸¡ì •
- ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í‰ê°€
- LLM First ë¶„ì„ ë¹„ìœ¨ ì¸¡ì •
- ì „ì²´ ì‹œìŠ¤í…œ í†µí•© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
"""

import pytest
import asyncio
import time
import statistics
import json
from typing import Dict, List, Any, Tuple
from datetime import datetime, timedelta
from pathlib import Path
import httpx

# Phase 2 ìµœì í™” ì»´í¬ë„ŒíŠ¸ import
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from core.monitoring.a2a_performance_profiler import A2APerformanceProfiler
from core.optimization.a2a_connection_optimizer import A2AConnectionOptimizer
from core.streaming.optimized_streaming_pipeline import OptimizedStreamingPipeline, BufferConfig, ChunkingConfig
from core.llm_enhancement.llm_first_analyzer import LLMFirstAnalyzer, AnalysisStrategy

class A2AOptimizationTestSuite:
    """A2A ìµœì í™” í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        self.test_results = {}
        self.performance_baseline = {
            "avg_response_time": 5.0,  # 5ì´ˆ ì´í•˜ ëª©í‘œ
            "success_rate": 0.95,      # 95% ì´ìƒ ëª©í‘œ
            "llm_usage_ratio": 0.9,    # 90% ì´ìƒ ëª©í‘œ
            "throughput_rps": 10       # ì´ˆë‹¹ 10 ìš”ì²­ ì²˜ë¦¬ ëª©í‘œ
        }
        
        # í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.test_duration = 300  # 5ë¶„ í…ŒìŠ¤íŠ¸
        self.test_iterations = 50
        self.concurrent_requests = 5
        
        # A2A ì—ì´ì „íŠ¸ êµ¬ì„±
        self.a2a_agents = {
            "orchestrator": "http://localhost:8100",
            "data_cleaning": "http://localhost:8306",
            "data_loader": "http://localhost:8307",
            "data_visualization": "http://localhost:8308",
            "data_wrangling": "http://localhost:8309",
            "feature_engineering": "http://localhost:8310",
            "sql_database": "http://localhost:8311",
            "eda_tools": "http://localhost:8312",
            "h2o_modeling": "http://localhost:8313",
            "mlflow_tracking": "http://localhost:8314"
        }
        
        # ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        self.results_dir = Path("tests/performance/results")
        self.results_dir.mkdir(parents=True, exist_ok=True)
    
    async def run_comprehensive_test(self) -> Dict[str, Any]:
        """ì¢…í•© ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ A2A ìµœì í™” ì¢…í•© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        test_results = {
            "test_start_time": datetime.now().isoformat(),
            "test_duration_seconds": self.test_duration,
            "performance_tests": {},
            "optimization_verification": {},
            "benchmark_comparison": {},
            "final_assessment": {}
        }
        
        try:
            # Phase 1: ë¸Œë¡œì»¤ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ í…ŒìŠ¤íŠ¸
            print("ğŸ“Š 1. ë¸Œë¡œì»¤ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ í…ŒìŠ¤íŠ¸...")
            profiler_results = await self._test_broker_performance()
            test_results["performance_tests"]["broker_profiling"] = profiler_results
            
            # Phase 2: ì—°ê²° ìµœì í™” íš¨ê³¼ í…ŒìŠ¤íŠ¸
            print("ğŸ”§ 2. ì—°ê²° ìµœì í™” íš¨ê³¼ í…ŒìŠ¤íŠ¸...")
            connection_results = await self._test_connection_optimization()
            test_results["performance_tests"]["connection_optimization"] = connection_results
            
            # Phase 3: ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            print("ğŸŒŠ 3. ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸...")
            streaming_results = await self._test_streaming_optimization()
            test_results["performance_tests"]["streaming_pipeline"] = streaming_results
            
            # Phase 4: LLM First ë¶„ì„ ë¹„ìœ¨ í…ŒìŠ¤íŠ¸
            print("ğŸ§  4. LLM First ë¶„ì„ ë¹„ìœ¨ í…ŒìŠ¤íŠ¸...")
            llm_results = await self._test_llm_first_optimization()
            test_results["performance_tests"]["llm_first_analysis"] = llm_results
            
            # Phase 5: í†µí•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹
            print("âš¡ 5. í†µí•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹...")
            integration_results = await self._test_integration_performance()
            test_results["performance_tests"]["integration_benchmark"] = integration_results
            
            # ìµœì¢… í‰ê°€
            final_assessment = self._calculate_final_assessment(test_results["performance_tests"])
            test_results["final_assessment"] = final_assessment
            
            # ê²°ê³¼ ì €ì¥
            await self._save_test_results(test_results)
            
            print(f"âœ… ì¢…í•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ìµœì¢… ì ìˆ˜: {final_assessment['overall_score']:.1f}/100")
            
            return test_results
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            test_results["error"] = str(e)
            return test_results
    
    async def _test_broker_performance(self) -> Dict[str, Any]:
        """ë¸Œë¡œì»¤ ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ í…ŒìŠ¤íŠ¸"""
        profiler = A2APerformanceProfiler()
        
        test_messages = [
            "ì•ˆë…•í•˜ì„¸ìš”",
            "ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
            "ì°¨íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”",
            "ìš”ì•½ í†µê³„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”",
            "ì´ìƒì¹˜ë¥¼ íƒì§€í•´ì£¼ì„¸ìš”"
        ]
        
        # ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰
        results = await profiler.profile_message_routing_performance(
            test_messages, iterations=10
        )
        
        # ê²°ê³¼ ë¶„ì„
        analysis = {
            "test_type": "broker_performance_profiling",
            "system_health": results["overall_analysis"]["system_health"],
            "avg_response_time": results["overall_analysis"]["performance_summary"]["avg_response_time"],
            "healthy_agents": len(results["overall_analysis"]["healthy_agents"]),
            "degraded_agents": len(results["overall_analysis"]["degraded_agents"]),
            "failed_agents": len(results["overall_analysis"]["failed_agents"]),
            "bottlenecks_found": len(results["overall_analysis"]["bottlenecks"]),
            "performance_grade": self._grade_performance(
                results["overall_analysis"]["performance_summary"]["avg_response_time"],
                results["overall_analysis"]["performance_summary"]["avg_error_rate"]
            )
        }
        
        return analysis
    
    async def _test_connection_optimization(self) -> Dict[str, Any]:
        """ì—°ê²° ìµœì í™” íš¨ê³¼ í…ŒìŠ¤íŠ¸"""
        optimizer = A2AConnectionOptimizer()
        
        try:
            await optimizer.initialize()
            
            # ìµœì í™” ë²¤ì¹˜ë§ˆí‚¹ (5ë¶„ê°„)
            benchmark_results = await optimizer.benchmark_optimization(test_duration_minutes=5)
            
            # ìµœì í™” íš¨ê³¼ ë¶„ì„
            optimization_analysis = {
                "test_type": "connection_optimization",
                "test_duration_minutes": 5,
                "total_agents_tested": len(benchmark_results["agent_results"]),
                "optimization_metrics": benchmark_results["optimization_metrics"],
                "recommendations_count": len(benchmark_results["recommendations"]),
                "efficiency_scores": {}
            }
            
            # ì—ì´ì „íŠ¸ë³„ íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚°
            for agent_name, result in benchmark_results["agent_results"].items():
                if "response_time_stats" in result:
                    efficiency_score = self._calculate_efficiency_score(result)
                    optimization_analysis["efficiency_scores"][agent_name] = efficiency_score
            
            avg_efficiency = statistics.mean(optimization_analysis["efficiency_scores"].values()) if optimization_analysis["efficiency_scores"] else 0
            optimization_analysis["average_efficiency"] = avg_efficiency
            
            return optimization_analysis
            
        finally:
            await optimizer.stop_monitoring()
    
    async def _test_streaming_optimization(self) -> Dict[str, Any]:
        """ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        # ìµœì í™”ëœ ì„¤ì •
        buffer_config = BufferConfig(
            max_buffer_size=1024*1024,  # 1MB
            high_watermark=0.8,
            low_watermark=0.3
        )
        
        chunking_config = ChunkingConfig(
            target_chunk_size=2048,
            adaptive_chunking=True
        )
        
        pipeline = OptimizedStreamingPipeline(buffer_config, chunking_config)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±ê¸°
        async def test_content_generator():
            for i in range(100):
                content = f"ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²­í¬ {i}. " * 20  # ì¶©ë¶„í•œ í¬ê¸°
                yield content
                await asyncio.sleep(0.05)  # 50ms ê°„ê²©
        
        # ìŠ¤íŠ¸ë¦¬ë° í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        start_time = time.time()
        chunk_count = 0
        total_bytes = 0
        
        try:
            content_source = test_content_generator()
            
            async for sse_chunk in pipeline.start_stream(content_source):
                chunk_count += 1
                total_bytes += len(sse_chunk)
                
                if chunk_count >= 200:  # 200ê°œ ì²­í¬ë¡œ ì œí•œ
                    pipeline.stop_stream()
                    break
            
            end_time = time.time()
            duration = end_time - start_time
            
            # ìŠ¤íŠ¸ë¦¬ë° ìƒíƒœ ìˆ˜ì§‘
            status = pipeline.get_streaming_status()
            
            streaming_analysis = {
                "test_type": "streaming_optimization",
                "chunks_processed": chunk_count,
                "total_bytes": total_bytes,
                "duration_seconds": duration,
                "throughput_bps": total_bytes / duration if duration > 0 else 0,
                "chunks_per_second": chunk_count / duration if duration > 0 else 0,
                "pipeline_status": status,
                "optimization_effectiveness": self._assess_streaming_optimization(status)
            }
            
            return streaming_analysis
            
        except Exception as e:
            return {
                "test_type": "streaming_optimization",
                "error": str(e),
                "chunks_processed": chunk_count,
                "total_bytes": total_bytes
            }
    
    async def _test_llm_first_optimization(self) -> Dict[str, Any]:
        """LLM First ë¶„ì„ ë¹„ìœ¨ í…ŒìŠ¤íŠ¸"""
        analyzer = LLMFirstAnalyzer()
        
        try:
            await analyzer.initialize()
            
            # í…ŒìŠ¤íŠ¸ ë¶„ì„ ìš”ì²­ë“¤
            test_queries = [
                "ë°ì´í„°ì˜ ì „ë°˜ì ì¸ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê³  ì›ì¸ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
                "ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”",
                "ì˜ˆì¸¡ ëª¨ë¸ì„ ìœ„í•œ íŠ¹ì„±ì„ ì¶”ì²œí•´ì£¼ì„¸ìš”",
                "ë°ì´í„° í’ˆì§ˆ ë¬¸ì œë¥¼ ì‹ë³„í•´ì£¼ì„¸ìš”",
                "ì‹œê³„ì—´ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
                "í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ í•´ì„í•´ì£¼ì„¸ìš”",
                "ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”"
            ]
            
            # ë‹¤ì–‘í•œ ì „ëµìœ¼ë¡œ í…ŒìŠ¤íŠ¸
            strategy_results = {}
            
            for strategy in [AnalysisStrategy.LLM_ONLY, AnalysisStrategy.LLM_PREFERRED]:
                strategy_name = strategy.value
                strategy_metrics = {
                    "total_requests": 0,
                    "successful_requests": 0,
                    "llm_usage_count": 0,
                    "fallback_usage_count": 0,
                    "avg_confidence": 0.0,
                    "response_times": []
                }
                
                # ê° ì¿¼ë¦¬ë¥¼ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰
                for query in test_queries[:5]:  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
                    for iteration in range(3):  # ê° ì¿¼ë¦¬ë¥¼ 3ë²ˆì”©
                        try:
                            context = {"test_iteration": iteration, "strategy": strategy_name}
                            
                            response = await analyzer.analyze_realtime(
                                query, context, priority=5, strategy=strategy
                            )
                            
                            strategy_metrics["total_requests"] += 1
                            
                            if response.success:
                                strategy_metrics["successful_requests"] += 1
                            
                            if not response.fallback_used:
                                strategy_metrics["llm_usage_count"] += 1
                            else:
                                strategy_metrics["fallback_usage_count"] += 1
                            
                            strategy_metrics["response_times"].append(response.response_time)
                            strategy_metrics["avg_confidence"] += response.confidence_score
                            
                        except Exception as e:
                            print(f"LLM í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
                            continue
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                if strategy_metrics["total_requests"] > 0:
                    strategy_metrics["llm_usage_ratio"] = (
                        strategy_metrics["llm_usage_count"] / strategy_metrics["total_requests"]
                    )
                    strategy_metrics["success_rate"] = (
                        strategy_metrics["successful_requests"] / strategy_metrics["total_requests"]
                    )
                    strategy_metrics["avg_confidence"] /= strategy_metrics["total_requests"]
                    strategy_metrics["avg_response_time"] = statistics.mean(strategy_metrics["response_times"]) if strategy_metrics["response_times"] else 0
                
                strategy_results[strategy_name] = strategy_metrics
            
            # ì „ì²´ ìƒíƒœ ìˆ˜ì§‘
            overall_status = analyzer.get_llm_first_status()
            
            llm_analysis = {
                "test_type": "llm_first_optimization",
                "strategy_results": strategy_results,
                "overall_status": overall_status,
                "llm_first_score": overall_status["global_metrics"]["llm_first_score"],
                "target_achievement": {
                    "llm_ratio_target": 0.9,
                    "actual_llm_ratio": overall_status["global_metrics"]["llm_usage_ratio"],
                    "target_achieved": overall_status["global_metrics"]["llm_usage_ratio"] >= 0.9
                }
            }
            
            return llm_analysis
            
        finally:
            await analyzer.shutdown()
    
    async def _test_integration_performance(self) -> Dict[str, Any]:
        """í†µí•© ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹"""
        
        # ì‹¤ì œ A2A ì‹œìŠ¤í…œì— ëŒ€í•œ í†µí•© í…ŒìŠ¤íŠ¸
        integration_metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "response_times": [],
            "error_types": {},
            "agent_performance": {}
        }
        
        test_scenarios = [
            {
                "agent": "orchestrator", 
                "message": "ì „ì²´ ë°ì´í„° ë¶„ì„ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”",
                "expected_response_time": 10.0
            },
            {
                "agent": "data_loader", 
                "message": "CSV íŒŒì¼ì„ ë¡œë“œí•´ì£¼ì„¸ìš”",
                "expected_response_time": 5.0
            },
            {
                "agent": "eda_tools", 
                "message": "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”",
                "expected_response_time": 8.0
            },
            {
                "agent": "data_visualization", 
                "message": "ë°ì´í„° ì‹œê°í™”ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”",
                "expected_response_time": 7.0
            }
        ]
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            # ê° ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì—¬ëŸ¬ ë²ˆ í…ŒìŠ¤íŠ¸
            for scenario in test_scenarios:
                agent_name = scenario["agent"]
                agent_url = self.a2a_agents.get(agent_name)
                
                if not agent_url:
                    continue
                
                agent_metrics = {
                    "requests": 0,
                    "successes": 0,
                    "response_times": [],
                    "errors": []
                }
                
                # ê° ì—ì´ì „íŠ¸ì— 10ë²ˆì”© ìš”ì²­
                for i in range(10):
                    start_time = time.time()
                    
                    try:
                        payload = {
                            "jsonrpc": "2.0",
                            "id": f"integration_test_{i}",
                            "method": "message/send",
                            "params": {
                                "message": {
                                    "role": "user",
                                    "parts": [{"kind": "text", "text": scenario["message"]}],
                                    "messageId": f"integration_msg_{time.time()}"
                                },
                                "metadata": {"test": "integration_performance"}
                            }
                        }
                        
                        response = await client.post(agent_url, json=payload)
                        response_time = time.time() - start_time
                        
                        agent_metrics["requests"] += 1
                        integration_metrics["total_requests"] += 1
                        
                        if response.status_code == 200:
                            agent_metrics["successes"] += 1
                            integration_metrics["successful_requests"] += 1
                        else:
                            error_type = f"HTTP_{response.status_code}"
                            agent_metrics["errors"].append(error_type)
                            integration_metrics["error_types"][error_type] = integration_metrics["error_types"].get(error_type, 0) + 1
                        
                        agent_metrics["response_times"].append(response_time)
                        integration_metrics["response_times"].append(response_time)
                        
                    except Exception as e:
                        response_time = time.time() - start_time
                        error_type = str(type(e).__name__)
                        
                        agent_metrics["requests"] += 1
                        agent_metrics["errors"].append(error_type)
                        agent_metrics["response_times"].append(response_time)
                        
                        integration_metrics["total_requests"] += 1
                        integration_metrics["error_types"][error_type] = integration_metrics["error_types"].get(error_type, 0) + 1
                        integration_metrics["response_times"].append(response_time)
                
                # ì—ì´ì „íŠ¸ë³„ ë©”íŠ¸ë¦­ ê³„ì‚°
                if agent_metrics["requests"] > 0:
                    integration_metrics["agent_performance"][agent_name] = {
                        "success_rate": agent_metrics["successes"] / agent_metrics["requests"],
                        "avg_response_time": statistics.mean(agent_metrics["response_times"]),
                        "target_met": statistics.mean(agent_metrics["response_times"]) <= scenario["expected_response_time"],
                        "error_count": len(agent_metrics["errors"])
                    }
        
        # ì „ì²´ í†µí•© ì„±ëŠ¥ ê³„ì‚°
        integration_results = {
            "test_type": "integration_performance",
            "overall_metrics": {
                "total_requests": integration_metrics["total_requests"],
                "overall_success_rate": (
                    integration_metrics["successful_requests"] / integration_metrics["total_requests"]
                    if integration_metrics["total_requests"] > 0 else 0
                ),
                "avg_response_time": (
                    statistics.mean(integration_metrics["response_times"]) 
                    if integration_metrics["response_times"] else 0
                ),
                "throughput_rps": (
                    integration_metrics["total_requests"] / self.test_duration
                    if self.test_duration > 0 else 0
                )
            },
            "agent_performance": integration_metrics["agent_performance"],
            "error_distribution": integration_metrics["error_types"],
            "performance_targets": {
                "success_rate_target": self.performance_baseline["success_rate"],
                "response_time_target": self.performance_baseline["avg_response_time"],
                "throughput_target": self.performance_baseline["throughput_rps"]
            }
        }
        
        return integration_results
    
    def _calculate_final_assessment(self, performance_tests: Dict[str, Any]) -> Dict[str, Any]:
        """ìµœì¢… í‰ê°€ ê³„ì‚°"""
        
        scores = {}
        
        # 1. ë¸Œë¡œì»¤ ì„±ëŠ¥ ì ìˆ˜ (25ì )
        broker_test = performance_tests.get("broker_profiling", {})
        if broker_test.get("performance_grade"):
            grade_scores = {"A": 25, "B": 20, "C": 15, "D": 10, "F": 5}
            scores["broker_performance"] = grade_scores.get(broker_test["performance_grade"], 5)
        else:
            scores["broker_performance"] = 0
        
        # 2. ì—°ê²° ìµœì í™” ì ìˆ˜ (25ì )
        connection_test = performance_tests.get("connection_optimization", {})
        avg_efficiency = connection_test.get("average_efficiency", 0)
        scores["connection_optimization"] = min(25, avg_efficiency * 25)
        
        # 3. ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” ì ìˆ˜ (25ì )
        streaming_test = performance_tests.get("streaming_pipeline", {})
        streaming_effectiveness = streaming_test.get("optimization_effectiveness", 0)
        scores["streaming_optimization"] = min(25, streaming_effectiveness * 25)
        
        # 4. LLM First ì ìˆ˜ (25ì )
        llm_test = performance_tests.get("llm_first_analysis", {})
        llm_score = llm_test.get("llm_first_score", 0)
        scores["llm_first_analysis"] = min(25, llm_score / 4)  # 100ì  ìŠ¤ì¼€ì¼ì„ 25ì ìœ¼ë¡œ ë³€í™˜
        
        # ì´ì  ê³„ì‚°
        total_score = sum(scores.values())
        
        # ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        targets_met = {
            "response_time": False,
            "success_rate": False,
            "llm_ratio": False,
            "throughput": False
        }
        
        # í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ëª©í‘œ ë‹¬ì„± í‰ê°€
        integration_test = performance_tests.get("integration_benchmark", {})
        if integration_test:
            overall_metrics = integration_test.get("overall_metrics", {})
            targets_met["response_time"] = overall_metrics.get("avg_response_time", 999) <= self.performance_baseline["avg_response_time"]
            targets_met["success_rate"] = overall_metrics.get("overall_success_rate", 0) >= self.performance_baseline["success_rate"]
            targets_met["throughput"] = overall_metrics.get("throughput_rps", 0) >= self.performance_baseline["throughput_rps"]
        
        if llm_test:
            target_achievement = llm_test.get("target_achievement", {})
            targets_met["llm_ratio"] = target_achievement.get("target_achieved", False)
        
        # ì „ì²´ ë“±ê¸‰
        if total_score >= 90:
            overall_grade = "A"
        elif total_score >= 80:
            overall_grade = "B"
        elif total_score >= 70:
            overall_grade = "C"
        elif total_score >= 60:
            overall_grade = "D"
        else:
            overall_grade = "F"
        
        return {
            "overall_score": total_score,
            "overall_grade": overall_grade,
            "component_scores": scores,
            "targets_met": targets_met,
            "targets_achieved_count": sum(targets_met.values()),
            "total_targets": len(targets_met),
            "recommendation": self._generate_improvement_recommendations(scores, targets_met)
        }
    
    def _grade_performance(self, avg_response_time: float, avg_error_rate: float) -> str:
        """ì„±ëŠ¥ ë“±ê¸‰ ê³„ì‚°"""
        if avg_response_time <= 2.0 and avg_error_rate <= 0.05:
            return "A"
        elif avg_response_time <= 3.0 and avg_error_rate <= 0.1:
            return "B"
        elif avg_response_time <= 5.0 and avg_error_rate <= 0.15:
            return "C"
        elif avg_response_time <= 8.0 and avg_error_rate <= 0.25:
            return "D"
        else:
            return "F"
    
    def _calculate_efficiency_score(self, result: Dict[str, Any]) -> float:
        """íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        response_time_stats = result.get("response_time_stats", {})
        success_rate = result.get("success_rate", 0)
        
        # ì‘ë‹µì‹œê°„ ì ìˆ˜ (0-1)
        avg_response_time = response_time_stats.get("avg", 999)
        time_score = max(0, min(1, (5.0 - avg_response_time) / 5.0))
        
        # ì„±ê³µë¥  ì ìˆ˜ (0-1)
        success_score = success_rate
        
        # ì¢…í•© íš¨ìœ¨ì„± ì ìˆ˜
        efficiency = (time_score * 0.6 + success_score * 0.4)
        return efficiency
    
    def _assess_streaming_optimization(self, status: Dict[str, Any]) -> float:
        """ìŠ¤íŠ¸ë¦¬ë° ìµœì í™” íš¨ê³¼ í‰ê°€"""
        metrics = status.get("metrics", {})
        
        # ì²˜ë¦¬ëŸ‰ ì ìˆ˜
        throughput = metrics.get("avg_throughput_bps", 0)
        throughput_score = min(1.0, throughput / 1000000)  # 1Mbps ê¸°ì¤€
        
        # ì—ëŸ¬ìœ¨ ì ìˆ˜
        error_count = metrics.get("error_count", 0)
        total_chunks = metrics.get("total_chunks", 1)
        error_rate = error_count / total_chunks
        error_score = max(0, 1 - error_rate * 10)
        
        # ë²„í¼ íš¨ìœ¨ì„± ì ìˆ˜
        buffer_status = status.get("buffer_status", {})
        if isinstance(buffer_status, dict):
            usage_ratio = buffer_status.get("usage_ratio", 0)
            # ì ì • ì‚¬ìš©ë¥  (30-80%) ê¸°ì¤€
            if 0.3 <= usage_ratio <= 0.8:
                buffer_score = 1.0
            else:
                buffer_score = max(0, 1 - abs(usage_ratio - 0.55) * 2)
        else:
            buffer_score = 0.5
        
        # ì¢…í•© ì ìˆ˜
        optimization_effectiveness = (throughput_score * 0.4 + error_score * 0.4 + buffer_score * 0.2)
        return optimization_effectiveness
    
    def _generate_improvement_recommendations(self, scores: Dict[str, float], targets_met: Dict[str, bool]) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì ìˆ˜ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if scores.get("broker_performance", 0) < 20:
            recommendations.append("ë¸Œë¡œì»¤ ì„±ëŠ¥ ìµœì í™” í•„ìš”: ì—°ê²° í’€ í¬ê¸° ì¡°ì •, íƒ€ì„ì•„ì›ƒ ì„¤ì • ê²€í† ")
        
        if scores.get("connection_optimization", 0) < 20:
            recommendations.append("ì—°ê²° ìµœì í™” ê°•í™”: ì ì‘ì  ì—°ê²° í’€ ì„¤ì • ë¯¸ì„¸ ì¡°ì •")
        
        if scores.get("streaming_optimization", 0) < 20:
            recommendations.append("ìŠ¤íŠ¸ë¦¬ë° íŒŒì´í”„ë¼ì¸ ê°œì„ : ì²­í¬ í¬ê¸° ë° ë²„í¼ë§ ì „ëµ ì¬ê²€í† ")
        
        if scores.get("llm_first_analysis", 0) < 20:
            recommendations.append("LLM First ë¹„ìœ¨ í–¥ìƒ: í´ë°± ë¡œì§ ìµœì†Œí™”, LLM ë¼ìš°íŒ… ê°œì„ ")
        
        # ëª©í‘œ ë¯¸ë‹¬ì„± ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if not targets_met.get("response_time", True):
            recommendations.append("ì‘ë‹µ ì‹œê°„ ëª©í‘œ ë¯¸ë‹¬ì„±: ë³‘ë ¬ ì²˜ë¦¬ ê°•í™” ë° ìºì‹± ì „ëµ ë„ì…")
        
        if not targets_met.get("success_rate", True):
            recommendations.append("ì„±ê³µë¥  ëª©í‘œ ë¯¸ë‹¬ì„±: ì—ëŸ¬ ì²˜ë¦¬ ë¡œì§ ê°•í™” ë° ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ê°œì„ ")
        
        if not targets_met.get("llm_ratio", True):
            recommendations.append("LLM ì‚¬ìš© ë¹„ìœ¨ ëª©í‘œ ë¯¸ë‹¬ì„±: í´ë°± ë°©ì§€ ì „ëµ ê°•í™”")
        
        if not targets_met.get("throughput", True):
            recommendations.append("ì²˜ë¦¬ëŸ‰ ëª©í‘œ ë¯¸ë‹¬ì„±: ë™ì‹œ ì²˜ë¦¬ ëŠ¥ë ¥ í™•ì¥ ë° ë¶€í•˜ ë¶„ì‚° ê°œì„ ")
        
        if not recommendations:
            recommendations.append("ëª¨ë“  ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±! ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ ë° ë¯¸ì„¸ ì¡°ì • ê¶Œì¥")
        
        return recommendations
    
    async def _save_test_results(self, results: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        file_path = self.results_dir / f"a2a_optimization_test_results_{timestamp}.json"
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {file_path}")


# pytest í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤
class TestA2AOptimization:
    """pytest A2A ìµœì í™” í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture(scope="class")
    def test_suite(self):
        """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ í”½ìŠ¤ì²˜"""
        return A2AOptimizationTestSuite()
    
    @pytest.mark.asyncio
    @pytest.mark.timeout(1800)  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
    async def test_comprehensive_optimization(self, test_suite):
        """ì¢…í•© ìµœì í™” í…ŒìŠ¤íŠ¸"""
        results = await test_suite.run_comprehensive_test()
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê²€ì¦
        assert "final_assessment" in results
        assert results["final_assessment"]["overall_score"] >= 60  # ìµœì†Œ D ë“±ê¸‰
        
        # ê°œë³„ ì»´í¬ë„ŒíŠ¸ ì ìˆ˜ ê²€ì¦
        component_scores = results["final_assessment"]["component_scores"]
        
        # ê° ì»´í¬ë„ŒíŠ¸ê°€ ìµœì†Œ ê¸°ì¤€ ì´ìƒì´ì–´ì•¼ í•¨
        assert component_scores.get("broker_performance", 0) >= 10
        assert component_scores.get("connection_optimization", 0) >= 10
        assert component_scores.get("streaming_optimization", 0) >= 10
        assert component_scores.get("llm_first_analysis", 0) >= 10
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ¯ A2A ìµœì í™” í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"   ì´ì : {results['final_assessment']['overall_score']:.1f}/100")
        print(f"   ë“±ê¸‰: {results['final_assessment']['overall_grade']}")
        print(f"   ëª©í‘œ ë‹¬ì„±: {results['final_assessment']['targets_achieved_count']}/{results['final_assessment']['total_targets']}")
    
    @pytest.mark.asyncio
    async def test_broker_performance_baseline(self, test_suite):
        """ë¸Œë¡œì»¤ ì„±ëŠ¥ ê¸°ì¤€ì„  í…ŒìŠ¤íŠ¸"""
        profiler = A2APerformanceProfiler()
        
        # ê°„ë‹¨í•œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        test_messages = ["Hello", "ì•ˆë…•í•˜ì„¸ìš”"]
        results = await profiler.profile_message_routing_performance(test_messages, iterations=3)
        
        # ê¸°ë³¸ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ ê²€ì¦
        avg_response_time = results["overall_analysis"]["performance_summary"]["avg_response_time"]
        assert avg_response_time <= 10.0  # 10ì´ˆ ì´í•˜
        
        # ìµœì†Œ ì ˆë°˜ ì´ìƒì˜ ì—ì´ì „íŠ¸ê°€ ì •ìƒ ì‘ë™í•´ì•¼ í•¨
        total_agents = len(results["agent_results"])
        healthy_agents = len(results["overall_analysis"]["healthy_agents"])
        assert healthy_agents >= total_agents * 0.5
    
    @pytest.mark.asyncio
    async def test_llm_first_ratio_requirement(self, test_suite):
        """LLM First ë¹„ìœ¨ ìš”êµ¬ì‚¬í•­ í…ŒìŠ¤íŠ¸"""
        analyzer = LLMFirstAnalyzer()
        
        try:
            await analyzer.initialize()
            
            # ê°„ë‹¨í•œ LLM ë¶„ì„ í…ŒìŠ¤íŠ¸
            response = await analyzer.analyze_realtime(
                "ê°„ë‹¨í•œ ë°ì´í„° ë¶„ì„ì„ í•´ì£¼ì„¸ìš”",
                {"test": True},
                strategy=AnalysisStrategy.LLM_PREFERRED
            )
            
            # LLM ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            assert not response.fallback_used or response.success
            
            # ì „ì²´ ìƒíƒœ í™•ì¸
            status = analyzer.get_llm_first_status()
            
            # ìµœì†Œ ìš”êµ¬ì‚¬í•­: LLM ì‚¬ìš© ë¹„ìœ¨ 70% ì´ìƒ
            assert status["global_metrics"]["llm_usage_ratio"] >= 0.7
            
        finally:
            await analyzer.shutdown()


# ë…ë¦½ ì‹¤í–‰ìš© ë©”ì¸ í•¨ìˆ˜
async def main():
    """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ë…ë¦½ ì‹¤í–‰"""
    test_suite = A2AOptimizationTestSuite()
    results = await test_suite.run_comprehensive_test()
    
    print("\n" + "="*80)
    print("ğŸ† A2A ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*80)
    
    final_assessment = results["final_assessment"]
    print(f"ğŸ“Š ìµœì¢… ì ìˆ˜: {final_assessment['overall_score']:.1f}/100 ({final_assessment['overall_grade']})")
    print(f"ğŸ¯ ëª©í‘œ ë‹¬ì„±: {final_assessment['targets_achieved_count']}/{final_assessment['total_targets']}")
    
    print("\nğŸ“‹ ì»´í¬ë„ŒíŠ¸ë³„ ì ìˆ˜:")
    for component, score in final_assessment["component_scores"].items():
        print(f"   â€¢ {component}: {score:.1f}/25")
    
    print("\nğŸ’¡ ê°œì„  ê¶Œì¥ì‚¬í•­:")
    for i, rec in enumerate(final_assessment["recommendation"][:3], 1):
        print(f"   {i}. {rec}")

if __name__ == "__main__":
    asyncio.run(main()) 