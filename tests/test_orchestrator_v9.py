#!/usr/bin/env python3
"""
CherryAI v9 Intelligent Dynamic Orchestrator í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import pytest
from unittest.mock import Mock, AsyncMock, patch
from typing import Dict, List

# í…ŒìŠ¤íŠ¸ ëŒ€ìƒ ëª¨ë“ˆë“¤
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from a2a_orchestrator import (
    IntelligentContextManager,
    DynamicWorkflowPlanner,
    IntelligentFinalAnswerEngine,
    CherryAI_v9_IntelligentDynamicOrchestrator
)


class TestIntelligentContextManager:
    """ì§€ëŠ¥í˜• ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def context_manager(self):
        """ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return IntelligentContextManager()
    
    @pytest.fixture
    def mock_openai_client(self):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ëª¨í‚¹"""
        mock_client = AsyncMock()
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = json.dumps({
            "task_objective": "ë°ì´í„° ë¡œë”© ë° ê²€ì¦ ìˆ˜í–‰",
            "context_summary": "ì‚¬ìš©ìê°€ ìš”ì²­í•œ ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ì´ˆê¸° ë°ì´í„° ë¡œë”©",
            "specific_instructions": "CSV, Excel íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë”©í•˜ê³  ê¸°ë³¸ ê²€ì¦ ìˆ˜í–‰",
            "expected_output": "ë¡œë”©ëœ ë°ì´í„°í”„ë ˆì„ê³¼ ê¸°ë³¸ í†µê³„ ì •ë³´",
            "success_criteria": "ë°ì´í„° ë¬´ê²°ì„± í™•ì¸ ë° ì˜¤ë¥˜ ì—†ëŠ” ë¡œë”©",
            "dependencies": "íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´"
        })
        mock_client.chat.completions.create.return_value = mock_response
        return mock_client
    
    def test_basic_context_creation(self, context_manager):
        """ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        context = context_manager._create_basic_context(
            "Test Agent", 
            "í…ŒìŠ¤íŠ¸ ì‘ì—…", 
            "ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
        )
        
        assert context["agent_name"] == "Test Agent"
        assert context["task_objective"] == "í…ŒìŠ¤íŠ¸ ì‘ì—…"
        assert "ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”" in context["context_summary"]
        assert "created_at" in context
    
    @pytest.mark.asyncio
    async def test_specialized_context_creation_with_openai(self, mock_openai_client):
        """OpenAIë¥¼ í™œìš©í•œ ì „ë¬¸í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        context_manager = IntelligentContextManager(mock_openai_client)
        
        context = await context_manager.create_specialized_context(
            "Data Loader Agent",
            "ë°ì´í„° ë¡œë”© ì‘ì—…",
            "ê³ ê° ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
            []
        )
        
        assert context["agent_name"] == "Data Loader Agent"
        assert context["task_objective"] == "ë°ì´í„° ë¡œë”© ë° ê²€ì¦ ìˆ˜í–‰"
        assert "created_at" in context
        mock_openai_client.chat.completions.create.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_context_creation_with_previous_results(self, mock_openai_client):
        """ì´ì „ ê²°ê³¼ë¥¼ í¬í•¨í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        context_manager = IntelligentContextManager(mock_openai_client)
        
        previous_results = [
            {"agent_name": "Data Loader", "output": "ë°ì´í„° ë¡œë”© ì™„ë£Œ"}
        ]
        
        context = await context_manager.create_specialized_context(
            "EDA Tools Agent",
            "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„",
            "ë°ì´í„° íŒ¨í„´ì„ ì°¾ì•„ì£¼ì„¸ìš”",
            previous_results
        )
        
        assert "EDA Tools Agent" in context["agent_name"]
        assert mock_openai_client.chat.completions.create.called
    
    @pytest.mark.asyncio
    async def test_context_creation_fallback(self):
        """OpenAI ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
        context_manager = IntelligentContextManager(None)  # OpenAI í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ
        
        context = await context_manager.create_specialized_context(
            "Test Agent",
            "í…ŒìŠ¤íŠ¸ ì‘ì—…",
            "í…ŒìŠ¤íŠ¸ ìš”ì²­",
            []
        )
        
        assert context["agent_name"] == "Test Agent"
        assert context["task_objective"] == "í…ŒìŠ¤íŠ¸ ì‘ì—…"


class TestDynamicWorkflowPlanner:
    """ë™ì  ì›Œí¬í”Œë¡œìš° í”Œë˜ë„ˆ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def workflow_planner(self):
        """ì›Œí¬í”Œë¡œìš° í”Œë˜ë„ˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return DynamicWorkflowPlanner()
    
    @pytest.fixture
    def sample_agents(self):
        """í…ŒìŠ¤íŠ¸ìš© ì—ì´ì „íŠ¸ ëª©ë¡"""
        return [
            {
                "name": "Data Loader Agent",
                "description": "ë°ì´í„° ë¡œë”© ì „ë¬¸ ì—ì´ì „íŠ¸",
                "capabilities": ["data_loading", "file_processing"]
            },
            {
                "name": "EDA Tools Agent", 
                "description": "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸",
                "capabilities": ["exploratory_analysis", "statistical_analysis"]
            },
            {
                "name": "Data Visualization Agent",
                "description": "ë°ì´í„° ì‹œê°í™” ì „ë¬¸ ì—ì´ì „íŠ¸", 
                "capabilities": ["data_visualization", "charting"]
            }
        ]
    
    @pytest.fixture
    def mock_openai_client_planner(self):
        """í”Œë˜ë„ˆìš© OpenAI í´ë¼ì´ì–¸íŠ¸ ëª¨í‚¹"""
        mock_client = AsyncMock()
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = json.dumps({
            "workflow_rationale": "ê³ ê° ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ ì²´ê³„ì  ì ‘ê·¼",
            "steps": [
                {
                    "step_id": "step_1",
                    "agent_name": "Data Loader Agent",
                    "task_description": "ê³ ê° ë°ì´í„° ë¡œë”© ë° ì´ˆê¸° ê²€ì¦",
                    "input_requirements": "CSV íŒŒì¼ ê²½ë¡œ",
                    "output_expectations": "ê²€ì¦ëœ ë°ì´í„°í”„ë ˆì„",
                    "priority": "high",
                    "dependencies": []
                },
                {
                    "step_id": "step_2", 
                    "agent_name": "EDA Tools Agent",
                    "task_description": "ê³ ê° ë°ì´í„° íŒ¨í„´ ë¶„ì„",
                    "input_requirements": "ë¡œë”©ëœ ë°ì´í„°í”„ë ˆì„",
                    "output_expectations": "í†µê³„ ë¶„ì„ ê²°ê³¼",
                    "priority": "high",
                    "dependencies": ["step_1"]
                }
            ]
        })
        mock_client.chat.completions.create.return_value = mock_response
        return mock_client
    
    def test_fallback_plan_creation(self, workflow_planner, sample_agents):
        """ê¸°ë³¸ ê³„íš ìƒì„± í…ŒìŠ¤íŠ¸"""
        plan = workflow_planner._create_fallback_plan(sample_agents)
        
        assert len(plan) >= 1
        assert all("step_id" in step for step in plan)
        assert all("agent_name" in step for step in plan)
        assert all("task_description" in step for step in plan)
    
    @pytest.mark.asyncio
    async def test_intelligent_plan_creation(self, mock_openai_client_planner, sample_agents):
        """LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ê³„íš ìƒì„± í…ŒìŠ¤íŠ¸"""
        planner = DynamicWorkflowPlanner(mock_openai_client_planner)
        
        plan = await planner.create_intelligent_plan(
            "ê³ ê° ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ íŒ¨í„´ì„ ì°¾ì•„ì£¼ì„¸ìš”",
            sample_agents
        )
        
        assert len(plan) == 2
        assert plan[0]["agent_name"] == "Data Loader Agent"
        assert plan[1]["agent_name"] == "EDA Tools Agent"
        assert "step_1" in plan[0]["step_id"]
        mock_openai_client_planner.chat.completions.create.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_replan_not_needed(self, mock_openai_client_planner):
        """ì¬ê³„íš ë¶ˆí•„ìš” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = json.dumps({
            "replan_needed": False,
            "replan_rationale": "ëª¨ë“  ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨",
            "alternative_steps": []
        })
        mock_openai_client_planner.chat.completions.create.return_value = mock_response
        
        planner = DynamicWorkflowPlanner(mock_openai_client_planner)
        
        original_plan = [{"step_id": "step_1", "agent_name": "Test Agent"}]
        execution_results = [{"status": "success", "agent_name": "Test Agent"}]
        
        should_replan, new_plan = await planner.replan_if_needed(
            original_plan, execution_results, "í…ŒìŠ¤íŠ¸ ìš”ì²­"
        )
        
        assert should_replan is False
        assert new_plan == original_plan
    
    @pytest.mark.asyncio
    async def test_replan_needed(self, mock_openai_client_planner):
        """ì¬ê³„íš í•„ìš” ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = json.dumps({
            "replan_needed": True,
            "replan_rationale": "ì¼ë¶€ ë‹¨ê³„ ì‹¤íŒ¨ë¡œ ëŒ€ì²´ ë°©ë²• í•„ìš”",
            "alternative_steps": [
                {
                    "step_id": "alt_step_1",
                    "agent_name": "Alternative Agent",
                    "task_description": "ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì‘ì—… ìˆ˜í–‰",
                    "rationale": "ì›ë³¸ ì—ì´ì „íŠ¸ ì‹¤íŒ¨ë¡œ ëŒ€ì²´"
                }
            ]
        })
        mock_openai_client_planner.chat.completions.create.return_value = mock_response
        
        planner = DynamicWorkflowPlanner(mock_openai_client_planner)
        
        original_plan = [{"step_id": "step_1", "agent_name": "Failed Agent"}]
        execution_results = [{"status": "failed", "agent_name": "Failed Agent"}]
        
        should_replan, new_plan = await planner.replan_if_needed(
            original_plan, execution_results, "í…ŒìŠ¤íŠ¸ ìš”ì²­"
        )
        
        assert should_replan is True
        assert len(new_plan) == 1
        assert new_plan[0]["agent_name"] == "Alternative Agent"


class TestIntelligentFinalAnswerEngine:
    """ì§€ëŠ¥í˜• ìµœì¢… ë‹µë³€ ìƒì„± ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def answer_engine(self):
        """ë‹µë³€ ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return IntelligentFinalAnswerEngine()
    
    @pytest.fixture
    def sample_execution_results(self):
        """í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ê²°ê³¼"""
        return [
            {
                "agent_name": "Data Loader Agent",
                "status": "success",
                "output": "ê³ ê° ë°ì´í„° 10,000ê±´ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë”©í–ˆìŠµë‹ˆë‹¤.",
                "execution_time": 2.5
            },
            {
                "agent_name": "EDA Tools Agent",
                "status": "success", 
                "output": "ê³ ê° ì—°ë ¹ëŒ€ë³„ êµ¬ë§¤ íŒ¨í„´ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. 30-40ëŒ€ê°€ ì „ì²´ êµ¬ë§¤ì˜ 65%ë¥¼ ì°¨ì§€í•©ë‹ˆë‹¤.",
                "execution_time": 5.2
            }
        ]
    
    @pytest.fixture
    def mock_openai_client_answer(self):
        """ë‹µë³€ ì—”ì§„ìš© OpenAI í´ë¼ì´ì–¸íŠ¸ ëª¨í‚¹"""
        mock_client = AsyncMock()
        mock_response = Mock()
        mock_response.choices = [Mock()]
        mock_response.choices[0].message.content = """# ğŸ¯ ê³ ê° ë°ì´í„° ë¶„ì„ ê²°ê³¼

## ğŸ“Š í•µì‹¬ ë°œê²¬ì‚¬í•­
- ì´ 10,000ê±´ì˜ ê³ ê° ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„í–ˆìŠµë‹ˆë‹¤
- 30-40ëŒ€ ê³ ê°ì´ ì „ì²´ êµ¬ë§¤ì˜ 65%ë¥¼ ì°¨ì§€í•˜ëŠ” í•µì‹¬ ê³ ê°ì¸µì…ë‹ˆë‹¤
- ë°ì´í„° í’ˆì§ˆì´ ìš°ìˆ˜í•˜ì—¬ ì‹ ë¢°ì„± ìˆëŠ” ë¶„ì„ì´ ê°€ëŠ¥í–ˆìŠµë‹ˆë‹¤

## ğŸ“ˆ êµ¬ì²´ì  ë¶„ì„ ê²°ê³¼
**ë°ì´í„° ë¡œë”© ê²°ê³¼**: 10,000ê±´ì˜ ê³ ê° ë°ì´í„°ë¥¼ 2.5ì´ˆ ë§Œì— ë¡œë”© ì™„ë£Œ
**íŒ¨í„´ ë¶„ì„ ê²°ê³¼**: 30-40ëŒ€ ê³ ê°ì¸µì˜ êµ¬ë§¤ íŒ¨í„´ì´ ê°€ì¥ í™œë°œí•¨ (65% ì ìœ ìœ¨)

## ğŸ’¡ ì‹¤ë¬´ ì ìš© ê¶Œì¥ì‚¬í•­
1. 30-40ëŒ€ íƒ€ê²Ÿ ë§ˆì¼€íŒ… ì „ëµ ê°•í™”
2. í•´ë‹¹ ì—°ë ¹ëŒ€ ì„ í˜¸ ìƒí’ˆêµ° ì§‘ì¤‘ ê°œë°œ
3. ì •ê¸°ì ì¸ ê³ ê° í–‰ë™ íŒ¨í„´ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•

## ğŸ” ë¶„ì„ ì‹ ë¢°ì„±
- ë°ì´í„° ì™„ì„±ë„: ë†’ìŒ (10,000ê±´ ì „ì²´ ìœ íš¨)
- ë¶„ì„ ë°©ë²•ë¡ : í†µê³„ì  ìœ ì˜ì„± í™•ë³´
- ê²°ê³¼ ê²€ì¦: ë‹¤ì¤‘ ì—ì´ì „íŠ¸ êµì°¨ ê²€ì¦ ì™„ë£Œ

## ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
ì¶”ê°€ì ì¸ ì„¸ê·¸ë¨¼í…Œì´ì…˜ ë¶„ì„ ë° ì˜ˆì¸¡ ëª¨ë¸ë§ì„ í†µí•´ ë” ì •êµí•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ê°€ëŠ¥í•©ë‹ˆë‹¤."""
        mock_client.chat.completions.create.return_value = mock_response
        return mock_client
    
    def test_summarize_execution_results(self, answer_engine, sample_execution_results):
        """ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ í…ŒìŠ¤íŠ¸"""
        summary = answer_engine._summarize_execution_results(sample_execution_results)
        
        assert "Data Loader Agent" in summary
        assert "EDA Tools Agent" in summary
        assert "success" in summary
        assert "10,000ê±´" in summary
    
    def test_format_execution_results(self, answer_engine, sample_execution_results):
        """ì‹¤í–‰ ê²°ê³¼ í¬ë§·íŒ… í…ŒìŠ¤íŠ¸"""
        formatted = answer_engine._format_execution_results(sample_execution_results)
        
        assert "**1. Data Loader Agent**" in formatted
        assert "**2. EDA Tools Agent**" in formatted
        assert "ìƒíƒœ: success" in formatted
    
    def test_structured_fallback_generation(self, answer_engine, sample_execution_results):
        """êµ¬ì¡°í™”ëœ ê¸°ë³¸ ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸"""
        answer = answer_engine._generate_structured_fallback(
            "ê³ ê° ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
            sample_execution_results
        )
        
        assert "# ğŸ¯" in answer
        assert "## ğŸ“Š í•µì‹¬ ë°œê²¬ì‚¬í•­" in answer
        assert "## ğŸ“ˆ êµ¬ì²´ì  ë¶„ì„ ê²°ê³¼" in answer
        assert "## ğŸ’¡ ì‹¤ë¬´ ì ìš© ê¶Œì¥ì‚¬í•­" in answer
        assert "Data Loader Agent" in answer
        assert "EDA Tools Agent" in answer
    
    @pytest.mark.asyncio
    async def test_comprehensive_answer_generation(self, mock_openai_client_answer, sample_execution_results):
        """ì¢…í•©ì ì¸ ìµœì¢… ë‹µë³€ ìƒì„± í…ŒìŠ¤íŠ¸"""
        engine = IntelligentFinalAnswerEngine(mock_openai_client_answer)
        
        answer = await engine.generate_comprehensive_answer(
            "ê³ ê° ë°ì´í„°ì˜ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
            sample_execution_results,
            {"context_id": "test_context"}
        )
        
        assert "# ğŸ¯ ê³ ê° ë°ì´í„° ë¶„ì„ ê²°ê³¼" in answer
        assert "30-40ëŒ€" in answer
        assert "65%" in answer
        assert "10,000ê±´" in answer
        mock_openai_client_answer.chat.completions.create.assert_called_once()


class TestCherryAI_v9_Orchestrator:
    """CherryAI v9 ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í†µí•© í…ŒìŠ¤íŠ¸"""
    
    @pytest.fixture
    def orchestrator(self):
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return CherryAI_v9_IntelligentDynamicOrchestrator()
    
    def test_orchestrator_initialization(self, orchestrator):
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        assert orchestrator.context_manager is not None
        assert orchestrator.workflow_planner is not None
        assert orchestrator.answer_engine is not None
        assert isinstance(orchestrator.discovered_agents, dict)
        assert isinstance(orchestrator.execution_history, list)
    
    def test_extract_user_input(self, orchestrator):
        """ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        # Mock RequestContext ìƒì„±
        mock_context = Mock()
        mock_message = Mock()
        mock_part = Mock()
        mock_part.root.kind = 'text'
        mock_part.root.text = 'ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”'
        mock_message.parts = [mock_part]
        mock_context.message = mock_message
        
        user_input = orchestrator._extract_user_input(mock_context)
        assert user_input == 'ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”'
    
    def test_analyze_agent_capabilities(self, orchestrator):
        """ì—ì´ì „íŠ¸ ëŠ¥ë ¥ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        agent_card = {
            "name": "Data Loader Agent",
            "description": "ë°ì´í„° ë¡œë”© ì „ë¬¸ ì—ì´ì „íŠ¸",
            "skills": [
                {
                    "tags": ["data_loading", "file_processing"]
                }
            ]
        }
        
        capabilities = orchestrator._analyze_agent_capabilities(agent_card)
        assert "data_loading" in capabilities
        assert "file_processing" in capabilities
    
    @pytest.mark.asyncio
    async def test_discover_intelligent_agents(self, orchestrator):
        """ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ ë°œê²¬ í…ŒìŠ¤íŠ¸"""
        with patch('httpx.AsyncClient') as mock_client:
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.json.return_value = {
                "name": "Test Agent",
                "description": "í…ŒìŠ¤íŠ¸ ì—ì´ì „íŠ¸",
                "version": "1.0.0",
                "skills": []
            }
            
            mock_client.return_value.__aenter__.return_value.get.return_value = mock_response
            
            agents = await orchestrator._discover_intelligent_agents()
            
            # ìµœì†Œí•œ í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ëŠ” ë°œê²¬ë˜ì–´ì•¼ í•¨
            assert len(agents) >= 0
    
    @pytest.mark.asyncio 
    async def test_execute_agent_with_context(self, orchestrator):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì—ì´ì „íŠ¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸"""
        step = {
            "step_id": "test_step",
            "agent_name": "Test Agent",
            "task_description": "í…ŒìŠ¤íŠ¸ ì‘ì—… ìˆ˜í–‰",
            "priority": "high"
        }
        
        agent_context = {
            "task_objective": "í…ŒìŠ¤íŠ¸ ëª©í‘œ",
            "context_summary": "í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸"
        }
        
        result = await orchestrator._execute_agent_with_context(
            step, agent_context, "test_context"
        )
        
        assert result["agent_name"] == "Test Agent"
        assert result["status"] in ["success", "failed"]
        assert "execution_time" in result
        assert "context_used" in result


@pytest.mark.asyncio
class TestOrchestrator_Integration:
    """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° í†µí•© í…ŒìŠ¤íŠ¸"""
    
    async def test_end_to_end_workflow(self):
        """ì—”ë“œíˆ¬ì—”ë“œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # Mock ì»´í¬ë„ŒíŠ¸ë“¤ì„ ì‚¬ìš©í•œ ì „ì²´ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸
        orchestrator = CherryAI_v9_IntelligentDynamicOrchestrator()
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ì–´ë„ ê¸°ë³¸ ê¸°ëŠ¥ì´ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸
        assert orchestrator.context_manager._create_basic_context(
            "Test Agent", "í…ŒìŠ¤íŠ¸", "ìš”ì²­"
        ) is not None
        
        assert orchestrator.workflow_planner._create_fallback_plan([]) is not None
        
        assert orchestrator.answer_engine._generate_structured_fallback(
            "í…ŒìŠ¤íŠ¸ ìš”ì²­", []
        ) is not None


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"]) 