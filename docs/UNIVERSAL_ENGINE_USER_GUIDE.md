# Universal Engine User Guide

## ğŸš€ ê°œìš”

Universal Engineì€ CherryAIì˜ í•µì‹¬ ì§€ëŠ¥í˜• ì‹œìŠ¤í…œìœ¼ë¡œ, ì œë¡œ í•˜ë“œì½”ë”© ì² í•™ì„ ë°”íƒ•ìœ¼ë¡œ êµ¬ì¶•ëœ LLM-First ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. DeepSeek-R1ì—ì„œ ì˜ê°ì„ ë°›ì€ 4ë‹¨ê³„ ë©”íƒ€ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ë‹¤ì–‘í•œ ë°ì´í„° ë¶„ì„ ìš”ì²­ì— ì ì‘ì ìœ¼ë¡œ ëŒ€ì‘í•©ë‹ˆë‹¤.

## ğŸ¯ ì£¼ìš” íŠ¹ì§•

### 1. LLM-First ì•„í‚¤í…ì²˜
- **ì œë¡œ í•˜ë“œì½”ë”©**: ëª¨ë“  ë¡œì§ì´ LLMì„ í†µí•´ ë™ì ìœ¼ë¡œ ìƒì„±
- **ì ì‘ì  ì¶”ë¡ **: ê° ìš”ì²­ì— ë§ëŠ” ìµœì ì˜ ì²˜ë¦¬ ë°©ì‹ ìë™ ê²°ì •
- **ë©”íƒ€ ì¶”ë¡ **: 4ë‹¨ê³„ ì¶”ë¡  ê³¼ì •ìœ¼ë¡œ ë³µì¡í•œ ë¬¸ì œ í•´ê²°

### 2. Universal Query Processing
- **ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬**: ì¼ë°˜ ì–¸ì–´ë¡œ ë°ì´í„° ë¶„ì„ ìš”ì²­
- **ë‹¤ì–‘í•œ ë°ì´í„° í˜•ì‹ ì§€ì›**: CSV, JSON, DataFrame ë“±
- **ì»¨í…ìŠ¤íŠ¸ ì¸ì‹**: ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ê³¼ê±° ìƒí˜¸ì‘ìš© ê¸°ë°˜ ë§ì¶¤ ì‘ë‹µ

### 3. A2A (Agent-to-Agent) í†µí•©
- **10ê°œ ì „ë¬¸ ì—ì´ì „íŠ¸**: í¬íŠ¸ 8306-8315ì—ì„œ ìš´ì˜
  - 8306: ë°ì´í„° ì •ë¦¬, 8307: ë°ì´í„° ë¡œë”, 8308: ì‹œê°í™”, 8309: ë°ì´í„° ê°€ê³µ
  - 8310: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§, 8311: SQL ë¶„ì„, 8312: EDA ë„êµ¬, 8313: H2O ML
  - 8314: MLflow ë„êµ¬, 8315: ë³´ê³ ì„œ ìƒì„±
- **ìë™ ì—ì´ì „íŠ¸ ë°œê²¬**: ë™ì  ì—ì´ì „íŠ¸ íƒì§€ ë° í• ë‹¹
- **ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**: ë³µì¡í•œ ì‘ì—…ì˜ ìë™ ë¶„ì‚° ì²˜ë¦¬

### 4. ì ì‘ì  ì‚¬ìš©ì ì´í•´
- **ì‚¬ìš©ì ë ˆë²¨ ì¶”ì •**: ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€ ìë™ íŒë³„
- **Progressive Disclosure**: ì‚¬ìš©ì ìˆ˜ì¤€ì— ë§ëŠ” ì •ë³´ ì œê³µ
- **ì‹¤ì‹œê°„ í•™ìŠµ**: ì‚¬ìš©ì ìƒí˜¸ì‘ìš©ì„ í†µí•œ ì§€ì†ì  ê°œì„ 

## ğŸ“‹ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### ìµœì†Œ ìš”êµ¬ì‚¬í•­
- **Python**: 3.9 ì´ìƒ
- **ë©”ëª¨ë¦¬**: 4GB RAM ì´ìƒ
- **ë””ìŠ¤í¬**: 2GB ì—¬ìœ  ê³µê°„
- **LLM Provider**: Ollama, OpenAI, ë˜ëŠ” ê¸°íƒ€ ì§€ì› ëª¨ë¸

### ê¶Œì¥ ìš”êµ¬ì‚¬í•­
- **Python**: 3.11 ì´ìƒ
- **ë©”ëª¨ë¦¬**: 8GB RAM ì´ìƒ
- **CPU**: 4ì½”ì–´ ì´ìƒ
- **GPU**: CUDA ì§€ì› (ì„ íƒì‚¬í•­, ì„±ëŠ¥ í–¥ìƒ)

## ğŸ›  ì„¤ì¹˜ ë° ì„¤ì •

### 1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

```bash
# LLM Provider ì„¤ì •
export LLM_PROVIDER="ollama"  # ë˜ëŠ” "openai"
export OLLAMA_MODEL="llama2"  # Ollama ì‚¬ìš© ì‹œ
export OPENAI_API_KEY="your-key"  # OpenAI ì‚¬ìš© ì‹œ

# A2A ì—ì´ì „íŠ¸ ì„¤ì •
export A2A_PORT_START="8306"
export A2A_PORT_END="8315"
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source .venv/bin/activate

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. ì‹œìŠ¤í…œ ì´ˆê¸°í™”

```python
from core.universal_engine.initialization.system_initializer import UniversalEngineInitializer

# ì‹œìŠ¤í…œ ì´ˆê¸°í™”
initializer = UniversalEngineInitializer()
await initializer.initialize_system()
```

## ğŸš€ ê¸°ë³¸ ì‚¬ìš©ë²•

### 1. ê°„ë‹¨í•œ ì¿¼ë¦¬ ì²˜ë¦¬

```python
from core.universal_engine.universal_query_processor import UniversalQueryProcessor
import pandas as pd

# ë°ì´í„° ì¤€ë¹„
data = pd.read_csv("your_data.csv")

# Query Processor ì´ˆê¸°í™”
processor = UniversalQueryProcessor()

# ìì—°ì–´ ì¿¼ë¦¬ ì‹¤í–‰
result = await processor.process_query(
    query="ì´ ë°ì´í„°ì˜ ì£¼ìš” íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
    data=data,
    context={"session_id": "user123"}
)

print(result)
```

### 2. ì„¸ì…˜ ê´€ë¦¬

```python
from core.universal_engine.session.session_management_system import SessionManager

# ì„¸ì…˜ ë§¤ë‹ˆì € ì´ˆê¸°í™”
session_manager = SessionManager()

# ì„¸ì…˜ ìƒì„±
session = {
    "session_id": "unique_session_123",
    "user_id": "user456",
    "messages": [],
    "user_profile": {"expertise": "intermediate"}
}

# ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
context = await session_manager.extract_comprehensive_context(session)
```

### 3. A2A ì—ì´ì „íŠ¸ í™œìš©

```python
from core.universal_engine.a2a_integration.a2a_workflow_orchestrator import A2AWorkflowOrchestrator

# ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì´ˆê¸°í™”
orchestrator = A2AWorkflowOrchestrator()

# ë³µì¡í•œ ë¶„ì„ ì‘ì—… ì‹¤í–‰
workflow_result = await orchestrator.execute_workflow(
    query="ë°ì´í„° ì •ë¦¬ í›„ ê³ ê¸‰ í†µê³„ ë¶„ì„ ìˆ˜í–‰",
    data=large_dataset,
    required_agents=["data_cleaner", "statistical_analyst"]
)
```

## ğŸ› ê³ ê¸‰ ì„¤ì •

### 1. ì‚¬ìš©ì ë ˆë²¨ ì»¤ìŠ¤í„°ë§ˆì´ì§•

```python
from core.universal_engine.adaptive_user_understanding import AdaptiveUserUnderstanding

# ì‚¬ìš©ì ì´í•´ ëª¨ë“ˆ ì´ˆê¸°í™”
user_understanding = AdaptiveUserUnderstanding()

# ì‚¬ìš©ì í”„ë¡œí•„ ì„¤ì •
user_profile = await user_understanding.estimate_user_level(
    query="ë³µì¡í•œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ì„ ìœ„í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§",
    historical_queries=["íšŒê·€ë¶„ì„", "êµì°¨ê²€ì¦", "í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"],
    user_feedback={"complexity_preference": "high"}
)
```

### 2. ë™ì  ì»¨í…ìŠ¤íŠ¸ ë°œê²¬

```python
from core.universal_engine.dynamic_context_discovery import DynamicContextDiscovery

# ì»¨í…ìŠ¤íŠ¸ ë°œê²¬ ëª¨ë“ˆ ì´ˆê¸°í™”
context_discovery = DynamicContextDiscovery()

# ë™ì  ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
discovered_context = await context_discovery.discover_context(
    query="ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸",
    data=customer_data,
    domain_knowledge={"industry": "telecommunications"}
)
```

### 3. ë©”íƒ€ ì¶”ë¡  ì—”ì§„ í™œìš©

```python
from core.universal_engine.meta_reasoning_engine import MetaReasoningEngine

# ë©”íƒ€ ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™”
meta_engine = MetaReasoningEngine()

# 4ë‹¨ê³„ ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
reasoning_result = await meta_engine.analyze_request(
    query="ì‹œê³„ì—´ ë°ì´í„°ì˜ ì´ìƒì¹˜ íƒì§€ ë° ì˜ˆì¸¡",
    data=time_series_data,
    context={"domain": "finance", "urgency": "high"}
)
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì„±ëŠ¥

### 1. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
from core.universal_engine.monitoring.performance_monitoring_system import PerformanceMonitoringSystem

# ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
monitor = PerformanceMonitoringSystem()

# ì„±ëŠ¥ ë©”íŠ¸ë¦­ í™•ì¸
metrics = monitor.get_performance_metrics()
print(f"í‰ê·  ì‘ë‹µì‹œê°„: {metrics['avg_response_time']:.2f}ì´ˆ")
print(f"ì„±ê³µë¥ : {metrics['success_rate']:.1f}%")
```

### 2. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸

```python
# A2A ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸
from core.universal_engine.a2a_integration.a2a_agent_discovery import A2AAgentDiscoverySystem

discovery = A2AAgentDiscoverySystem()
agent_status = await discovery.check_agent_health()

for agent_id, status in agent_status.items():
    print(f"ì—ì´ì „íŠ¸ {agent_id}: {status['status']} (í¬íŠ¸: {status['port']})")
```

## ğŸ”§ ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. LLM ì—°ê²° ì˜¤ë¥˜
```
Error: LLM service unavailable
```

**í•´ê²° ë°©ë²•:**
1. LLM_PROVIDER í™˜ê²½ë³€ìˆ˜ í™•ì¸
2. Ollama ì„œë¹„ìŠ¤ ì‹¤í–‰ ìƒíƒœ í™•ì¸: `ollama serve`
3. OpenAI API í‚¤ ìœ íš¨ì„± ê²€ì¦

#### 2. A2A ì—ì´ì „íŠ¸ ì—°ê²° ì‹¤íŒ¨
```
Error: A2A agents not responding
```

**í•´ê²° ë°©ë²•:**
1. í¬íŠ¸ ë²”ìœ„ í™•ì¸ (8306-8315)
2. ë°©í™”ë²½ ì„¤ì • ê²€í† 
3. ì—ì´ì „íŠ¸ ì„œë¹„ìŠ¤ ì¬ì‹œì‘

#### 3. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```
Error: Out of memory
```

**í•´ê²° ë°©ë²•:**
1. ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
2. ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬ ì¦ì„¤
3. ë°°ì¹˜ í¬ê¸° ì¡°ì •

### ë¡œê·¸ í™•ì¸

```python
import logging

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
logging.basicConfig(level=logging.INFO)

# Universal Engine ë¡œê·¸ í™•ì¸
logger = logging.getLogger('universal_engine')
logger.info("ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸")
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ

### 1. ë°ì´í„° ì²˜ë¦¬ ìµœì í™”
- **ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• 
- **ìºì‹± í™œìš©**: ë°˜ë³µì ì¸ ì¿¼ë¦¬ ê²°ê³¼ ìºì‹œ ì €ì¥
- **ì¸ë±ì‹±**: ìì£¼ ì‚¬ìš©í•˜ëŠ” ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ ìƒì„±

### 2. LLM í˜¸ì¶œ ìµœì í™”
- **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ì„ ë¬¶ì–´ì„œ ì²˜ë¦¬
- **í”„ë¡¬í”„íŠ¸ ìµœì í™”**: ê°„ê²°í•˜ê³  ëª…í™•í•œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
- **í† í° ì œí•œ**: ì…ë ¥ í† í° ìˆ˜ ê´€ë¦¬

### 3. A2A ì—ì´ì „íŠ¸ ìµœì í™”
- **ë¡œë“œ ë°¸ëŸ°ì‹±**: ì—ì´ì „íŠ¸ ê°„ ì‘ì—… ë¶„ì‚°
- **Connection Pooling**: ì—°ê²° ì¬ì‚¬ìš©
- **Circuit Breaker**: ì¥ì•  ì—ì´ì „íŠ¸ ìë™ ì°¨ë‹¨

## ğŸ”’ ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### 1. ë°ì´í„° ë³´ì•ˆ
- **ë¯¼ê° ì •ë³´ ë§ˆìŠ¤í‚¹**: SSN, ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸ ë“± ìë™ ë§ˆìŠ¤í‚¹
- **ì ‘ê·¼ ê¶Œí•œ ê´€ë¦¬**: ì‚¬ìš©ìë³„ ë°ì´í„° ì ‘ê·¼ ì œí•œ
- **ì•”í˜¸í™”**: ì €ì¥ ë° ì „ì†¡ ì¤‘ ë°ì´í„° ì•”í˜¸í™”

### 2. ì„¸ì…˜ ë³´ì•ˆ
- **ì„¸ì…˜ ë§Œë£Œ**: ë¹„í™œì„± ì„¸ì…˜ ìë™ ì •ë¦¬
- **ì„¸ì…˜ ê²©ë¦¬**: ì‚¬ìš©ì ê°„ ë°ì´í„° ë¶„ë¦¬
- **ë³´ì•ˆ í—¤ë”**: ì ì ˆí•œ HTTP ë³´ì•ˆ í—¤ë” ì„¤ì •

### 3. ì…ë ¥ ê²€ì¦
- **SQL ì¸ì ì…˜ ë°©ì§€**: ì•…ì˜ì  ì¿¼ë¦¬ ì°¨ë‹¨
- **XSS ë°©ì§€**: ìŠ¤í¬ë¦½íŠ¸ ì¸ì ì…˜ ë°©ì§€
- **ì…ë ¥ í¬ê¸° ì œí•œ**: ê³¼ë„í•œ ì…ë ¥ ì°¨ë‹¨

## ğŸ¯ ì‚¬ìš© ì‚¬ë¡€

### 1. ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€
```python
# ë§¤ì¶œ íŠ¸ë Œë“œ ë¶„ì„
result = await processor.process_query(
    query="ì§€ë‚œ 6ê°œì›” ë§¤ì¶œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ê³  ë‹¤ìŒ ë¶„ê¸° ì˜ˆì¸¡ì„ í•´ì£¼ì„¸ìš”",
    data=sales_data,
    context={"user_role": "analyst", "expertise": "intermediate"}
)
```

### 2. ë°ì´í„° ê³¼í•™ì
```python
# ê³ ê¸‰ ë¨¸ì‹ ëŸ¬ë‹ ë¶„ì„
result = await processor.process_query(
    query="ê³ ê° ì„¸ê·¸ë¨¼í…Œì´ì…˜ì„ ìœ„í•œ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ í›„ ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ íŠ¹ì„± ë¶„ì„",
    data=customer_data,
    context={"user_role": "data_scientist", "expertise": "expert"}
)
```

### 3. ë¹„ê°œë°œì
```python
# ê°„ë‹¨í•œ ë°ì´í„° ìš”ì•½
result = await processor.process_query(
    query="ì´ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ê²ƒë“¤ì„ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”",
    data=survey_data,
    context={"user_role": "business_user", "expertise": "beginner"}
)
```

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

- **API ì°¸ì¡°**: [UNIVERSAL_ENGINE_API_REFERENCE.md](./UNIVERSAL_ENGINE_API_REFERENCE.md)
- **ë°°í¬ ê°€ì´ë“œ**: [UNIVERSAL_ENGINE_DEPLOYMENT_GUIDE.md](./UNIVERSAL_ENGINE_DEPLOYMENT_GUIDE.md)
- **ë¬¸ì œ í•´ê²°**: [UNIVERSAL_ENGINE_TROUBLESHOOTING.md](./UNIVERSAL_ENGINE_TROUBLESHOOTING.md)
- **ì „ì²´ ì‹œìŠ¤í…œ ê°€ì´ë“œ**: [USER_GUIDE.md](./USER_GUIDE.md)

## ğŸ†˜ ì§€ì› ë° ì»¤ë®¤ë‹ˆí‹°

ë¬¸ì œê°€ ë°œìƒí•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´:

1. **ë¬¸ì œ í•´ê²° ê°€ì´ë“œ** í™•ì¸
2. **ë¡œê·¸ íŒŒì¼** ë¶„ì„
3. **GitHub Issues**ì— ë¬¸ì œ ë³´ê³ 
4. **ì»¤ë®¤ë‹ˆí‹° í¬ëŸ¼** ì°¸ì—¬

---

**Universal Engine v1.0** - LLM-First ë°ì´í„° ë¶„ì„ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„