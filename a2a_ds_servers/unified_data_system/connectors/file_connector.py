"""
í–¥ìƒëœ íŒŒì¼ ì»¤ë„¥í„° (Enhanced File Connector)

pandas_agentì˜ FileConnector íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ í–¥ìƒëœ íŒŒì¼ ë°ì´í„° ì†ŒìŠ¤ ì»¤ë„¥í„°
ë‹¤ì¤‘ ì¸ì½”ë”© ì§€ì›, ì§€ëŠ¥í˜• ë¡œë”© ì „ëµ, ìºì‹± ë“±ì˜ ê³ ê¸‰ ê¸°ëŠ¥ ì œê³µ

í•µì‹¬ ì›ì¹™:
- pandas_agent FileConnector 100% í˜¸í™˜
- UTF-8 ì¸ì½”ë”© ë¬¸ì œ ìë™ í•´ê²°
- LLM First: ë¡œë”© ì „ëµì„ LLMì´ ë™ì  ê²°ì •
- ì„±ëŠ¥ ìµœì í™”: ìºì‹± ë° ì²­í¬ ë¡œë”© ì§€ì›
"""

import pandas as pd
import logging
import asyncio
from typing import Dict, List, Optional, Any, Union
from pathlib import Path
import json
import os

from ..core.unified_data_interface import LoadingStrategy, A2AContext
from ..core.smart_dataframe import SmartDataFrame
from ..utils.encoding_detector import EncodingDetector
from ..core.cache_manager import CacheManager

logger = logging.getLogger(__name__)


class EnhancedFileConnector:
    """
    í–¥ìƒëœ íŒŒì¼ ì»¤ë„¥í„°
    
    pandas_agentì˜ FileConnector íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬í˜„ëœ
    ì§€ëŠ¥í˜• íŒŒì¼ ë¡œë”© ì»¤ë„¥í„°
    """
    
    def __init__(self, cache_manager: Optional[CacheManager] = None):
        """
        í–¥ìƒëœ íŒŒì¼ ì»¤ë„¥í„° ì´ˆê¸°í™”
        
        Args:
            cache_manager: ìºì‹œ ë§¤ë‹ˆì € (ì„ íƒì )
        """
        self.encoding_detector = EncodingDetector()
        self.cache_manager = cache_manager
        
        # ì§€ì›í•˜ëŠ” íŒŒì¼ í˜•ì‹
        self.supported_formats = {
            '.csv': self._load_csv,
            '.xlsx': self._load_excel,
            '.xls': self._load_excel,
            '.json': self._load_json,
            '.parquet': self._load_parquet,
            '.feather': self._load_feather,
            '.txt': self._load_text,
            '.tsv': self._load_tsv
        }
        
        logger.info("âœ… EnhancedFileConnector ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def load_file(self, 
                       file_path: str, 
                       strategy: LoadingStrategy,
                       context: Optional[A2AContext] = None) -> SmartDataFrame:
        """
        íŒŒì¼ ë¡œë”© (ë©”ì¸ ë©”ì„œë“œ)
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            strategy: ë¡œë”© ì „ëµ
            context: A2A ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            SmartDataFrame: ë¡œë”©ëœ ì§€ëŠ¥í˜• DataFrame
        """
        try:
            # ìºì‹œ í™•ì¸
            if self.cache_manager and strategy.use_cache:
                cache_key = self._generate_cache_key(file_path, strategy)
                cached_df = await self.cache_manager.get(cache_key)
                
                if cached_df is not None:
                    logger.info(f"âœ… ìºì‹œì—ì„œ ë¡œë”©: {file_path}")
                    return cached_df
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            file_obj = Path(file_path)
            if not file_obj.exists():
                raise FileNotFoundError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}")
            
            # íŒŒì¼ í˜•ì‹ ê°ì§€
            file_extension = file_obj.suffix.lower()
            if file_extension not in self.supported_formats:
                raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")
            
            # íŒŒì¼ ë¡œë”©
            loader_func = self.supported_formats[file_extension]
            df = await loader_func(file_path, strategy)
            
            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = await self._generate_metadata(file_path, strategy, context)
            
            # SmartDataFrame ìƒì„±
            smart_df = SmartDataFrame(df, metadata)
            
            # ìºì‹œ ì €ì¥
            if self.cache_manager and strategy.use_cache:
                await self.cache_manager.set(
                    cache_key, 
                    smart_df, 
                    ttl=strategy.cache_ttl,
                    tags={f"file:{file_obj.name}", f"ext:{file_extension}"}
                )
            
            logger.info(f"âœ… íŒŒì¼ ë¡œë”© ì™„ë£Œ: {file_path} ({smart_df.shape})")
            return smart_df
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ {file_path}: {e}")
            raise
    
    async def _load_csv(self, file_path: str, strategy: LoadingStrategy) -> pd.DataFrame:
        """CSV íŒŒì¼ ë¡œë”©"""
        try:
            # ì¸ì½”ë”© ê°ì§€ ë˜ëŠ” ì „ëµ ì‚¬ìš©
            encoding = await self._determine_encoding(file_path, strategy)
            
            # ë¡œë”© íŒŒë¼ë¯¸í„° ì„¤ì •
            kwargs = {
                'encoding': encoding,
                'low_memory': False  # íƒ€ì… ì¶”ë¡  ê°œì„ 
            }
            
            # ì²­í¬ ë¡œë”© ì§€ì›
            if strategy.chunk_size:
                kwargs['chunksize'] = strategy.chunk_size
                
                # ì²­í¬ ë‹¨ìœ„ë¡œ ì½ì–´ì„œ í•©ì¹˜ê¸°
                chunks = []
                for chunk in pd.read_csv(file_path, **kwargs):
                    chunks.append(chunk)
                    
                    # ìƒ˜í”Œë§ì´ í•„ìš”í•œ ê²½ìš°
                    if strategy.sample_ratio and len(chunks) * strategy.chunk_size >= 10000:
                        break
                
                df = pd.concat(chunks, ignore_index=True)
            else:
                df = pd.read_csv(file_path, **kwargs)
            
            # ìƒ˜í”Œë§ ì ìš©
            if strategy.sample_ratio and len(df) > 1000:
                sample_size = int(len(df) * strategy.sample_ratio)
                df = df.sample(n=sample_size, random_state=42)
                logger.info(f"ğŸ¯ ìƒ˜í”Œë§ ì ìš©: {len(df)}í–‰ ({strategy.sample_ratio:.1%})")
            
            return df
            
        except UnicodeDecodeError as e:
            # í´ë°± ì¸ì½”ë”© ì‹œë„
            logger.warning(f"âš ï¸ ì¸ì½”ë”© ì˜¤ë¥˜, í´ë°± ì‹œë„: {e}")
            return await self._load_csv_with_fallback(file_path, strategy)
    
    async def _load_csv_with_fallback(self, file_path: str, strategy: LoadingStrategy) -> pd.DataFrame:
        """CSV í´ë°± ë¡œë”© (ë‹¤ì¤‘ ì¸ì½”ë”© ì‹œë„)"""
        for encoding in strategy.fallback_encodings:
            try:
                logger.info(f"ğŸ”„ í´ë°± ì¸ì½”ë”© ì‹œë„: {encoding}")
                df = pd.read_csv(file_path, encoding=encoding, low_memory=False)
                logger.info(f"âœ… í´ë°± ì¸ì½”ë”© ì„±ê³µ: {encoding}")
                return df
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        # ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨ ì‹œ errors='ignore' ì‚¬ìš©
        logger.warning("âš ï¸ ëª¨ë“  ì¸ì½”ë”© ì‹¤íŒ¨, errors='ignore' ì‚¬ìš©")
        return pd.read_csv(file_path, encoding='utf-8', errors='ignore', low_memory=False)
    
    async def _load_excel(self, file_path: str, strategy: LoadingStrategy) -> pd.DataFrame:
        """Excel íŒŒì¼ ë¡œë”©"""
        try:
            kwargs = {}
            
            # ì—”ì§„ ìë™ ì„ íƒ
            if file_path.endswith('.xlsx'):
                kwargs['engine'] = 'openpyxl'
            elif file_path.endswith('.xls'):
                kwargs['engine'] = 'xlrd'
            
            df = pd.read_excel(file_path, **kwargs)
            
            # ìƒ˜í”Œë§ ì ìš©
            if strategy.sample_ratio and len(df) > 1000:
                sample_size = int(len(df) * strategy.sample_ratio)
                df = df.sample(n=sample_size, random_state=42)
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Excel ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _load_json(self, file_path: str, strategy: LoadingStrategy) -> pd.DataFrame:
        """JSON íŒŒì¼ ë¡œë”©"""
        try:
            # ì¸ì½”ë”© ê°ì§€
            encoding = await self._determine_encoding(file_path, strategy)
            
            # JSON ë¡œë”© ì‹œë„ (ì—¬ëŸ¬ í˜•ì‹ ì§€ì›)
            try:
                df = pd.read_json(file_path, encoding=encoding)
            except ValueError:
                # lines=True ì‹œë„ (JSONL í˜•ì‹)
                df = pd.read_json(file_path, lines=True, encoding=encoding)
            
            # ìƒ˜í”Œë§ ì ìš©
            if strategy.sample_ratio and len(df) > 1000:
                sample_size = int(len(df) * strategy.sample_ratio)
                df = df.sample(n=sample_size, random_state=42)
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ JSON ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _load_parquet(self, file_path: str, strategy: LoadingStrategy) -> pd.DataFrame:
        """Parquet íŒŒì¼ ë¡œë”©"""
        try:
            df = pd.read_parquet(file_path)
            
            # ìƒ˜í”Œë§ ì ìš©
            if strategy.sample_ratio and len(df) > 1000:
                sample_size = int(len(df) * strategy.sample_ratio)
                df = df.sample(n=sample_size, random_state=42)
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Parquet ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _load_feather(self, file_path: str, strategy: LoadingStrategy) -> pd.DataFrame:
        """Feather íŒŒì¼ ë¡œë”©"""
        try:
            df = pd.read_feather(file_path)
            
            # ìƒ˜í”Œë§ ì ìš©
            if strategy.sample_ratio and len(df) > 1000:
                sample_size = int(len(df) * strategy.sample_ratio)
                df = df.sample(n=sample_size, random_state=42)
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ Feather ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _load_text(self, file_path: str, strategy: LoadingStrategy) -> pd.DataFrame:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ë¡œë”© (ë‹¨ìˆœ ë¼ì¸ ê¸°ë°˜)"""
        try:
            encoding = await self._determine_encoding(file_path, strategy)
            
            with open(file_path, 'r', encoding=encoding) as f:
                lines = f.readlines()
            
            # ë¼ì¸ì„ DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame({'text': [line.strip() for line in lines]})
            
            # ìƒ˜í”Œë§ ì ìš©
            if strategy.sample_ratio and len(df) > 1000:
                sample_size = int(len(df) * strategy.sample_ratio)
                df = df.sample(n=sample_size, random_state=42)
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ í…ìŠ¤íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _load_tsv(self, file_path: str, strategy: LoadingStrategy) -> pd.DataFrame:
        """TSV íŒŒì¼ ë¡œë”©"""
        try:
            encoding = await self._determine_encoding(file_path, strategy)
            
            kwargs = {
                'encoding': encoding,
                'sep': '\t',
                'low_memory': False
            }
            
            df = pd.read_csv(file_path, **kwargs)
            
            # ìƒ˜í”Œë§ ì ìš©
            if strategy.sample_ratio and len(df) > 1000:
                sample_size = int(len(df) * strategy.sample_ratio)
                df = df.sample(n=sample_size, random_state=42)
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ TSV ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _determine_encoding(self, file_path: str, strategy: LoadingStrategy) -> str:
        """ì¸ì½”ë”© ê²°ì •"""
        # ì „ëµì— ëª…ì‹œì  ì¸ì½”ë”©ì´ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if strategy.encoding and strategy.encoding != 'auto':
            return strategy.encoding
        
        # ìë™ ê°ì§€
        detected_encoding = await self.encoding_detector.detect_encoding(file_path)
        return detected_encoding
    
    async def _generate_metadata(self, 
                                file_path: str, 
                                strategy: LoadingStrategy,
                                context: Optional[A2AContext]) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° ìƒì„±"""
        file_obj = Path(file_path)
        
        metadata = {
            "source": "file",
            "file_path": str(file_obj.absolute()),
            "file_name": file_obj.name,
            "file_extension": file_obj.suffix.lower(),
            "file_size": file_obj.stat().st_size,
            "encoding": strategy.encoding,
            "loading_strategy": {
                "chunk_size": strategy.chunk_size,
                "sample_ratio": strategy.sample_ratio,
                "use_cache": strategy.use_cache,
                "cache_ttl": strategy.cache_ttl
            },
            "loaded_at": pd.Timestamp.now().isoformat()
        }
        
        # A2A ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
        if context:
            metadata.update({
                "session_id": context.session_id,
                "user_id": context.user_id,
                "request_id": context.request_id
            })
        
        return metadata
    
    def _generate_cache_key(self, file_path: str, strategy: LoadingStrategy) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        file_obj = Path(file_path)
        
        # íŒŒì¼ ê²½ë¡œ, ìˆ˜ì • ì‹œê°„, ì „ëµì„ í¬í•¨í•œ í‚¤ ìƒì„±
        key_components = [
            str(file_obj.absolute()),
            str(file_obj.stat().st_mtime),
            strategy.encoding,
            str(strategy.chunk_size),
            str(strategy.sample_ratio)
        ]
        
        return "|".join(str(comp) for comp in key_components)
    
    async def validate_file(self, file_path: str) -> Dict[str, Any]:
        """íŒŒì¼ ìœ íš¨ì„± ê²€ì¦"""
        try:
            file_obj = Path(file_path)
            
            validation_result = {
                "valid": False,
                "exists": file_obj.exists(),
                "readable": False,
                "supported_format": False,
                "estimated_size_mb": 0,
                "encoding_issues": [],
                "recommendations": []
            }
            
            if not validation_result["exists"]:
                validation_result["recommendations"].append("íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return validation_result
            
            # ì½ê¸° ê¶Œí•œ í™•ì¸
            validation_result["readable"] = file_obj.is_file() and os.access(file_obj, os.R_OK)
            
            # ì§€ì› í˜•ì‹ í™•ì¸
            file_extension = file_obj.suffix.lower()
            validation_result["supported_format"] = file_extension in self.supported_formats
            
            # íŒŒì¼ í¬ê¸°
            file_size = file_obj.stat().st_size
            validation_result["estimated_size_mb"] = file_size / (1024 * 1024)
            
            # ì¸ì½”ë”© ë¶„ì„ (í…ìŠ¤íŠ¸ íŒŒì¼ë§Œ)
            if file_extension in ['.csv', '.txt', '.tsv', '.json']:
                encoding_analysis = await self.encoding_detector.analyze_file_encoding_issues(file_path)
                validation_result["encoding_issues"] = encoding_analysis.get("recommendations", [])
            
            # ê¶Œì¥ì‚¬í•­ ìƒì„±
            if validation_result["estimated_size_mb"] > 100:
                validation_result["recommendations"].append("ëŒ€ìš©ëŸ‰ íŒŒì¼ì…ë‹ˆë‹¤. ìƒ˜í”Œë§ ì˜µì…˜ì„ ê³ ë ¤í•˜ì„¸ìš”")
            
            if not validation_result["supported_format"]:
                validation_result["recommendations"].append(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì…ë‹ˆë‹¤: {file_extension}")
            
            # ì „ì²´ ìœ íš¨ì„±
            validation_result["valid"] = (
                validation_result["exists"] and 
                validation_result["readable"] and 
                validation_result["supported_format"]
            )
            
            return validation_result
            
        except Exception as e:
            return {
                "valid": False,
                "error": str(e),
                "recommendations": ["íŒŒì¼ ìœ íš¨ì„± ê²€ì¦ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤"]
            }
    
    def get_supported_formats(self) -> List[str]:
        """ì§€ì›ë˜ëŠ” íŒŒì¼ í˜•ì‹ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜"""
        return list(self.supported_formats.keys())
    
    async def preview_file(self, file_path: str, lines: int = 10) -> Dict[str, Any]:
        """íŒŒì¼ ë¯¸ë¦¬ë³´ê¸°"""
        try:
            # ê¸°ë³¸ ì „ëµìœ¼ë¡œ ë¯¸ë¦¬ë³´ê¸° ë¡œë”©
            strategy = LoadingStrategy(
                encoding='utf-8',
                sample_ratio=0.1 if lines > 5 else None
            )
            
            smart_df = await self.load_file(file_path, strategy)
            
            preview = {
                "shape": smart_df.shape,
                "columns": list(smart_df.columns),
                "dtypes": {col: str(dtype) for col, dtype in smart_df.dtypes.items()},
                "sample_data": smart_df.head(lines).to_dict('records'),
                "metadata": smart_df.metadata
            }
            
            return preview
            
        except Exception as e:
            logger.error(f"âŒ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e}")
            return {
                "error": str(e),
                "recommendations": ["íŒŒì¼ì„ ì§ì ‘ í™•ì¸í•´ë³´ì„¸ìš”"]
            } 