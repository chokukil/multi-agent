"""
ìºì‹œ ë§¤ë‹ˆì € (Cache Manager)

pandas_agentì˜ ìºì‹± íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ì„±ëŠ¥ ìµœì í™” ìºì‹œ ì‹œìŠ¤í…œ
LRU + TTL + íƒœê·¸ ê¸°ë°˜ ì§€ëŠ¥í˜• ìºì‹± êµ¬í˜„

í•µì‹¬ ì›ì¹™:
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: LRU ê¸°ë°˜ ìë™ ì •ë¦¬
- ì‹œê°„ ê¸°ë°˜ ë§Œë£Œ: TTL ì§€ì›
- íƒœê·¸ ê¸°ë°˜ ë¬´íš¨í™”: ê´€ë ¨ ë°ì´í„° ì¼ê´„ ì •ë¦¬
- ìŠ¤ë ˆë“œ ì•ˆì „ì„±: ë™ì‹œ ì ‘ê·¼ ì•ˆì „ ë³´ì¥
"""

import asyncio
import logging
import time
import hashlib
import pickle
import weakref
from typing import Dict, List, Optional, Any, Union, Set, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import OrderedDict
from threading import RLock
import os
from pathlib import Path

logger = logging.getLogger(__name__)


@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë³´"""
    key: str
    value: Any
    created_at: float
    accessed_at: float
    ttl: Optional[int] = None
    tags: Set[str] = field(default_factory=set)
    size_bytes: int = 0
    access_count: int = 0
    
    def is_expired(self) -> bool:
        """ë§Œë£Œ ì—¬ë¶€ í™•ì¸"""
        if self.ttl is None:
            return False
        return time.time() - self.created_at > self.ttl
    
    def update_access(self):
        """ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.accessed_at = time.time()
        self.access_count += 1


class CacheManager:
    """
    ì§€ëŠ¥í˜• ìºì‹œ ë§¤ë‹ˆì €
    
    pandas_agentì˜ ìºì‹± íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ êµ¬í˜„ëœ
    LRU + TTL + íƒœê·¸ ê¸°ë°˜ ê³ ì„±ëŠ¥ ìºì‹œ ì‹œìŠ¤í…œ
    """
    
    def __init__(self, 
                 max_size_mb: int = 100,
                 default_ttl: int = 3600,
                 cleanup_interval: int = 300,
                 persistent: bool = False,
                 cache_dir: Optional[str] = None):
        """
        ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        
        Args:
            max_size_mb: ìµœëŒ€ ìºì‹œ í¬ê¸° (MB)
            default_ttl: ê¸°ë³¸ TTL (ì´ˆ)
            cleanup_interval: ì •ë¦¬ ì£¼ê¸° (ì´ˆ)
            persistent: ì˜êµ¬ ì €ì¥ ì—¬ë¶€
            cache_dir: ì˜êµ¬ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.max_size_bytes = max_size_mb * 1024 * 1024
        self.default_ttl = default_ttl
        self.cleanup_interval = cleanup_interval
        self.persistent = persistent
        self.cache_dir = Path(cache_dir) if cache_dir else Path("./cache")
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (LRU ìˆœì„œ ìœ ì§€)
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self._current_size = 0
        self._lock = RLock()
        
        # íƒœê·¸ ì¸ë±ìŠ¤
        self._tag_index: Dict[str, Set[str]] = {}
        
        # í†µê³„
        self._stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "size_evictions": 0,
            "ttl_evictions": 0
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‘ì—…
        self._cleanup_task = None
        self._shutdown = False
        
        # ì˜êµ¬ ì €ì¥ ì´ˆê¸°í™”
        if self.persistent:
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            self._load_persistent_cache()
        
        # ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‹œì‘
        self._start_cleanup_task()
        
        logger.info(f"âœ… CacheManager ì´ˆê¸°í™”: ìµœëŒ€ {max_size_mb}MB, TTL {default_ttl}ì´ˆ")
    
    async def get(self, key: str) -> Optional[Any]:
        """
        ìºì‹œì—ì„œ ê°’ ì¡°íšŒ
        
        Args:
            key: ìºì‹œ í‚¤
            
        Returns:
            ìºì‹œëœ ê°’ ë˜ëŠ” None
        """
        with self._lock:
            cache_key = self._normalize_key(key)
            
            if cache_key not in self._cache:
                self._stats["misses"] += 1
                
                # ì˜êµ¬ ì €ì¥ì—ì„œ ë³µì› ì‹œë„
                if self.persistent:
                    value = await self._load_from_persistent(cache_key)
                    if value is not None:
                        # ì„ì‹œë¡œ ë©”ëª¨ë¦¬ì— ë¡œë“œ
                        await self.set(key, value, ttl=self.default_ttl)
                        return value
                
                return None
            
            entry = self._cache[cache_key]
            
            # ë§Œë£Œ í™•ì¸
            if entry.is_expired():
                await self._remove_entry(cache_key)
                self._stats["misses"] += 1
                self._stats["ttl_evictions"] += 1
                return None
            
            # ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸
            entry.update_access()
            
            # LRU ìˆœì„œ ê°±ì‹  (ê°€ì¥ ìµœê·¼ ì‚¬ìš©ìœ¼ë¡œ ì´ë™)
            self._cache.move_to_end(cache_key)
            
            self._stats["hits"] += 1
            return entry.value
    
    async def set(self, 
                  key: str, 
                  value: Any, 
                  ttl: Optional[int] = None,
                  tags: Optional[Set[str]] = None) -> bool:
        """
        ìºì‹œì— ê°’ ì €ì¥
        
        Args:
            key: ìºì‹œ í‚¤
            value: ì €ì¥í•  ê°’
            ttl: TTL (ì´ˆ, Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
            tags: íƒœê·¸ ì§‘í•©
            
        Returns:
            ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with self._lock:
                cache_key = self._normalize_key(key)
                current_time = time.time()
                
                # ê¸°ì¡´ ì—”íŠ¸ë¦¬ ì œê±° (í¬ê¸° ê³„ì‚°ì„ ìœ„í•´)
                if cache_key in self._cache:
                    await self._remove_entry(cache_key)
                
                # í¬ê¸° ê³„ì‚°
                size_bytes = self._calculate_size(value)
                
                # í¬ê¸° ì œí•œ í™•ì¸
                if size_bytes > self.max_size_bytes:
                    logger.warning(f"âš ï¸ ìºì‹œ ì•„ì´í…œì´ ë„ˆë¬´ í¼: {size_bytes} bytes")
                    return False
                
                # ê³µê°„ í™•ë³´
                await self._ensure_space(size_bytes)
                
                # ìºì‹œ ì—”íŠ¸ë¦¬ ìƒì„±
                entry = CacheEntry(
                    key=cache_key,
                    value=value,
                    created_at=current_time,
                    accessed_at=current_time,
                    ttl=ttl or self.default_ttl,
                    tags=tags or set(),
                    size_bytes=size_bytes,
                    access_count=1
                )
                
                # ìºì‹œì— ì¶”ê°€
                self._cache[cache_key] = entry
                self._current_size += size_bytes
                
                # íƒœê·¸ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
                for tag in entry.tags:
                    if tag not in self._tag_index:
                        self._tag_index[tag] = set()
                    self._tag_index[tag].add(cache_key)
                
                # ì˜êµ¬ ì €ì¥
                if self.persistent:
                    await self._save_to_persistent(cache_key, value)
                
                logger.debug(f"âœ… ìºì‹œ ì €ì¥: {key} ({size_bytes} bytes)")
                return True
                
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def remove(self, key: str) -> bool:
        """
        ìºì‹œì—ì„œ í‚¤ ì œê±°
        
        Args:
            key: ì œê±°í•  í‚¤
            
        Returns:
            ì œê±° ì„±ê³µ ì—¬ë¶€
        """
        with self._lock:
            cache_key = self._normalize_key(key)
            
            if cache_key in self._cache:
                await self._remove_entry(cache_key)
                return True
            
            return False
    
    async def remove_by_tags(self, tags: Set[str]) -> int:
        """
        íƒœê·¸ë¡œ ìºì‹œ í•­ëª©ë“¤ ì œê±°
        
        Args:
            tags: ì œê±°í•  íƒœê·¸ë“¤
            
        Returns:
            ì œê±°ëœ í•­ëª© ìˆ˜
        """
        removed_count = 0
        
        with self._lock:
            keys_to_remove = set()
            
            for tag in tags:
                if tag in self._tag_index:
                    keys_to_remove.update(self._tag_index[tag])
            
            for cache_key in keys_to_remove:
                if cache_key in self._cache:
                    await self._remove_entry(cache_key)
                    removed_count += 1
        
        logger.info(f"âœ… íƒœê·¸ ê¸°ë°˜ ì œê±°: {removed_count}ê°œ í•­ëª©")
        return removed_count
    
    async def clear(self):
        """ìºì‹œ ì „ì²´ ì •ë¦¬"""
        with self._lock:
            self._cache.clear()
            self._tag_index.clear()
            self._current_size = 0
            
            # ì˜êµ¬ ì €ì¥ ì •ë¦¬
            if self.persistent and self.cache_dir.exists():
                import shutil
                shutil.rmtree(self.cache_dir)
                self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info("âœ… ìºì‹œ ì „ì²´ ì •ë¦¬ ì™„ë£Œ")
    
    async def cleanup_expired(self) -> int:
        """ë§Œë£Œëœ í•­ëª©ë“¤ ì •ë¦¬"""
        removed_count = 0
        current_time = time.time()
        
        with self._lock:
            expired_keys = []
            
            for cache_key, entry in self._cache.items():
                if entry.is_expired():
                    expired_keys.append(cache_key)
            
            for cache_key in expired_keys:
                await self._remove_entry(cache_key)
                removed_count += 1
                self._stats["ttl_evictions"] += 1
        
        if removed_count > 0:
            logger.info(f"âœ… ë§Œë£Œ í•­ëª© ì •ë¦¬: {removed_count}ê°œ")
        
        return removed_count
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        with self._lock:
            total_requests = self._stats["hits"] + self._stats["misses"]
            hit_rate = self._stats["hits"] / total_requests if total_requests > 0 else 0
            
            return {
                "size_mb": self._current_size / (1024 * 1024),
                "max_size_mb": self.max_size_bytes / (1024 * 1024),
                "utilization": self._current_size / self.max_size_bytes,
                "entry_count": len(self._cache),
                "hit_rate": hit_rate,
                "stats": self._stats.copy(),
                "tag_count": len(self._tag_index)
            }
    
    async def _ensure_space(self, required_bytes: int):
        """í•„ìš”í•œ ê³µê°„ í™•ë³´ (LRU ë°©ì‹)"""
        while self._current_size + required_bytes > self.max_size_bytes and self._cache:
            # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±° (LRU)
            oldest_key = next(iter(self._cache))
            await self._remove_entry(oldest_key)
            self._stats["evictions"] += 1
            self._stats["size_evictions"] += 1
    
    async def _remove_entry(self, cache_key: str):
        """ìºì‹œ ì—”íŠ¸ë¦¬ ì œê±°"""
        if cache_key not in self._cache:
            return
        
        entry = self._cache[cache_key]
        
        # í¬ê¸° ì—…ë°ì´íŠ¸
        self._current_size -= entry.size_bytes
        
        # íƒœê·¸ ì¸ë±ìŠ¤ì—ì„œ ì œê±°
        for tag in entry.tags:
            if tag in self._tag_index:
                self._tag_index[tag].discard(cache_key)
                if not self._tag_index[tag]:
                    del self._tag_index[tag]
        
        # ìºì‹œì—ì„œ ì œê±°
        del self._cache[cache_key]
        
        # ì˜êµ¬ ì €ì¥ì—ì„œ ì œê±°
        if self.persistent:
            await self._remove_from_persistent(cache_key)
    
    def _normalize_key(self, key: str) -> str:
        """í‚¤ ì •ê·œí™”"""
        # í•´ì‹œë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ ê¸¸ì´ ì œí•œ ë° íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬
        return hashlib.md5(key.encode()).hexdigest()
    
    def _calculate_size(self, value: Any) -> int:
        """ê°ì²´ í¬ê¸° ê³„ì‚°"""
        try:
            # pickle ì§ë ¬í™” í¬ê¸°ë¡œ ì¶”ì •
            return len(pickle.dumps(value))
        except:
            # ì¶”ì • ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ê°’
            return 1024
    
    async def _save_to_persistent(self, cache_key: str, value: Any):
        """ì˜êµ¬ ì €ì¥ì— ì €ì¥"""
        if not self.persistent:
            return
        
        try:
            file_path = self.cache_dir / f"{cache_key}.cache"
            with open(file_path, 'wb') as f:
                pickle.dump(value, f)
        except Exception as e:
            logger.error(f"âŒ ì˜êµ¬ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _load_from_persistent(self, cache_key: str) -> Optional[Any]:
        """ì˜êµ¬ ì €ì¥ì—ì„œ ë¡œë“œ"""
        if not self.persistent:
            return None
        
        try:
            file_path = self.cache_dir / f"{cache_key}.cache"
            if file_path.exists():
                with open(file_path, 'rb') as f:
                    return pickle.load(f)
        except Exception as e:
            logger.error(f"âŒ ì˜êµ¬ ì €ì¥ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _remove_from_persistent(self, cache_key: str):
        """ì˜êµ¬ ì €ì¥ì—ì„œ ì œê±°"""
        if not self.persistent:
            return
        
        try:
            file_path = self.cache_dir / f"{cache_key}.cache"
            if file_path.exists():
                file_path.unlink()
        except Exception as e:
            logger.error(f"âŒ ì˜êµ¬ ì €ì¥ ì œê±° ì‹¤íŒ¨: {e}")
    
    def _load_persistent_cache(self):
        """ì˜êµ¬ ì €ì¥ëœ ìºì‹œ ë¡œë“œ"""
        if not self.persistent or not self.cache_dir.exists():
            return
        
        try:
            cache_files = list(self.cache_dir.glob("*.cache"))
            logger.info(f"ğŸ”„ ì˜êµ¬ ìºì‹œ ë¡œë“œ ì¤‘: {len(cache_files)}ê°œ íŒŒì¼")
            
            # ì˜êµ¬ ì €ì¥ì€ í•„ìš”ì‹œì—ë§Œ ë©”ëª¨ë¦¬ë¡œ ë¡œë“œí•˜ë¯€ë¡œ
            # ì—¬ê¸°ì„œëŠ” íŒŒì¼ ì¡´ì¬ë§Œ í™•ì¸
            
        except Exception as e:
            logger.error(f"âŒ ì˜êµ¬ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def _start_cleanup_task(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì‘ì—… ì‹œì‘"""
        async def cleanup_loop():
            while not self._shutdown:
                try:
                    await asyncio.sleep(self.cleanup_interval)
                    if not self._shutdown:
                        await self.cleanup_expired()
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"âŒ ë°±ê·¸ë¼ìš´ë“œ ì •ë¦¬ ì˜¤ë¥˜: {e}")
        
        try:
            loop = asyncio.get_event_loop()
            self._cleanup_task = loop.create_task(cleanup_loop())
        except RuntimeError:
            # ì´ë²¤íŠ¸ ë£¨í”„ê°€ ì—†ëŠ” ê²½ìš° (í…ŒìŠ¤íŠ¸ í™˜ê²½ ë“±)
            pass
    
    async def shutdown(self):
        """ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self._shutdown = True
        
        if self._cleanup_task:
            self._cleanup_task.cancel()
            try:
                await self._cleanup_task
            except asyncio.CancelledError:
                pass
        
        logger.info("âœ… CacheManager ì¢…ë£Œ ì™„ë£Œ")
    
    def __del__(self):
        """ì†Œë©¸ì"""
        if not self._shutdown:
            try:
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    loop.create_task(self.shutdown())
            except:
                pass


# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ (ì‹±ê¸€í†¤ íŒ¨í„´)
_global_cache_manager: Optional[CacheManager] = None


def get_cache_manager(**kwargs) -> CacheManager:
    """
    ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜
    
    Args:
        **kwargs: CacheManager ì´ˆê¸°í™” íŒŒë¼ë¯¸í„°
        
    Returns:
        CacheManager ì¸ìŠ¤í„´ìŠ¤
    """
    global _global_cache_manager
    
    if _global_cache_manager is None:
        _global_cache_manager = CacheManager(**kwargs)
    
    return _global_cache_manager


async def clear_global_cache():
    """ì „ì—­ ìºì‹œ ì •ë¦¬"""
    global _global_cache_manager
    
    if _global_cache_manager:
        await _global_cache_manager.clear()


async def shutdown_global_cache():
    """ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ"""
    global _global_cache_manager
    
    if _global_cache_manager:
        await _global_cache_manager.shutdown()
        _global_cache_manager = None 