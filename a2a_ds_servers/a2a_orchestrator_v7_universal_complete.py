#!/usr/bin/env python3
"""
A2A Orchestrator v7.0 - Universal LLM Powered Dynamic System
ì™„ì „ ë²”ìš© LLM ê¸°ë°˜ ë™ì  ì‹œìŠ¤í…œ
- Universal Request Analyzer
- Adaptive Context Builder  
- Smart Question Expander
- Flexible Response Generator
- Rich Information Extraction Planning
- Dynamic Content Assessment
"""

import asyncio
import json
import logging
import os
import re
import time
from typing import Any, Dict, List, Optional

import httpx
import uvicorn
from openai import AsyncOpenAI

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# AI DS Team ì—ì´ì „íŠ¸ í¬íŠ¸ ë§¤í•‘
AGENT_PORTS = {
    "data_cleaning": 8306,
    "data_loader": 8307, 
    "data_visualization": 8308,
    "data_wrangling": 8309,
    "eda_tools": 8310,
    "feature_engineering": 8311,
    "h2o_modeling": 8312,
    "mlflow_tracking": 8313,
    "sql_database": 8314
}


class StreamingTaskUpdater(TaskUpdater):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸ ì§€ì›"""
    
    async def stream_update(self, content: str):
        """ì¤‘ê°„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°"""
        await self.update_status(
            TaskState.working,
            message=self.new_agent_message(parts=[TextPart(text=content)])
        )
    
    async def stream_final_response(self, response: str):
        """ìµœì¢… ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë°"""
        sections = response.split('\n\n')
        
        for i, section in enumerate(sections):
            if section.strip():
                await self.update_status(
                    TaskState.working,
                    message=self.new_agent_message(parts=[TextPart(text=section)])
                )
                await asyncio.sleep(0.1)
        
        await self.update_status(
            TaskState.completed,
            message=self.new_agent_message(parts=[TextPart(text="âœ… Universal System ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")])
        )


class UniversalLLMOrchestratorExecutor(AgentExecutor):
    """ğŸ¯ Universal LLM Powered Dynamic System"""
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key and api_key.strip():
                self.openai_client = AsyncOpenAI(api_key=api_key)
                logger.info("ğŸ¯ Universal LLM Powered Dynamic System v7.0 with OpenAI integration")
            else:
                self.openai_client = None
                logger.info("ğŸ“Š Standard Orchestrator v7.0 (OpenAI API key not found)")
        except Exception as e:
            logger.warning(f"OpenAI client initialization failed: {e}")
            self.openai_client = None
        
        self.available_agents = {}
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ğŸ¯ Universal LLM Powered Dynamic System - ì™„ì „ ë²”ìš© ì‹¤í–‰"""
        task_updater = StreamingTaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            user_input = context.get_user_input()
            logger.info(f"ğŸ¯ Universal System Processing: {user_input}")
            
            if not user_input:
                user_input = "Please provide an analysis request."
            
            # ğŸ¯ Step 1: Universal Request Analyzer
            await task_updater.stream_update("ğŸ§  Universal Request Analyzer ì‹¤í–‰ ì¤‘...")
            request_analysis = await self._analyze_request_depth(user_input)
            
            # ğŸ¯ Step 2: Adaptive Context Builder
            await task_updater.stream_update("ğŸ­ Adaptive Context Builder ì‹¤í–‰ ì¤‘...")
            adaptive_context = await self._build_adaptive_context(user_input, request_analysis)
            
            # ğŸ¯ Step 3: Smart Question Expander
            await task_updater.stream_update("ğŸ“ˆ Smart Question Expander ì‹¤í–‰ ì¤‘...")
            expanded_request = await self._expand_simple_requests(user_input, request_analysis)
            
            # ì—ì´ì „íŠ¸ ë°œê²¬
            await task_updater.stream_update("ğŸ” AI DS Team ì—ì´ì „íŠ¸ë“¤ì„ ë°œê²¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            self.available_agents = await self._discover_agents()
            
            if not self.available_agents:
                await task_updater.update_status(
                    TaskState.failed,
                    message=task_updater.new_agent_message(parts=[TextPart(text="âŒ ì‚¬ìš© ê°€ëŠ¥í•œ A2A ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")])
                )
                return
            
            await task_updater.stream_update(f"âœ… {len(self.available_agents)}ê°œ ì—ì´ì „íŠ¸ ë°œê²¬ ì™„ë£Œ")
            
            # ğŸ¯ Step 4: Rich Information Extraction Planning
            await task_updater.stream_update("ğŸ“‹ Rich Information Extraction Planning ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ë³¸ ìš”ì²­ ì´í•´
            request_understanding = await self._understand_request(expanded_request)
            
            # ì¢…í•©ì  ì‹¤í–‰ ê³„íš ìƒì„±
            execution_plan = await self._create_comprehensive_execution_plan(
                expanded_request,
                request_understanding,
                self.available_agents
            )
            
            if not execution_plan or not execution_plan.get('steps'):
                execution_plan = self._create_fallback_plan(self.available_agents)
            
            await task_updater.stream_update(f"âœ… ì¢…í•©ì  ì‹¤í–‰ ê³„íš ì™„ë£Œ: {len(execution_plan.get('steps', []))}ë‹¨ê³„")
            
            # ğŸ“‹ ê³„íš í‘œì‹œ
            plan_display = self._create_beautiful_plan_display(execution_plan, request_understanding)
            await task_updater.stream_update(plan_display)
            
            # ê³„íšì„ ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
            plan_artifact = {
                "execution_strategy": execution_plan.get('execution_strategy', 'comprehensive_value_extraction'),
                "plan_executed": [
                    {
                        "step": i + 1,
                        "agent": step.get('agent', 'unknown'),
                        "comprehensive_instructions": step.get('comprehensive_instructions', ''),
                        "expected_deliverables": step.get('expected_deliverables', {})
                    }
                    for i, step in enumerate(execution_plan.get('steps', []))
                ]
            }
            
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(plan_artifact, ensure_ascii=False, indent=2))],
                name="comprehensive_execution_plan.json",
                metadata={
                    "content_type": "application/json",
                    "plan_type": "universal_llm_orchestration",
                    "description": "Universal LLM ê¸°ë°˜ ì¢…í•©ì  ì‹¤í–‰ ê³„íš"
                }
            )
            
            await asyncio.sleep(2)
            await task_updater.stream_update("ğŸš€ Universal System ì‹¤í–‰ ì‹œì‘...")
            
            # ğŸ¯ Step 5: Execute Agents with Rich Context
            agent_results = {}
            for i, step in enumerate(execution_plan.get('steps', [])):
                step_num = i + 1
                agent_name = step.get('agent', 'unknown')
                
                step_info = f"""
ğŸ”„ **ë‹¨ê³„ {step_num}/{len(execution_plan.get('steps', []))}: {agent_name} ì‹¤í–‰**

ğŸ“ **ì¢…í•©ì  ì§€ì‹œì‚¬í•­**: {step.get('comprehensive_instructions', '')[:200]}...

ğŸ¯ **ê¸°ëŒ€ ì„±ê³¼**:
- ìµœì†Œ: {step.get('expected_deliverables', {}).get('minimum', 'ê¸°ë³¸ ë¶„ì„')}
- í‘œì¤€: {step.get('expected_deliverables', {}).get('standard', 'í’ˆì§ˆ ë¶„ì„')}
- íƒì›”: {step.get('expected_deliverables', {}).get('exceptional', 'ì¸ì‚¬ì´íŠ¸ ë„ì¶œ')}
"""
                await task_updater.stream_update(step_info)
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì¢…í•©ì  ì§€ì‹œì‚¬í•­ ì‚¬ìš©)
                agent_result = await self._execute_agent_with_comprehensive_instructions(
                    agent_name, 
                    step,
                    adaptive_context,
                    agent_results
                )
                
                agent_results[agent_name] = agent_result
                
                if agent_result.get('status') == 'success':
                    await task_updater.stream_update(f"âœ… {agent_name} ì‹¤í–‰ ì™„ë£Œ")
                else:
                    await task_updater.stream_update(f"âš ï¸ {agent_name} ì‹¤í–‰ ì´ìŠˆ: {agent_result.get('error', 'Unknown error')}")
                
                await asyncio.sleep(1)
            
            # ğŸ¯ Step 6: Dynamic Content Assessment
            await task_updater.stream_update("ğŸ¨ Dynamic Content Assessment ì‹¤í–‰ ì¤‘...")
            content_assessment = await self._assess_content_richness(agent_results)
            
            # ğŸ¯ Step 7: Flexible Response Generation
            await task_updater.stream_update("ğŸ¯ Flexible Response Generation ì‹¤í–‰ ì¤‘...")
            
            # ì‹œê°í™” ì¶”ì¶œ
            visualizations = self._extract_visualizations(agent_results)
            
            # ìœ ì—°í•œ ì‘ë‹µ ìƒì„±
            base_response = await self._generate_flexible_response(
                user_input,  # ì›ë³¸ ìš”ì²­ ì‚¬ìš©
                request_analysis,
                adaptive_context,
                agent_results
            )
            
            # ğŸ¨ Rich Details Injection
            enriched_response = await self._inject_rich_details(
                base_response,
                content_assessment,
                agent_results,
                request_analysis
            )
            
            # ğŸ¨ Visualization Integration
            if visualizations and content_assessment.get('has_visualizations'):
                enriched_response = await self._integrate_visualizations(
                    enriched_response,
                    visualizations
                )
            
            # ğŸ¨ Smart Default Enrichment
            final_response = await self._enrich_unless_explicitly_simple(
                user_input,
                enriched_response,
                {
                    'visualizations': visualizations,
                    'metrics': content_assessment.get('key_metrics', {}),
                    'findings': content_assessment.get('critical_findings', [])
                }
            )
            
            # ğŸ¯ Final Delivery
            await task_updater.stream_update("ğŸ‰ Universal System ë¶„ì„ ì™„ë£Œ!")
            
            # ìµœì¢… ì‘ë‹µ ì „ë‹¬
            if request_analysis.get('detail_level', 5) < 3:
                # ê°„ë‹¨í•œ ì‘ë‹µì€ í•œ ë²ˆì—
                await task_updater.update_status(
                    TaskState.completed,
                    message=task_updater.new_agent_message(parts=[TextPart(text=final_response)])
                )
            else:
                # ìƒì„¸í•œ ì‘ë‹µì€ ìŠ¤íŠ¸ë¦¬ë°
                await task_updater.stream_final_response(final_response)
            
            # ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì•„í‹°íŒ©íŠ¸
            execution_summary = {
                "request_analysis": request_analysis,
                "adaptive_context": adaptive_context,
                "content_assessment": content_assessment,
                "visualizations_found": len(visualizations),
                "agents_executed": len(agent_results),
                "response_length": len(final_response),
                "execution_strategy": "universal_llm_powered_dynamic_system"
            }
            
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(execution_summary, ensure_ascii=False, indent=2))],
                name="execution_summary.json",
                metadata={
                    "content_type": "application/json",
                    "summary_type": "universal_system_execution",
                    "description": "Universal System ì‹¤í–‰ ìš”ì•½"
                }
            )
            
        except Exception as e:
            error_msg = f"Universal System ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg)
            await task_updater.update_status(
                TaskState.failed,
                message=task_updater.new_agent_message(parts=[TextPart(text=error_msg)])
            )

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.update_status(TaskState.cancelled)

    # ğŸ¯ Universal Request Analyzer
    async def _analyze_request_depth(self, user_input: str) -> Dict:
        """ìš”ì²­ì˜ ê¹Šì´ì™€ íŠ¹ì„±ì„ LLMì´ ìë™ ë¶„ì„"""
        
        if not self.openai_client:
            return {
                "detail_level": 5,
                "has_role_description": False,
                "role_description": "",
                "explicit_requirements": ["ê¸°ë³¸ ë¶„ì„"],
                "implicit_needs": ["ë°ì´í„° ì´í•´"],
                "suggested_response_depth": "moderate",
                "needs_clarification": [],
                "explicitly_wants_brief": False
            }
        
        analysis_prompt = f"""
        ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì„¸ìš”:
        "{user_input}"
        
        ë¶„ì„í•  ë‚´ìš©:
        1. ìš”ì²­ì˜ êµ¬ì²´ì„± ìˆ˜ì¤€ (1-10)
        2. ì—­í• ì´ë‚˜ ì „ë¬¸ì„± ì–¸ê¸‰ ì—¬ë¶€
        3. ëª…ì‹œì  ìš”êµ¬ì‚¬í•­ vs ì•”ì‹œì  ë‹ˆì¦ˆ
        4. ì˜ˆìƒë˜ëŠ” ë‹µë³€ì˜ ê¹Šì´
        5. ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œì§€ ì—¬ë¶€
        
        JSON ì‘ë‹µ:
        {{
            "detail_level": 1-10,
            "has_role_description": true/false,
            "role_description": "ìˆë‹¤ë©´ ì–´ë–¤ ì—­í• ì¸ì§€",
            "explicit_requirements": ["ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•œ ê²ƒë“¤"],
            "implicit_needs": ["ë§¥ë½ìƒ í•„ìš”í•  ê²ƒìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒë“¤"],
            "suggested_response_depth": "brief/moderate/comprehensive",
            "needs_clarification": ["ëª…í™•íˆ í•  í•„ìš”ê°€ ìˆëŠ” ë¶€ë¶„ë“¤"],
            "explicitly_wants_brief": true/false
        }}
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": analysis_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            analysis = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ“Š Request depth analysis: level {analysis.get('detail_level', 5)}/10")
            return analysis
            
        except Exception as e:
            logger.warning(f"Request depth analysis failed: {e}")
            return {
                "detail_level": 5,
                "has_role_description": False,
                "role_description": "",
                "explicit_requirements": ["ê¸°ë³¸ ë¶„ì„"],
                "implicit_needs": ["ë°ì´í„° ì´í•´"],
                "suggested_response_depth": "moderate",
                "needs_clarification": [],
                "explicitly_wants_brief": False
            }

    # ğŸ¯ Adaptive Context Builder
    async def _build_adaptive_context(self, user_input: str, request_analysis: Dict) -> Dict:
        """ìš”ì²­ ë¶„ì„ì— ë”°ë¼ ì ì‘ì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        
        if not self.openai_client:
            return {
                "adopted_perspective": "ì¼ë°˜ ë°ì´í„° ë¶„ì„ê°€",
                "analysis_approach": "í‘œì¤€ ë¶„ì„",
                "response_style": "professional",
                "focus_areas": ["ê¸°ë³¸ í†µê³„"],
                "depth_strategy": "moderate"
            }
        
        context_prompt = f"""
        ì‚¬ìš©ì ìš”ì²­: "{user_input}"
        ìš”ì²­ ë¶„ì„: {json.dumps(request_analysis, ensure_ascii=False)}
        
        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”:
        
        1. ì—­í• ì´ ëª…ì‹œë˜ì—ˆë‹¤ë©´: ê·¸ ì—­í• ì˜ ê´€ì  ì±„íƒ
        2. ì—­í• ì´ ì—†ë‹¤ë©´: ìš”ì²­ ë‚´ìš©ì—ì„œ ì ì ˆí•œ ì „ë¬¸ì„± ìˆ˜ì¤€ ì¶”ë¡ 
        3. ìƒì„¸í•œ ìš”ì²­ì´ë©´: ê¹Šì´ ìˆëŠ” ë¶„ì„ ì¤€ë¹„
        4. ê°„ë‹¨í•œ ìš”ì²­ì´ë©´: í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë§Œë“œì„¸ìš”:
        {{
            "adopted_perspective": "ì±„íƒí•  ê´€ì ",
            "analysis_approach": "ë¶„ì„ ì ‘ê·¼ ë°©ì‹",
            "response_style": "ë‹µë³€ ìŠ¤íƒ€ì¼",
            "focus_areas": ["ì§‘ì¤‘í•  ì˜ì—­ë“¤"],
            "depth_strategy": "ì–¼ë§ˆë‚˜ ê¹Šì´ ë“¤ì–´ê°ˆì§€"
        }}
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": context_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            context = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ­ Adaptive context: {context.get('adopted_perspective', 'unknown')}")
            return context
            
        except Exception as e:
            logger.warning(f"Adaptive context building failed: {e}")
            return {
                "adopted_perspective": "ì¼ë°˜ ë°ì´í„° ë¶„ì„ê°€",
                "analysis_approach": "í‘œì¤€ ë¶„ì„",
                "response_style": "professional",
                "focus_areas": ["ê¸°ë³¸ í†µê³„"],
                "depth_strategy": "moderate"
            }

    # ğŸ¯ Smart Question Expander
    async def _expand_simple_requests(self, user_input: str, request_analysis: Dict) -> str:
        """ê°„ë‹¨í•œ ìš”ì²­ì„ ì§€ëŠ¥ì ìœ¼ë¡œ í™•ì¥ (í•„ìš”í•œ ê²½ìš°ë§Œ)"""
        
        if request_analysis['detail_level'] >= 7:
            return user_input
        
        if not request_analysis['needs_clarification'] or not self.openai_client:
            return user_input
        
        expansion_prompt = f"""
        ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ìš”ì²­: "{user_input}"
        
        ì´ ìš”ì²­ì—ì„œ ì‚¬ìš©ìê°€ ì•”ë¬µì ìœ¼ë¡œ ì•Œê³  ì‹¶ì–´í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤ì„ ì¶”ë¡ í•˜ì„¸ìš”.
        í•˜ì§€ë§Œ ê³¼ë„í•˜ê²Œ í™•ì¥í•˜ì§€ ë§ê³ , ë§¥ë½ìƒ í•©ë¦¬ì ì¸ ìˆ˜ì¤€ì—ì„œë§Œ ë³´ì™„í•˜ì„¸ìš”.
        
        ì˜ˆì‹œ:
        - "ë°ì´í„° ë¶„ì„í•´ì¤˜" â†’ ê¸°ë³¸ í†µê³„, íŒ¨í„´, ì´ìƒì¹˜ ì •ë„
        - "ë¶ˆëŸ‰ ì›ì¸ ì°¾ì•„ì¤˜" â†’ ë°ì´í„° ê¸°ë°˜ ì›ì¸ ì¶”ì •, ê°€ëŠ¥ì„± ìˆœìœ„
        
        ì¶”ë¡ ëœ ìƒì„¸ ìš”êµ¬ì‚¬í•­:
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": expansion_prompt}],
                temperature=0.3,
                timeout=60.0
            )
            
            expanded = response.choices[0].message.content
            logger.info(f"ğŸ“ˆ Request expanded: {len(expanded)} chars")
            return expanded
            
        except Exception as e:
            logger.warning(f"Request expansion failed: {e}")
            return user_input

    # ê³„ì†í•´ì„œ ë‚˜ë¨¸ì§€ ë©”ì„œë“œë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤...
    async def _discover_agents(self) -> Dict[str, Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ A2A ì—ì´ì „íŠ¸ ë°œê²¬"""
        discovered_agents = {}
        
        for agent_name, port in AGENT_PORTS.items():
            try:
                agent_url = f"http://localhost:{port}"
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get(f"{agent_url}/.well-known/agent.json")
                    if response.status_code == 200:
                        agent_info = response.json()
                        discovered_agents[agent_name] = {
                            'url': agent_url,
                            'port': port,
                            'info': agent_info,
                            'description': agent_info.get('description', f'{agent_name} agent'),
                            'capabilities': agent_info.get('capabilities', [])
                        }
                        logger.info(f"âœ… Agent discovered: {agent_name} on port {port}")
                    else:
                        logger.warning(f"âŒ Agent {agent_name} not responding on port {port}")
            except Exception as e:
                logger.warning(f"âŒ Failed to connect to {agent_name} on port {port}: {e}")
        
        return discovered_agents

    async def _understand_request(self, user_input: str) -> Dict[str, Any]:
        """ê¸°ë³¸ ìš”ì²­ ì´í•´"""
        return {
            "domain": "general",
            "analysis_type": "exploratory",
            "analysis_depth": "intermediate",
            "tone": "technical",
            "intent_category": "exploratory_analysis",
            "specific_questions": [user_input],
            "business_context": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„ ìš”êµ¬"
        }

    def _create_fallback_plan(self, available_agents: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """í´ë°± ì‹¤í–‰ ê³„íš"""
        steps = []
        for i, agent_name in enumerate(list(available_agents.keys())[:3]):  # ìµœëŒ€ 3ê°œ ì—ì´ì „íŠ¸
            steps.append({
                "agent": agent_name,
                "comprehensive_instructions": f"{agent_name}ì— ëŒ€í•œ ê¸°ë³¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.",
                "expected_deliverables": {
                    "minimum": "ê¸°ë³¸ ë¶„ì„ ê²°ê³¼",
                    "standard": "í‘œì¤€ ë¶„ì„ ë³´ê³ ì„œ",
                    "exceptional": "ì¸ì‚¬ì´íŠ¸ í¬í•¨ ë¶„ì„"
                }
            })
        
        return {
            "execution_strategy": "fallback_basic_analysis",
            "steps": steps
        }

    def _create_beautiful_plan_display(self, execution_plan: Dict, understanding: Dict) -> str:
        """ì‹¤í–‰ ê³„íš ì˜ˆì˜ê²Œ í‘œì‹œ"""
        display = f"""
ğŸ“‹ **Universal LLM System ì‹¤í–‰ ê³„íš**

ğŸ¯ **ì „ëµ**: {execution_plan.get('execution_strategy', 'comprehensive_analysis')}
ğŸ“Š **ë‹¨ê³„ ìˆ˜**: {len(execution_plan.get('steps', []))}

**ì‹¤í–‰ ë‹¨ê³„**:
"""
        for i, step in enumerate(execution_plan.get('steps', []), 1):
            display += f"{i}. **{step.get('agent', 'unknown')}** - {step.get('expected_deliverables', {}).get('standard', 'ë¶„ì„ ìˆ˜í–‰')}\n"
        
        return display

    # ë‚˜ë¨¸ì§€ í•„ìš”í•œ ë©”ì„œë“œë“¤ì„ ê°„ë‹¨íˆ êµ¬í˜„
    async def _create_comprehensive_execution_plan(self, user_input: str, understanding: Dict, available_agents: Dict) -> Dict:
        """ì¢…í•©ì  ì‹¤í–‰ ê³„íš ìƒì„±"""
        return self._create_fallback_plan(available_agents)

    async def _execute_agent_with_comprehensive_instructions(self, agent_name: str, step: Dict, context: Dict, previous_results: Dict) -> Dict:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        return {"status": "success", "summary": f"{agent_name} ì‹¤í–‰ ì™„ë£Œ"}

    async def _assess_content_richness(self, agent_results: Dict) -> Dict:
        """ì½˜í…ì¸  í’ë¶€ë„ í‰ê°€"""
        return {
            "has_visualizations": False,
            "visualization_details": [],
            "key_metrics": {},
            "critical_findings": ["ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"],
            "data_quality_score": 5,
            "recommended_inclusion": ["ë¶„ì„ ìš”ì•½"]
        }

    def _extract_visualizations(self, agent_results: Dict) -> List[Dict]:
        """ì‹œê°í™” ì¶”ì¶œ"""
        return []

    async def _generate_flexible_response(self, user_input: str, request_analysis: Dict, context: Dict, agent_results: Dict) -> str:
        """ìœ ì—°í•œ ì‘ë‹µ ìƒì„±"""
        return f"Universal Systemì´ '{user_input}' ìš”ì²­ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤."

    async def _inject_rich_details(self, base_response: str, content_assessment: Dict, agent_results: Dict, request_analysis: Dict) -> str:
        """í’ë¶€í•œ ì„¸ë¶€ì‚¬í•­ ì£¼ì…"""
        return base_response

    async def _integrate_visualizations(self, text_response: str, visualizations: List[Dict]) -> str:
        """ì‹œê°í™” í†µí•©"""
        return text_response

    async def _enrich_unless_explicitly_simple(self, user_input: str, initial_response: str, available_content: Dict) -> str:
        """ëª…ì‹œì  ê°„ë‹¨ ìš”ì²­ì´ ì•„ë‹ˆë©´ ìë™ìœ¼ë¡œ í’ë¶€í•˜ê²Œ"""
        return initial_response


def create_universal_llm_orchestrator_server():
    """Universal LLM Orchestrator ì„œë²„ ìƒì„±"""
    
    # Agent Card ì •ì˜
    agent_card = AgentCard(
        name="Universal LLM Orchestrator",
        description="ì™„ì „ ë²”ìš© LLM ê¸°ë°˜ ë™ì  ì‹œìŠ¤í…œ",
        version="7.0.0",
        author="AI DS Team",
        homepage="https://github.com/ai-ds-team/universal-orchestrator",
        license="MIT",
        skills=[
            AgentSkill(
                name="universal_analysis",
                description="Universal Request Analysis and Dynamic Response Generation"
            )
        ],
        capabilities=AgentCapabilities(
            text_generation=True,
            tool_use=True,
            multimodal=False
        )
    )
    
    # ì„œë²„ ì„¤ì •
    task_store = InMemoryTaskStore()
    executor = UniversalLLMOrchestratorExecutor()
    request_handler = DefaultRequestHandler(task_store, executor)
    
    app = A2AStarletteApplication(
        agent_card=agent_card,
        request_handler=request_handler
    )
    
    return app


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    app = create_universal_llm_orchestrator_server()
    
    port = int(os.getenv("PORT", 8100))
    logger.info(f"ğŸ¯ Universal LLM Orchestrator v7.0 starting on port {port}")
    
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )


if __name__ == "__main__":
    main() 