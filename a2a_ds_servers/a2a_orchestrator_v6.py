#!/usr/bin/env python3
"""
A2A Orchestrator v6.0 - LLM Powered Dynamic Context-Aware Orchestrator
LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
"""

import asyncio
import json
import logging
import os
import re
import time
from typing import Any, Dict, List, Optional

import httpx
import uvicorn
from openai import AsyncOpenAI

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# AI DS Team ì—ì´ì „íŠ¸ í¬íŠ¸ ë§¤í•‘
AGENT_PORTS = {
    "data_cleaning": 8306,
    "data_loader": 8307, 
    "data_visualization": 8308,
    "data_wrangling": 8309,
    "eda_tools": 8310,
    "feature_engineering": 8311,
    "h2o_modeling": 8312,
    "mlflow_tracking": 8313,
    "sql_database": 8314
}


class StreamingTaskUpdater(TaskUpdater):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸ ì§€ì›"""
    
    async def stream_update(self, content: str):
        """ì¤‘ê°„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°"""
        await self.update_status(
            TaskState.working,
            message=self.new_agent_message(parts=[TextPart(text=content)])
        )
    
    async def stream_final_response(self, response: str):
        """ìµœì¢… ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë°"""
        # Markdown ì„¹ì…˜ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        sections = response.split('\n\n')
        
        for i, section in enumerate(sections):
            if section.strip():
                await self.update_status(
                    TaskState.working,
                    message=self.new_agent_message(parts=[TextPart(text=section)])
                )
                await asyncio.sleep(0.1)  # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°
        
        # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        await self.update_status(
            TaskState.completed,
            message=self.new_agent_message(parts=[TextPart(text="âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")])
        )


class LLMPoweredOrchestratorExecutor(AgentExecutor):
    """LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì˜µì…˜)
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key and api_key.strip():
                self.openai_client = AsyncOpenAI(api_key=api_key)
                logger.info("ğŸ¤– LLM Powered Dynamic Orchestrator v6 with OpenAI integration")
            else:
                self.openai_client = None
                logger.info("ğŸ“Š Standard Orchestrator v6 (OpenAI API key not found)")
        except Exception as e:
            logger.warning(f"OpenAI client initialization failed: {e}")
            self.openai_client = None
        
        self.available_agents = {}
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """LLMì´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´í•´í•˜ê³  ì¡°ì •í•˜ëŠ” ì‹¤í–‰"""
        task_updater = StreamingTaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            user_input = context.get_user_input()
            logger.info(f"ğŸ“¥ Processing orchestration query: {user_input}")
            
            if not user_input:
                user_input = "Please provide an analysis request."
            
            # ì—ì´ì „íŠ¸ ë°œê²¬
            await task_updater.stream_update("ğŸ” AI DS Team ì—ì´ì „íŠ¸ë“¤ì„ ë°œê²¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            self.available_agents = await self._discover_agents()
            
            if not self.available_agents:
                await task_updater.update_status(
                    TaskState.failed,
                    message=task_updater.new_agent_message(parts=[TextPart(text="âŒ ì‚¬ìš© ê°€ëŠ¥í•œ A2A ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")])
                )
                return
            
            await task_updater.stream_update(f"âœ… {len(self.available_agents)}ê°œ ì—ì´ì „íŠ¸ ë°œê²¬ ì™„ë£Œ")
            
            # 1. ì‚¬ìš©ì ì…ë ¥ì„ LLMì´ ì™„ì „íˆ ì´í•´
            await task_updater.stream_update("ğŸ§  ì‚¬ìš©ì ìš”ì²­ì„ ì‹¬ì¸µ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            request_understanding = await self._understand_request(user_input)
            
            # ğŸ¯ NEW: ì§ˆë¬¸ì—ì„œ ë‹µë³€ êµ¬ì¡° ì¶”ì¶œ
            await task_updater.stream_update("ğŸ¯ ì§ˆë¬¸ì—ì„œ í•„ìš”í•œ ë‹µë³€ êµ¬ì¡°ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            answer_structure = await self._extract_answer_structure_from_question(user_input)
            
            # ë„ë©”ì¸ ìë™ ì ì‘
            domain_adaptation = await self._auto_adapt_to_domain(user_input)
            
            # 2. ğŸ¯ NEW: êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë™ì  ì‹¤í–‰ ê³„íš ìƒì„±
            await task_updater.stream_update("ğŸ“‹ ë‹µë³€ êµ¬ì¡°ì— ë§ì¶˜ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            execution_plan = await self._create_structure_aware_plan(
                request_understanding, 
                answer_structure,
                self.available_agents
            )
            
            if not execution_plan or not execution_plan.get('steps'):
                execution_plan = self._create_fallback_plan(self.available_agents)
            
            await task_updater.stream_update(f"âœ… {len(execution_plan.get('steps', []))}ë‹¨ê³„ ì‹¤í–‰ ê³„íš ì™„ë£Œ")
            
            # ğŸ“‹ ì˜ˆìœ ê³„íš í‘œì‹œë¥¼ ìœ„í•œ ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€
            plan_display = self._create_beautiful_plan_display(execution_plan, request_understanding)
            await task_updater.stream_update(plan_display)
            
            # ê³„íšì„ ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡ (í´ë¼ì´ì–¸íŠ¸ íŒŒì‹±ìš©)
            plan_artifact = {
                "plan_executed": [
                    {
                        "step": i + 1,
                        "agent": step.get('agent', step.get('agent_name', 'unknown')),
                        "task_description": step.get('enriched_task', step.get('purpose', '')),
                        "reasoning": step.get('purpose', ''),
                        "expected_output": step.get('expected_output', '')
                    }
                    for i, step in enumerate(execution_plan.get('steps', []))
                ]
            }
            
            # ğŸ“‹ ì‹¤í–‰ ê³„íšì„ Artifactë¡œ ë¨¼ì € ì „ì†¡
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(plan_artifact, ensure_ascii=False, indent=2))],
                name="execution_plan.json",
                metadata={
                    "content_type": "application/json",
                    "plan_type": "ai_ds_team_orchestration",
                    "description": "LLM ê¸°ë°˜ ë™ì  ì‹¤í–‰ ê³„íš"
                }
            )
            
            # ê³„íš í™•ì¸ ì‹œê°„ ì œê³µ
            await asyncio.sleep(2)
            await task_updater.stream_update("ğŸš€ ì‹¤í–‰ ê³„íšì— ë”°ë¼ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # 3. ê° ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
            agent_results = {}
            for i, step in enumerate(execution_plan.get('steps', [])):
                step_num = i + 1
                agent_name = step.get('agent', step.get('agent_name', 'unknown'))
                
                # ë‹¨ê³„ë³„ ìƒì„¸ ì •ë³´ í‘œì‹œ
                step_info = f"""
ğŸ”„ **ë‹¨ê³„ {step_num}/{len(execution_plan.get('steps', []))}: {agent_name} ì‹¤í–‰**

ğŸ“ **ì‘ì—…**: {step.get('enriched_task', step.get('purpose', ''))[:100]}...
ğŸ¯ **ëª©ì **: {step.get('purpose', '')}
"""
                await task_updater.stream_update(step_info)
                
                try:
                    # ğŸ¯ NEW: êµ¬ì¡° ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì—ì´ì „íŠ¸ ì‹¤í–‰
                    result = await self._execute_agent_with_structure_context(
                        agent_name=agent_name,
                        step=step,
                        answer_structure=answer_structure,
                        full_context=request_understanding,
                        previous_results=agent_results
                    )
                    agent_results[agent_name] = result
                    
                    # ì‹¤ì‹œê°„ í”¼ë“œë°±
                    summary = result.get('summary', 'Processing completed')
                    success_msg = f"âœ… **{agent_name} ì™„ë£Œ**: {summary}"
                    await task_updater.stream_update(success_msg)
                    
                except Exception as agent_error:
                    logger.warning(f"Agent {agent_name} execution failed: {agent_error}")
                    agent_results[agent_name] = {
                        'status': 'failed',
                        'error': str(agent_error),
                        'summary': f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(agent_error)}"
                    }
                    error_msg = f"âš ï¸ **{agent_name} ì˜¤ë¥˜**: {str(agent_error)[:100]}... (ê³„ì† ì§„í–‰)"
                    await task_updater.stream_update(error_msg)
            
            # 4. LLMì´ ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
            await task_updater.stream_update("ğŸ¯ ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            final_response = await self._synthesize_with_llm(
                original_request=user_input,
                understanding=request_understanding,
                all_results=agent_results,
                task_updater=task_updater
            )
            
            # ğŸ“Š ìµœì¢… ë‹µë³€ì„ ë¨¼ì € ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ í‘œì‹œ
            final_display = self._create_beautiful_final_display(
                final_response, 
                execution_plan, 
                agent_results, 
                request_understanding
            )
            await task_updater.stream_update(final_display)
            
            # ğŸ“Š ìµœì¢… ë‹µë³€ì„ Artifactë¡œë„ ì „ì†¡
            await task_updater.add_artifact(
                parts=[TextPart(text=final_response)],
                name="final_analysis_report.md",
                metadata={
                    "content_type": "text/markdown",
                    "report_type": "comprehensive_analysis",
                    "description": "LLM ê¸°ë°˜ ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ"
                }
            )
            
            # 5. ì™„ë£Œ ë©”ì‹œì§€ (ë” ìƒì„¸í•˜ê³  ì˜ˆì˜ê²Œ)
            completion_summary = f"""## ğŸ‰ LLM ê¸°ë°˜ ë™ì  ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì™„ë£Œ

### ğŸ“Š ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
- **ğŸ¤– ì—ì´ì „íŠ¸ ë°œê²¬**: {len(self.available_agents)}ê°œ
- **ğŸ“‹ ì‹¤í–‰ ê³„íš**: {len(execution_plan.get('steps', []))}ë‹¨ê³„
- **âœ… ì„±ê³µí•œ ì—ì´ì „íŠ¸**: {len([r for r in agent_results.values() if r.get('status') == 'success'])}ê°œ
- **âŒ ì‹¤íŒ¨í•œ ì—ì´ì „íŠ¸**: {len([r for r in agent_results.values() if r.get('status') == 'failed'])}ê°œ
- **ğŸ¢ ë„ë©”ì¸**: {request_understanding.get('domain', 'ë°ì´í„° ë¶„ì„')}
- **ğŸ“ˆ ë¶„ì„ ê¹Šì´**: {request_understanding.get('analysis_depth', 'intermediate')}

### ğŸ“‹ ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸
1. **execution_plan.json**: ë™ì  ì‹¤í–‰ ê³„íš (JSON í˜•ì‹)
2. **final_analysis_report.md**: ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ (Markdown í˜•ì‹)

### ğŸ¯ ë¶„ì„ ì™„ë£Œ
ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ìƒì„¸í•œ ê²°ê³¼ì™€ ì•„í‹°íŒ©íŠ¸ì—ì„œ ì „ì²´ ë¶„ì„ ë‚´ìš©ì„ í™•ì¸í•˜ì„¸ìš”.

---
*ğŸ¤– Powered by LLM Dynamic Context-Aware Orchestrator v6*"""
            
            await task_updater.update_status(
                TaskState.completed,
                message=task_updater.new_agent_message(parts=[TextPart(text=completion_summary)])
            )
            
        except Exception as e:
            logger.error(f"Error in LLM Powered Orchestrator: {e}", exc_info=True)
            await task_updater.update_status(
                TaskState.failed,
                message=task_updater.new_agent_message(parts=[TextPart(text=f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")])
            )

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Cancel the operation"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.reject()
        logger.info(f"Operation cancelled for context {context.context_id}")

    async def _discover_agents(self) -> Dict[str, Dict[str, Any]]:
        """A2A í‘œì¤€ ì—ì´ì „íŠ¸ ë°œê²¬"""
        available_agents = {}
        
        async with httpx.AsyncClient(timeout=10.0) as client:  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            for agent_name, port in AGENT_PORTS.items():
                try:
                    response = await client.get(f"http://localhost:{port}/.well-known/agent.json")
                    if response.status_code == 200:
                        agent_card = response.json()
                        available_agents[agent_name] = {
                            "name": agent_card.get("name", agent_name),
                            "url": f"http://localhost:{port}",
                            "port": port,
                            "description": agent_card.get("description", ""),
                            "status": "available"
                        }
                        logger.info(f"âœ… {agent_name} agent discovered on port {port}")
                except Exception as e:
                    logger.warning(f"âš ï¸ {agent_name} agent on port {port} not available: {e}")
        
        logger.info(f"ğŸ” Total discovered agents: {len(available_agents)}")
        return available_agents

    async def _understand_request(self, user_input: str) -> Dict[str, Any]:
        """ê°•í™”ëœ LLM ê¸°ë°˜ ìš”ì²­ ì´í•´ - ì‚¬ìš©ì ì˜ë„ë¥¼ ì™„ì „íˆ íŒŒì•…"""
        
        if not self.openai_client:
            return {
                "domain": "general",
                "analysis_type": "exploratory",
                "analysis_depth": "intermediate",
                "tone": "technical",
                "intent_category": "exploratory_analysis",
                "specific_questions": [user_input],
                "business_context": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„ ìš”êµ¬"
            }

        understanding_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ê¹Šì´ ë¶„ì„í•˜ì—¬ ì™„ì „íˆ ì´í•´í•˜ì„¸ìš”:

"{user_input}"

ì‚¬ìš©ìì˜ ëª…ì‹œì /ì•”ì‹œì  ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ íŒŒì•…í•˜ê³ , 
ì–´ë–¤ ì¢…ë¥˜ì˜ ë¶„ì„ê³¼ ê²°ê³¼ë¬¼ì„ ì›í•˜ëŠ”ì§€ ì •í™•íˆ íŒë‹¨í•˜ì„¸ìš”.

íŠ¹íˆ ë‹¤ìŒì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:
1. ì‚¬ìš©ìê°€ ì†í•œ ë„ë©”ì¸/ì—…ê³„ëŠ” ë¬´ì—‡ì¸ê°€?
2. ì–´ë–¤ ìˆ˜ì¤€ì˜ ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆëŠ”ê°€?
3. ì–´ë–¤ êµ¬ì²´ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ê³  í•˜ëŠ”ê°€?
4. ì–´ë–¤ í˜•íƒœì˜ ê²°ê³¼ë¬¼ì„ ê¸°ëŒ€í•˜ëŠ”ê°€?
5. ë¹„ì¦ˆë‹ˆìŠ¤/ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€?

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "domain": "ê°ì§€ëœ ë„ë©”ì¸ (ì˜ˆ: semiconductor, finance, healthcare, general)",
    "analysis_type": "ë¶„ì„ ìœ í˜• (descriptive/diagnostic/predictive/prescriptive)",
    "analysis_depth": "ë¶„ì„ ê¹Šì´ (basic/intermediate/expert)",
    "urgency": "ê¸´ê¸‰ë„ (low/medium/high)",
    "tone": "ì ì ˆí•œ ì‘ë‹µ í†¤ (casual/professional/technical/academic)",
    "intent_category": "ì˜ë„ ì¹´í…Œê³ ë¦¬",
    "specific_questions": ["ì‚¬ìš©ìê°€ ë‹µì„ ì›í•˜ëŠ” êµ¬ì²´ì  ì§ˆë¬¸ë“¤"],
    "business_context": "ë¹„ì¦ˆë‹ˆìŠ¤/ì—…ë¬´ ë§¥ë½",
    "expertise_claimed": "ì‚¬ìš©ìê°€ ì£¼ì¥í•˜ëŠ” ì „ë¬¸ì„± ìˆ˜ì¤€",
    "expected_deliverables": ["ê¸°ëŒ€í•˜ëŠ” ê²°ê³¼ë¬¼ ìœ í˜•ë“¤"],
    "stakeholder_considerations": ["ê³ ë ¤í•´ì•¼ í•  ì´í•´ê´€ê³„ìë“¤"]
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": understanding_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            understanding = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ“‹ Request understanding: {understanding.get('domain')} domain, {understanding.get('analysis_depth')} depth")
            return understanding
            
        except Exception as e:
            logger.warning(f"Request understanding failed: {e}")
            return {
                "domain": "general",
                "analysis_type": "exploratory", 
                "analysis_depth": "intermediate",
                "tone": "technical",
                "intent_category": "exploratory_analysis",
                "specific_questions": [user_input],
                "business_context": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„ ìš”êµ¬"
            }

    async def _extract_answer_structure_from_question(self, user_input: str) -> Dict:
        """ğŸ¯ NEW: ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•„ìš”í•œ ë‹µë³€ êµ¬ì¡°ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œ"""
        
        if not self.openai_client:
            return {
                "required_sections": [
                    {
                        "name": "ë¶„ì„ ê²°ê³¼",
                        "purpose": "ë°ì´í„° ê¸°ë°˜ ë¶„ì„ ìˆ˜í–‰",
                        "required_data": ["ê¸°ë³¸ í†µê³„", "íŒ¨í„´ ë¶„ì„"],
                        "expected_format": "í…ìŠ¤íŠ¸"
                    }
                ],
                "overall_structure": "ê¸°ë³¸ ë¶„ì„ ë³´ê³ ì„œ",
                "key_questions_to_answer": [user_input]
            }
        
        structure_extraction_prompt = f"""
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ êµ¬ì¡°ì˜ ë‹µë³€ì„ ì›í•˜ëŠ”ì§€ íŒŒì•…í•˜ì„¸ìš”.
        
        ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"
        
        ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•˜ëŠ” ê²ƒë“¤ì„ ì¶”ì¶œí•˜ì—¬ ë‹µë³€ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ì„¸ìš”.
        ì˜ˆë¥¼ ë“¤ì–´:
        - "ê³µì • ì´ìƒ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³ " â†’ ì´ìƒ ì—¬ë¶€ ì§„ë‹¨ ì„¹ì…˜ í•„ìš”
        - "ì›ì¸ì„ ì„¤ëª…í•˜ë©°" â†’ ì›ì¸ ë¶„ì„ ì„¹ì…˜ í•„ìš”  
        - "ì¡°ì¹˜ ë°©í–¥ì„ ì œì•ˆ" â†’ ì¡°ì¹˜ ë°©ì•ˆ ì„¹ì…˜ í•„ìš”
        
        í•˜ì§€ë§Œ ì´ê²ƒì€ ì˜ˆì‹œì¼ ë¿ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìê°€ "íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì¤˜"ë¼ê³  í•˜ë©´ íŠ¸ë Œë“œ ë¶„ì„ êµ¬ì¡°ê°€ í•„ìš”í•˜ê³ ,
        "Aì™€ Bë¥¼ ë¹„êµí•´ì¤˜"ë¼ê³  í•˜ë©´ ë¹„êµ ë¶„ì„ êµ¬ì¡°ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
        {{
            "required_sections": [
                {{
                    "name": "ì„¹ì…˜ ì´ë¦„",
                    "purpose": "ì´ ì„¹ì…˜ì˜ ëª©ì ",
                    "required_data": ["í•„ìš”í•œ ë°ì´í„° ìœ í˜•ë“¤"],
                    "expected_format": "í‘œ/ê·¸ë˜í”„/í…ìŠ¤íŠ¸/ëª©ë¡ ë“±"
                }}
            ],
            "overall_structure": "ì „ì²´ì ì¸ ë‹µë³€ íë¦„",
            "key_questions_to_answer": ["ë‹µí•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸ë“¤"]
        }}
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": structure_extraction_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            answer_structure = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ¯ Answer structure extracted: {len(answer_structure.get('required_sections', []))} sections")
            return answer_structure
            
        except Exception as e:
            logger.warning(f"Answer structure extraction failed: {e}")
            return {
                "required_sections": [
                    {
                        "name": "ë¶„ì„ ê²°ê³¼",
                        "purpose": "ë°ì´í„° ê¸°ë°˜ ë¶„ì„ ìˆ˜í–‰",
                        "required_data": ["ê¸°ë³¸ í†µê³„", "íŒ¨í„´ ë¶„ì„"],
                        "expected_format": "í…ìŠ¤íŠ¸"
                    }
                ],
                "overall_structure": "ê¸°ë³¸ ë¶„ì„ ë³´ê³ ì„œ",
                "key_questions_to_answer": [user_input]
            }

    async def _auto_adapt_to_domain(self, user_input: str) -> Dict:
        """ì–´ë–¤ ë„ë©”ì¸ì´ë“  ìë™ìœ¼ë¡œ ì ì‘"""
        
        if not self.openai_client:
            return {"adaptation": "ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ì ‘ê·¼ë²•"}
        
        adaptation_prompt = f"""
ë‹¤ìŒ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ë„ë©”ì¸ê³¼ í•„ìš”í•œ ë¶„ì„ ë°©ë²•ì„ íŒŒì•…í•˜ì„¸ìš”:

{user_input}

ì´ ìš”ì²­ì´ ì–´ë–¤ ë„ë©”ì¸ì¸ì§€, ì–´ë–¤ ì¢…ë¥˜ì˜ ë¶„ì„ì´ í•„ìš”í•œì§€, 
ì–´ë–¤ ì „ë¬¸ ì§€ì‹ì´ í•„ìš”í•œì§€ íŒŒì•…í•˜ì—¬ ìµœì ì˜ ì ‘ê·¼ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”.

íŠ¹íˆ ì‚¬ìš©ìê°€ íŠ¹ì • ì—­í• ì´ë‚˜ ì „ë¬¸ì„±ì„ ì–¸ê¸‰í–ˆë‹¤ë©´, 
ê·¸ì— ë§ëŠ” ë¶„ì„ ê¹Šì´ì™€ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "detected_domain": "ê°ì§€ëœ ë„ë©”ì¸",
    "required_expertise": "í•„ìš”í•œ ì „ë¬¸ì„±",
    "analysis_approach": "ê¶Œì¥ ë¶„ì„ ì ‘ê·¼ë²•",
    "terminology_level": "ìš©ì–´ ìˆ˜ì¤€ (basic/intermediate/expert)",
    "special_considerations": ["íŠ¹ë³„ ê³ ë ¤ì‚¬í•­ë“¤"]
}}
"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": adaptation_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.warning(f"Domain adaptation failed: {e}")
            return {"adaptation": "ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ì ‘ê·¼ë²•"}

    async def _create_structure_aware_plan(self, understanding: Dict, 
                                          answer_structure: Dict,
                                          available_agents: Dict) -> Dict:
        """ì™„ì „ LLM ê¸°ë°˜ ë™ì  ê³„íš ìƒì„± - í•˜ë“œì½”ë”© ì œê±°, ë²”ìš©ì  ì ‘ê·¼"""
        
        if not self.openai_client:
            return self._create_fallback_plan(available_agents)
        
        # ì—ì´ì „íŠ¸ë“¤ì˜ ìƒì„¸ ì •ë³´ êµ¬ì¡°í™”
        agents_details = {}
        for name, info in available_agents.items():
            agents_details[name] = {
                "description": info.get('description', ''),
                "capabilities": info.get('capabilities', []),
                "typical_use_cases": info.get('use_cases', [])
            }
        
        planning_prompt = f"""ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³ , ê°€ì¥ íš¨ê³¼ì ì¸ ì—ì´ì „íŠ¸ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ“‹ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ê²°ê³¼
{json.dumps(understanding, ensure_ascii=False, indent=2)}

## ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë“¤
{json.dumps(agents_details, ensure_ascii=False, indent=2)}

## ğŸ¯ ê³„íš ìˆ˜ë¦½ ì§€ì¹¨
1. **ìš”ì²­ ì¤‘ì‹¬ ì ‘ê·¼**: ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²°ê³¼ì— ì§‘ì¤‘í•˜ì—¬ í•„ìš”í•œ ì—ì´ì „íŠ¸ë§Œ ì„ íƒ
2. **ë…¼ë¦¬ì  ìˆœì„œ**: ë°ì´í„° íë¦„ê³¼ ì˜ì¡´ì„±ì„ ê³ ë ¤í•œ ìˆœì„œ ê²°ì •
3. **íš¨ìœ¨ì„± ìµœì í™”**: ë¶ˆí•„ìš”í•œ ë‹¨ê³„ ì œê±°, í•µì‹¬ ë¶„ì„ì— ì§‘ì¤‘
4. **ë„ë©”ì¸ ì ì‘**: {understanding.get('domain', 'ì¼ë°˜')} ë„ë©”ì¸ íŠ¹ì„± ë°˜ì˜
5. **ì‚¬ìš©ì ìˆ˜ì¤€ ê³ ë ¤**: {understanding.get('expertise_claimed', 'ì¼ë°˜ ì‚¬ìš©ì')} ìˆ˜ì¤€ì— ë§ëŠ” ë¶„ì„ ê¹Šì´

## ğŸš€ ë™ì  ì—ì´ì „íŠ¸ ì„ íƒ ê¸°ì¤€
- ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ê°€ ë¬´ì—‡ì¸ê°€?
- ì–´ë–¤ ì¢…ë¥˜ì˜ ë¶„ì„ì´ ì‹¤ì œë¡œ í•„ìš”í•œê°€?
- ê° ì—ì´ì „íŠ¸ê°€ ì œê³µí•  ìˆ˜ ìˆëŠ” ê°€ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€?
- ìµœì†Œí•œì˜ ë‹¨ê³„ë¡œ ìµœëŒ€í•œì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ìœ¼ë ¤ë©´?

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "analysis_strategy": "ì´ ìš”ì²­ì— ëŒ€í•œ ì „ì²´ì  ë¶„ì„ ì „ëµ",
    "agent_selection_reasoning": "ì„ íƒëœ ì—ì´ì „íŠ¸ë“¤ê³¼ ê·¸ ì´ìœ ",
    "steps": [
        {{
            "step_number": 1,
            "agent": "ì„ íƒëœ_ì—ì´ì „íŠ¸ëª…",
            "purpose": "ì´ ë‹¨ê³„ì˜ êµ¬ì²´ì  ëª©ì ",
            "enriched_task": "ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•  ìƒì„¸í•œ ì‘ì—… ì§€ì‹œ (ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)",
            "expected_output": "ì´ ë‹¨ê³„ì—ì„œ ê¸°ëŒ€í•˜ëŠ” êµ¬ì²´ì  ê²°ê³¼",
            "success_criteria": "ì„±ê³µ íŒë‹¨ ê¸°ì¤€",
            "context_for_next": "ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•  í•µì‹¬ ì •ë³´"
        }}
    ],
    "final_synthesis_strategy": "ëª¨ë“  ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ ì¢…í•©í•  ê²ƒì¸ê°€",
    "potential_insights": "ì´ ê³„íšìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ì˜ˆìƒ ì¸ì‚¬ì´íŠ¸ë“¤"
}}

ì¤‘ìš”: í…œí”Œë¦¿ì´ë‚˜ ê³ ì •ëœ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì´ íŠ¹ì • ìš”ì²­ì— ìµœì í™”ëœ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”."""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system", 
                        "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš° ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ìš”ì²­ì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ íŒŒì•…í•˜ê³ , ê°€ì¥ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ ë¶„ì„ ê²½ë¡œë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤. í•˜ë“œì½”ë”©ëœ í…œí”Œë¦¿ì´ ì•„ë‹Œ, ìš”ì²­ë³„ ë§ì¶¤í˜• ì ‘ê·¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    },
                    {"role": "user", "content": planning_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.4,  # ì°½ì˜ì  ê³„íš ìˆ˜ë¦½ì„ ìœ„í•´ ì•½ê°„ ë†’ì„
                max_tokens=3000,
                timeout=90.0
            )
            
            plan = json.loads(response.choices[0].message.content)
            
            # ê³„íš ê²€ì¦ ë° ë³´ì •
            validated_plan = self._validate_and_enhance_plan(plan, available_agents, understanding)
            
            return validated_plan
            
        except Exception as e:
            logger.warning(f"Dynamic planning failed: {e}")
            return self._create_fallback_plan(available_agents)

    def _validate_and_enhance_plan(self, plan: Dict, available_agents: Dict, understanding: Dict) -> Dict:
        """ìƒì„±ëœ ê³„íšì„ ê²€ì¦í•˜ê³  ë³´ê°•"""
        try:
            # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
            if not plan.get('steps'):
                logger.warning("Plan has no steps, using fallback")
                return self._create_fallback_plan(available_agents)
            
            # ì—ì´ì „íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ë³´ì •
            valid_steps = []
            for step in plan.get('steps', []):
                agent_name = step.get('agent', '')
                if agent_name in available_agents:
                    # í•„ìˆ˜ í•„ë“œ ë³´ì™„
                    enhanced_step = {
                        "agent": agent_name,
                        "purpose": step.get('purpose', f'{agent_name} ë¶„ì„ ìˆ˜í–‰'),
                        "enriched_task": step.get('enriched_task', step.get('purpose', f'{agent_name} ì‘ì—… ìˆ˜í–‰')),
                        "expected_output": step.get('expected_output', f'{agent_name} ë¶„ì„ ê²°ê³¼'),
                        "pass_to_next": step.get('context_for_next', step.get('pass_to_next', ['ë¶„ì„ ê²°ê³¼']))
                    }
                    valid_steps.append(enhanced_step)
                else:
                    logger.warning(f"Agent {agent_name} not available, skipping step")
            
            if not valid_steps:
                logger.warning("No valid steps after validation, using fallback")
                return self._create_fallback_plan(available_agents)
            
            # í–¥ìƒëœ ê³„íš ë°˜í™˜
            enhanced_plan = {
                "reasoning": plan.get('analysis_strategy', plan.get('reasoning', 'ì‚¬ìš©ì ìš”ì²­ì— ìµœì í™”ëœ ë¶„ì„ ì›Œí¬í”Œë¡œìš°')),
                "steps": valid_steps,
                "final_synthesis_guide": plan.get('final_synthesis_strategy', plan.get('final_synthesis_guide', 'ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ')),
                "potential_insights": plan.get('potential_insights', ['ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ']),
                "agent_selection_reasoning": plan.get('agent_selection_reasoning', 'ìš”ì²­ì— ìµœì í™”ëœ ì—ì´ì „íŠ¸ ì„ íƒ')
            }
            
            return enhanced_plan
            
        except Exception as e:
            logger.error(f"Plan validation failed: {e}")
            return self._create_fallback_plan(available_agents)

    async def _execute_agent_with_structure_context(self, agent_name: str, step: Dict, 
                                                    answer_structure: Dict,
                                                    full_context: Dict,
                                                    previous_results: Dict) -> Dict:
        """ê° ì—ì´ì „íŠ¸ì— í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì‘ì—… ì „ë‹¬"""
        
        if agent_name not in self.available_agents:
            return {
                'status': 'failed',
                'error': f'Agent {agent_name} not available',
                'summary': f'ì—ì´ì „íŠ¸ {agent_name}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }
        
        # LLMì´ ì´ì „ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ë³´ê°•
        enriched_prompt = await self._enrich_agent_task(
            agent_name, step.get('enriched_task', step.get('purpose', '')),
            full_context, previous_results
        )
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        agent_url = self.available_agents[agent_name]['url']
        
        payload = {
            "jsonrpc": "2.0",
            "method": "message/send",
            "params": {
                "message": {
                    "messageId": f"req_{agent_name}_{int(time.time())}",
                    "role": "user",
                    "parts": [{
                        "kind": "text",
                        "text": enriched_prompt
                    }]
                }
            },
            "id": f"req_{agent_name}_{int(time.time())}"
        }
        
        try:
            async with httpx.AsyncClient(timeout=180.0) as client:  # 3ë¶„ìœ¼ë¡œ ëŒ€í­ ì¦ê°€
                response = await client.post(
                    agent_url,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return self._parse_agent_response(result, agent_name)
                else:
                    return {
                        'status': 'failed',
                        'error': f'HTTP {response.status_code}',
                        'summary': f'ì—ì´ì „íŠ¸ í˜¸ì¶œ ì‹¤íŒ¨ (HTTP {response.status_code})'
                    }
                    
        except Exception as e:
            logger.error(f"Agent {agent_name} execution error: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'summary': f'ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }

    async def _enrich_agent_task(self, agent_name: str, base_task: str, 
                                context: Dict, previous_results: Dict) -> str:
        """LLMì´ ê° ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ë³´ê°•"""
        
        if not self.openai_client:
            return base_task
        
        # ì´ì „ ì—ì´ì „íŠ¸ë“¤ì˜ í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
        previous_insights = self._extract_key_insights(previous_results)
        
        enrichment_prompt = f"""
ì—ì´ì „íŠ¸: {agent_name}
ê¸°ë³¸ ì‘ì—…: {base_task}

ì „ì²´ ë§¥ë½:
- ë„ë©”ì¸: {context['domain']}
- ì‚¬ìš©ì ì—­í• /ì „ë¬¸ì„±: {context.get('expertise_claimed', 'ì¼ë°˜ ì‚¬ìš©ì')}
- ìµœì¢… ëª©í‘œ: {context['key_objectives']}
- ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸: {context.get('domain_context', '')}

ì´ì „ ë¶„ì„ ê²°ê³¼:
{json.dumps(previous_insights, ensure_ascii=False)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {agent_name} ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•´ì•¼ í•  êµ¬ì²´ì ì¸ ì‘ì—…ì„ ì‘ì„±í•˜ì„¸ìš”.
ë„ë©”ì¸ ì§€ì‹ê³¼ ì´ì „ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë¶„ì„ì´ ë˜ë„ë¡ ì§€ì‹œí•˜ì„¸ìš”.
"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´
                messages=[{"role": "user", "content": enrichment_prompt}],
                temperature=0.2,
                max_tokens=1000,
                timeout=60.0  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.warning(f"Task enrichment failed: {e}")
            return base_task

    async def _synthesize_with_llm(self, original_request: str, 
                                  understanding: Dict, all_results: Dict,
                                  task_updater: StreamingTaskUpdater) -> str:
        """ğŸ¯ NEW: Question-Driven Dynamic Structure ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        
        logger.info("ğŸ¯ Question-Driven í•©ì„± ì‹œì‘")
        
        if not self.openai_client:
            logger.warning("âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, fallback ì‚¬ìš©")
            return self._create_fallback_synthesis(original_request, all_results)
        
        try:
            # 1ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ ë‹µë³€ êµ¬ì¡° ì¶”ì¶œ
            logger.info("ğŸ“‹ 1ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ ë‹µë³€ êµ¬ì¡° ì¶”ì¶œ")
            answer_structure = await self._extract_answer_structure_from_question(original_request)
            logger.info(f"âœ… ë‹µë³€ êµ¬ì¡° ì¶”ì¶œ ì™„ë£Œ: {len(answer_structure.get('required_sections', []))} ì„¹ì…˜")
            
            # 2ë‹¨ê³„: ì‹¤ì œ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
            logger.info("ğŸ“Š 2ë‹¨ê³„: ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ")
            data_context = await self._extract_data_context(all_results)
            logger.info(f"âœ… ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {data_context.get('data_quality', 'unknown')} í’ˆì§ˆ")
            
            # 3ë‹¨ê³„: ì—ì´ì „íŠ¸ ê²°ê³¼ êµ¬ì¡°í™”
            logger.info("ğŸ” 3ë‹¨ê³„: ì—ì´ì „íŠ¸ ê²°ê³¼ êµ¬ì¡°í™”")
            structured_results = self._structure_agent_results(all_results)
            logger.info(f"âœ… ê²°ê³¼ êµ¬ì¡°í™” ì™„ë£Œ: {len(structured_results)} ë¬¸ì")
            
            # 4ë‹¨ê³„: ğŸ¯ NEW - ë™ì  êµ¬ì¡° ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            logger.info("ğŸ¨ 4ë‹¨ê³„: ë™ì  êµ¬ì¡° ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±")
            synthesis_prompt = f"""ë‹¹ì‹ ì€ {understanding.get('domain', 'ë°ì´í„° ë¶„ì„')} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ğŸ¯ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
"{original_request}"

## ğŸ“‹ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë‹µë³€ êµ¬ì¡° (ì§ˆë¬¸ì—ì„œ ì¶”ì¶œ)
ì „ì²´ ë‹µë³€ íë¦„: {answer_structure.get('overall_structure', 'ì§ì ‘ ë‹µë³€')}

í•„ìš”í•œ ì„¹ì…˜ë“¤:
{json.dumps(answer_structure.get('required_sections', []), ensure_ascii=False, indent=2)}

ë‹µí•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸ë“¤:
{json.dumps(answer_structure.get('key_questions_to_answer', []), ensure_ascii=False, indent=2)}

## ğŸ“Š ì‹¤ì œ ë¶„ì„ëœ ë°ì´í„° ì •ë³´ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
- ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°: {len(data_context.get('available_data', []))}ê°œ ì†ŒìŠ¤
- ë°ì´í„° í’ˆì§ˆ: {data_context.get('data_quality', 'unknown')}
- í†µê³„ì  ì¦ê±°: {', '.join(data_context.get('statistical_evidence', [])[:10])}
- ë°ì´í„° ì œí•œì‚¬í•­: {', '.join(data_context.get('limitations', []))}

## ğŸ” ê° ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼
{structured_results}

## âœ… í•„ìˆ˜ ì¤€ìˆ˜ì‚¬í•­ (Question-Driven ë°©ì‹)
1. **ì§ˆë¬¸ êµ¬ì¡° ì™„ì „ ì¤€ìˆ˜**: ìœ„ì—ì„œ ì¶”ì¶œí•œ ë‹µë³€ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë”°ë¥´ì„¸ìš”
2. **ì„¹ì…˜ë³„ ë§ì¶¤ ì‘ì„±**: ê° required_sectionì˜ purposeì™€ expected_formatì— ë§ê²Œ ì‘ì„±
3. **ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©**: ìœ„ ë¶„ì„ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
4. **í•µì‹¬ ì§ˆë¬¸ ì™„ì „ ë‹µë³€**: key_questions_to_answerì˜ ëª¨ë“  ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”
5. **êµ¬ì²´ì  ê·¼ê±° ì œì‹œ**: ëª¨ë“  ì£¼ì¥ì— ëŒ€í•´ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê·¼ê±° ì œì‹œ

## âŒ ì ˆëŒ€ ê¸ˆì§€
- ë¯¸ë¦¬ ì •ì˜ëœ í…œí”Œë¦¿ ì‚¬ìš© (ì‚¬ìš©ì ì§ˆë¬¸ êµ¬ì¡°ì™€ ë‹¤ë¥¸ ê²½ìš°)
- ë¶„ì„ë˜ì§€ ì•Šì€ ë‚´ìš© ì¶”ì¸¡
- ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•˜ì§€ ì•Šì€ ì„¹ì…˜ ì¶”ê°€
- ë§‰ì—°í•œ í‘œí˜„ ("ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ", "ëŒ€ì²´ë¡œ" ë“±)

ğŸ¯ ì¤‘ìš”: ì‚¬ìš©ìê°€ ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•œ êµ¬ì¡° ê·¸ëŒ€ë¡œ ë‹µë³€í•˜ì„¸ìš”. 
ì˜ˆë¥¼ ë“¤ì–´ "ì´ìƒ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³  ì›ì¸ì„ ì„¤ëª…í•˜ë©° ì¡°ì¹˜ë¥¼ ì œì•ˆ"ì´ë¼ê³  í–ˆë‹¤ë©´, 
ì •í™•íˆ ê·¸ 3ê°€ì§€ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”."""

            logger.info(f"âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {len(synthesis_prompt)} ë¬¸ì")
            
            # 5ë‹¨ê³„: LLM í˜¸ì¶œ
            logger.info("ğŸ¤– 5ë‹¨ê³„: LLM í˜¸ì¶œ")
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": synthesis_prompt}],
                max_tokens=4000,
                temperature=0.3,
                timeout=180
            )
            
            llm_response = response.choices[0].message.content
            logger.info(f"âœ… LLM ì‘ë‹µ ìˆ˜ì‹ : {len(llm_response)} ë¬¸ì")
            
            # 6ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ (í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬)
            logger.info("ğŸ” 6ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦")
            quality_ok = await self._validate_response_quality(llm_response, data_context, original_request)
            
            if quality_ok:
                logger.info("âœ… í’ˆì§ˆ ê²€ì¦ í†µê³¼, ìµœì¢… ì‘ë‹µ ë°˜í™˜")
                return llm_response
            else:
                logger.warning("âš ï¸ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨, ê°•í™” í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„")
                # í’ˆì§ˆì´ ë¶€ì¡±í•˜ë©´ ë” ê°•í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
                retry_result = await self._retry_with_stronger_prompt(
                    original_request, understanding, structured_results, data_context, answer_structure
                )
                logger.info("âœ… ì¬ì‹œë„ ì™„ë£Œ")
                return retry_result
                                                           
        except Exception as e:
            logger.error(f"âŒ Question-Driven í•©ì„± ì‹¤íŒ¨: {e}", exc_info=True)
            logger.warning("ğŸ”„ fallback_synthesisë¡œ ì „í™˜")
            return self._create_fallback_synthesis(original_request, all_results)

    async def _validate_response_quality(self, response: str, data_context: Dict, 
                                       original_request: str) -> bool:
        """ğŸ” ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ - í• ë£¨ì‹œë„¤ì´ì…˜ ë° í”¼ìƒì  ë‹µë³€ ê°ì§€"""
        
        # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
        if len(response) < 300:
            return False
            
        # í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ íŒ¨í„´
        hallucination_patterns = [
            r'ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„', r'ë³´í†µ \w+ëŠ”', r'ëŒ€ì²´ë¡œ', r'í†µìƒì ìœ¼ë¡œ',
            r'ê²½í—˜ìƒ', r'ì¼ë°˜ì ì¸ ê²½ìš°', r'ë³´í¸ì ìœ¼ë¡œ'
        ]
        
        import re
        for pattern in hallucination_patterns:
            if re.search(pattern, response):
                logger.warning(f"í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨í„´ ê°ì§€: {pattern}")
                return False
        
        # í”¼ìƒì  ë‹µë³€ ê°ì§€
        superficial_patterns = [
            r'ë¶„ì„ì„ í†µí•´ í™•ì¸', r'ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ', r'ë°ì´í„°ë¥¼ í†µí•´',
            r'ì¶”ê°€ ë¶„ì„ì´ í•„ìš”', r'í–¥í›„ ì—°êµ¬ê°€ í•„ìš”'
        ]
        
        superficial_count = sum(1 for pattern in superficial_patterns 
                               if re.search(pattern, response))
        
        if superficial_count > 2:
            logger.warning(f"í”¼ìƒì  í‘œí˜„ ê³¼ë‹¤ ê°ì§€: {superficial_count}ê°œ")
            return False
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ê´€ë ¨ì„± ì²´í¬
        key_terms = original_request.split()[:5]  # ì²« 5ê°œ ë‹¨ì–´
        relevance_score = sum(1 for term in key_terms if term in response)
        
        if relevance_score < 2:
            logger.warning(f"ì§ˆë¬¸ ê´€ë ¨ì„± ë¶€ì¡±: {relevance_score}")
            return False
            
        return True

    async def _retry_with_stronger_prompt(self, original_request: str, understanding: Dict,
                                        structured_results: str, data_context: Dict, answer_structure: Dict) -> str:
        """ğŸ”¥ í’ˆì§ˆ ë¶€ì¡± ì‹œ ë” ê°•í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„"""
        
        stronger_prompt = f"""ğŸš¨ ì¤‘ìš”: ì´ì „ ë‹µë³€ì´ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{original_request}"

## ğŸ¯ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•  ìš”êµ¬ì‚¬í•­
1. ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ì§ì ‘ ë‹µë³€
2. ì•„ë˜ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë§Œ ì‚¬ìš© (ì¶”ì¸¡ ê¸ˆì§€)
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°œê²¬ì‚¬í•­ì´ ìˆë‹¤ë©´ ëª…ì‹œ
4. "ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ", "ëŒ€ì²´ë¡œ" ê°™ì€ ë§‰ì—°í•œ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
5. ìµœì†Œ 500ë‹¨ì–´ ì´ìƒì˜ ìƒì„¸í•œ ë‹µë³€

## ğŸ“Š ì‹¤ì œ ë¶„ì„ ë°ì´í„°
{structured_results}

## ğŸ” ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
- í†µê³„ì  ì¦ê±°: {', '.join(data_context.get('statistical_evidence', []))}
- ë°ì´í„° ì œí•œì‚¬í•­: {', '.join(data_context.get('limitations', []))}

ì‚¬ìš©ìê°€ ì •í™•íˆ ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ íŒŒì•…í•˜ê³ , ë¶„ì„ëœ ì‹¤ì œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": stronger_prompt}],
                max_tokens=4000,
                temperature=0.2,
                timeout=180
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
            return self._create_fallback_synthesis(original_request, {})

    async def _analyze_user_intent_and_structure(self, user_input: str, understanding: Dict) -> Dict:
        """ğŸ¯ ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ êµ¬ì¡°ë¥¼ ë™ì  ìƒì„±"""
        
        if not self.openai_client:
            return {"structure_type": "direct_answer", "guidelines": []}
        
        try:
            structure_prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë‹µë³€ êµ¬ì¡°ë¥¼ ê²°ì •í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"
ì§ˆë¬¸ ì˜ë„: {understanding}

ë‹¤ìŒ ì¤‘ì—ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë‹µë³€ í˜•íƒœë¥¼ ì„ íƒí•˜ê³  êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ì„¸ìš”:

1. **ì§ì ‘ ë‹µë³€í˜•**: ì§ˆë¬¸ì— ë°”ë¡œ ë‹µí•˜ëŠ” í˜•íƒœ (ì˜ˆ: "ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ ì¤‘ìš”í•œê°€?")
2. **ë¶„ì„ ë³´ê³ ì„œí˜•**: ì²´ê³„ì ì¸ ë¶„ì„ ê²°ê³¼ (ì˜ˆ: "ë°ì´í„°ë¥¼ ë¶„ì„í•´ì¤˜")
3. **ì‹¤í–‰ ê°€ì´ë“œí˜•**: êµ¬ì²´ì  í–‰ë™ ë°©ì•ˆ (ì˜ˆ: "ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì¤˜")
4. **ë¹„êµ ë¶„ì„í˜•**: ì—¬ëŸ¬ ì˜µì…˜ ë¹„êµ (ì˜ˆ: "ì–´ë–¤ ë°©ë²•ì´ ë” ì¢‹ì€ê°€?")
5. **ë¬¸ì œ í•´ê²°í˜•**: ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²°ì±… (ì˜ˆ: "ë¶ˆëŸ‰ë¥ ì„ ì¤„ì´ê³  ì‹¶ì–´")
6. **íƒìƒ‰ì  ë¶„ì„í˜•**: íŒ¨í„´ ë°œê²¬ ë° ì¸ì‚¬ì´íŠ¸ (ì˜ˆ: "ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì°¾ì•„ì¤˜")

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "structure_type": "ì„ íƒëœ ë‹µë³€ í˜•íƒœ",
    "reasoning": "ì„ íƒ ì´ìœ ",
    "key_elements": ["ë‹µë³€ì— ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•  í•µì‹¬ ìš”ì†Œë“¤"],
    "tone": "ë‹µë³€ í†¤ (professional/technical/conversational/urgent)",
    "focus_areas": ["ì§‘ì¤‘í•´ì•¼ í•  ì˜ì—­ë“¤"],
    "avoid": ["í”¼í•´ì•¼ í•  ë‚´ìš©ë“¤"]
}}"""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": structure_prompt}],
                max_tokens=800,
                temperature=0.3
            )
            
            # JSON íŒŒì‹±ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            response_content = response.choices[0].message.content.strip()
            logger.info(f"ğŸ” êµ¬ì¡° ë¶„ì„ ì‘ë‹µ: {response_content[:200]}...")
            
            # JSON ë¸”ë¡ ì¶”ì¶œ (```json...``` í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
            if "```json" in response_content:
                json_start = response_content.find("```json") + 7
                json_end = response_content.find("```", json_start)
                if json_end != -1:
                    response_content = response_content[json_start:json_end].strip()
            elif "```" in response_content:
                json_start = response_content.find("```") + 3
                json_end = response_content.find("```", json_start)
                if json_end != -1:
                    response_content = response_content[json_start:json_end].strip()
            
            try:
                structure_info = json.loads(response_content)
                logger.info(f"âœ… êµ¬ì¡° ë¶„ì„ ì„±ê³µ: {structure_info.get('structure_type', 'unknown')}")
                return structure_info
            except json.JSONDecodeError as json_error:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}, ì›ë³¸ ì‘ë‹µ: {response_content[:200]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    "structure_type": "direct_answer",
                    "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©",
                    "key_elements": ["êµ¬ì²´ì  ë‹µë³€", "ë°ì´í„° ê¸°ë°˜ ê·¼ê±°"],
                    "tone": "professional",
                    "focus_areas": ["ì‚¬ìš©ì ì§ˆë¬¸ ì§ì ‘ ë‹µë³€"],
                    "avoid": ["ë¶ˆí•„ìš”í•œ í˜•ì‹ì  êµ¬ì¡°"]
                }
            
        except Exception as e:
            logger.warning(f"êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©: {e}")
            return {
                "structure_type": "direct_answer",
                "reasoning": "ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©",
                "key_elements": ["êµ¬ì²´ì  ë‹µë³€", "ë°ì´í„° ê¸°ë°˜ ê·¼ê±°"],
                "tone": "professional",
                "focus_areas": ["ì‚¬ìš©ì ì§ˆë¬¸ ì§ì ‘ ë‹µë³€"],
                "avoid": ["ë¶ˆí•„ìš”í•œ í˜•ì‹ì  êµ¬ì¡°"]
            }

    async def _extract_data_context(self, all_results: Dict) -> Dict:
        """ğŸ“Š ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹¤ì œ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€"""
        
        data_context = {
            "available_data": [],
            "key_findings": [],
            "statistical_evidence": [],
            "limitations": [],
            "data_quality": "unknown"
        }
        
        try:
            for agent_name, result in all_results.items():
                if isinstance(result, dict):
                    # ì‹¤ì œ ë°ì´í„° ì •ë³´ ì¶”ì¶œ
                    if 'artifacts' in result:
                        for artifact in result['artifacts']:
                            if isinstance(artifact, dict):
                                data_context["available_data"].append({
                                    "source": agent_name,
                                    "type": artifact.get('contentType', 'unknown'),
                                    "description": artifact.get('name', 'unnamed')
                                })
                    
                    # í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
                    if 'response' in result:
                        response_text = str(result['response'])
                        # í†µê³„ì  ì¦ê±°ë‚˜ êµ¬ì²´ì  ìˆ˜ì¹˜ ì¶”ì¶œ
                        numbers = re.findall(r'\d+\.?\d*%|\d+\.?\d*', response_text)
                        if numbers:
                            data_context["statistical_evidence"].extend(numbers[:5])  # ìµœëŒ€ 5ê°œ
                        
                        # í•µì‹¬ ë°œê²¬ì‚¬í•­ í‚¤ì›Œë“œ ì¶”ì¶œ
                        keywords = re.findall(r'(ì¤‘ìš”|í•µì‹¬|ì£¼ìš”|ë°œê²¬|ê²°ê³¼|ë¶„ì„|ìƒê´€ê´€ê³„|íŒ¨í„´)', response_text)
                        if keywords:
                            data_context["key_findings"].append(f"{agent_name}ì—ì„œ {len(keywords)}ê°œ í•µì‹¬ ë°œê²¬")
            
            # ë°ì´í„° í’ˆì§ˆ í‰ê°€
            if len(data_context["available_data"]) > 3:
                data_context["data_quality"] = "good"
            elif len(data_context["available_data"]) > 1:
                data_context["data_quality"] = "moderate"
            else:
                data_context["data_quality"] = "limited"
                data_context["limitations"].append("ì œí•œëœ ë°ì´í„° ì†ŒìŠ¤")
                
        except Exception as e:
            logger.warning(f"ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            data_context["limitations"].append("ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì œí•œ")
        
        return data_context

    def _structure_agent_results(self, all_results: Dict) -> str:
        """ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ LLMì´ ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì¡°í™”"""
        structured = "### ì—ì´ì „íŠ¸ë³„ ìƒì„¸ ë¶„ì„ ê²°ê³¼\n\n"
        
        for agent_name, result in all_results.items():
            status = result.get('status', 'unknown')
            structured += f"#### ğŸ¤– {agent_name} ì—ì´ì „íŠ¸\n"
            structured += f"- **ì‹¤í–‰ ìƒíƒœ**: {status}\n"
            
            if status == 'success':
                # ì„±ê³µí•œ ê²½ìš° ê²°ê³¼ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                agent_result = result.get('result', {})
                structured += f"- **ìš”ì•½**: {result.get('summary', 'ì‘ì—… ì™„ë£Œ')}\n"
                
                # ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
                if isinstance(agent_result, dict):
                    if 'artifacts' in agent_result:
                        artifacts = agent_result['artifacts']
                        structured += f"- **ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸**: {len(artifacts)}ê°œ\n"
                        for artifact in artifacts[:3]:  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ í‘œì‹œ
                            artifact_name = artifact.get('name', 'ì´ë¦„ ì—†ìŒ')
                            artifact_type = artifact.get('metadata', {}).get('content_type', 'íƒ€ì… ë¯¸ì§€ì •')
                            structured += f"  - {artifact_name} ({artifact_type})\n"
                    
                    if 'message' in agent_result:
                        message = agent_result['message']
                        if isinstance(message, dict) and 'parts' in message:
                            parts = message['parts']
                            if parts and len(parts) > 0:
                                first_part = parts[0]
                                if hasattr(first_part, 'text'):
                                    text_preview = first_part.text[:200] + "..." if len(first_part.text) > 200 else first_part.text
                                    structured += f"- **ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°**: {text_preview}\n"
                
                # ì›ì‹œ ê²°ê³¼ ë°ì´í„°ë„ í¬í•¨ (JSON í˜•íƒœ)
                structured += f"- **ìƒì„¸ ê²°ê³¼**: {json.dumps(agent_result, ensure_ascii=False, indent=2)[:500]}...\n"
                
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ì˜¤ë¥˜ ì •ë³´
                error_msg = result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                structured += f"- **ì˜¤ë¥˜ ë‚´ìš©**: {error_msg}\n"
                structured += f"- **ì˜í–¥**: ì´ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ëŠ” ìµœì¢… ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤\n"
            
            structured += "\n"
        
        # ì „ì²´ ìš”ì•½ ì •ë³´
        total_agents = len(all_results)
        successful_agents = len([r for r in all_results.values() if r.get('status') == 'success'])
        failed_agents = total_agents - successful_agents
        
        structured += f"### ğŸ“Š ì „ì²´ ì‹¤í–‰ ìš”ì•½\n"
        structured += f"- **ì´ ì—ì´ì „íŠ¸ ìˆ˜**: {total_agents}ê°œ\n"
        structured += f"- **ì„±ê³µ**: {successful_agents}ê°œ\n"
        structured += f"- **ì‹¤íŒ¨**: {failed_agents}ê°œ\n"
        structured += f"- **ì„±ê³µë¥ **: {(successful_agents/total_agents*100):.1f}%\n\n"
        
        return structured

    def _extract_key_insights(self, previous_results: Dict) -> Dict:
        """ì´ì „ ê²°ê³¼ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = {}
        for agent_name, result in previous_results.items():
            if result.get('status') == 'success':
                insights[agent_name] = result.get('summary', 'ì‘ì—… ì™„ë£Œ')
            else:
                insights[agent_name] = f"ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        return insights

    def _parse_agent_response(self, response: Dict, agent_name: str) -> Dict:
        """ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹±"""
        try:
            if 'result' in response:
                result = response['result']
                if isinstance(result, dict):
                    # A2A í‘œì¤€ ì‘ë‹µ ì²˜ë¦¬
                    status = result.get('status', {})
                    if isinstance(status, dict) and status.get('state') == 'completed':
                        return {
                            'status': 'success',
                            'result': result,
                            'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ì—… ì™„ë£Œ'
                        }
                    else:
                        return {
                            'status': 'partial',
                            'result': result,
                            'summary': f'{agent_name} ì—ì´ì „íŠ¸ ë¶€ë¶„ ì™„ë£Œ'
                        }
                else:
                    return {
                        'status': 'success',
                        'result': result,
                        'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ì—… ì™„ë£Œ'
                    }
            else:
                return {
                    'status': 'failed',
                    'error': 'No result in response',
                    'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ë‹µ ì˜¤ë¥˜'
                }
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
                'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜'
            }

    def _create_fallback_plan(self, available_agents: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """LLMì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•Œì˜ í´ë°± ê³„íš"""
        steps = []
        step_number = 1
        
        # ê¸°ë³¸ì ì¸ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš°
        basic_workflow = [
            ("data_loader", "ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ê²€ì¦", "ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ì ì¸ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤"),
            ("data_cleaning", "ë°ì´í„° í’ˆì§ˆ í™•ì¸ ë° ì •ë¦¬", "ë°ì´í„°ì˜ í’ˆì§ˆì„ í™•ì¸í•˜ê³  í•„ìš”í•œ ì •ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"),
            ("eda_tools", "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ìˆ˜í–‰", "ë°ì´í„°ì˜ ë¶„í¬ì™€ íŒ¨í„´ì„ íƒìƒ‰í•˜ê³  ê¸°ì´ˆ í†µê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"),
            ("data_visualization", "ë°ì´í„° ì‹œê°í™” ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ", "ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤")
        ]
        
        for agent_name, purpose, task_desc in basic_workflow:
            if agent_name in available_agents:
                steps.append({
                    "agent": agent_name,
                    "purpose": purpose,
                    "enriched_task": task_desc,
                    "expected_output": f"{purpose} ê²°ê³¼",
                    "pass_to_next": ["ë¶„ì„ ê²°ê³¼", "ë°ì´í„° ì •ë³´"]
                })
                step_number += 1
        
        return {
            "reasoning": "ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ í¬ê´„ì ì¸ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í‘œì¤€ ì›Œí¬í”Œë¡œìš°",
            "steps": steps,
            "final_synthesis_guide": "ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ"
        }

    def _create_fallback_synthesis(self, original_request: str, all_results: Dict) -> str:
        """í´ë°± ìµœì¢… ë‹µë³€ ìƒì„±"""
        
        successful_agents = [name for name, result in all_results.items() 
                           if result.get('status') == 'success']
        failed_agents = [name for name, result in all_results.items() 
                        if result.get('status') == 'failed']
        
        synthesis = f"""## ğŸ“Š ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì¢…í•©

### ğŸ¯ ìš”ì²­ ì‚¬í•­
{original_request}

### âœ… ì™„ë£Œëœ ë¶„ì„ ë‹¨ê³„
"""
        
        for agent_name in successful_agents:
            result = all_results[agent_name]
            synthesis += f"- **{agent_name}**: {result.get('summary', 'ì‘ì—… ì™„ë£Œ')}\n"
        
        if failed_agents:
            synthesis += f"\n### âš ï¸ ì¼ë¶€ ì œí•œì‚¬í•­\n"
            for agent_name in failed_agents:
                result = all_results[agent_name]
                synthesis += f"- **{agent_name}**: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n"
        
        synthesis += f"""

### ğŸ‰ ê²°ë¡ 
ì´ {len(successful_agents)}ê°œì˜ ë¶„ì„ ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. 
ê° ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ê²°ê³¼ë¬¼ê³¼ ì•„í‹°íŒ©íŠ¸ë¥¼ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ë” ìì„¸í•œ ë¶„ì„ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ìš”ì²­í•´ ì£¼ì„¸ìš”.
"""
        
        return synthesis

    def _create_beautiful_plan_display(self, execution_plan: Dict, understanding: Dict) -> str:
        """ì˜ˆìœ ì‹¤í–‰ ê³„íš í‘œì‹œ ìƒì„±"""
        
        plan_display = f"""
## ğŸ“‹ LLM ê¸°ë°˜ ë™ì  ì‹¤í–‰ ê³„íš

### ğŸ¯ ë¶„ì„ ê°œìš”
- **ë„ë©”ì¸**: {understanding.get('domain', 'ë°ì´í„° ë¶„ì„')}
- **ëª©í‘œ**: {', '.join(understanding.get('key_objectives', ['ë°ì´í„° ë¶„ì„ ìˆ˜í–‰']))}
- **ë¶„ì„ ê¹Šì´**: {understanding.get('analysis_depth', 'intermediate')}
- **ì´ ë‹¨ê³„**: {len(execution_plan.get('steps', []))}ê°œ

### ğŸš€ ì‹¤í–‰ ë‹¨ê³„ë³„ ê³„íš

"""
        
        for i, step in enumerate(execution_plan.get('steps', [])):
            step_num = i + 1
            agent_name = step.get('agent', step.get('agent_name', 'unknown'))
            purpose = step.get('purpose', '')
            task = step.get('enriched_task', step.get('task_description', ''))
            expected = step.get('expected_output', '')
            
            plan_display += f"""**{step_num}. {agent_name} ì—ì´ì „íŠ¸**
   - ğŸ¯ **ëª©ì **: {purpose}
   - ğŸ“ **ì‘ì—…**: {task[:150]}{'...' if len(task) > 150 else ''}
   - ğŸ“Š **ì˜ˆìƒ ê²°ê³¼**: {expected}

"""
        
        plan_display += f"""
### ğŸ§  ê³„íš ê·¼ê±°
{execution_plan.get('reasoning', 'ì‚¬ìš©ì ìš”ì²­ì— ìµœì í™”ëœ ë¶„ì„ ì›Œí¬í”Œë¡œìš°')}

---
"""
        
        return plan_display

    def _create_beautiful_final_display(self, final_response: str, execution_plan: Dict, 
                                      agent_results: Dict, understanding: Dict) -> str:
        """ì˜ˆìœ ìµœì¢… ê²°ê³¼ í‘œì‹œ ìƒì„±"""
        
        successful_agents = [name for name, result in agent_results.items() 
                           if result.get('status') == 'success']
        failed_agents = [name for name, result in agent_results.items() 
                        if result.get('status') == 'failed']
        
        final_display = f"""
## ğŸ‰ LLM ê¸°ë°˜ ì¢…í•© ë¶„ì„ ê²°ê³¼

### ğŸ“ˆ ì‹¤í–‰ ì„±ê³¼
- âœ… **ì„±ê³µí•œ ë‹¨ê³„**: {len(successful_agents)}ê°œ
- âŒ **ì‹¤íŒ¨í•œ ë‹¨ê³„**: {len(failed_agents)}ê°œ
- ğŸ“Š **ì „ì²´ ì„±ê³µë¥ **: {(len(successful_agents) / len(agent_results) * 100):.1f}%

### ğŸ” ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼
"""
        
        for i, step in enumerate(execution_plan.get('steps', [])):
            step_num = i + 1
            agent_name = step.get('agent', step.get('agent_name', 'unknown'))
            result = agent_results.get(agent_name, {})
            status = result.get('status', 'unknown')
            summary = result.get('summary', 'ê²°ê³¼ ì—†ìŒ')
            
            status_icon = "âœ…" if status == 'success' else "âŒ" if status == 'failed' else "âš ï¸"
            
            final_display += f"""
**{step_num}. {agent_name}** {status_icon}
   - ğŸ“ **ê²°ê³¼**: {summary}
"""
        
        final_display += f"""

### ğŸ¯ ìµœì¢… ì¢…í•© ë¶„ì„

{final_response}

---
*ğŸ¤– AI DS Team LLM Powered Dynamic Orchestrator v6*
"""
        
        return final_display


def create_llm_powered_orchestrator_server():
    """LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„œë²„ ìƒì„±"""
    
    agent_card = AgentCard(
        name="AI DS Team LLM Powered Dynamic Orchestrator v6",
        description="LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›",
        url="http://localhost:8100",
        version="6.0.0",
        capabilities=AgentCapabilities(
            streaming=True,
            pushNotifications=True,
            stateTransitionHistory=True
        ),
        defaultInputModes=["text/plain"],
        defaultOutputModes=["text/plain", "application/json"],
        skills=[
            AgentSkill(
                id="llm_powered_orchestration",
                name="LLM Powered Dynamic Context-Aware Orchestration",
                description="LLMì´ ì‚¬ìš©ì ìš”ì²­ì„ ê¹Šì´ ì´í•´í•˜ê³  ë™ì ìœ¼ë¡œ ìµœì ì˜ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ì—¬ AI DS Team ì—ì´ì „íŠ¸ë“¤ì„ ì¡°ì •í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
                tags=["llm-powered", "dynamic-orchestration", "context-aware", "streaming", "multi-agent", "data-science"]
            )
        ]
    )
    
    executor = LLMPoweredOrchestratorExecutor()
    task_store = InMemoryTaskStore()
    request_handler = DefaultRequestHandler(
        agent_executor=executor,
        task_store=task_store
    )
    
    return A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting LLM Powered Dynamic Context-Aware Orchestrator v6.0")
    
    app = create_llm_powered_orchestrator_server()
    
    uvicorn.run(
        app.build(),
        host="0.0.0.0",
        port=8100,
        log_level="info"
    )


if __name__ == "__main__":
    main() 