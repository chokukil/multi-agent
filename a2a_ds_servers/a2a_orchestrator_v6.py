#!/usr/bin/env python3
"""
A2A Orchestrator v6.0 - LLM Powered Dynamic Context-Aware Orchestrator
LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
"""

import asyncio
import json
import logging
import os
import time
from typing import Any, Dict, List, Optional

import httpx
import uvicorn
from openai import AsyncOpenAI

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# AI DS Team ì—ì´ì „íŠ¸ í¬íŠ¸ ë§¤í•‘
AGENT_PORTS = {
    "data_cleaning": 8306,
    "data_loader": 8307, 
    "data_visualization": 8308,
    "data_wrangling": 8309,
    "eda_tools": 8310,
    "feature_engineering": 8311,
    "h2o_modeling": 8312,
    "mlflow_tracking": 8313,
    "sql_database": 8314
}


class StreamingTaskUpdater(TaskUpdater):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸ ì§€ì›"""
    
    async def stream_update(self, content: str):
        """ì¤‘ê°„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°"""
        await self.update_status(
            TaskState.working,
            message=self.new_agent_message(parts=[TextPart(text=content)])
        )
    
    async def stream_final_response(self, response: str):
        """ìµœì¢… ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë°"""
        # Markdown ì„¹ì…˜ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        sections = response.split('\n\n')
        
        for i, section in enumerate(sections):
            if section.strip():
                await self.update_status(
                    TaskState.working,
                    message=self.new_agent_message(parts=[TextPart(text=section)])
                )
                await asyncio.sleep(0.1)  # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°
        
        # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        await self.update_status(
            TaskState.completed,
            message=self.new_agent_message(parts=[TextPart(text="âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")])
        )


class LLMPoweredOrchestratorExecutor(AgentExecutor):
    """LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì˜µì…˜)
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key and api_key.strip():
                self.openai_client = AsyncOpenAI(api_key=api_key)
                logger.info("ğŸ¤– LLM Powered Dynamic Orchestrator v6 with OpenAI integration")
            else:
                self.openai_client = None
                logger.info("ğŸ“Š Standard Orchestrator v6 (OpenAI API key not found)")
        except Exception as e:
            logger.warning(f"OpenAI client initialization failed: {e}")
            self.openai_client = None
        
        self.available_agents = {}
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """LLMì´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´í•´í•˜ê³  ì¡°ì •í•˜ëŠ” ì‹¤í–‰"""
        task_updater = StreamingTaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            user_input = context.get_user_input()
            logger.info(f"ğŸ“¥ Processing orchestration query: {user_input}")
            
            if not user_input:
                user_input = "Please provide an analysis request."
            
            # ì—ì´ì „íŠ¸ ë°œê²¬
            await task_updater.stream_update("ğŸ” AI DS Team ì—ì´ì „íŠ¸ë“¤ì„ ë°œê²¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            self.available_agents = await self._discover_agents()
            
            if not self.available_agents:
                await task_updater.update_status(
                    TaskState.failed,
                    message=task_updater.new_agent_message(parts=[TextPart(text="âŒ ì‚¬ìš© ê°€ëŠ¥í•œ A2A ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")])
                )
                return
            
            await task_updater.stream_update(f"âœ… {len(self.available_agents)}ê°œ ì—ì´ì „íŠ¸ ë°œê²¬ ì™„ë£Œ")
            
            # 1. ì‚¬ìš©ì ì…ë ¥ì„ LLMì´ ì™„ì „íˆ ì´í•´
            await task_updater.stream_update("ğŸ§  ì‚¬ìš©ì ìš”ì²­ì„ ì‹¬ì¸µ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            request_understanding = await self._understand_request(user_input)
            
            # 2. ë™ì  ì‹¤í–‰ ê³„íš ìƒì„±
            await task_updater.stream_update("ğŸ“‹ ë™ì  ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            execution_plan = await self._create_dynamic_plan(
                request_understanding, 
                self.available_agents
            )
            
            if not execution_plan or not execution_plan.get('steps'):
                execution_plan = self._create_fallback_plan(self.available_agents)
            
            await task_updater.stream_update(f"âœ… {len(execution_plan.get('steps', []))}ë‹¨ê³„ ì‹¤í–‰ ê³„íš ì™„ë£Œ")
            
            # ê³„íšì„ ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡ (í´ë¼ì´ì–¸íŠ¸ íŒŒì‹±ìš©)
            plan_artifact = {
                "plan_executed": [
                    {
                        "step": i + 1,
                        "agent": step.get('agent', step.get('agent_name', 'unknown')),
                        "task_description": step.get('enriched_task', step.get('purpose', '')),
                        "reasoning": step.get('purpose', ''),
                        "expected_output": step.get('expected_output', '')
                    }
                    for i, step in enumerate(execution_plan.get('steps', []))
                ]
            }
            
            # ì•„í‹°íŒ©íŠ¸ë¡œ ê³„íš ì „ì†¡
            try:
                await task_updater.add_artifact(
                    parts=[TextPart(text=json.dumps(plan_artifact, ensure_ascii=False))],
                    name="execution_plan",
                    metadata={
                        "content_type": "application/json",
                        "plan_type": "ai_ds_team_orchestration"
                    }
                )
            except Exception as artifact_error:
                logger.warning(f"Failed to send plan artifact: {artifact_error}")
            
            # 3. ê° ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
            agent_results = {}
            for i, step in enumerate(execution_plan.get('steps', [])):
                step_num = i + 1
                agent_name = step.get('agent', step.get('agent_name', 'unknown'))
                
                await task_updater.stream_update(f"ğŸ”„ ë‹¨ê³„ {step_num}: {agent_name} ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘...")
                
                try:
                    result = await self._execute_agent_with_context(
                        agent_name=agent_name,
                        task=step.get('enriched_task', step.get('task_description', '')),
                        full_context=request_understanding,
                        previous_results=agent_results
                    )
                    agent_results[agent_name] = result
                    
                    # ì‹¤ì‹œê°„ í”¼ë“œë°±
                    summary = result.get('summary', 'Processing completed')
                    await task_updater.stream_update(f"âœ… {agent_name}: {summary}")
                    
                except Exception as agent_error:
                    logger.warning(f"Agent {agent_name} execution failed: {agent_error}")
                    agent_results[agent_name] = {
                        'status': 'failed',
                        'error': str(agent_error),
                        'summary': f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(agent_error)}"
                    }
                    await task_updater.stream_update(f"âš ï¸ {agent_name}: ì˜¤ë¥˜ ë°œìƒí•˜ì˜€ìœ¼ë‚˜ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤")
            
            # 4. LLMì´ ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
            await task_updater.stream_update("ğŸ¯ ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            
            final_response = await self._synthesize_with_llm(
                original_request=user_input,
                understanding=request_understanding,
                all_results=agent_results,
                task_updater=task_updater
            )
            
            # 5. ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ì „ë‹¬
            await task_updater.stream_final_response(final_response)
            
        except Exception as e:
            logger.error(f"Error in LLM Powered Orchestrator: {e}", exc_info=True)
            await task_updater.update_status(
                TaskState.failed,
                message=task_updater.new_agent_message(parts=[TextPart(text=f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")])
            )

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Cancel the operation"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.reject()
        logger.info(f"Operation cancelled for context {context.context_id}")

    async def _discover_agents(self) -> Dict[str, Dict[str, Any]]:
        """A2A í‘œì¤€ ì—ì´ì „íŠ¸ ë°œê²¬"""
        available_agents = {}
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            for agent_name, port in AGENT_PORTS.items():
                try:
                    response = await client.get(f"http://localhost:{port}/.well-known/agent.json")
                    if response.status_code == 200:
                        agent_card = response.json()
                        available_agents[agent_name] = {
                            "name": agent_card.get("name", agent_name),
                            "url": f"http://localhost:{port}",
                            "port": port,
                            "description": agent_card.get("description", ""),
                            "status": "available"
                        }
                        logger.info(f"âœ… {agent_name} agent discovered on port {port}")
                except Exception as e:
                    logger.warning(f"âš ï¸ {agent_name} agent on port {port} not available: {e}")
        
        logger.info(f"ğŸ” Total discovered agents: {len(available_agents)}")
        return available_agents

    async def _understand_request(self, user_input: str) -> Dict[str, Any]:
        """LLMì´ ìš”ì²­ì„ ê¹Šì´ ì´í•´í•˜ê³  êµ¬ì¡°í™”"""
        
        if not self.openai_client:
            # Fallback: ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                "domain": "ë°ì´í„° ë¶„ì„",
                "expertise_claimed": "ì¼ë°˜ ì‚¬ìš©ì",
                "key_objectives": ["ë°ì´í„° ë¶„ì„ ìˆ˜í–‰"],
                "required_outputs": ["ë¶„ì„ ê²°ê³¼"],
                "domain_context": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„",
                "data_mentioned": "ì—…ë¡œë“œëœ ë°ì´í„°",
                "analysis_depth": "intermediate",
                "tone": "technical"
            }
        
        prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”í•˜ì„¸ìš”:

{user_input}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "domain": "ì‹ë³„ëœ ë„ë©”ì¸ (ì˜ˆ: ë°˜ë„ì²´, ê¸ˆìœµ, ì˜ë£Œ, ì œì¡°ì—… ë“±)",
    "expertise_claimed": "ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì „ë¬¸ì„±ì´ë‚˜ ì—­í• ",
    "key_objectives": ["ëª©í‘œ1", "ëª©í‘œ2"],
    "required_outputs": ["í•„ìš”í•œ ì‚°ì¶œë¬¼1", "ì‚°ì¶œë¬¼2"],
    "domain_context": "ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ì´ë‚˜ ê·œì¹™ ìš”ì•½",
    "data_mentioned": "ì–¸ê¸‰ëœ ë°ì´í„°ë‚˜ íŒŒì¼",
    "analysis_depth": "ìš”êµ¬ë˜ëŠ” ë¶„ì„ ê¹Šì´ (basic/intermediate/expert)",
    "tone": "ì‘ë‹µ í†¤ (technical/business/educational)"
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.warning(f"Request understanding failed: {e}")
            return {
                "domain": "ë°ì´í„° ë¶„ì„",
                "expertise_claimed": "ì¼ë°˜ ì‚¬ìš©ì",
                "key_objectives": ["ë°ì´í„° ë¶„ì„ ìˆ˜í–‰"],
                "required_outputs": ["ë¶„ì„ ê²°ê³¼"],
                "domain_context": user_input,
                "data_mentioned": "ì—…ë¡œë“œëœ ë°ì´í„°",
                "analysis_depth": "intermediate",
                "tone": "technical"
            }

    async def _create_dynamic_plan(self, understanding: Dict, 
                                  available_agents: Dict) -> Dict:
        """ìš”ì²­ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ìµœì ì˜ ì‹¤í–‰ ê³„íš ìƒì„±"""
        
        if not self.openai_client:
            return self._create_fallback_plan(available_agents)
        
        agents_info = json.dumps(
            {name: info['description'] for name, info in available_agents.items()},
            ensure_ascii=False
        )
        
        planning_prompt = f"""
ì‚¬ìš©ì ìš”ì²­ ë¶„ì„:
{json.dumps(understanding, ensure_ascii=False, indent=2)}

ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸:
{agents_info}

ì´ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ìµœì ì˜ ì—ì´ì „íŠ¸ ì‹¤í–‰ ìˆœì„œë¥¼ ê³„íší•˜ì„¸ìš”.
ê° ë‹¨ê³„ì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•´ì•¼ í•˜ëŠ”ì§€ë„ ëª…ì‹œí•˜ì„¸ìš”.

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "reasoning": "ì´ ê³„íšì„ ì„ íƒí•œ ì´ìœ ",
    "steps": [
        {{
            "agent": "ì—ì´ì „íŠ¸ëª…",
            "purpose": "ì´ ë‹¨ê³„ì˜ ëª©ì ",
            "enriched_task": "êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œ (ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)",
            "expected_output": "ì˜ˆìƒ ì‚°ì¶œë¬¼",
            "pass_to_next": ["ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•  ì •ë³´ë“¤"]
        }}
    ],
    "final_synthesis_guide": "ìµœì¢… ì¢…í•© ì‹œ ì¤‘ì ì‚¬í•­"
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": planning_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.warning(f"Dynamic planning failed: {e}")
            return self._create_fallback_plan(available_agents)

    async def _execute_agent_with_context(self, agent_name: str, task: str, 
                                        full_context: Dict, previous_results: Dict) -> Dict:
        """ê° ì—ì´ì „íŠ¸ì— í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì‘ì—… ì „ë‹¬"""
        
        if agent_name not in self.available_agents:
            return {
                'status': 'failed',
                'error': f'Agent {agent_name} not available',
                'summary': f'ì—ì´ì „íŠ¸ {agent_name}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }
        
        # LLMì´ ì´ì „ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ë³´ê°•
        enriched_prompt = await self._enrich_agent_task(
            agent_name, task, full_context, previous_results
        )
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        agent_url = self.available_agents[agent_name]['url']
        
        payload = {
            "jsonrpc": "2.0",
            "method": "message/send",
            "params": {
                "message": {
                    "messageId": f"req_{agent_name}_{int(time.time())}",
                    "role": "user",
                    "parts": [{
                        "kind": "text",
                        "text": enriched_prompt
                    }]
                }
            },
            "id": f"req_{agent_name}_{int(time.time())}"
        }
        
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    agent_url,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return self._parse_agent_response(result, agent_name)
                else:
                    return {
                        'status': 'failed',
                        'error': f'HTTP {response.status_code}',
                        'summary': f'ì—ì´ì „íŠ¸ í˜¸ì¶œ ì‹¤íŒ¨ (HTTP {response.status_code})'
                    }
                    
        except Exception as e:
            logger.error(f"Agent {agent_name} execution error: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'summary': f'ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }

    async def _enrich_agent_task(self, agent_name: str, base_task: str, 
                                context: Dict, previous_results: Dict) -> str:
        """LLMì´ ê° ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ë³´ê°•"""
        
        if not self.openai_client:
            return base_task
        
        # ì´ì „ ì—ì´ì „íŠ¸ë“¤ì˜ í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
        previous_insights = self._extract_key_insights(previous_results)
        
        enrichment_prompt = f"""
ì—ì´ì „íŠ¸: {agent_name}
ê¸°ë³¸ ì‘ì—…: {base_task}

ì „ì²´ ë§¥ë½:
- ë„ë©”ì¸: {context['domain']}
- ì‚¬ìš©ì ì—­í• /ì „ë¬¸ì„±: {context.get('expertise_claimed', 'ì¼ë°˜ ì‚¬ìš©ì')}
- ìµœì¢… ëª©í‘œ: {context['key_objectives']}
- ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸: {context.get('domain_context', '')}

ì´ì „ ë¶„ì„ ê²°ê³¼:
{json.dumps(previous_insights, ensure_ascii=False)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {agent_name} ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•´ì•¼ í•  êµ¬ì²´ì ì¸ ì‘ì—…ì„ ì‘ì„±í•˜ì„¸ìš”.
ë„ë©”ì¸ ì§€ì‹ê³¼ ì´ì „ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë¶„ì„ì´ ë˜ë„ë¡ ì§€ì‹œí•˜ì„¸ìš”.
"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´
                messages=[{"role": "user", "content": enrichment_prompt}],
                temperature=0.2,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.warning(f"Task enrichment failed: {e}")
            return base_task

    async def _synthesize_with_llm(self, original_request: str, 
                                  understanding: Dict, all_results: Dict,
                                  task_updater: StreamingTaskUpdater) -> str:
        """LLMì´ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±"""
        
        if not self.openai_client:
            return self._create_fallback_synthesis(original_request, all_results)
        
        # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ìš”ì•½
        results_summary = json.dumps(all_results, ensure_ascii=False, indent=2)
        
        synthesis_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ë°ì´í„° ë¶„ì„ íŒ€ì˜ ìˆ˜ì„ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        
ì‚¬ìš©ìì˜ ì›ë˜ ìš”ì²­:
{original_request}

ìš”ì²­ ë¶„ì„:
- ë„ë©”ì¸: {understanding['domain']}
- ëª©í‘œ: {', '.join(understanding['key_objectives'])}
- í•„ìš” ì‚°ì¶œë¬¼: {', '.join(understanding['required_outputs'])}

ê° ì—ì´ì „íŠ¸ì˜ ë¶„ì„ ê²°ê³¼:
{results_summary}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬:
1. ì‚¬ìš©ìê°€ ìš”ì²­í•œ ëª¨ë“  ì‚¬í•­ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”
2. ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì „ë¬¸ì ì¸ í•´ì„ì„ ì œê³µí•˜ì„¸ìš”
3. êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸ì™€ ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”
4. {understanding['tone']} í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

ì¤‘ìš”: ì‚¬ìš©ìê°€ íŠ¹ì • ì—­í• ì´ë‚˜ ì „ë¬¸ì„±ì„ ì–¸ê¸‰í–ˆë‹¤ë©´ ({understanding.get('expertise_claimed', 'N/A')}), 
ê·¸ ê´€ì ì—ì„œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”."""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ì „ë¬¸ ì§€ì‹ì„ ì¢…í•©í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": synthesis_prompt}
                ],
                temperature=0.3,
                max_tokens=4000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"LLM synthesis failed: {e}")
            return self._create_fallback_synthesis(original_request, all_results)

    def _extract_key_insights(self, previous_results: Dict) -> Dict:
        """ì´ì „ ê²°ê³¼ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = {}
        for agent_name, result in previous_results.items():
            if result.get('status') == 'success':
                insights[agent_name] = result.get('summary', 'ì‘ì—… ì™„ë£Œ')
            else:
                insights[agent_name] = f"ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        return insights

    def _parse_agent_response(self, response: Dict, agent_name: str) -> Dict:
        """ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹±"""
        try:
            if 'result' in response:
                result = response['result']
                if isinstance(result, dict):
                    # A2A í‘œì¤€ ì‘ë‹µ ì²˜ë¦¬
                    status = result.get('status', {})
                    if isinstance(status, dict) and status.get('state') == 'completed':
                        return {
                            'status': 'success',
                            'result': result,
                            'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ì—… ì™„ë£Œ'
                        }
                    else:
                        return {
                            'status': 'partial',
                            'result': result,
                            'summary': f'{agent_name} ì—ì´ì „íŠ¸ ë¶€ë¶„ ì™„ë£Œ'
                        }
                else:
                    return {
                        'status': 'success',
                        'result': result,
                        'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ì—… ì™„ë£Œ'
                    }
            else:
                return {
                    'status': 'failed',
                    'error': 'No result in response',
                    'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ë‹µ ì˜¤ë¥˜'
                }
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
                'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜'
            }

    def _create_fallback_plan(self, available_agents: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """A2A í‘œì¤€ í´ë°± ê³„íš ìƒì„±"""
        steps = []
        step_number = 1
        
        # ê¸°ë³¸ì ì¸ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš°
        basic_workflow = [
            ("data_loader", "ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ê²€ì¦", "ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ì ì¸ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤"),
            ("data_cleaning", "ë°ì´í„° í’ˆì§ˆ í™•ì¸ ë° ì •ë¦¬", "ë°ì´í„°ì˜ í’ˆì§ˆì„ í™•ì¸í•˜ê³  í•„ìš”í•œ ì •ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"),
            ("eda_tools", "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ìˆ˜í–‰", "ë°ì´í„°ì˜ ë¶„í¬ì™€ íŒ¨í„´ì„ íƒìƒ‰í•˜ê³  ê¸°ì´ˆ í†µê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"),
            ("data_visualization", "ë°ì´í„° ì‹œê°í™” ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ", "ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤")
        ]
        
        for agent_name, purpose, task_desc in basic_workflow:
            if agent_name in available_agents:
                steps.append({
                    "agent": agent_name,
                    "purpose": purpose,
                    "enriched_task": task_desc,
                    "expected_output": f"{purpose} ê²°ê³¼",
                    "pass_to_next": ["ë¶„ì„ ê²°ê³¼", "ë°ì´í„° ì •ë³´"]
                })
                step_number += 1
        
        return {
            "reasoning": "ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ í¬ê´„ì ì¸ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í‘œì¤€ ì›Œí¬í”Œë¡œìš°",
            "steps": steps,
            "final_synthesis_guide": "ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ"
        }

    def _create_fallback_synthesis(self, original_request: str, all_results: Dict) -> str:
        """í´ë°± ìµœì¢… ë‹µë³€ ìƒì„±"""
        
        successful_agents = [name for name, result in all_results.items() 
                           if result.get('status') == 'success']
        failed_agents = [name for name, result in all_results.items() 
                        if result.get('status') == 'failed']
        
        synthesis = f"""## ğŸ“Š ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì¢…í•©

### ğŸ¯ ìš”ì²­ ì‚¬í•­
{original_request}

### âœ… ì™„ë£Œëœ ë¶„ì„ ë‹¨ê³„
"""
        
        for agent_name in successful_agents:
            result = all_results[agent_name]
            synthesis += f"- **{agent_name}**: {result.get('summary', 'ì‘ì—… ì™„ë£Œ')}\n"
        
        if failed_agents:
            synthesis += f"\n### âš ï¸ ì¼ë¶€ ì œí•œì‚¬í•­\n"
            for agent_name in failed_agents:
                result = all_results[agent_name]
                synthesis += f"- **{agent_name}**: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n"
        
        synthesis += f"""
### ğŸ‰ ê²°ë¡ 
ì´ {len(successful_agents)}ê°œì˜ ë¶„ì„ ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. 
ê° ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ê²°ê³¼ë¬¼ê³¼ ì•„í‹°íŒ©íŠ¸ë¥¼ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ë” ìì„¸í•œ ë¶„ì„ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ìš”ì²­í•´ ì£¼ì„¸ìš”.
"""
        
        return synthesis


def create_llm_powered_orchestrator_server():
    """LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„œë²„ ìƒì„±"""
    
    agent_card = AgentCard(
        name="AI DS Team LLM Powered Dynamic Orchestrator v6",
        description="LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›",
        url="http://localhost:8100",
        version="6.0.0",
        capabilities=AgentCapabilities(
            streaming=True,
            pushNotifications=True,
            stateTransitionHistory=True
        ),
        defaultInputModes=["text/plain"],
        defaultOutputModes=["text/plain", "application/json"],
        skills=[
            AgentSkill(
                id="llm_powered_orchestration",
                name="LLM Powered Dynamic Context-Aware Orchestration",
                description="LLMì´ ì‚¬ìš©ì ìš”ì²­ì„ ê¹Šì´ ì´í•´í•˜ê³  ë™ì ìœ¼ë¡œ ìµœì ì˜ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ì—¬ AI DS Team ì—ì´ì „íŠ¸ë“¤ì„ ì¡°ì •í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
                tags=["llm-powered", "dynamic-orchestration", "context-aware", "streaming", "multi-agent", "data-science"]
            )
        ]
    )
    
    executor = LLMPoweredOrchestratorExecutor()
    task_store = InMemoryTaskStore()
    request_handler = DefaultRequestHandler(
        agent_executor=executor,
        task_store=task_store
    )
    
    app = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler
    )
    
    return app


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting LLM Powered Dynamic Context-Aware Orchestrator v6.0")
    
    app = create_llm_powered_orchestrator_server()
    
    uvicorn.run(
        app.build(),
        host="localhost",
        port=8100,
        log_level="info"
    )


if __name__ == "__main__":
    main() 