#!/usr/bin/env python3
"""
A2A Orchestrator v6.0 - LLM Powered Dynamic Context-Aware Orchestrator
ë²”ìš© LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
"""

import asyncio
import json
import logging
import os
import time
from typing import Any, Dict, List, Optional

import httpx
import uvicorn
from openai import AsyncOpenAI

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# AI DS Team ì—ì´ì „íŠ¸ í¬íŠ¸ ë§¤í•‘
AGENT_PORTS = {
    "data_cleaning": 8306,
    "data_loader": 8307, 
    "data_visualization": 8308,
    "data_wrangling": 8309,
    "feature_engineering": 8310,
    "sql_database": 8311,
    "eda_tools": 8312,
    "h2o_ml": 8313,
    "mlflow_tools": 8314,
}


class StreamingTaskUpdater(TaskUpdater):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸ ì§€ì›"""
    
    async def stream_update(self, content: str):
        """ì¤‘ê°„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°"""
        await self.add_message(
            self.new_agent_message(parts=[TextPart(text=content)])
        )
    
    async def stream_final_response(self, response: str):
        """ìµœì¢… ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë°"""
        # Markdown ì„¹ì…˜ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        sections = response.split('\n\n')
        
        for section in sections:
            if section.strip():
                await self.add_message(
                    self.new_agent_message(parts=[TextPart(text=section)])
                )
                await asyncio.sleep(0.1)  # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°
        
        # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        await self.update_status(TaskState.completed)


class LLMPoweredOrchestrator(AgentExecutor):
    """LLM ê¸°ë°˜ ë²”ìš© ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.openai_client = AsyncOpenAI(
            api_key=os.getenv("OPENAI_API_KEY")
        ) if os.getenv("OPENAI_API_KEY") else None
        
        self.available_agents = {}
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """LLMì´ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´í•´í•˜ê³  ì¡°ì •í•˜ëŠ” ì‹¤í–‰"""
        task_updater = StreamingTaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            user_input = context.get_user_input()
            logger.info(f"ğŸ“¥ LLM Orchestrator processing: {user_input}")
            
            if not user_input:
                user_input = "ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”."
            
            # 1. ì‚¬ìš©ì ì…ë ¥ì„ LLMì´ ì™„ì „íˆ ì´í•´
            await task_updater.stream_update("ğŸ§  ìš”ì²­ ë¶„ì„ ì¤‘...")
            request_understanding = await self._understand_request(user_input)
            
            # 2. ì—ì´ì „íŠ¸ ë°œê²¬
            await task_updater.stream_update("ğŸ” ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ íƒìƒ‰ ì¤‘...")
            self.available_agents = await self._discover_agents()
            
            if not self.available_agents:
                await task_updater.update_status(
                    TaskState.failed,
                    message=task_updater.new_agent_message(parts=[TextPart(text="âŒ ì‚¬ìš© ê°€ëŠ¥í•œ A2A ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")])
                )
                return
            
            # 3. ë™ì  ì‹¤í–‰ ê³„íš ìƒì„±
            await task_updater.stream_update("ğŸ“‹ ë™ì  ì‹¤í–‰ ê³„íš ìƒì„± ì¤‘...")
            execution_plan = await self._create_dynamic_plan(request_understanding, self.available_agents)
            
            # 4. ê° ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬)
            agent_results = {}
            for i, step in enumerate(execution_plan['steps'], 1):
                await task_updater.stream_update(f"ğŸ”„ ë‹¨ê³„ {i}/{len(execution_plan['steps'])}: {step['agent']} ì‹¤í–‰ ì¤‘...")
                
                result = await self._execute_agent_with_context(
                    agent_name=step['agent'],
                    task=step['enriched_task'],
                    full_context=request_understanding,
                    previous_results=agent_results
                )
                agent_results[step['agent']] = result
                
                # ì‹¤ì‹œê°„ í”¼ë“œë°±
                summary = result.get('summary', 'ì²˜ë¦¬ ì™„ë£Œ')
                await task_updater.stream_update(f"âœ… {step['agent']}: {summary}")
            
            # 5. LLMì´ ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±
            await task_updater.stream_update("ğŸ¯ ìµœì¢… ê²°ê³¼ ì¢…í•© ì¤‘...")
            final_response = await self._synthesize_with_llm(
                original_request=user_input,
                understanding=request_understanding,
                all_results=agent_results,
                task_updater=task_updater
            )
            
            # 6. ì‹¤í–‰ ê³„íšì„ Artifactë¡œ ì €ì¥
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(execution_plan, ensure_ascii=False, indent=2))],
                name="execution_plan",
                metadata={
                    "content_type": "application/json",
                    "plan_type": "llm_powered_orchestration"
                }
            )
            
            # 7. ìµœì¢… ì™„ë£Œ
            await task_updater.update_status(TaskState.completed)
            
        except Exception as e:
            logger.error(f"Error in LLM Orchestrator: {e}", exc_info=True)
            await task_updater.update_status(
                TaskState.failed,
                message=task_updater.new_agent_message(parts=[TextPart(text=f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")])
            )

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Cancel the operation"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.reject()
        logger.info(f"LLM Orchestrator operation cancelled for context {context.context_id}")

    async def _understand_request(self, user_input: str) -> Dict[str, Any]:
        """LLMì´ ìš”ì²­ì„ ê¹Šì´ ì´í•´í•˜ê³  êµ¬ì¡°í™”"""
        
        if not self.openai_client:
            # Fallback: ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
            return {
                "domain": "ë°ì´í„° ë¶„ì„",
                "expertise_claimed": "ì¼ë°˜ ì‚¬ìš©ì",
                "key_objectives": ["ë°ì´í„° ë¶„ì„ ìˆ˜í–‰"],
                "required_outputs": ["ë¶„ì„ ê²°ê³¼"],
                "domain_context": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„",
                "data_mentioned": "ì—…ë¡œë“œëœ ë°ì´í„°",
                "analysis_depth": "intermediate",
                "tone": "technical"
            }
        
        prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”í•˜ì„¸ìš”:

{user_input}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "domain": "ì‹ë³„ëœ ë„ë©”ì¸ (ì˜ˆ: ë°˜ë„ì²´, ê¸ˆìœµ, ì˜ë£Œ, ì œì¡°ì—… ë“±)",
    "expertise_claimed": "ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì „ë¬¸ì„±ì´ë‚˜ ì—­í• ",
    "key_objectives": ["ëª©í‘œ1", "ëª©í‘œ2", ...],
    "required_outputs": ["í•„ìš”í•œ ì‚°ì¶œë¬¼1", "ì‚°ì¶œë¬¼2", ...],
    "domain_context": "ë„ë©”ì¸ íŠ¹í™” ì§€ì‹ì´ë‚˜ ê·œì¹™ ìš”ì•½",
    "data_mentioned": "ì–¸ê¸‰ëœ ë°ì´í„°ë‚˜ íŒŒì¼",
    "analysis_depth": "ìš”êµ¬ë˜ëŠ” ë¶„ì„ ê¹Šì´ (basic/intermediate/expert)",
    "tone": "ì‘ë‹µ í†¤ (technical/business/educational)"
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=1000
            )
            
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            logger.warning(f"Request understanding failed: {e}")
            return {
                "domain": "ë°ì´í„° ë¶„ì„",
                "expertise_claimed": "ì¼ë°˜ ì‚¬ìš©ì",
                "key_objectives": [user_input],
                "required_outputs": ["ë¶„ì„ ê²°ê³¼"],
                "domain_context": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„",
                "data_mentioned": "ë°ì´í„°",
                "analysis_depth": "intermediate",
                "tone": "technical"
            }

    async def _create_dynamic_plan(self, understanding: Dict, available_agents: Dict) -> Dict:
        """ìš”ì²­ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ìµœì ì˜ ì‹¤í–‰ ê³„íš ìƒì„±"""
        
        if not self.openai_client:
            return self._create_fallback_plan(available_agents)
        
        agents_info = json.dumps(
            {name: info['description'] for name, info in available_agents.items()},
            ensure_ascii=False
        )
        
        planning_prompt = f"""
ì‚¬ìš©ì ìš”ì²­ ë¶„ì„:
{json.dumps(understanding, ensure_ascii=False, indent=2)}

ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸:
{agents_info}

ì´ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•œ ìµœì ì˜ ì—ì´ì „íŠ¸ ì‹¤í–‰ ìˆœì„œë¥¼ ê³„íší•˜ì„¸ìš”.
ê° ë‹¨ê³„ì—ì„œ ì–´ë–¤ ì •ë³´ë¥¼ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•´ì•¼ í•˜ëŠ”ì§€ë„ ëª…ì‹œí•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹:
{{
    "plan_type": "llm_powered_orchestration",
    "objective": "ë¶„ì„ ëª©í‘œ ìš”ì•½",
    "reasoning": "ì´ ê³„íšì„ ì„ íƒí•œ ì´ìœ ",
    "steps": [
        {{
            "step_number": 1,
            "agent": "ì—ì´ì „íŠ¸ëª…",
            "purpose": "ì´ ë‹¨ê³„ì˜ ëª©ì ",
            "enriched_task": "êµ¬ì²´ì ì¸ ì‘ì—… ì§€ì‹œ (ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)",
            "expected_output": "ì˜ˆìƒ ì‚°ì¶œë¬¼",
            "pass_to_next": ["ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•  ì •ë³´ë“¤"]
        }}
    ],
    "final_synthesis_guide": "ìµœì¢… ì¢…í•© ì‹œ ì¤‘ì ì‚¬í•­"
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": planning_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                max_tokens=2000
            )
            
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            logger.warning(f"Dynamic planning failed: {e}")
            return self._create_fallback_plan(available_agents)

    async def _execute_agent_with_context(self, agent_name: str, task: str, 
                                        full_context: Dict, previous_results: Dict) -> Dict:
        """ê° ì—ì´ì „íŠ¸ì— í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì‘ì—… ì „ë‹¬"""
        
        # LLMì´ ì´ì „ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ë³´ê°•
        enriched_prompt = await self._enrich_agent_task(
            agent_name, task, full_context, previous_results
        )
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        if agent_name not in self.available_agents:
            return {"error": f"Agent {agent_name} not available", "summary": "ì—ì´ì „íŠ¸ ì‚¬ìš© ë¶ˆê°€"}
        
        agent_url = self.available_agents[agent_name]['url']
        
        payload = {
            "jsonrpc": "2.0",
            "method": "message/send",
            "params": {
                "message": {
                    "messageId": f"llm_orchestrator_{agent_name}_{int(time.time())}",
                    "role": "user",
                    "parts": [{
                        "kind": "text",
                        "text": enriched_prompt
                    }]
                }
            },
            "id": f"req_{agent_name}_{int(time.time())}"
        }
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(agent_url, json=payload)
                
                if response.status_code == 200:
                    result = self._parse_agent_response(response.json())
                    return result
                else:
                    return {"error": f"HTTP {response.status_code}", "summary": "ì—ì´ì „íŠ¸ ì‘ë‹µ ì˜¤ë¥˜"}
                    
        except Exception as e:
            logger.error(f"Error executing agent {agent_name}: {e}")
            return {"error": str(e), "summary": "ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨"}

    async def _enrich_agent_task(self, agent_name: str, base_task: str, 
                                context: Dict, previous_results: Dict) -> str:
        """LLMì´ ê° ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ë³´ê°•"""
        
        if not self.openai_client:
            return base_task
        
        # ì´ì „ ì—ì´ì „íŠ¸ë“¤ì˜ í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
        previous_insights = self._extract_key_insights(previous_results)
        
        enrichment_prompt = f"""
ì—ì´ì „íŠ¸: {agent_name}
ê¸°ë³¸ ì‘ì—…: {base_task}

ì „ì²´ ë§¥ë½:
- ë„ë©”ì¸: {context['domain']}
- ì‚¬ìš©ì ì—­í• /ì „ë¬¸ì„±: {context.get('expertise_claimed', 'ì¼ë°˜ ì‚¬ìš©ì')}
- ìµœì¢… ëª©í‘œ: {context['key_objectives']}
- ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸: {context.get('domain_context', '')}

ì´ì „ ë¶„ì„ ê²°ê³¼:
{json.dumps(previous_insights, ensure_ascii=False)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {agent_name} ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•´ì•¼ í•  êµ¬ì²´ì ì¸ ì‘ì—…ì„ ì‘ì„±í•˜ì„¸ìš”.
ë„ë©”ì¸ ì§€ì‹ê³¼ ì´ì „ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë¶„ì„ì´ ë˜ë„ë¡ ì§€ì‹œí•˜ì„¸ìš”.
"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´
                messages=[{"role": "user", "content": enrichment_prompt}],
                temperature=0.2,
                max_tokens=1000
            )
            
            return response.choices[0].message.content
        except Exception as e:
            logger.warning(f"Task enrichment failed: {e}")
            return base_task

    async def _synthesize_with_llm(self, original_request: str, understanding: Dict, 
                                  all_results: Dict, task_updater: StreamingTaskUpdater) -> str:
        """LLMì´ ëª¨ë“  ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±"""
        
        if not self.openai_client:
            # Fallback: ê°„ë‹¨í•œ ìš”ì•½
            summary = "## ë¶„ì„ ê²°ê³¼ ìš”ì•½\n\n"
            for agent, result in all_results.items():
                summary += f"### {agent}\n{result.get('summary', 'ë¶„ì„ ì™„ë£Œ')}\n\n"
            await task_updater.stream_update(summary)
            return summary
        
        # ê° ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ ìš”ì•½
        results_summary = json.dumps(all_results, ensure_ascii=False, indent=2)
        
        synthesis_prompt = f"""ë‹¹ì‹ ì€ ì „ë¬¸ ë°ì´í„° ë¶„ì„ íŒ€ì˜ ìˆ˜ì„ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
        
ì‚¬ìš©ìì˜ ì›ë˜ ìš”ì²­:
{original_request}

ìš”ì²­ ë¶„ì„:
- ë„ë©”ì¸: {understanding['domain']}
- ëª©í‘œ: {', '.join(understanding['key_objectives'])}
- í•„ìš” ì‚°ì¶œë¬¼: {', '.join(understanding['required_outputs'])}

ê° ì—ì´ì „íŠ¸ì˜ ë¶„ì„ ê²°ê³¼:
{results_summary}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬:
1. ì‚¬ìš©ìê°€ ìš”ì²­í•œ ëª¨ë“  ì‚¬í•­ì— ëŒ€í•´ ë‹µë³€í•˜ì„¸ìš”
2. ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì „ë¬¸ì ì¸ í•´ì„ì„ ì œê³µí•˜ì„¸ìš”
3. êµ¬ì²´ì ì¸ ì¸ì‚¬ì´íŠ¸ì™€ ì‹¤í–‰ ê°€ëŠ¥í•œ ê¶Œê³ ì‚¬í•­ì„ í¬í•¨í•˜ì„¸ìš”
4. {understanding['tone']} í†¤ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”

ì¤‘ìš”: ì‚¬ìš©ìê°€ íŠ¹ì • ì—­í• ì´ë‚˜ ì „ë¬¸ì„±ì„ ì–¸ê¸‰í–ˆë‹¤ë©´ ({understanding.get('expertise_claimed', 'N/A')}), 
ê·¸ ê´€ì ì—ì„œ ë¶„ì„í•˜ê³  ë‹µë³€í•˜ì„¸ìš”."""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ì „ë¬¸ ì§€ì‹ì„ ì¢…í•©í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": synthesis_prompt}
                ],
                temperature=0.3,
                max_tokens=4000,
                stream=True  # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            )
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ì „ì†¡
            full_response = ""
            current_chunk = ""
            
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    full_response += content
                    current_chunk += content
                    
                    # ë¬¸ì¥ì´ ì™„ë£Œë˜ë©´ ìŠ¤íŠ¸ë¦¬ë°
                    if content in ['.', '!', '?', '\n\n']:
                        await task_updater.stream_update(current_chunk)
                        current_chunk = ""
            
            # ë§ˆì§€ë§‰ ë‚¨ì€ ë‚´ìš© ì „ì†¡
            if current_chunk:
                await task_updater.stream_update(current_chunk)
            
            return full_response
            
        except Exception as e:
            logger.error(f"LLM synthesis failed: {e}")
            # Fallback ì‘ë‹µ
            fallback = f"## ğŸ“Š ë¶„ì„ ì™„ë£Œ\n\nìš”ì²­: {original_request}\n\n"
            for agent, result in all_results.items():
                fallback += f"**{agent}**: {result.get('summary', 'ë¶„ì„ ì™„ë£Œ')}\n\n"
            await task_updater.stream_update(fallback)
            return fallback

    async def _discover_agents(self) -> Dict[str, Dict[str, Any]]:
        """A2A í‘œì¤€ ì—ì´ì „íŠ¸ ë°œê²¬"""
        available_agents = {}
        
        async with httpx.AsyncClient(timeout=5.0) as client:
            for agent_name, port in AGENT_PORTS.items():
                try:
                    response = await client.get(f"http://localhost:{port}/.well-known/agent.json")
                    if response.status_code == 200:
                        agent_card = response.json()
                        available_agents[agent_name] = {
                            "name": agent_card.get("name", agent_name),
                            "url": f"http://localhost:{port}",
                            "port": port,
                            "description": agent_card.get("description", ""),
                            "status": "available"
                        }
                        logger.info(f"âœ… {agent_name} agent discovered on port {port}")
                except Exception as e:
                    logger.warning(f"âš ï¸ {agent_name} agent on port {port} not available: {e}")
        
        logger.info(f"ğŸ” Total discovered agents: {len(available_agents)}")
        return available_agents

    def _extract_key_insights(self, previous_results: Dict) -> Dict:
        """ì´ì „ ê²°ê³¼ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = {}
        for agent, result in previous_results.items():
            if isinstance(result, dict):
                insights[agent] = {
                    "summary": result.get("summary", ""),
                    "key_findings": result.get("key_findings", []),
                    "status": result.get("status", "completed")
                }
        return insights

    def _parse_agent_response(self, response_data: Dict) -> Dict:
        """ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹±"""
        try:
            if "result" in response_data and "status" in response_data["result"]:
                status = response_data["result"]["status"]
                if "message" in status and "parts" in status["message"]:
                    parts = status["message"]["parts"]
                    if parts and "text" in parts[0]:
                        text = parts[0]["text"]
                        return {
                            "content": text,
                            "summary": text[:200] + "..." if len(text) > 200 else text,
                            "status": "completed"
                        }
            
            return {"content": str(response_data), "summary": "ì‘ë‹µ ìˆ˜ì‹ ", "status": "completed"}
        except Exception as e:
            return {"error": str(e), "summary": "ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨", "status": "failed"}

    def _create_fallback_plan(self, available_agents: Dict) -> Dict:
        """Fallback ì‹¤í–‰ ê³„íš ìƒì„±"""
        steps = []
        step_num = 1
        
        # ê¸°ë³¸ ìˆœì„œ: ë°ì´í„° ë¡œë”© -> EDA -> ì‹œê°í™”
        preferred_order = ["data_loader", "eda_tools", "data_visualization"]
        
        for agent_name in preferred_order:
            if agent_name in available_agents:
                steps.append({
                    "step_number": step_num,
                    "agent": agent_name,
                    "purpose": f"{agent_name} ë¶„ì„ ìˆ˜í–‰",
                    "enriched_task": f"{agent_name} ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.",
                    "expected_output": "ë¶„ì„ ê²°ê³¼",
                    "pass_to_next": ["ë¶„ì„ ê²°ê³¼"]
                })
                step_num += 1
        
        return {
            "plan_type": "llm_powered_orchestration",
            "objective": "ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ìˆ˜í–‰",
            "reasoning": "ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë¥¼ í™œìš©í•œ ê¸°ë³¸ ë¶„ì„ ê³„íš",
            "steps": steps,
            "final_synthesis_guide": "ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ ì œê³µ"
        }


def create_llm_orchestrator_server():
    """LLM ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„œë²„ ìƒì„±"""
    


def create_llm_orchestrator_server():
    """LLM ê¸°ë°˜ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„œë²„ ìƒì„±"""
    
    # Agent Card ì •ì˜
    agent_card = AgentCard(
        name="AI_DS_Team LLM Orchestrator v6",
        description="LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë²”ìš© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ê¹Šì´ ì´í•´í•˜ê³ , ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ìµœì ì˜ ì—ì´ì „íŠ¸ ì‹¤í–‰ ê³„íšì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.",
        url="http://localhost:8100",
        version="6.0.0",
        capabilities=AgentCapabilities(
            streaming=True,
            pushNotifications=False,
            stateTransitionHistory=True
        ),
        defaultInputModes=["text/plain"],
        defaultOutputModes=["text/plain", "application/json"],
        skills=[
            AgentSkill(
                id="llm-orchestration",
                name="LLM Powered Dynamic Orchestration",
                description="LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë²”ìš© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°. ì‚¬ìš©ì ìš”ì²­ì„ ê¹Šì´ ì´í•´í•˜ê³  ìµœì ì˜ ì—ì´ì „íŠ¸ ì‹¤í–‰ ê³„íšì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.",
                tags=["orchestration", "llm", "dynamic", "context-aware", "ai"]
            )
        ]
    )
    
    # Executorì™€ Request Handler ìƒì„±
    executor = LLMPoweredOrchestrator()
    task_store = InMemoryTaskStore()
    request_handler = DefaultRequestHandler(
        agent_executor=executor,
        task_store=task_store
    )
    
    # A2A Server ìƒì„± (v5ì™€ ë™ì¼í•œ ë°©ì‹)
    app = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler
    )
    
    return app


def main():
    """LLM ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„œë²„ ì‹¤í–‰"""
    logger.info("ğŸš€ Starting AI_DS_Team LLM Orchestrator v6...")
    
    # OpenAI API í‚¤ í™•ì¸
    if not os.getenv("OPENAI_API_KEY"):
        logger.warning("âš ï¸ OPENAI_API_KEY not found. LLM features will use fallback mode.")
    else:
        logger.info("âœ… OpenAI API key found. LLM features enabled.")
    
    app = create_llm_orchestrator_server()
    
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        app.build(),
        host="localhost",
        port=8100,
        log_level="info"
    )


if __name__ == "__main__":
    main()
