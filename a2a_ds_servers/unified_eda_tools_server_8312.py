#!/usr/bin/env python3
"""
CherryAI Unified EDA Tools Server - Port 8312
A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜ + UnifiedDataInterface íŒ¨í„´

ğŸ“Š í•µì‹¬ ê¸°ëŠ¥:
- ğŸ” LLM ê¸°ë°˜ ì§€ëŠ¥í˜• íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì „ëµ
- ğŸ“ˆ í†µê³„ ê³„ì‚° ì˜¤ë¥˜ ì™„ì „ í•´ê²° (ì„¤ê³„ ë¬¸ì„œ ì£¼ìš” ë¬¸ì œì  í•´ê²°)
- ğŸ“‹ í¬ê´„ì  5ë‹¨ê³„ EDA í”„ë¡œì„¸ìŠ¤ (ê¸°ìˆ í†µê³„, ë¶„í¬, ìƒê´€ê´€ê³„, ì´ìƒê°’, íŒ¨í„´)
- ğŸ¨ Interactive ì‹œê°í™” + í†µê³„ ë¦¬í¬íŠ¸ í†µí•©
- ğŸ§  LLM ì¸ì‚¬ì´íŠ¸ ìƒì„± ë° í•´ì„
- ğŸ¯ A2A í‘œì¤€ TaskUpdater + ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

ê¸°ë°˜: pandas_agent íŒ¨í„´ + unified_data_loader ì„±ê³µ ì‚¬ë¡€
"""

import asyncio
import logging
import os
import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import pandas as pd
import numpy as np
import plotly.graph_objects as go
import plotly.express as px
import plotly.figure_factory as ff
from plotly.subplots import make_subplots
import scipy.stats as stats
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A SDK 0.2.9 í‘œì¤€ imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard, AgentSkill, AgentCapabilities,
    TaskState, TextPart
)
from a2a.utils import new_agent_text_message
import uvicorn

# CherryAI Core imports
from core.llm_factory import LLMFactory
from a2a_ds_servers.unified_data_system.core.unified_data_interface import UnifiedDataInterface
from a2a_ds_servers.unified_data_system.core.smart_dataframe import SmartDataFrame
from a2a_ds_servers.unified_data_system.core.llm_first_data_engine import LLMFirstDataEngine
from a2a_ds_servers.unified_data_system.core.cache_manager import CacheManager
from a2a_ds_servers.unified_data_system.utils.file_scanner import FileScanner

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class EDAInsight:
    """EDA ì¸ì‚¬ì´íŠ¸ ì •ì˜"""
    category: str  # statistical, distribution, correlation, outlier, pattern
    finding: str
    confidence: float
    evidence: Dict[str, Any]
    recommendation: str

@dataclass
class StatisticalAnalysis:
    """í†µê³„ ë¶„ì„ ê²°ê³¼"""
    descriptive_stats: Dict[str, Any]
    distribution_analysis: Dict[str, Any]
    correlation_analysis: Dict[str, Any]
    outlier_analysis: Dict[str, Any]
    pattern_analysis: Dict[str, Any]

class UnifiedEDAToolsExecutor(AgentExecutor, UnifiedDataInterface):
    """
    Unified EDA Tools Executor
    
    pandas_agent íŒ¨í„´ + data_loader ì„±ê³µ ì‚¬ë¡€ ê¸°ë°˜
    - LLM First íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
    - í†µê³„ ê³„ì‚° ì•ˆì •ì„± ë³´ì¥
    - í¬ê´„ì  5ë‹¨ê³„ EDA í”„ë¡œì„¸ìŠ¤
    - A2A SDK 0.2.9 ì™„ì „ ì¤€ìˆ˜
    """
    
    def __init__(self):
        super().__init__()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.data_engine = LLMFirstDataEngine()
        self.cache_manager = CacheManager()
        self.file_scanner = FileScanner()
        self.llm_factory = LLMFactory()
        
        # EDA ì „ë¬¸ ì„¤ì •
        self.eda_components = {
            'descriptive_statistics': [
                'basic_stats', 'central_tendency', 'variability', 'shape_measures',
                'quantiles', 'missing_value_analysis'
            ],
            'distribution_analysis': [
                'histograms', 'density_plots', 'qq_plots', 'normality_tests',
                'skewness_kurtosis', 'distribution_fitting'
            ],
            'correlation_analysis': [
                'correlation_matrix', 'correlation_heatmap', 'scatter_matrix',
                'partial_correlations', 'association_measures'
            ],
            'outlier_analysis': [
                'box_plots', 'z_score_analysis', 'iqr_analysis', 'isolation_forest',
                'local_outlier_factor', 'outlier_visualization'
            ],
            'pattern_analysis': [
                'trend_analysis', 'seasonality_detection', 'clustering_tendency',
                'feature_importance', 'anomaly_detection'
            ]
        }
        
        # í†µê³„ ì•ˆì „ì„± ì„¤ì • (í•µì‹¬ ë¬¸ì œ í•´ê²°)
        self.statistical_safety = {
            'min_sample_size': 5,  # ìµœì†Œ ìƒ˜í”Œ í¬ê¸°
            'max_categories': 50,   # ì¹´í…Œê³ ë¦¬ ìµœëŒ€ ê°œìˆ˜
            'correlation_threshold': 0.01,  # ìƒê´€ê´€ê³„ ì„ê³„ê°’
            'outlier_detection_methods': ['iqr', 'zscore', 'isolation'],
            'normality_test_methods': ['shapiro', 'jarque_bera', 'kolmogorov']
        }
        
        # ì‹œê°í™” ì„¤ì •
        self.visualization_themes = {
            'default': 'plotly_white',
            'professional': 'plotly',
            'minimal': 'simple_white',
            'academic': 'ggplot2'
        }
        
        logger.info("âœ… Unified EDA Tools Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """
        A2A í‘œì¤€ ì‹¤í–‰: 7ë‹¨ê³„ í¬ê´„ì  íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ í”„ë¡œì„¸ìŠ¤
        
        ğŸ” 1ë‹¨ê³„: LLM EDA ì˜ë„ ë¶„ì„
        ğŸ“‚ 2ë‹¨ê³„: ë°ì´í„° ê²€ìƒ‰ ë° ì•ˆì „ ë¡œë”©
        ğŸ“Š 3ë‹¨ê³„: ê¸°ìˆ í†µê³„ ì•ˆì „ ê³„ì‚° (í†µê³„ ì˜¤ë¥˜ í•´ê²°)
        ğŸ“ˆ 4ë‹¨ê³„: ë¶„í¬ ë¶„ì„ ë° ì‹œê°í™”
        ğŸ”— 5ë‹¨ê³„: ìƒê´€ê´€ê³„ ë° ì—°ê´€ì„± ë¶„ì„
        âš ï¸ 6ë‹¨ê³„: ì´ìƒê°’ ë° íŒ¨í„´ íƒì§€
        ğŸ§  7ë‹¨ê³„: LLM ì¸ì‚¬ì´íŠ¸ ìƒì„± ë° ì¢…í•© ë³´ê³ ì„œ
        """
        try:
            # ì‘ì—… ì‹œì‘
            await task_updater.submit()
            await task_updater.start_work()
            
            start_time = time.time()
            
            # ğŸ” 1ë‹¨ê³„: EDA ì˜ë„ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§‘ğŸ» **íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì‹œì‘** - 1ë‹¨ê³„: EDA ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘...")
            )
            
            user_query = self._extract_user_query(context)
            logger.info(f"ğŸ“Š EDA Analysis Query: {user_query}")
            
            # LLM ê¸°ë°˜ EDA ì˜ë„ ë¶„ì„
            eda_intent = await self._analyze_eda_intent(user_query)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì˜ë„ ë¶„ì„ ì™„ë£Œ**\n"
                    f"- ë¶„ì„ ìœ í˜•: {eda_intent['analysis_type']}\n"
                    f"- ì¤‘ì  ì˜ì—­: {', '.join(eda_intent['focus_areas'])}\n"
                    f"- ìƒì„¸ë„ ìˆ˜ì¤€: {eda_intent['detail_level']}\n"
                    f"- ì‹ ë¢°ë„: {eda_intent['confidence']:.2f}\n\n"
                    f"**2ë‹¨ê³„**: ë°ì´í„° ê²€ìƒ‰ ì¤‘..."
                )
            )
            
            # ğŸ“‚ 2ë‹¨ê³„: ë°ì´í„° ê²€ìƒ‰ ë° ë¡œë”©
            available_files = await self._scan_available_files()
            
            if not available_files:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(
                        "âš ï¸ **ë°ì´í„° ì—†ìŒ**: ë¶„ì„í•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                        "**í•´ê²°ì±…**:\n"
                        "1. `a2a_ds_servers/artifacts/data/` í´ë”ì— ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”\n"
                        "2. ì§€ì› í˜•ì‹: CSV, Excel (.xlsx/.xls), JSON, Parquet\n"
                        "3. ê¶Œì¥ ìµœì†Œ í¬ê¸°: 100í–‰ ì´ìƒ (í†µê³„ì  ìœ ì˜ì„±)"
                    )
                )
                return
            
            # ìµœì  íŒŒì¼ ì„ íƒ
            selected_file = await self._select_optimal_file_for_eda(available_files, eda_intent)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **íŒŒì¼ ì„ íƒ ì™„ë£Œ**\n"
                    f"- íŒŒì¼: {selected_file['name']}\n"
                    f"- í¬ê¸°: {selected_file['size']:,} bytes\n"
                    f"- EDA ì í•©ë„: {selected_file.get('eda_suitability', 'N/A')}\n\n"
                    f"**3ë‹¨ê³„**: ì•ˆì „í•œ ë°ì´í„° ë¡œë”© ì¤‘..."
                )
            )
            
            # ğŸ“Š 3ë‹¨ê³„: ì•ˆì „í•œ ë°ì´í„° ë¡œë”©
            smart_df = await self._load_data_safely_for_eda(selected_file)
            
            # ğŸ“ˆ 4ë‹¨ê³„: ê¸°ìˆ í†µê³„ ì•ˆì „ ê³„ì‚° (í•µì‹¬ ë¬¸ì œ í•´ê²°)
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ë°ì´í„° ë¡œë”© ì™„ë£Œ**\n"
                    f"- í˜•íƒœ: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´\n"
                    f"- ìˆ«ìí˜• ì»¬ëŸ¼: {len(smart_df.data.select_dtypes(include=[np.number]).columns)}ê°œ\n"
                    f"- ë²”ì£¼í˜• ì»¬ëŸ¼: {len(smart_df.data.select_dtypes(include=['object']).columns)}ê°œ\n\n"
                    f"**4ë‹¨ê³„**: ê¸°ìˆ í†µê³„ ì•ˆì „ ê³„ì‚° ì¤‘..."
                )
            )
            
            # í†µê³„ ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€ ì‹œìŠ¤í…œ
            statistical_analysis = await self._comprehensive_statistical_analysis(smart_df)
            
            # ğŸ“ˆ 5ë‹¨ê³„: ë¶„í¬ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ê¸°ìˆ í†µê³„ ì™„ë£Œ**\n"
                    f"- ê³„ì‚°ëœ í†µê³„: {len(statistical_analysis.descriptive_stats)}ê°œ ì»¬ëŸ¼\n"
                    f"- ì•ˆì „ì„± ê²€ì¦: í†µê³¼\n\n"
                    f"**5ë‹¨ê³„**: ë¶„í¬ ë¶„ì„ ë° ì‹œê°í™” ì¤‘..."
                )
            )
            
            distribution_results = await self._distribution_analysis(smart_df, task_updater)
            
            # ğŸ”— 6ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ë¶„í¬ ë¶„ì„ ì™„ë£Œ**\n"
                    f"- ìƒì„±ëœ ì°¨íŠ¸: {len(distribution_results['charts'])}ê°œ\n\n"
                    f"**6ë‹¨ê³„**: ìƒê´€ê´€ê³„ ë° ì—°ê´€ì„± ë¶„ì„ ì¤‘..."
                )
            )
            
            correlation_results = await self._correlation_analysis(smart_df, task_updater)
            
            # âš ï¸ 7ë‹¨ê³„: ì´ìƒê°’ ë° íŒ¨í„´ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ**\n"
                    f"- ìƒê´€ê´€ê³„ ê³„ìˆ˜: {correlation_results['correlation_count']}ê°œ\n\n"
                    f"**7ë‹¨ê³„**: ì´ìƒê°’ ë° íŒ¨í„´ íƒì§€ ì¤‘..."
                )
            )
            
            outlier_pattern_results = await self._outlier_and_pattern_analysis(smart_df, task_updater)
            
            # ğŸ§  8ë‹¨ê³„: LLM ì¸ì‚¬ì´íŠ¸ ìƒì„±
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("**8ë‹¨ê³„**: LLM ì¸ì‚¬ì´íŠ¸ ìƒì„± ë° ì¢…í•© ë³´ê³ ì„œ ì‘ì„± ì¤‘...")
            )
            
            llm_insights = await self._generate_llm_insights(
                smart_df, statistical_analysis, distribution_results, 
                correlation_results, outlier_pattern_results, eda_intent
            )
            
            # ìµœì¢… ê²°ê³¼ í†µí•©
            final_results = await self._finalize_eda_results(
                smart_df=smart_df,
                statistical_analysis=statistical_analysis,
                distribution_results=distribution_results,
                correlation_results=correlation_results,
                outlier_pattern_results=outlier_pattern_results,
                llm_insights=llm_insights,
                task_updater=task_updater
            )
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(
                    f"âœ… **íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì™„ë£Œ!**\n\n"
                    f"ğŸ“Š **ë¶„ì„ ê²°ê³¼**:\n"
                    f"- ë°ì´í„° í¬ê¸°: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´\n"
                    f"- ìƒì„±ëœ ì°¨íŠ¸: {final_results['total_charts']}ê°œ\n"
                    f"- LLM ì¸ì‚¬ì´íŠ¸: {len(final_results['insights'])}ê°œ\n"
                    f"- ë°œê²¬ëœ íŒ¨í„´: {final_results['patterns_found']}ê°œ\n"
                    f"- ì´ìƒê°’ ê°ì§€: {final_results['outliers_detected']}ê°œ\n"
                    f"- ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n\n"
                    f"ğŸ“ **ì €ì¥ ìœ„ì¹˜**: {final_results['report_path']}\n"
                    f"ğŸ“Š **Interactive ì°¨íŠ¸**: Streamlitì—ì„œ í™•ì¸ ê°€ëŠ¥\n"
                    f"ğŸ“‹ **ì¢…í•© ë³´ê³ ì„œ**: ì•„í‹°íŒ©íŠ¸ë¡œ ìƒì„±ë¨"
                )
            )
            
            # ì•„í‹°íŒ©íŠ¸ ìƒì„±
            await self._create_eda_artifacts(final_results, task_updater)
            
        except Exception as e:
            logger.error(f"âŒ EDA Analysis Error: {e}", exc_info=True)
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"âŒ **EDA ë¶„ì„ ì‹¤íŒ¨**: {str(e)}")
            )
    
    async def _analyze_eda_intent(self, user_query: str) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ EDA ì˜ë„ ë¶„ì„"""
        llm = await self.llm_factory.get_llm()
        
        prompt = f"""
        ì‚¬ìš©ìì˜ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ EDA ì „ëµì„ ê²°ì •í•´ì£¼ì„¸ìš”:
        
        ìš”ì²­: {user_query}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ EDA êµ¬ì„±ìš”ì†Œë“¤:
        {json.dumps(self.eda_components, indent=2, ensure_ascii=False)}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "analysis_type": "comprehensive|focused|statistical|visual|exploratory",
            "focus_areas": ["descriptive_statistics", "distribution_analysis", "correlation_analysis", "outlier_analysis", "pattern_analysis"],
            "detail_level": "summary|detailed|comprehensive",
            "confidence": 0.0-1.0,
            "visualization_priority": "high|medium|low",
            "statistical_rigor": "basic|intermediate|advanced",
            "target_insights": ["ì°¾ê³ ì í•˜ëŠ” ì¸ì‚¬ì´íŠ¸ë“¤"]
        }}
        """
        
        response = await llm.agenerate([prompt])
        try:
            intent = json.loads(response.generations[0][0].text)
            return intent
        except:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "analysis_type": "comprehensive",
                "focus_areas": ["descriptive_statistics", "distribution_analysis", "correlation_analysis"],
                "detail_level": "detailed",
                "confidence": 0.8,
                "visualization_priority": "high",
                "statistical_rigor": "intermediate",
                "target_insights": ["ë°ì´í„° íŒ¨í„´", "ë¶„í¬ íŠ¹ì„±", "ìƒê´€ê´€ê³„"]
            }
    
    async def _scan_available_files(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ê²€ìƒ‰ (unified pattern)"""
        try:
            data_directories = [
                "ai_ds_team/data",
                "a2a_ds_servers/artifacts/data",
                "test_datasets"
            ]
            
            discovered_files = []
            for directory in data_directories:
                if os.path.exists(directory):
                    files = self.file_scanner.scan_data_files(directory)
                    discovered_files.extend(files)
            
            logger.info(f"ğŸ“‚ ë°œê²¬ëœ íŒŒì¼: {len(discovered_files)}ê°œ")
            return discovered_files
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
            return []
    
    async def _select_optimal_file_for_eda(self, available_files: List[Dict], eda_intent: Dict) -> Dict[str, Any]:
        """EDAì— ìµœì í™”ëœ íŒŒì¼ ì„ íƒ"""
        if len(available_files) == 1:
            return available_files[0]
        
        # EDA ì í•©ë„ ì ìˆ˜ ê³„ì‚°
        for file_info in available_files:
            suitability_score = await self._calculate_eda_suitability(file_info)
            file_info['eda_suitability'] = suitability_score
        
        # ì í•©ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        available_files.sort(key=lambda x: x.get('eda_suitability', 0), reverse=True)
        
        # LLM ê¸°ë°˜ ìµœì¢… ì„ íƒ
        llm = await self.llm_factory.get_llm()
        
        files_info = "\n".join([
            f"- {f['name']} (í¬ê¸°: {f['size']} bytes, EDAì í•©ë„: {f.get('eda_suitability', 'N/A')})"
            for f in available_files[:5]  # ìƒìœ„ 5ê°œë§Œ í‘œì‹œ
        ])
        
        prompt = f"""
        íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ì— ê°€ì¥ ì í•©í•œ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”:
        
        EDA ì˜ë„: {eda_intent['analysis_type']} - {eda_intent['detail_level']}
        ì¤‘ì  ì˜ì—­: {eda_intent['focus_areas']}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:
        {files_info}
        
        ê°€ì¥ ì í•©í•œ íŒŒì¼ëª…ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        """
        
        response = await llm.agenerate([prompt])
        selected_name = response.generations[0][0].text.strip()
        
        # íŒŒì¼ëª… ë§¤ì¹­
        for file_info in available_files:
            if selected_name in file_info['name'] or file_info['name'] in selected_name:
                return file_info
        
        return available_files[0]
    
    async def _calculate_eda_suitability(self, file_info: Dict) -> float:
        """EDA ì í•©ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            suitability_factors = []
            
            # 1. íŒŒì¼ í¬ê¸° (EDAì— ì í•©í•œ í¬ê¸°)
            size = file_info['size']
            if 10000 <= size <= 50_000_000:  # 10KB ~ 50MB (EDAì— ì í•©)
                suitability_factors.append(1.0)
            elif size < 10000:
                suitability_factors.append(0.4)  # ë„ˆë¬´ ì‘ìŒ
            elif size > 100_000_000:  # 100MB ì´ìƒ
                suitability_factors.append(0.6)  # ë„ˆë¬´ í¼ (ìƒ˜í”Œë§ í•„ìš”)
            else:
                suitability_factors.append(0.8)
            
            # 2. íŒŒì¼ í˜•ì‹ (EDA ì¹œí™”ì„±)
            extension = file_info.get('extension', '').lower()
            if extension == '.csv':
                suitability_factors.append(1.0)  # ìµœê³  ì í•©ì„±
            elif extension in ['.xlsx', '.xls']:
                suitability_factors.append(0.9)  # ì¢‹ì€ ì í•©ì„±
            elif extension == '.json':
                suitability_factors.append(0.7)  # ë³´í†µ ì í•©ì„±
            else:
                suitability_factors.append(0.5)
            
            # 3. íŒŒì¼ëª… ë¶„ì„ (ë°ì´í„° ìœ í˜• ì¶”ì •)
            filename = file_info['name'].lower()
            if any(keyword in filename for keyword in ['sales', 'customer', 'financial', 'survey']):
                suitability_factors.append(0.9)  # ë¶„ì„í•˜ê¸° ì¢‹ì€ ë°ì´í„°
            elif any(keyword in filename for keyword in ['sample', 'test', 'demo']):
                suitability_factors.append(0.8)  # ìƒ˜í”Œ ë°ì´í„°
            elif any(keyword in filename for keyword in ['log', 'temp', 'backup']):
                suitability_factors.append(0.3)  # ë¶„ì„ì— ë¶€ì í•©
            else:
                suitability_factors.append(0.6)
            
            suitability_score = sum(suitability_factors) / len(suitability_factors)
            return round(suitability_score, 3)
            
        except Exception as e:
            logger.warning(f"EDA ì í•©ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _load_data_safely_for_eda(self, file_info: Dict[str, Any]) -> SmartDataFrame:
        """EDAì— ìµœì í™”ëœ ì•ˆì „í•œ ë°ì´í„° ë¡œë”©"""
        file_path = file_info['path']
        
        try:
            # unified patternì˜ ë‹¤ì¤‘ ì¸ì½”ë”© ì‹œë„
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1', 'utf-16']
            df = None
            used_encoding = None
            
            for encoding in encodings:
                try:
                    if file_path.endswith('.csv'):
                        df = pd.read_csv(file_path, encoding=encoding)
                    elif file_path.endswith(('.xlsx', '.xls')):
                        df = pd.read_excel(file_path)
                        used_encoding = 'excel_auto'
                    elif file_path.endswith('.json'):
                        df = pd.read_json(file_path, encoding=encoding)
                    elif file_path.endswith('.parquet'):
                        df = pd.read_parquet(file_path)
                        used_encoding = 'parquet_auto'
                    
                    if df is not None and not df.empty:
                        used_encoding = used_encoding or encoding
                        break
                        
                except (UnicodeDecodeError, Exception):
                    continue
            
            if df is None or df.empty:
                raise ValueError("ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            
            # EDAìš© ë°ì´í„° ìµœì í™”
            df = await self._optimize_data_for_eda(df)
            
            # SmartDataFrame ìƒì„±
            metadata = {
                'source_file': file_path,
                'encoding': used_encoding,
                'load_timestamp': datetime.now().isoformat(),
                'original_shape': df.shape,
                'eda_optimized': True,
                'eda_suitability': file_info.get('eda_suitability', 0.5)
            }
            
            smart_df = SmartDataFrame(df, metadata)
            logger.info(f"âœ… EDA ìµœì í™” ë¡œë”© ì™„ë£Œ: {smart_df.shape}")
            
            return smart_df
            
        except Exception as e:
            logger.error(f"EDA ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _optimize_data_for_eda(self, df: pd.DataFrame) -> pd.DataFrame:
        """EDAì— ìµœì í™”ëœ ë°ì´í„° ì „ì²˜ë¦¬"""
        try:
            # 1. ê·¹ë‹¨ì ìœ¼ë¡œ í° ë°ì´í„° ìƒ˜í”Œë§
            if len(df) > 50000:  # 5ë§Œ í–‰ ì´ìƒì´ë©´ ìƒ˜í”Œë§
                df = df.sample(n=50000, random_state=42)
                logger.info(f"ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒ˜í”Œë§: 50,000í–‰ìœ¼ë¡œ ì¶•ì†Œ")
            
            # 2. ê·¹ë‹¨ì ìœ¼ë¡œ ë§ì€ ì»¬ëŸ¼ ì œí•œ
            if df.shape[1] > 100:  # 100ê°œ ì»¬ëŸ¼ ì´ìƒì´ë©´ ì œí•œ
                # ìˆ«ìí˜• ìš°ì„ , ê·¸ ë‹¤ìŒ ë²”ì£¼í˜•
                numeric_cols = df.select_dtypes(include=[np.number]).columns[:50]
                categorical_cols = df.select_dtypes(include=['object']).columns[:30]
                selected_cols = list(numeric_cols) + list(categorical_cols)[:100]
                df = df[selected_cols]
                logger.info(f"ì»¬ëŸ¼ ìˆ˜ ì œí•œ: {len(selected_cols)}ê°œë¡œ ì¶•ì†Œ")
            
            # 3. ë°ì´í„° íƒ€ì… ìµœì í™”
            for col in df.columns:
                if df[col].dtype == 'object':
                    # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
                    try:
                        numeric_series = pd.to_numeric(df[col], errors='coerce')
                        if not numeric_series.isna().all():
                            df[col] = numeric_series
                    except:
                        pass
            
            return df
            
        except Exception as e:
            logger.warning(f"EDA ìµœì í™” ì‹¤íŒ¨: {e}")
            return df
    
    async def _comprehensive_statistical_analysis(self, smart_df: SmartDataFrame) -> StatisticalAnalysis:
        """
        í¬ê´„ì  í†µê³„ ë¶„ì„ (í†µê³„ ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€ ì‹œìŠ¤í…œ í¬í•¨)
        ì„¤ê³„ ë¬¸ì„œ ì£¼ìš” ë¬¸ì œì  í•´ê²°
        """
        df = smart_df.data
        
        try:
            # ê¸°ìˆ í†µê³„ ì•ˆì „ ê³„ì‚°
            descriptive_stats = await self._safe_descriptive_statistics(df)
            
            # ë¶„í¬ ë¶„ì„ ì•ˆì „ ê³„ì‚°
            distribution_analysis = await self._safe_distribution_analysis(df)
            
            # ìƒê´€ê´€ê³„ ë¶„ì„ ì•ˆì „ ê³„ì‚°
            correlation_analysis = await self._safe_correlation_analysis(df)
            
            # ì´ìƒê°’ ë¶„ì„ ì•ˆì „ ê³„ì‚°
            outlier_analysis = await self._safe_outlier_analysis(df)
            
            # íŒ¨í„´ ë¶„ì„ ì•ˆì „ ê³„ì‚°
            pattern_analysis = await self._safe_pattern_analysis(df)
            
            return StatisticalAnalysis(
                descriptive_stats=descriptive_stats,
                distribution_analysis=distribution_analysis,
                correlation_analysis=correlation_analysis,
                outlier_analysis=outlier_analysis,
                pattern_analysis=pattern_analysis
            )
            
        except Exception as e:
            logger.error(f"í†µê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ë¹ˆ ê²°ê³¼ ë°˜í™˜
            return StatisticalAnalysis(
                descriptive_stats={},
                distribution_analysis={},
                correlation_analysis={},
                outlier_analysis={},
                pattern_analysis={}
            )
    
    async def _safe_descriptive_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì•ˆì „í•œ ê¸°ìˆ í†µê³„ ê³„ì‚° (ì˜¤ë¥˜ ë°©ì§€)"""
        stats_results = {}
        
        try:
            # ìˆ«ìí˜• ì»¬ëŸ¼ ë¶„ì„
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_columns:
                try:
                    col_data = df[col].dropna()  # NaN ì œê±°
                    
                    if len(col_data) < self.statistical_safety['min_sample_size']:
                        continue  # ìƒ˜í”Œ í¬ê¸° ë¶€ì¡± ì‹œ ê±´ë„ˆë›°ê¸°
                    
                    col_stats = {
                        'count': len(col_data),
                        'mean': float(col_data.mean()),
                        'std': float(col_data.std()),
                        'min': float(col_data.min()),
                        'max': float(col_data.max()),
                        'median': float(col_data.median()),
                        'q1': float(col_data.quantile(0.25)),
                        'q3': float(col_data.quantile(0.75)),
                        'missing_count': int(df[col].isna().sum()),
                        'missing_percentage': float((df[col].isna().sum() / len(df)) * 100)
                    }
                    
                    # ì¶”ê°€ í†µê³„ (ì•ˆì „í•œ ê³„ì‚°)
                    try:
                        col_stats['skewness'] = float(col_data.skew())
                        col_stats['kurtosis'] = float(col_data.kurtosis())
                    except:
                        col_stats['skewness'] = None
                        col_stats['kurtosis'] = None
                    
                    stats_results[col] = col_stats
                    
                except Exception as e:
                    logger.warning(f"ì»¬ëŸ¼ {col} í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}")
                    continue
            
            # ë²”ì£¼í˜• ì»¬ëŸ¼ ë¶„ì„
            categorical_columns = df.select_dtypes(include=['object', 'category']).columns
            
            for col in categorical_columns:
                try:
                    col_data = df[col].dropna()
                    
                    if len(col_data) == 0:
                        continue
                    
                    value_counts = col_data.value_counts()
                    
                    # ì¹´í…Œê³ ë¦¬ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œí•œ
                    if len(value_counts) > self.statistical_safety['max_categories']:
                        value_counts = value_counts.head(self.statistical_safety['max_categories'])
                    
                    col_stats = {
                        'count': len(col_data),
                        'unique_count': len(value_counts),
                        'most_frequent': str(value_counts.index[0]) if len(value_counts) > 0 else None,
                        'most_frequent_count': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
                        'missing_count': int(df[col].isna().sum()),
                        'missing_percentage': float((df[col].isna().sum() / len(df)) * 100),
                        'top_categories': {str(k): int(v) for k, v in value_counts.head(10).items()}
                    }
                    
                    stats_results[f"{col}_categorical"] = col_stats
                    
                except Exception as e:
                    logger.warning(f"ë²”ì£¼í˜• ì»¬ëŸ¼ {col} ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            logger.info(f"âœ… ê¸°ìˆ í†µê³„ ê³„ì‚° ì™„ë£Œ: {len(stats_results)}ê°œ ì»¬ëŸ¼")
            return stats_results
            
        except Exception as e:
            logger.error(f"ê¸°ìˆ í†µê³„ ê³„ì‚° ì „ì²´ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _safe_distribution_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì•ˆì „í•œ ë¶„í¬ ë¶„ì„"""
        distribution_results = {}
        
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_columns:
                try:
                    col_data = df[col].dropna()
                    
                    if len(col_data) < self.statistical_safety['min_sample_size']:
                        continue
                    
                    # ì •ê·œì„± ê²€ì • (ì•ˆì „í•œ ì‹¤í–‰)
                    normality_tests = {}
                    
                    # Shapiro-Wilk í…ŒìŠ¤íŠ¸ (ìƒ˜í”Œ í¬ê¸° ì œí•œ)
                    if len(col_data) <= 5000:  # ShapiroëŠ” 5000ê°œ ì´í•˜ì—ì„œë§Œ ìœ íš¨
                        try:
                            shapiro_stat, shapiro_p = stats.shapiro(col_data.sample(min(len(col_data), 1000)))
                            normality_tests['shapiro'] = {
                                'statistic': float(shapiro_stat),
                                'p_value': float(shapiro_p),
                                'is_normal': shapiro_p > 0.05
                            }
                        except:
                            pass
                    
                    # Jarque-Bera í…ŒìŠ¤íŠ¸
                    try:
                        jb_stat, jb_p = stats.jarque_bera(col_data)
                        normality_tests['jarque_bera'] = {
                            'statistic': float(jb_stat),
                            'p_value': float(jb_p),
                            'is_normal': jb_p > 0.05
                        }
                    except:
                        pass
                    
                    distribution_results[col] = {
                        'normality_tests': normality_tests,
                        'skewness': float(col_data.skew()) if len(col_data) > 0 else None,
                        'kurtosis': float(col_data.kurtosis()) if len(col_data) > 0 else None,
                        'distribution_shape': self._classify_distribution_shape(col_data)
                    }
                    
                except Exception as e:
                    logger.warning(f"ì»¬ëŸ¼ {col} ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            return distribution_results
            
        except Exception as e:
            logger.error(f"ë¶„í¬ ë¶„ì„ ì „ì²´ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _safe_correlation_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì•ˆì „í•œ ìƒê´€ê´€ê³„ ë¶„ì„"""
        correlation_results = {}
        
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) < 2:
                return {"message": "ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ìˆ«ìí˜• ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤"}
            
            # ìˆ«ìí˜• ë°ì´í„°ë§Œ ì¶”ì¶œ
            numeric_df = df[numeric_columns].dropna()
            
            if len(numeric_df) < self.statistical_safety['min_sample_size']:
                return {"message": "ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤"}
            
            # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
            try:
                correlation_matrix = numeric_df.corr()
                
                # NaN ê°’ ì²˜ë¦¬
                correlation_matrix = correlation_matrix.fillna(0)
                
                correlation_results['correlation_matrix'] = correlation_matrix.to_dict()
                
                # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸°
                strong_correlations = []
                for i in range(len(correlation_matrix.columns)):
                    for j in range(i+1, len(correlation_matrix.columns)):
                        corr_value = correlation_matrix.iloc[i, j]
                        if abs(corr_value) > 0.5:  # ê°•í•œ ìƒê´€ê´€ê³„
                            strong_correlations.append({
                                'variable1': correlation_matrix.columns[i],
                                'variable2': correlation_matrix.columns[j],
                                'correlation': float(corr_value),
                                'strength': 'strong' if abs(corr_value) > 0.7 else 'moderate'
                            })
                
                correlation_results['strong_correlations'] = strong_correlations
                correlation_results['correlation_count'] = len(strong_correlations)
                
            except Exception as e:
                logger.warning(f"ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚° ì‹¤íŒ¨: {e}")
                correlation_results['error'] = str(e)
            
            return correlation_results
            
        except Exception as e:
            logger.error(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì „ì²´ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _safe_outlier_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì•ˆì „í•œ ì´ìƒê°’ ë¶„ì„"""
        outlier_results = {}
        
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_columns:
                try:
                    col_data = df[col].dropna()
                    
                    if len(col_data) < self.statistical_safety['min_sample_size']:
                        continue
                    
                    # IQR ë°©ë²•
                    q1 = col_data.quantile(0.25)
                    q3 = col_data.quantile(0.75)
                    iqr = q3 - q1
                    lower_bound = q1 - 1.5 * iqr
                    upper_bound = q3 + 1.5 * iqr
                    
                    iqr_outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]
                    
                    # Z-Score ë°©ë²•
                    z_scores = np.abs(stats.zscore(col_data))
                    z_outliers = col_data[z_scores > 3]
                    
                    outlier_results[col] = {
                        'iqr_method': {
                            'count': len(iqr_outliers),
                            'percentage': float((len(iqr_outliers) / len(col_data)) * 100),
                            'lower_bound': float(lower_bound),
                            'upper_bound': float(upper_bound)
                        },
                        'zscore_method': {
                            'count': len(z_outliers),
                            'percentage': float((len(z_outliers) / len(col_data)) * 100)
                        }
                    }
                    
                except Exception as e:
                    logger.warning(f"ì»¬ëŸ¼ {col} ì´ìƒê°’ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            return outlier_results
            
        except Exception as e:
            logger.error(f"ì´ìƒê°’ ë¶„ì„ ì „ì²´ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _safe_pattern_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ì•ˆì „í•œ íŒ¨í„´ ë¶„ì„"""
        pattern_results = {}
        
        try:
            # ê¸°ë³¸ íŒ¨í„´ ë¶„ì„
            pattern_results['data_shape'] = {
                'rows': int(df.shape[0]),
                'columns': int(df.shape[1]),
                'density': float((df.notna().sum().sum()) / (df.shape[0] * df.shape[1]))
            }
            
            # ë°ì´í„° íƒ€ì… ë¶„í¬
            dtype_counts = df.dtypes.value_counts()
            pattern_results['data_types'] = {str(k): int(v) for k, v in dtype_counts.items()}
            
            # ê²°ì¸¡ê°’ íŒ¨í„´
            missing_pattern = df.isnull().sum()
            missing_cols = missing_pattern[missing_pattern > 0]
            
            if len(missing_cols) > 0:
                pattern_results['missing_patterns'] = {
                    'columns_with_missing': {str(k): int(v) for k, v in missing_cols.items()},
                    'total_missing_cells': int(df.isnull().sum().sum())
                }
            
            return pattern_results
            
        except Exception as e:
            logger.error(f"íŒ¨í„´ ë¶„ì„ ì „ì²´ ì‹¤íŒ¨: {e}")
            return {}
    
    def _classify_distribution_shape(self, data: pd.Series) -> str:
        """ë¶„í¬ í˜•íƒœ ë¶„ë¥˜"""
        try:
            skewness = data.skew()
            kurtosis = data.kurtosis()
            
            if abs(skewness) < 0.5:
                skew_desc = "symmetric"
            elif skewness > 0.5:
                skew_desc = "right_skewed"
            else:
                skew_desc = "left_skewed"
            
            if kurtosis > 3:
                kurt_desc = "heavy_tailed"
            elif kurtosis < -1:
                kurt_desc = "light_tailed"
            else:
                kurt_desc = "normal_tailed"
            
            return f"{skew_desc}_{kurt_desc}"
            
        except:
            return "unknown"
    
    async def _distribution_analysis(self, smart_df: SmartDataFrame, task_updater: TaskUpdater) -> Dict[str, Any]:
        """ë¶„í¬ ë¶„ì„ ë° ì‹œê°í™”"""
        df = smart_df.data
        charts = {}
        
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns[:6]  # ìµœëŒ€ 6ê°œ
            
            for col in numeric_columns:
                try:
                    col_data = df[col].dropna()
                    
                    if len(col_data) < 5:
                        continue
                    
                    # íˆìŠ¤í† ê·¸ë¨
                    fig_hist = px.histogram(
                        df, x=col,
                        title=f"Distribution of {col}",
                        template='plotly_white'
                    )
                    charts[f"{col}_histogram"] = fig_hist
                    
                    # ë°•ìŠ¤í”Œë¡¯
                    fig_box = px.box(
                        df, y=col,
                        title=f"Box Plot of {col}",
                        template='plotly_white'
                    )
                    charts[f"{col}_boxplot"] = fig_box
                    
                except Exception as e:
                    logger.warning(f"ì»¬ëŸ¼ {col} ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                    continue
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(f"ğŸ’ ë¶„í¬ ì°¨íŠ¸ ìƒì„± ì™„ë£Œ: {len(charts)}ê°œ")
            )
            
            return {
                'charts': charts,
                'chart_count': len(charts)
            }
            
        except Exception as e:
            logger.error(f"ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'charts': {}, 'chart_count': 0}
    
    async def _correlation_analysis(self, smart_df: SmartDataFrame, task_updater: TaskUpdater) -> Dict[str, Any]:
        """ìƒê´€ê´€ê³„ ë¶„ì„ ë° ì‹œê°í™”"""
        df = smart_df.data
        charts = {}
        correlation_count = 0
        
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) >= 2:
                # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤
                corr_matrix = df[numeric_columns].corr()
                
                # íˆíŠ¸ë§µ
                fig_heatmap = px.imshow(
                    corr_matrix,
                    title="Correlation Matrix",
                    template='plotly_white',
                    aspect="auto"
                )
                charts['correlation_heatmap'] = fig_heatmap
                
                # ê°•í•œ ìƒê´€ê´€ê³„ ê°œìˆ˜
                strong_corr_mask = (abs(corr_matrix) > 0.5) & (abs(corr_matrix) < 1.0)
                correlation_count = strong_corr_mask.sum().sum() // 2  # ëŒ€ì¹­ ë§¤íŠ¸ë¦­ìŠ¤ì´ë¯€ë¡œ 2ë¡œ ë‚˜ëˆ”
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(f"ğŸ’ ìƒê´€ê´€ê³„ ë¶„ì„ ì™„ë£Œ: {correlation_count}ê°œ ê°•í•œ ìƒê´€ê´€ê³„")
            )
            
            return {
                'charts': charts,
                'correlation_count': correlation_count
            }
            
        except Exception as e:
            logger.error(f"ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'charts': {}, 'correlation_count': 0}
    
    async def _outlier_and_pattern_analysis(self, smart_df: SmartDataFrame, task_updater: TaskUpdater) -> Dict[str, Any]:
        """ì´ìƒê°’ ë° íŒ¨í„´ ë¶„ì„"""
        df = smart_df.data
        charts = {}
        outliers_detected = 0
        patterns_found = 0
        
        try:
            numeric_columns = df.select_dtypes(include=[np.number]).columns[:4]  # ìµœëŒ€ 4ê°œ
            
            for col in numeric_columns:
                try:
                    col_data = df[col].dropna()
                    
                    if len(col_data) < 5:
                        continue
                    
                    # ë°•ìŠ¤í”Œë¡¯ (ì´ìƒê°’ ì‹œê°í™”)
                    fig_box = px.box(
                        df, y=col,
                        title=f"Outlier Detection: {col}",
                        template='plotly_white'
                    )
                    charts[f"{col}_outlier_boxplot"] = fig_box
                    
                    # ì´ìƒê°’ ê°œìˆ˜ ê³„ì‚°
                    q1 = col_data.quantile(0.25)
                    q3 = col_data.quantile(0.75)
                    iqr = q3 - q1
                    outlier_mask = (col_data < q1 - 1.5*iqr) | (col_data > q3 + 1.5*iqr)
                    outliers_detected += outlier_mask.sum()
                    
                except Exception as e:
                    logger.warning(f"ì»¬ëŸ¼ {col} ì´ìƒê°’ ë¶„ì„ ì‹¤íŒ¨: {e}")
                    continue
            
            # ê°„ë‹¨í•œ íŒ¨í„´ íƒì§€
            patterns_found = len(df.select_dtypes(include=[np.number]).columns)  # ìˆ«ìí˜• ì»¬ëŸ¼ ìˆ˜
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(f"ğŸ’ ì´ìƒê°’ ë° íŒ¨í„´ ë¶„ì„ ì™„ë£Œ: {outliers_detected}ê°œ ì´ìƒê°’, {patterns_found}ê°œ íŒ¨í„´")
            )
            
            return {
                'charts': charts,
                'outliers_detected': outliers_detected,
                'patterns_found': patterns_found
            }
            
        except Exception as e:
            logger.error(f"ì´ìƒê°’ ë° íŒ¨í„´ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'charts': {}, 'outliers_detected': 0, 'patterns_found': 0}
    
    async def _generate_llm_insights(self, smart_df: SmartDataFrame, statistical_analysis: StatisticalAnalysis,
                                   distribution_results: Dict, correlation_results: Dict, outlier_pattern_results: Dict,
                                   eda_intent: Dict) -> List[EDAInsight]:
        """LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            llm = await self.llm_factory.get_llm()
            
            # ë¶„ì„ ê²°ê³¼ ìš”ì•½
            analysis_summary = {
                'data_shape': smart_df.shape,
                'descriptive_stats_count': len(statistical_analysis.descriptive_stats),
                'correlation_count': correlation_results.get('correlation_count', 0),
                'outliers_detected': outlier_pattern_results.get('outliers_detected', 0),
                'charts_generated': (
                    len(distribution_results.get('charts', {})) + 
                    len(correlation_results.get('charts', {})) + 
                    len(outlier_pattern_results.get('charts', {}))
                )
            }
            
            prompt = f"""
            íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
            
            ë°ì´í„° ê°œìš”:
            - í˜•íƒœ: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´
            - ë¶„ì„ëœ ì»¬ëŸ¼: {analysis_summary['descriptive_stats_count']}ê°œ
            
            ë¶„ì„ ê²°ê³¼ ìš”ì•½:
            {json.dumps(analysis_summary, indent=2, ensure_ascii=False)}
            
            ì‚¬ìš©ì ì˜ë„:
            {json.dumps(eda_intent, indent=2, ensure_ascii=False)}
            
            ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ 3-5ê°œì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
            [
                {{
                    "category": "statistical|distribution|correlation|outlier|pattern",
                    "finding": "êµ¬ì²´ì ì¸ ë°œê²¬ì‚¬í•­",
                    "confidence": 0.0-1.0,
                    "evidence": {{"ê·¼ê±°": "ê°’"}},
                    "recommendation": "ê¶Œì¥ ì‚¬í•­"
                }}
            ]
            """
            
            response = await llm.agenerate([prompt])
            insights_data = json.loads(response.generations[0][0].text)
            
            insights = []
            for insight_data in insights_data:
                insight = EDAInsight(
                    category=insight_data.get('category', 'general'),
                    finding=insight_data.get('finding', ''),
                    confidence=insight_data.get('confidence', 0.5),
                    evidence=insight_data.get('evidence', {}),
                    recommendation=insight_data.get('recommendation', '')
                )
                insights.append(insight)
            
            return insights
            
        except Exception as e:
            logger.error(f"LLM ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì¸ì‚¬ì´íŠ¸ ë°˜í™˜
            return [
                EDAInsight(
                    category="general",
                    finding=f"ë°ì´í„°ì…‹ì€ {smart_df.shape[0]}í–‰ {smart_df.shape[1]}ì—´ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
                    confidence=1.0,
                    evidence={"data_shape": smart_df.shape},
                    recommendation="ì¶”ê°€ ë¶„ì„ì„ í†µí•´ ë” ìì„¸í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                )
            ]
    
    async def _finalize_eda_results(self, smart_df: SmartDataFrame, statistical_analysis: StatisticalAnalysis,
                                  distribution_results: Dict, correlation_results: Dict, outlier_pattern_results: Dict,
                                  llm_insights: List[EDAInsight], task_updater: TaskUpdater) -> Dict[str, Any]:
        """EDA ê²°ê³¼ ìµœì¢…í™”"""
        
        # ëª¨ë“  ì°¨íŠ¸ í†µí•©
        all_charts = {}
        all_charts.update(distribution_results.get('charts', {}))
        all_charts.update(correlation_results.get('charts', {}))
        all_charts.update(outlier_pattern_results.get('charts', {}))
        
        # ì°¨íŠ¸ ì €ì¥
        save_dir = Path("a2a_ds_servers/artifacts/eda_reports")
        save_dir.mkdir(exist_ok=True, parents=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ì°¨íŠ¸ íŒŒì¼ ì €ì¥
        chart_paths = []
        for chart_name, fig in all_charts.items():
            try:
                html_filename = f"{chart_name}_{timestamp}.html"
                html_path = save_dir / html_filename
                fig.write_html(str(html_path))
                chart_paths.append(str(html_path))
            except Exception as e:
                logger.warning(f"ì°¨íŠ¸ ì €ì¥ ì‹¤íŒ¨ ({chart_name}): {e}")
        
        # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        report_filename = f"eda_report_{timestamp}.json"
        report_path = save_dir / report_filename
        
        comprehensive_report = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'data_source': smart_df.metadata.get('source_file', 'Unknown'),
                'data_shape': smart_df.shape,
                'analysis_duration': 'completed'
            },
            'statistical_summary': statistical_analysis.descriptive_stats,
            'insights': [
                {
                    'category': insight.category,
                    'finding': insight.finding,
                    'confidence': insight.confidence,
                    'recommendation': insight.recommendation
                } for insight in llm_insights
            ],
            'charts_generated': list(all_charts.keys()),
            'saved_files': chart_paths
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        return {
            'total_charts': len(all_charts),
            'insights': llm_insights,
            'patterns_found': outlier_pattern_results.get('patterns_found', 0),
            'outliers_detected': outlier_pattern_results.get('outliers_detected', 0),
            'report_path': str(report_path),
            'chart_paths': chart_paths,
            'comprehensive_report': comprehensive_report
        }
    
    async def _create_eda_artifacts(self, results: Dict[str, Any], task_updater: TaskUpdater) -> None:
        """EDA ì•„í‹°íŒ©íŠ¸ ìƒì„±"""
        
        # EDA ë³´ê³ ì„œ ì•„í‹°íŒ©íŠ¸
        eda_report = {
            'exploratory_data_analysis_report': {
                'timestamp': datetime.now().isoformat(),
                'analysis_summary': {
                    'total_charts_generated': results['total_charts'],
                    'insights_discovered': len(results['insights']),
                    'patterns_identified': results['patterns_found'],
                    'outliers_detected': results['outliers_detected']
                },
                'key_insights': [
                    {
                        'category': insight.category,
                        'finding': insight.finding,
                        'confidence': insight.confidence,
                        'recommendation': insight.recommendation
                    } for insight in results['insights']
                ],
                'files_generated': {
                    'report_path': results['report_path'],
                    'chart_count': len(results['chart_paths']),
                    'interactive_charts': True
                }
            }
        }
        
        # A2A ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
        await task_updater.add_artifact(
            parts=[TextPart(text=json.dumps(eda_report, indent=2, ensure_ascii=False))],
            name="eda_analysis_report",
            metadata={"content_type": "application/json", "category": "exploratory_data_analysis"}
        )
        
        # ì¢…í•© ë³´ê³ ì„œë„ ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
        await task_updater.add_artifact(
            parts=[TextPart(text=json.dumps(results['comprehensive_report'], indent=2, ensure_ascii=False))],
            name="comprehensive_eda_report",
            metadata={"content_type": "application/json", "category": "comprehensive_analysis"}
        )
        
        logger.info("âœ… EDA ë¶„ì„ ì•„í‹°íŒ©íŠ¸ ìƒì„± ì™„ë£Œ")
    
    def _extract_user_query(self, context: RequestContext) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ (A2A í‘œì¤€)"""
        user_query = ""
        if context.message and context.message.parts:
            for part in context.message.parts:
                if hasattr(part, 'root') and hasattr(part.root, 'text'):
                    user_query += part.root.text + " "
        return user_query.strip() or "ë°ì´í„°ë¥¼ íƒìƒ‰ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”"
    
    async def cancel(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        await task_updater.reject()
        logger.info(f"EDA Analysis ì‘ì—… ì·¨ì†Œë¨: {context.context_id}")

# A2A ì„œë²„ ì„¤ì •
def create_eda_tools_agent_card() -> AgentCard:
    """EDA Tools Agent Card ìƒì„±"""
    return AgentCard(
        name="Unified EDA Tools Agent",
        description="ğŸ“Š LLM First ì§€ëŠ¥í˜• íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ - í†µê³„ ê³„ì‚° ì˜¤ë¥˜ ì™„ì „ í•´ê²°, í¬ê´„ì  5ë‹¨ê³„ EDA, A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜",
        skills=[
            AgentSkill(
                name="intelligent_eda_planning",
                description="LLM ê¸°ë°˜ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì „ëµ ìˆ˜ë¦½"
            ),
            AgentSkill(
                name="safe_statistical_calculation", 
                description="í†µê³„ ê³„ì‚° ì˜¤ë¥˜ ë°©ì§€ ì‹œìŠ¤í…œ (ì£¼ìš” ë¬¸ì œì  í•´ê²°)"
            ),
            AgentSkill(
                name="comprehensive_descriptive_statistics",
                description="ê¸°ìˆ í†µê³„, ì¤‘ì‹¬ê²½í–¥ì„±, ë³€ì‚°ì„±, ë¶„í¬ í˜•íƒœ ë¶„ì„"
            ),
            AgentSkill(
                name="distribution_analysis",
                description="ì •ê·œì„± ê²€ì •, ë¶„í¬ ì í•©ì„±, ì™œë„-ì²¨ë„ ë¶„ì„"
            ),
            AgentSkill(
                name="correlation_analysis",
                description="ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤, ì—°ê´€ì„± ì¸¡ì •, ë‹¤ì¤‘ê³µì„ ì„± ì§„ë‹¨"
            ),
            AgentSkill(
                name="outlier_detection",
                description="IQR, Z-score, Isolation Forest ê¸°ë°˜ ì´ìƒê°’ íƒì§€"
            ),
            AgentSkill(
                name="pattern_recognition",
                description="íŠ¸ë Œë“œ, ê³„ì ˆì„±, í´ëŸ¬ìŠ¤í„°ë§ ê²½í–¥ ë¶„ì„"
            ),
            AgentSkill(
                name="llm_insight_generation",
                description="LLM ê¸°ë°˜ ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ë°œê²¬ ë° í•´ì„"
            ),
            AgentSkill(
                name="interactive_eda_visualization",
                description="Interactive Plotly ê¸°ë°˜ EDA ì‹œê°í™”"
            )
        ],
        capabilities=AgentCapabilities(
            supports_streaming=True,
            supports_artifacts=True,
            max_execution_time=240,
            supported_formats=["csv", "excel", "json", "parquet"]
        )
    )

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # A2A ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    task_store = InMemoryTaskStore()
    executor = UnifiedEDAToolsExecutor()
    agent_card = create_eda_tools_agent_card()
    
    request_handler = DefaultRequestHandler(
        agent_card=agent_card,
        task_store=task_store,
        agent_executor=executor
    )
    
    app = A2AStarletteApplication(request_handler=request_handler)
    
    # ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Unified EDA Tools Server ì‹œì‘ - Port 8312")
    logger.info("ğŸ“Š ê¸°ëŠ¥: LLM First íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ + í†µê³„ ê³„ì‚° ì•ˆì •ì„±")
    logger.info("ğŸ¯ A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜")
    
    uvicorn.run(app, host="0.0.0.0", port=8312) 