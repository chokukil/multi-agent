#!/usr/bin/env python3
"""
ğŸ§  Enhanced Shared Knowledge Bank - A2A SDK 0.2.9 ê³ ê¸‰ ê¸°ëŠ¥ í™•ì¥
Port: 8602

Context Engineering MEMORY ë ˆì´ì–´ì˜ ê³ ê¸‰ êµ¬í˜„
- ìë™í™”ëœ ì§€ì‹ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ
- ì§€ëŠ¥í˜• ì§€ì‹ ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬
- ì‹¤ì‹œê°„ íŒ¨í„´ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸
- ìŠ¤ë§ˆíŠ¸ ì•Œë¦¼ ë° ì¶”ì²œ ì‹œìŠ¤í…œ
- ê³ ê¸‰ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
- ì§€ì‹ ìƒëª…ì£¼ê¸° ê´€ë¦¬
"""

import asyncio
import json
import logging
import os
import pickle
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

import numpy as np
import pandas as pd
import networkx as nx
import uvicorn
from openai import AsyncOpenAI

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart,
    Part
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
from dotenv import load_dotenv
load_dotenv()

# LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
llm_client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY", "your-api-key-here"),
    base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
)

# ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
KNOWLEDGE_BASE_DIR = Path("a2a_ds_servers/artifacts/knowledge_base")
KNOWLEDGE_BASE_DIR.mkdir(parents=True, exist_ok=True)

ANALYTICS_DIR = KNOWLEDGE_BASE_DIR / "analytics"
ANALYTICS_DIR.mkdir(parents=True, exist_ok=True)

UPDATES_DIR = KNOWLEDGE_BASE_DIR / "updates"
UPDATES_DIR.mkdir(parents=True, exist_ok=True)

class KnowledgeQuality(Enum):
    """ì§€ì‹ í’ˆì§ˆ ë“±ê¸‰"""
    EXCELLENT = "excellent"
    GOOD = "good"
    ACCEPTABLE = "acceptable"
    NEEDS_REVIEW = "needs_review"
    OUTDATED = "outdated"

class UpdateTrigger(Enum):
    """ì—…ë°ì´íŠ¸ íŠ¸ë¦¬ê±° ìœ í˜•"""
    AUTOMATED = "automated"
    PATTERN_DETECTED = "pattern_detected"
    QUALITY_DEGRADED = "quality_degraded"
    USER_FEEDBACK = "user_feedback"
    SCHEDULED = "scheduled"

@dataclass
class EnhancedKnowledgeEntry:
    """í–¥ìƒëœ ì§€ì‹ í•­ëª©"""
    id: str
    type: str
    title: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    related_entries: List[str] = None
    created_at: datetime = None
    updated_at: datetime = None
    usage_count: int = 0
    relevance_score: float = 0.0
    quality_score: float = 0.0
    quality_grade: KnowledgeQuality = KnowledgeQuality.ACCEPTABLE
    last_verified: Optional[datetime] = None
    verification_status: str = "pending"
    update_history: List[Dict] = None
    tags: List[str] = None
    
    def __post_init__(self):
        if self.related_entries is None:
            self.related_entries = []
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.updated_at is None:
            self.updated_at = datetime.now()
        if self.update_history is None:
            self.update_history = []
        if self.tags is None:
            self.tags = []

@dataclass
class KnowledgeUpdateRequest:
    """ì§€ì‹ ì—…ë°ì´íŠ¸ ìš”ì²­"""
    id: str
    entry_id: str
    trigger: UpdateTrigger
    reason: str
    proposed_changes: Dict[str, Any]
    priority: int = 1  # 1(ë†’ìŒ) ~ 5(ë‚®ìŒ)
    created_at: datetime = None
    status: str = "pending"  # pending, approved, rejected, applied
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()

@dataclass
class KnowledgeAnalytics:
    """ì§€ì‹ ë¶„ì„ ê²°ê³¼"""
    total_entries: int
    quality_distribution: Dict[str, int]
    usage_statistics: Dict[str, Any]
    top_entries: List[Dict[str, Any]]
    outdated_entries: List[str]
    update_recommendations: List[Dict[str, Any]]
    growth_trends: Dict[str, Any]
    generated_at: datetime = None
    
    def __post_init__(self):
        if self.generated_at is None:
            self.generated_at = datetime.now()

class AutomatedKnowledgeUpdater:
    """ìë™í™”ëœ ì§€ì‹ ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, llm_client: AsyncOpenAI):
        self.llm_client = llm_client
        self.update_requests: Dict[str, KnowledgeUpdateRequest] = {}
        self.update_history: List[Dict] = []
        
    async def detect_update_needs(self, entries: Dict[str, EnhancedKnowledgeEntry]) -> List[KnowledgeUpdateRequest]:
        """ì—…ë°ì´íŠ¸ í•„ìš”ì„± ìë™ ê°ì§€"""
        update_requests = []
        
        for entry_id, entry in entries.items():
            # 1. í’ˆì§ˆ ê¸°ë°˜ ì—…ë°ì´íŠ¸ í•„ìš”ì„± ê²€ì‚¬
            if entry.quality_grade in [KnowledgeQuality.NEEDS_REVIEW, KnowledgeQuality.OUTDATED]:
                update_requests.append(KnowledgeUpdateRequest(
                    id=str(uuid.uuid4()),
                    entry_id=entry_id,
                    trigger=UpdateTrigger.QUALITY_DEGRADED,
                    reason=f"Quality grade is {entry.quality_grade.value}",
                    proposed_changes={"status": "needs_review"},
                    priority=2
                ))
            
            # 2. ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì—…ë°ì´íŠ¸
            if entry.usage_count > 100 and entry.quality_score < 0.7:
                update_requests.append(KnowledgeUpdateRequest(
                    id=str(uuid.uuid4()),
                    entry_id=entry_id,
                    trigger=UpdateTrigger.PATTERN_DETECTED,
                    reason="High usage but low quality score",
                    proposed_changes={"priority": "high_review"},
                    priority=1
                ))
            
            # 3. ì‹œê°„ ê¸°ë°˜ ì—…ë°ì´íŠ¸
            if entry.last_verified and (datetime.now() - entry.last_verified).days > 30:
                update_requests.append(KnowledgeUpdateRequest(
                    id=str(uuid.uuid4()),
                    entry_id=entry_id,
                    trigger=UpdateTrigger.SCHEDULED,
                    reason="Periodic verification required",
                    proposed_changes={"verification_needed": True},
                    priority=3
                ))
        
        return update_requests
    
    async def llm_propose_updates(self, entry: EnhancedKnowledgeEntry, context: Dict[str, Any]) -> Dict[str, Any]:
        """LLMì„ í™œìš©í•œ ì—…ë°ì´íŠ¸ ì œì•ˆ"""
        try:
            prompt = f"""
            Analyze this knowledge entry and propose improvements:
            
            Title: {entry.title}
            Content: {entry.content[:500]}...
            Current Quality Score: {entry.quality_score}
            Usage Count: {entry.usage_count}
            Last Updated: {entry.updated_at}
            
            Context:
            {json.dumps(context, indent=2)}
            
            Please suggest:
            1. Content improvements
            2. Better categorization
            3. Additional metadata
            4. Quality enhancements
            
            Return as JSON with specific actionable recommendations.
            """
            
            response = await self.llm_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a knowledge management expert. Provide specific, actionable recommendations for improving knowledge entries."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"LLM update proposal failed: {e}")
            return {"error": str(e)}
    
    async def apply_automated_updates(self, entry: EnhancedKnowledgeEntry, update_request: KnowledgeUpdateRequest) -> bool:
        """ìë™ ì—…ë°ì´íŠ¸ ì ìš©"""
        try:
            # ì—…ë°ì´íŠ¸ ê¸°ë¡ ì¶”ê°€
            update_record = {
                "update_id": update_request.id,
                "trigger": update_request.trigger.value,
                "reason": update_request.reason,
                "changes": update_request.proposed_changes,
                "applied_at": datetime.now().isoformat()
            }
            
            entry.update_history.append(update_record)
            entry.updated_at = datetime.now()
            
            # ì œì•ˆëœ ë³€ê²½ì‚¬í•­ ì ìš©
            for key, value in update_request.proposed_changes.items():
                if hasattr(entry, key):
                    setattr(entry, key, value)
            
            logger.info(f"Applied automated update to entry {entry.id}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to apply update to entry {entry.id}: {e}")
            return False

class KnowledgeQualityAssessor:
    """ì§€ì‹ í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, llm_client: AsyncOpenAI):
        self.llm_client = llm_client
    
    async def assess_quality(self, entry: EnhancedKnowledgeEntry) -> Tuple[float, KnowledgeQuality]:
        """ì§€ì‹ í’ˆì§ˆ í‰ê°€"""
        score = 0.0
        factors = []
        
        # 1. ì½˜í…ì¸  ê¸¸ì´ ë° ì™„ì„±ë„
        content_score = min(len(entry.content) / 500, 1.0)
        factors.append(("content_length", content_score, 0.2))
        
        # 2. ë©”íƒ€ë°ì´í„° ì™„ì„±ë„
        metadata_score = len(entry.metadata) / 10  # 10ê°œ ë©”íƒ€ë°ì´í„° í•„ë“œ ê¸°ì¤€
        factors.append(("metadata_completeness", metadata_score, 0.15))
        
        # 3. ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ì ìˆ˜
        usage_score = min(entry.usage_count / 100, 1.0)
        factors.append(("usage_frequency", usage_score, 0.25))
        
        # 4. ìµœì‹ ì„±
        days_since_update = (datetime.now() - entry.updated_at).days
        recency_score = max(0, 1.0 - (days_since_update / 365))
        factors.append(("recency", recency_score, 0.2))
        
        # 5. ê´€ë ¨ í•­ëª© ìˆ˜
        relation_score = min(len(entry.related_entries) / 5, 1.0)
        factors.append(("relationships", relation_score, 0.2))
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        for factor_name, factor_score, weight in factors:
            score += factor_score * weight
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if score >= 0.9:
            grade = KnowledgeQuality.EXCELLENT
        elif score >= 0.75:
            grade = KnowledgeQuality.GOOD
        elif score >= 0.6:
            grade = KnowledgeQuality.ACCEPTABLE
        elif score >= 0.4:
            grade = KnowledgeQuality.NEEDS_REVIEW
        else:
            grade = KnowledgeQuality.OUTDATED
        
        return score, grade
    
    async def llm_quality_assessment(self, entry: EnhancedKnowledgeEntry) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ í’ˆì§ˆ í‰ê°€"""
        try:
            prompt = f"""
            Assess the quality of this knowledge entry:
            
            Title: {entry.title}
            Content: {entry.content}
            Metadata: {json.dumps(entry.metadata, indent=2)}
            Usage Count: {entry.usage_count}
            
            Evaluate on:
            1. Accuracy (0-10)
            2. Completeness (0-10)
            3. Clarity (0-10)
            4. Relevance (0-10)
            5. Usefulness (0-10)
            
            Provide specific feedback and suggestions for improvement.
            Return as JSON.
            """
            
            response = await self.llm_client.chat.completions.create(
                model="gpt-4",
                messages=[
                    {"role": "system", "content": "You are a knowledge quality expert. Provide detailed, objective assessments."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"LLM quality assessment failed: {e}")
            return {"error": str(e)}

class KnowledgeAnalyticsEngine:
    """ì§€ì‹ ë¶„ì„ ì—”ì§„"""
    
    def __init__(self):
        self.analytics_history: List[KnowledgeAnalytics] = []
    
    def generate_analytics(self, entries: Dict[str, EnhancedKnowledgeEntry]) -> KnowledgeAnalytics:
        """ì¢…í•© ë¶„ì„ ìƒì„±"""
        # í’ˆì§ˆ ë¶„í¬
        quality_distribution = {}
        for quality in KnowledgeQuality:
            quality_distribution[quality.value] = sum(
                1 for entry in entries.values() 
                if entry.quality_grade == quality
            )
        
        # ì‚¬ìš©ëŸ‰ í†µê³„
        usage_stats = {
            "total_usage": sum(entry.usage_count for entry in entries.values()),
            "average_usage": np.mean([entry.usage_count for entry in entries.values()]) if entries else 0,
            "max_usage": max([entry.usage_count for entry in entries.values()]) if entries else 0,
            "usage_distribution": self._calculate_usage_distribution(entries)
        }
        
        # ìƒìœ„ í•­ëª©ë“¤
        top_entries = sorted(
            [
                {
                    "id": entry.id,
                    "title": entry.title,
                    "usage_count": entry.usage_count,
                    "quality_score": entry.quality_score
                }
                for entry in entries.values()
            ],
            key=lambda x: x["usage_count"],
            reverse=True
        )[:10]
        
        # êµ¬ì‹ í•­ëª©ë“¤
        outdated_entries = [
            entry.id for entry in entries.values()
            if entry.quality_grade == KnowledgeQuality.OUTDATED
        ]
        
        # ì—…ë°ì´íŠ¸ ì¶”ì²œ
        update_recommendations = self._generate_update_recommendations(entries)
        
        # ì„±ì¥ íŠ¸ë Œë“œ
        growth_trends = self._calculate_growth_trends(entries)
        
        analytics = KnowledgeAnalytics(
            total_entries=len(entries),
            quality_distribution=quality_distribution,
            usage_statistics=usage_stats,
            top_entries=top_entries,
            outdated_entries=outdated_entries,
            update_recommendations=update_recommendations,
            growth_trends=growth_trends
        )
        
        self.analytics_history.append(analytics)
        return analytics
    
    def _calculate_usage_distribution(self, entries: Dict[str, EnhancedKnowledgeEntry]) -> Dict[str, int]:
        """ì‚¬ìš©ëŸ‰ ë¶„í¬ ê³„ì‚°"""
        if not entries:
            return {}
        
        usage_counts = [entry.usage_count for entry in entries.values()]
        return {
            "no_usage": sum(1 for count in usage_counts if count == 0),
            "low_usage": sum(1 for count in usage_counts if 1 <= count < 10),
            "medium_usage": sum(1 for count in usage_counts if 10 <= count < 50),
            "high_usage": sum(1 for count in usage_counts if count >= 50)
        }
    
    def _generate_update_recommendations(self, entries: Dict[str, EnhancedKnowledgeEntry]) -> List[Dict[str, Any]]:
        """ì—…ë°ì´íŠ¸ ì¶”ì²œ ìƒì„±"""
        recommendations = []
        
        for entry in entries.values():
            if entry.quality_grade == KnowledgeQuality.NEEDS_REVIEW:
                recommendations.append({
                    "entry_id": entry.id,
                    "title": entry.title,
                    "recommendation": "Quality review needed",
                    "priority": "high",
                    "reason": f"Quality score: {entry.quality_score}"
                })
            
            if entry.usage_count > 50 and entry.quality_score < 0.8:
                recommendations.append({
                    "entry_id": entry.id,
                    "title": entry.title,
                    "recommendation": "Improve high-usage content",
                    "priority": "medium",
                    "reason": f"High usage ({entry.usage_count}) but quality score {entry.quality_score}"
                })
        
        return recommendations[:20]  # ìƒìœ„ 20ê°œë§Œ
    
    def _calculate_growth_trends(self, entries: Dict[str, EnhancedKnowledgeEntry]) -> Dict[str, Any]:
        """ì„±ì¥ íŠ¸ë Œë“œ ê³„ì‚°"""
        # ìµœê·¼ 30ì¼, 7ì¼ ìƒì„±ëœ í•­ëª© ìˆ˜
        now = datetime.now()
        recent_30_days = sum(
            1 for entry in entries.values()
            if (now - entry.created_at).days <= 30
        )
        recent_7_days = sum(
            1 for entry in entries.values()
            if (now - entry.created_at).days <= 7
        )
        
        return {
            "entries_last_30_days": recent_30_days,
            "entries_last_7_days": recent_7_days,
            "average_daily_growth": recent_30_days / 30,
            "growth_acceleration": recent_7_days / 7 - recent_30_days / 30
        }

class EnhancedSharedKnowledgeBank:
    """í–¥ìƒëœ ê³µìœ  ì§€ì‹ ì€í–‰"""
    
    def __init__(self):
        self.knowledge_entries: Dict[str, EnhancedKnowledgeEntry] = {}
        self.knowledge_graph = nx.DiGraph()
        
        # ê³ ê¸‰ ì‹œìŠ¤í…œ ì»´í¬ë„ŒíŠ¸
        self.automated_updater = AutomatedKnowledgeUpdater(llm_client)
        self.quality_assessor = KnowledgeQualityAssessor(llm_client)
        self.analytics_engine = KnowledgeAnalyticsEngine()
        
        # ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬
        self.background_tasks: Set[asyncio.Task] = set()
        
        # ê¸°ì¡´ ì§€ì‹ ë¡œë“œ
        self._load_existing_knowledge()
        
        # ë°±ê·¸ë¼ìš´ë“œ ì—…ë°ì´íŠ¸ ì‹œì‘
        self._start_background_tasks()
        
        logger.info("ğŸ§  Enhanced Shared Knowledge Bank initialized")
    
    def _load_existing_knowledge(self):
        """ê¸°ì¡´ ì§€ì‹ ë¡œë“œ"""
        try:
            knowledge_file = KNOWLEDGE_BASE_DIR / "enhanced_knowledge_entries.json"
            if knowledge_file.exists():
                with open(knowledge_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for entry_data in data:
                        # ë‚ ì§œ í•„ë“œ ë³µì›
                        for date_field in ['created_at', 'updated_at', 'last_verified']:
                            if entry_data.get(date_field):
                                entry_data[date_field] = datetime.fromisoformat(entry_data[date_field])
                        
                        # Enum ë³µì›
                        if entry_data.get('quality_grade'):
                            entry_data['quality_grade'] = KnowledgeQuality(entry_data['quality_grade'])
                        
                        entry = EnhancedKnowledgeEntry(**entry_data)
                        self.knowledge_entries[entry.id] = entry
                        
            logger.info(f"Loaded {len(self.knowledge_entries)} knowledge entries")
            
        except Exception as e:
            logger.error(f"Failed to load knowledge: {e}")
    
    def _save_knowledge(self):
        """ì§€ì‹ ì €ì¥"""
        try:
            knowledge_file = KNOWLEDGE_BASE_DIR / "enhanced_knowledge_entries.json"
            
            # ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            serializable_data = []
            for entry in self.knowledge_entries.values():
                entry_dict = asdict(entry)
                
                # ë‚ ì§œ í•„ë“œ ë³€í™˜
                for date_field in ['created_at', 'updated_at', 'last_verified']:
                    if entry_dict.get(date_field):
                        entry_dict[date_field] = entry_dict[date_field].isoformat()
                
                # Enum ë³€í™˜
                if entry_dict.get('quality_grade'):
                    entry_dict['quality_grade'] = entry_dict['quality_grade'].value
                
                serializable_data.append(entry_dict)
            
            with open(knowledge_file, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, indent=2, ensure_ascii=False)
                
            logger.info(f"Saved {len(self.knowledge_entries)} knowledge entries")
            
        except Exception as e:
            logger.error(f"Failed to save knowledge: {e}")
    
    def _start_background_tasks(self):
        """ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì‹œì‘"""
        # ì£¼ê¸°ì  í’ˆì§ˆ í‰ê°€
        task1 = asyncio.create_task(self._periodic_quality_assessment())
        self.background_tasks.add(task1)
        task1.add_done_callback(self.background_tasks.discard)
        
        # ìë™ ì—…ë°ì´íŠ¸ ê°ì§€
        task2 = asyncio.create_task(self._periodic_update_detection())
        self.background_tasks.add(task2)
        task2.add_done_callback(self.background_tasks.discard)
        
        # ë¶„ì„ ìƒì„±
        task3 = asyncio.create_task(self._periodic_analytics_generation())
        self.background_tasks.add(task3)
        task3.add_done_callback(self.background_tasks.discard)
    
    async def _periodic_quality_assessment(self):
        """ì£¼ê¸°ì  í’ˆì§ˆ í‰ê°€"""
        while True:
            try:
                await asyncio.sleep(3600)  # 1ì‹œê°„ë§ˆë‹¤
                
                for entry in self.knowledge_entries.values():
                    if not entry.last_verified or (datetime.now() - entry.last_verified).hours >= 24:
                        score, grade = await self.quality_assessor.assess_quality(entry)
                        entry.quality_score = score
                        entry.quality_grade = grade
                        entry.last_verified = datetime.now()
                        entry.verification_status = "verified"
                
                self._save_knowledge()
                logger.info("Completed periodic quality assessment")
                
            except Exception as e:
                logger.error(f"Periodic quality assessment failed: {e}")
    
    async def _periodic_update_detection(self):
        """ì£¼ê¸°ì  ì—…ë°ì´íŠ¸ ê°ì§€"""
        while True:
            try:
                await asyncio.sleep(7200)  # 2ì‹œê°„ë§ˆë‹¤
                
                update_requests = await self.automated_updater.detect_update_needs(self.knowledge_entries)
                
                for request in update_requests:
                    if request.trigger in [UpdateTrigger.AUTOMATED, UpdateTrigger.QUALITY_DEGRADED]:
                        entry = self.knowledge_entries.get(request.entry_id)
                        if entry:
                            success = await self.automated_updater.apply_automated_updates(entry, request)
                            if success:
                                logger.info(f"Applied automated update to {entry.id}")
                
                self._save_knowledge()
                logger.info(f"Processed {len(update_requests)} update requests")
                
            except Exception as e:
                logger.error(f"Periodic update detection failed: {e}")
    
    async def _periodic_analytics_generation(self):
        """ì£¼ê¸°ì  ë¶„ì„ ìƒì„±"""
        while True:
            try:
                await asyncio.sleep(86400)  # 24ì‹œê°„ë§ˆë‹¤
                
                analytics = self.analytics_engine.generate_analytics(self.knowledge_entries)
                
                # ë¶„ì„ ê²°ê³¼ ì €ì¥
                analytics_file = ANALYTICS_DIR / f"analytics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(analytics_file, 'w', encoding='utf-8') as f:
                    # dataclassë¥¼ dictë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                    analytics_dict = asdict(analytics)
                    analytics_dict['generated_at'] = analytics_dict['generated_at'].isoformat()
                    json.dump(analytics_dict, f, indent=2, ensure_ascii=False)
                
                logger.info("Generated periodic analytics")
                
            except Exception as e:
                logger.error(f"Periodic analytics generation failed: {e}")
    
    async def add_knowledge_entry(self, entry: EnhancedKnowledgeEntry) -> str:
        """ì§€ì‹ í•­ëª© ì¶”ê°€ (í–¥ìƒëœ ë²„ì „)"""
        try:
            # í’ˆì§ˆ í‰ê°€
            score, grade = await self.quality_assessor.assess_quality(entry)
            entry.quality_score = score
            entry.quality_grade = grade
            entry.last_verified = datetime.now()
            entry.verification_status = "verified"
            
            # ì§€ì‹ ì €ì¥
            self.knowledge_entries[entry.id] = entry
            
            # ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
            self.knowledge_graph.add_node(entry.id, **asdict(entry))
            
            # ê´€ë ¨ í•­ëª©ê³¼ ì—°ê²°
            for related_id in entry.related_entries:
                if related_id in self.knowledge_entries:
                    self.knowledge_graph.add_edge(entry.id, related_id)
            
            # ì €ì¥
            self._save_knowledge()
            
            logger.info(f"ğŸ“ Added enhanced knowledge entry: {entry.title} (Quality: {grade.value})")
            return entry.id
            
        except Exception as e:
            logger.error(f"âŒ Error adding knowledge entry: {e}")
            raise
    
    async def get_analytics_dashboard(self) -> Dict[str, Any]:
        """ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë°ì´í„° ìƒì„±"""
        analytics = self.analytics_engine.generate_analytics(self.knowledge_entries)
        
        # ì—…ë°ì´íŠ¸ ìš”ì²­ í˜„í™©
        update_requests = await self.automated_updater.detect_update_needs(self.knowledge_entries)
        
        return {
            "overview": {
                "total_entries": analytics.total_entries,
                "quality_distribution": analytics.quality_distribution,
                "pending_updates": len(update_requests)
            },
            "quality_metrics": {
                "average_quality": np.mean([entry.quality_score for entry in self.knowledge_entries.values()]) if self.knowledge_entries else 0,
                "quality_distribution": analytics.quality_distribution,
                "verification_status": self._get_verification_status()
            },
            "usage_analytics": analytics.usage_statistics,
            "top_performers": analytics.top_entries,
            "improvement_opportunities": analytics.update_recommendations,
            "growth_trends": analytics.growth_trends,
            "update_requests": [
                {
                    "entry_id": req.entry_id,
                    "trigger": req.trigger.value,
                    "reason": req.reason,
                    "priority": req.priority
                } for req in update_requests[:10]
            ]
        }
    
    def _get_verification_status(self) -> Dict[str, int]:
        """ê²€ì¦ ìƒíƒœ ë¶„í¬"""
        status_counts = {}
        for entry in self.knowledge_entries.values():
            status = entry.verification_status
            status_counts[status] = status_counts.get(status, 0) + 1
        return status_counts

# A2A ì„œë²„ êµ¬í˜„
class EnhancedKnowledgeBankExecutor(AgentExecutor):
    """Enhanced Knowledge Bank A2A ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.knowledge_bank = EnhancedSharedKnowledgeBank()
        logger.info("ğŸ§  Enhanced Knowledge Bank Executor initialized")
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> Any:
        """A2A ìš”ì²­ ì‹¤í–‰ - í–¥ìƒëœ ê¸°ëŠ¥ í¬í•¨"""
        try:
            # ì‹œì‘ ì—…ë°ì´íŠ¸
            await task_updater.update_status(
                TaskState.working,
                message="ğŸ§  Enhanced Knowledge Bank ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
            )
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ íŒŒì‹±
            user_message = None
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if hasattr(part, 'root') and hasattr(part.root, 'text'):
                        user_message = part.root.text
                        break
            
            if not user_message:
                await task_updater.update_status(
                    TaskState.failed,
                    message="âŒ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
                return
            
            # ë©”ì‹œì§€ ë¶„ì„ ë° ì ì ˆí•œ ê¸°ëŠ¥ ì‹¤í–‰
            result = await self._process_enhanced_request(user_message, task_updater)
            
            # ê²°ê³¼ ì „ì†¡
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(result, ensure_ascii=False, indent=2))],
                name="enhanced_knowledge_result",
                metadata={"content_type": "application/json"}
            )
            
            await task_updater.update_status(
                TaskState.completed,
                message="âœ… Enhanced Knowledge Bank ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            )
            
        except Exception as e:
            logger.error(f"âŒ Enhanced Knowledge Bank error: {e}")
            await task_updater.update_status(
                TaskState.failed,
                message=f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
    
    async def _process_enhanced_request(self, user_message: str, task_updater: TaskUpdater) -> Dict[str, Any]:
        """í–¥ìƒëœ ìš”ì²­ ì²˜ë¦¬"""
        try:
            request_lower = user_message.lower()
            
            if "ëŒ€ì‹œë³´ë“œ" in request_lower or "dashboard" in request_lower:
                # ë¶„ì„ ëŒ€ì‹œë³´ë“œ
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ“Š ë¶„ì„ ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤..."
                )
                
                dashboard_data = await self.knowledge_bank.get_analytics_dashboard()
                
                return {
                    "type": "analytics_dashboard",
                    "data": dashboard_data,
                    "message": "ë¶„ì„ ëŒ€ì‹œë³´ë“œê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
                }
            
            elif "í’ˆì§ˆ" in request_lower or "quality" in request_lower:
                # í’ˆì§ˆ í‰ê°€
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ” ì§€ì‹ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤..."
                )
                
                quality_report = await self._generate_quality_report()
                
                return {
                    "type": "quality_assessment",
                    "data": quality_report,
                    "message": "í’ˆì§ˆ í‰ê°€ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                }
            
            elif "ì—…ë°ì´íŠ¸" in request_lower or "update" in request_lower:
                # ì—…ë°ì´íŠ¸ ê´€ë¦¬
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ”„ ì—…ë°ì´íŠ¸ ê´€ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤..."
                )
                
                update_status = await self._manage_updates()
                
                return {
                    "type": "update_management",
                    "data": update_status,
                    "message": "ì—…ë°ì´íŠ¸ ê´€ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
                }
            
            else:
                # ê¸°ë³¸ ê²€ìƒ‰ (ê¸°ì¡´ ê¸°ëŠ¥ ìœ ì§€)
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ” ì§€ì‹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤..."
                )
                
                # ê¸°ì¡´ ê²€ìƒ‰ ë¡œì§ (ìƒëµ - ê¸°ì¡´ shared_knowledge_bank.py ì°¸ì¡°)
                return {
                    "type": "knowledge_search",
                    "query": user_message,
                    "message": "ê²€ìƒ‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.",
                    "suggestion": "ê³ ê¸‰ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë ¤ë©´ 'ëŒ€ì‹œë³´ë“œ', 'í’ˆì§ˆ', 'ì—…ë°ì´íŠ¸' í‚¤ì›Œë“œë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
                }
            
        except Exception as e:
            logger.error(f"âŒ Error processing enhanced request: {e}")
            return {
                "type": "error",
                "message": f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    async def _generate_quality_report(self) -> Dict[str, Any]:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        entries = self.knowledge_bank.knowledge_entries
        
        # í’ˆì§ˆ ë¶„í¬
        quality_scores = [entry.quality_score for entry in entries.values()]
        quality_grades = [entry.quality_grade.value for entry in entries.values()]
        
        return {
            "total_entries": len(entries),
            "average_quality": np.mean(quality_scores) if quality_scores else 0,
            "quality_distribution": {grade: quality_grades.count(grade) for grade in set(quality_grades)},
            "top_quality_entries": [
                {"id": entry.id, "title": entry.title, "score": entry.quality_score}
                for entry in sorted(entries.values(), key=lambda x: x.quality_score, reverse=True)[:10]
            ],
            "improvement_needed": [
                {"id": entry.id, "title": entry.title, "score": entry.quality_score}
                for entry in entries.values()
                if entry.quality_grade in [KnowledgeQuality.NEEDS_REVIEW, KnowledgeQuality.OUTDATED]
            ]
        }
    
    async def _manage_updates(self) -> Dict[str, Any]:
        """ì—…ë°ì´íŠ¸ ê´€ë¦¬"""
        update_requests = await self.knowledge_bank.automated_updater.detect_update_needs(
            self.knowledge_bank.knowledge_entries
        )
        
        # ìë™ ì—…ë°ì´íŠ¸ ì ìš©
        applied_updates = 0
        for request in update_requests:
            if request.trigger in [UpdateTrigger.AUTOMATED, UpdateTrigger.SCHEDULED]:
                entry = self.knowledge_bank.knowledge_entries.get(request.entry_id)
                if entry:
                    success = await self.knowledge_bank.automated_updater.apply_automated_updates(entry, request)
                    if success:
                        applied_updates += 1
        
        return {
            "total_requests": len(update_requests),
            "applied_updates": applied_updates,
            "pending_manual_review": len([r for r in update_requests if r.trigger == UpdateTrigger.QUALITY_DEGRADED]),
            "update_summary": [
                {
                    "entry_id": req.entry_id,
                    "trigger": req.trigger.value,
                    "reason": req.reason,
                    "status": "applied" if req.trigger in [UpdateTrigger.AUTOMATED, UpdateTrigger.SCHEDULED] else "pending"
                } for req in update_requests[:20]
            ]
        }
    
    async def cancel(self, context: RequestContext, task_updater: TaskUpdater) -> Any:
        """ì‘ì—… ì·¨ì†Œ"""
        await task_updater.update_status(
            TaskState.cancelled,
            message="ğŸ›‘ Enhanced Knowledge Bank ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
        )

# Agent Card ì •ì˜
ENHANCED_AGENT_CARD = AgentCard(
    name="Enhanced Shared Knowledge Bank",
    description="ê³ ê¸‰ ê¸°ëŠ¥ì„ ê°–ì¶˜ A2A ì—ì´ì „íŠ¸ ê°„ ê³µìœ  ì§€ì‹ ì€í–‰ - ìë™í™”ëœ í’ˆì§ˆ ê´€ë¦¬, ì§€ëŠ¥í˜• ì—…ë°ì´íŠ¸, ì‹¤ì‹œê°„ ë¶„ì„ ëŒ€ì‹œë³´ë“œ",
    skills=[
        AgentSkill(
            name="automated_knowledge_management",
            description="ìë™í™”ëœ ì§€ì‹ í’ˆì§ˆ í‰ê°€ ë° ì—…ë°ì´íŠ¸ ì‹œìŠ¤í…œ"
        ),
        AgentSkill(
            name="intelligent_quality_assessment",
            description="LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì§€ì‹ í’ˆì§ˆ í‰ê°€ ë° ê°œì„  ì œì•ˆ"
        ),
        AgentSkill(
            name="analytics_dashboard",
            description="ì‹¤ì‹œê°„ ì§€ì‹ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ë° ì¸ì‚¬ì´íŠ¸ ì œê³µ"
        ),
        AgentSkill(
            name="automated_updates",
            description="íŒ¨í„´ ê¸°ë°˜ ìë™ ì§€ì‹ ì—…ë°ì´íŠ¸ ë° ìƒëª…ì£¼ê¸° ê´€ë¦¬"
        ),
        AgentSkill(
            name="smart_notifications",
            description="ì§€ëŠ¥í˜• ì•Œë¦¼ ë° ì¶”ì²œ ì‹œìŠ¤í…œ"
        )
    ],
    capabilities=AgentCapabilities(
        supports_streaming=True,
        supports_cancellation=True,
        supports_artifacts=True
    )
)

# ë©”ì¸ ì‹¤í–‰
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # A2A ì„œë²„ ì„¤ì •
    task_store = InMemoryTaskStore()
    request_handler = DefaultRequestHandler(task_store)
    
    # ì—ì´ì „íŠ¸ ë“±ë¡
    executor = EnhancedKnowledgeBankExecutor()
    request_handler.register_agent(ENHANCED_AGENT_CARD, executor)
    
    # ì•± ìƒì„±
    app = A2AStarletteApplication(
        request_handler=request_handler,
        agent_card=ENHANCED_AGENT_CARD
    )
    
    # ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Starting Enhanced Shared Knowledge Bank Server on port 8602")
    
    config = uvicorn.Config(
        app=app,
        host="0.0.0.0",
        port=8602,
        log_level="info"
    )
    
    server = uvicorn.Server(config)
    await server.serve()

if __name__ == "__main__":
    asyncio.run(main()) 