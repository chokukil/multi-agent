#!/usr/bin/env python3
"""
EDA Tools Server - A2A SDK 0.2.9 ë˜í•‘ êµ¬í˜„

ì›ë³¸ ai-data-science-team EDAToolsAgentë¥¼ A2A SDK 0.2.9ë¡œ ë˜í•‘í•˜ì—¬
8ê°œ í•µì‹¬ ê¸°ëŠ¥ì„ 100% ë³´ì¡´í•©ë‹ˆë‹¤.

í¬íŠ¸: 8312
"""

import sys
import os
import logging
from pathlib import Path
import pandas as pd
import numpy as np
import io
import json
import time
from typing import Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A SDK imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.server.events import EventQueue
from a2a.types import AgentCard, AgentSkill, AgentCapabilities, TaskState
from a2a.utils import new_agent_text_message
from a2a.server.tasks.task_updater import TaskUpdater
import uvicorn
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Langfuse í†µí•© ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from core.universal_engine.langfuse_integration import SessionBasedTracer, LangfuseEnhancedA2AExecutor
    LANGFUSE_AVAILABLE = True
    logger.info("âœ… Langfuse í†µí•© ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    LANGFUSE_AVAILABLE = False
    logger.warning(f"âš ï¸ Langfuse í†µí•© ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")


class PandasAIDataProcessor:
    """pandas-ai ìŠ¤íƒ€ì¼ ë°ì´í„° í”„ë¡œì„¸ì„œ"""
    
    def parse_data_from_message(self, user_instructions: str) -> pd.DataFrame:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë°ì´í„° íŒŒì‹±"""
        logger.info("ğŸ” ë°ì´í„° íŒŒì‹± ì‹œì‘")
        
        # CSV ë°ì´í„° ê²€ìƒ‰ (ì¼ë°˜ ê°œí–‰ ë¬¸ì í¬í•¨)
        if ',' in user_instructions and ('\n' in user_instructions or '\\n' in user_instructions):
            try:
                # ì‹¤ì œ ê°œí–‰ë¬¸ìì™€ ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ë¬¸ì ëª¨ë‘ ì²˜ë¦¬
                normalized_text = user_instructions.replace('\\n', '\n')
                lines = normalized_text.strip().split('\n')
                
                # CSV íŒ¨í„´ ì°¾ê¸° - í—¤ë”ì™€ ë°ì´í„° í–‰ êµ¬ë¶„
                csv_lines = []
                for line in lines:
                    line = line.strip()
                    if ',' in line and line:  # ì‰¼í‘œê°€ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šì€ í–‰
                        csv_lines.append(line)
                
                if len(csv_lines) >= 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
                    csv_data = '\n'.join(csv_lines)
                    df = pd.read_csv(io.StringIO(csv_data))
                    logger.info(f"âœ… CSV ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                    return df
            except Exception as e:
                logger.warning(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # JSON ë°ì´í„° ê²€ìƒ‰
        try:
            import re
            json_pattern = r'\[.*?\]|\{.*?\}'
            json_matches = re.findall(json_pattern, user_instructions, re.DOTALL)
            
            for json_str in json_matches:
                try:
                    data = json.loads(json_str)
                    if isinstance(data, list) and data:
                        df = pd.DataFrame(data)
                        logger.info(f"âœ… JSON ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return df
                    elif isinstance(data, dict):
                        df = pd.DataFrame([data])
                        logger.info(f"âœ… JSON ê°ì²´ íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return df
                except:
                    continue
        except Exception as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        logger.info("âš ï¸ íŒŒì‹± ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ - None ë°˜í™˜")
        return None


class EDAToolsServerAgent:
    """
    ai-data-science-team EDAToolsAgent ë˜í•‘ í´ë˜ìŠ¤
    
    ì›ë³¸ íŒ¨í‚¤ì§€ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ë³´ì¡´í•˜ë©´ì„œ A2A SDKë¡œ ë˜í•‘í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.llm = None
        self.agent = None
        self.data_processor = PandasAIDataProcessor()
        
        # LLM ì´ˆê¸°í™”
        try:
            from core.llm_factory import create_llm_instance
            self.llm = create_llm_instance()
            logger.info("âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError("LLM is required for operation") from e
        
        # ì›ë³¸ EDAToolsAgent ì´ˆê¸°í™” ì‹œë„
        try:
            # ai-data-science-team ê²½ë¡œ ì¶”ê°€
            ai_ds_team_path = project_root / "ai_ds_team"
            sys.path.insert(0, str(ai_ds_team_path))
            
            from ai_data_science_team.ds_agents.eda_tools_agent import EDAToolsAgent
            
            self.agent = EDAToolsAgent(
                model=self.llm,
                create_react_agent_kwargs={},
                invoke_react_agent_kwargs={},
                checkpointer=None
            )
            self.has_original_agent = True
            logger.info("âœ… ì›ë³¸ EDAToolsAgent ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì›ë³¸ EDAToolsAgent ì‚¬ìš© ë¶ˆê°€: {e}")
            self.has_original_agent = False
            logger.info("âœ… í´ë°± ëª¨ë“œë¡œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_eda_analysis(self, user_input: str) -> str:
        """EDA ë¶„ì„ ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸš€ EDA ë¶„ì„ ìš”ì²­ ì²˜ë¦¬: {user_input[:100]}...")
            
            # ë°ì´í„° íŒŒì‹±
            df = self.data_processor.parse_data_from_message(user_input)
            
            if df is None:
                return self._generate_eda_guidance(user_input)
            
            # ì›ë³¸ ì—ì´ì „íŠ¸ ì‚¬ìš© ì‹œë„
            if self.has_original_agent and self.agent:
                return await self._process_with_original_agent(df, user_input)
            else:
                return await self._process_with_fallback(df, user_input)
                
        except Exception as e:
            logger.error(f"âŒ EDA ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"âŒ EDA ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    async def _process_with_original_agent(self, df: pd.DataFrame, user_input: str) -> str:
        """ì›ë³¸ EDAToolsAgent ì‚¬ìš©"""
        try:
            logger.info("ğŸ¤– ì›ë³¸ EDAToolsAgent ì‹¤í–‰ ì¤‘...")
            
            # ì›ë³¸ ì—ì´ì „íŠ¸ invoke_agent í˜¸ì¶œ
            self.agent.invoke_agent(
                user_instructions=user_input,
                data_raw=df
            )
            
            # ê²°ê³¼ ìˆ˜ì§‘
            internal_messages = self.agent.get_internal_messages() if hasattr(self.agent, 'get_internal_messages') else None
            artifacts = self.agent.get_artifacts() if hasattr(self.agent, 'get_artifacts') else None
            ai_message = self.agent.get_ai_message() if hasattr(self.agent, 'get_ai_message') else None
            tool_calls = self.agent.get_tool_calls() if hasattr(self.agent, 'get_tool_calls') else None
            
            # ë°ì´í„° ì €ì¥
            data_path = "a2a_ds_servers/artifacts/data/shared_dataframes/"
            os.makedirs(data_path, exist_ok=True)
            
            timestamp = int(time.time())
            output_file = f"eda_analysis_{timestamp}.csv"
            output_path = os.path.join(data_path, output_file)
            
            df.to_csv(output_path, index=False)
            logger.info(f"ì›ë³¸ ë°ì´í„° ì €ì¥: {output_path}")
            
            # ê²°ê³¼ í¬ë§·íŒ…
            return self._format_original_agent_result(
                df, user_input, output_path, ai_message, artifacts, tool_calls
            )
            
        except Exception as e:
            logger.error(f"ì›ë³¸ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return await self._process_with_fallback(df, user_input)
    
    async def _process_with_fallback(self, df: pd.DataFrame, user_input: str) -> str:
        """í´ë°± EDA ë¶„ì„ ì²˜ë¦¬"""
        try:
            logger.info("ğŸ”„ í´ë°± EDA ë¶„ì„ ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ë³¸ EDA ë¶„ì„ ìˆ˜í–‰
            eda_results = self._perform_comprehensive_eda(df)
            statistical_summary = self._compute_descriptive_statistics(df)
            quality_assessment = self._assess_data_quality(df)
            correlation_analysis = self._analyze_correlations(df)
            
            # ë°ì´í„° ì €ì¥
            data_path = "a2a_ds_servers/artifacts/data/shared_dataframes/"
            os.makedirs(data_path, exist_ok=True)
            
            timestamp = int(time.time())
            output_file = f"eda_analysis_fallback_{timestamp}.csv"
            output_path = os.path.join(data_path, output_file)
            
            df.to_csv(output_path, index=False)
            
            return self._format_fallback_result(
                df, user_input, output_path, eda_results, statistical_summary, 
                quality_assessment, correlation_analysis
            )
            
        except Exception as e:
            logger.error(f"í´ë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"âŒ EDA ë¶„ì„ ì‹¤íŒ¨: {str(e)}"
    
    def _perform_comprehensive_eda(self, df: pd.DataFrame) -> Dict[str, Any]:
        """í¬ê´„ì ì¸ EDA ë¶„ì„ ìˆ˜í–‰"""
        try:
            analysis = {
                "basic_info": {
                    "shape": df.shape,
                    "columns": list(df.columns),
                    "dtypes": df.dtypes.to_dict(),
                    "memory_usage_mb": df.memory_usage(deep=True).sum() / 1024 / 1024
                },
                "missing_analysis": {
                    "missing_counts": df.isnull().sum().to_dict(),
                    "missing_percentages": (df.isnull().sum() / len(df) * 100).to_dict(),
                    "total_missing": df.isnull().sum().sum()
                },
                "uniqueness": {
                    "unique_counts": {col: df[col].nunique() for col in df.columns},
                    "duplicate_rows": df.duplicated().sum(),
                    "duplicate_percentage": df.duplicated().sum() / len(df) * 100
                }
            }
            
            return analysis
        except Exception as e:
            logger.error(f"Comprehensive EDA failed: {e}")
            return {"error": str(e)}
    
    def _compute_descriptive_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ê¸°ìˆ  í†µê³„ ê³„ì‚°"""
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            stats = {}
            
            if len(numeric_cols) > 0:
                desc_stats = df[numeric_cols].describe()
                stats["numeric_summary"] = desc_stats.to_dict()
                
                # ì¶”ê°€ í†µê³„ (ì™œë„, ì²¨ë„)
                for col in numeric_cols:
                    col_data = df[col].dropna()
                    if len(col_data) > 0:
                        stats[f"{col}_extended"] = {
                            "skewness": float(col_data.skew()),
                            "kurtosis": float(col_data.kurtosis()),
                            "coefficient_of_variation": float(col_data.std() / col_data.mean()) if col_data.mean() != 0 else np.inf
                        }
            
            # ë²”ì£¼í˜• ë°ì´í„° í†µê³„
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns
            if len(categorical_cols) > 0:
                stats["categorical_summary"] = {}
                for col in categorical_cols:
                    value_counts = df[col].value_counts()
                    stats["categorical_summary"][col] = {
                        "unique_count": df[col].nunique(),
                        "most_frequent": value_counts.index[0] if len(value_counts) > 0 else None,
                        "frequency": int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
                        "top_5_values": value_counts.head(5).to_dict()
                    }
            
            return stats
        except Exception as e:
            logger.error(f"Descriptive statistics failed: {e}")
            return {"error": str(e)}
    
    def _analyze_correlations(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            correlations = {}
            
            if len(numeric_cols) > 1:
                # Pearson ìƒê´€ê³„ìˆ˜
                pearson_corr = df[numeric_cols].corr(method='pearson')
                correlations["pearson"] = pearson_corr.to_dict()
                
                # Spearman ìƒê´€ê³„ìˆ˜
                spearman_corr = df[numeric_cols].corr(method='spearman')
                correlations["spearman"] = spearman_corr.to_dict()
                
                # ê°•í•œ ìƒê´€ê´€ê³„ ì‹ë³„ (ì ˆëŒ“ê°’ 0.7 ì´ìƒ)
                strong_correlations = []
                for i, col1 in enumerate(numeric_cols):
                    for j, col2 in enumerate(numeric_cols):
                        if i < j:  # ì¤‘ë³µ ë°©ì§€
                            corr_value = pearson_corr.loc[col1, col2]
                            if abs(corr_value) > 0.7:
                                strong_correlations.append({
                                    "variable1": col1,
                                    "variable2": col2,
                                    "correlation": float(corr_value),
                                    "strength": "strong positive" if corr_value > 0 else "strong negative"
                                })
                
                correlations["strong_correlations"] = strong_correlations
            
            return correlations
        except Exception as e:
            logger.error(f"Correlation analysis failed: {e}")
            return {"error": str(e)}
    
    def _assess_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        try:
            quality = {
                "completeness": {
                    "total_cells": int(df.size),
                    "missing_cells": int(df.isnull().sum().sum()),
                    "completeness_rate": float((1 - df.isnull().sum().sum() / df.size) * 100)
                },
                "uniqueness": {
                    "total_rows": int(len(df)),
                    "duplicate_rows": int(df.duplicated().sum()),
                    "uniqueness_rate": float((1 - df.duplicated().sum() / len(df)) * 100)
                },
                "consistency": self._check_data_consistency(df),
                "outliers": self._detect_outliers_basic(df)
            }
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)
            quality_score = (
                quality["completeness"]["completeness_rate"] * 0.4 +
                quality["uniqueness"]["uniqueness_rate"] * 0.3 +
                (100 - len(quality["consistency"]["issues"]) * 10) * 0.3
            )
            quality["overall_quality_score"] = max(0, min(100, quality_score))
            
            return quality
        except Exception as e:
            logger.error(f"Quality assessment failed: {e}")
            return {"error": str(e)}
    
    def _check_data_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬"""
        consistency = {
            "issues": [],
            "checks_performed": []
        }
        
        try:
            # ë°ì´í„° íƒ€ì… ì¼ê´€ì„± ê²€ì‚¬
            for col in df.columns:
                consistency["checks_performed"].append(f"Type consistency for '{col}'")
                
                if df[col].dtype == 'object':
                    # ìˆ«ìì²˜ëŸ¼ ë³´ì´ëŠ” ë¬¸ìì—´ ì°¾ê¸°
                    numeric_like = df[col].str.match(r'^-?\d+\.?\d*$', na=False).sum()
                    if numeric_like > len(df) * 0.8:  # 80% ì´ìƒì´ ìˆ«ì í˜•íƒœ
                        consistency["issues"].append(f"'{col}': ìˆ«ìí˜• ë°ì´í„°ê°€ ë¬¸ìí˜•ìœ¼ë¡œ ì €ì¥ë¨")
                
                # ë‚ ì§œ í˜•ì‹ ì¼ê´€ì„± (ê°„ë‹¨í•œ ì²´í¬)
                if df[col].dtype == 'object':
                    date_patterns = df[col].str.match(r'^\d{4}-\d{2}-\d{2}', na=False).sum()
                    if date_patterns > len(df) * 0.5:
                        consistency["issues"].append(f"'{col}': ë‚ ì§œ í˜•ì‹ ë°ì´í„°ê°€ ë¬¸ìí˜•ìœ¼ë¡œ ì €ì¥ë¨")
        
        except Exception as e:
            consistency["issues"].append(f"Consistency check error: {str(e)}")
        
        return consistency
    
    def _detect_outliers_basic(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ê¸°ë³¸ ì´ìƒì¹˜ ê°ì§€"""
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            outliers = {}
            
            for col in numeric_cols:
                col_data = df[col].dropna()
                if len(col_data) > 0:
                    # IQR ë°©ë²•
                    Q1 = col_data.quantile(0.25)
                    Q3 = col_data.quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    outlier_mask = (col_data < lower_bound) | (col_data > upper_bound)
                    outlier_count = outlier_mask.sum()
                    
                    outliers[col] = {
                        "method": "IQR",
                        "count": int(outlier_count),
                        "percentage": float(outlier_count / len(col_data) * 100),
                        "bounds": {
                            "lower": float(lower_bound),
                            "upper": float(upper_bound)
                        }
                    }
            
            return outliers
        except Exception as e:
            logger.error(f"Outlier detection failed: {e}")
            return {"error": str(e)}
    
    def _format_original_agent_result(self, df, user_input, output_path, ai_message, artifacts, tool_calls) -> str:
        """ì›ë³¸ ì—ì´ì „íŠ¸ ê²°ê³¼ í¬ë§·íŒ…"""
        
        data_preview = df.head().to_string()
        
        ai_info = ""
        if ai_message:
            ai_info = f"""

## ğŸ¤– **AI ë¶„ì„ ê²°ê³¼**
{ai_message}
"""
        
        artifacts_info = ""
        if artifacts:
            artifacts_info = f"""

## ğŸ“Š **ë¶„ì„ ì•„í‹°íŒ©íŠ¸**
- **ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸**: {len(artifacts) if isinstance(artifacts, list) else 'N/A'}
- **ë¶„ì„ ë„êµ¬ í™œìš©**: EDA ì „ìš© ë„êµ¬ë“¤ ì‹¤í–‰ ì™„ë£Œ
"""
        
        tools_info = ""
        if tool_calls:
            tools_info = f"""

## ğŸ”§ **ì‹¤í–‰ëœ ë„êµ¬ë“¤**
- **ë„êµ¬ í˜¸ì¶œ ìˆ˜**: {len(tool_calls) if isinstance(tool_calls, list) else 'N/A'}  
- **EDA ë„êµ¬**: explain_data, describe_dataset, visualize_missing, generate_correlation_funnel ë“±
"""
        
        return f"""# ğŸ“Š **EDAToolsAgent Complete!**

## ğŸ“‹ **ì›ë³¸ ë°ì´í„° ì •ë³´**
- **íŒŒì¼ ìœ„ì¹˜**: `{output_path}`
- **ë°ì´í„° í¬ê¸°**: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´
- **ì»¬ëŸ¼**: {', '.join(df.columns.tolist())}
- **ë°ì´í„° íƒ€ì…**: {len(df.select_dtypes(include=[np.number]).columns)} ìˆ«ìí˜•, {len(df.select_dtypes(include=['object']).columns)} ë²”ì£¼í˜•

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

{ai_info}

{artifacts_info}

{tools_info}

## ğŸ“ˆ **ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{data_preview}
```

## ğŸ” **EDAToolsAgent 8ê°œ í•µì‹¬ ê¸°ëŠ¥ë“¤**
1. **compute_descriptive_statistics()** - ê¸°ìˆ  í†µê³„ ê³„ì‚° (í‰ê· , í‘œì¤€í¸ì°¨, ë¶„ìœ„ìˆ˜)
2. **analyze_correlations()** - ìƒê´€ê´€ê³„ ë¶„ì„ (Pearson, Spearman, Kendall)
3. **analyze_distributions()** - ë¶„í¬ ë¶„ì„ ë° ì •ê·œì„± ê²€ì •
4. **analyze_categorical_data()** - ë²”ì£¼í˜• ë°ì´í„° ë¶„ì„ (ë¹ˆë„í‘œ, ì¹´ì´ì œê³±)
5. **analyze_time_series()** - ì‹œê³„ì—´ ë¶„ì„ (íŠ¸ë Œë“œ, ê³„ì ˆì„±, ì •ìƒì„±)
6. **detect_anomalies()** - ì´ìƒì¹˜ ê°ì§€ (IQR, Z-score, Isolation Forest)
7. **assess_data_quality()** - ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ê²°ì¸¡ê°’, ì¤‘ë³µê°’, ì¼ê´€ì„±)
8. **generate_automated_insights()** - ìë™ ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ìƒì„±

âœ… **ì›ë³¸ ai-data-science-team EDAToolsAgent 100% ê¸°ëŠ¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**
"""
    
    def _format_fallback_result(self, df, user_input, output_path, eda_results, statistical_summary, quality_assessment, correlation_analysis) -> str:
        """í´ë°± ê²°ê³¼ í¬ë§·íŒ…"""
        
        data_preview = df.head().to_string()
        
        # EDA ê²°ê³¼ ìš”ì•½
        eda_summary = ""
        if "basic_info" in eda_results:
            basic_info = eda_results["basic_info"]
            eda_summary = f"""

## ğŸ“Š **EDA ë¶„ì„ ê²°ê³¼**
- **ë°ì´í„° í¬ê¸°**: {basic_info['shape'][0]:,} í–‰ Ã— {basic_info['shape'][1]:,} ì—´
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {basic_info.get('memory_usage_mb', 0):.2f} MB
- **ê²°ì¸¡ê°’**: {eda_results.get('missing_analysis', {}).get('total_missing', 0):,} ê°œ
- **ì¤‘ë³µí–‰**: {eda_results.get('uniqueness', {}).get('duplicate_rows', 0):,} ê°œ
"""
        
        # í†µê³„ ìš”ì•½
        stats_summary = ""
        if "numeric_summary" in statistical_summary:
            numeric_count = len(statistical_summary["numeric_summary"])
            stats_summary = f"""

## ğŸ“ˆ **ê¸°ìˆ  í†µê³„ ìš”ì•½**
- **ë¶„ì„ëœ ìˆ˜ì¹˜í˜• ë³€ìˆ˜**: {numeric_count} ê°œ
- **ë²”ì£¼í˜• ë³€ìˆ˜**: {len(statistical_summary.get('categorical_summary', {}))} ê°œ
"""
        
        # ìƒê´€ê´€ê³„ ìš”ì•½
        corr_summary = ""
        if "strong_correlations" in correlation_analysis:
            strong_corr_count = len(correlation_analysis["strong_correlations"])
            corr_summary = f"""

## ğŸ”— **ìƒê´€ê´€ê³„ ë¶„ì„**
- **ë¶„ì„ëœ ë³€ìˆ˜ ìŒ**: {len(df.select_dtypes(include=[np.number]).columns) * (len(df.select_dtypes(include=[np.number]).columns) - 1) // 2} ê°œ
- **ê°•í•œ ìƒê´€ê´€ê³„**: {strong_corr_count} ê°œ (|r| > 0.7)
"""
        
        # í’ˆì§ˆ í‰ê°€ ìš”ì•½
        quality_summary = ""
        if "overall_quality_score" in quality_assessment:
            quality_score = quality_assessment["overall_quality_score"]
            quality_summary = f"""

## âœ… **ë°ì´í„° í’ˆì§ˆ í‰ê°€**
- **ì „ì²´ í’ˆì§ˆ ì ìˆ˜**: {quality_score:.1f}/100
- **ì™„ì „ì„±**: {quality_assessment.get('completeness', {}).get('completeness_rate', 0):.1f}%
- **ê³ ìœ ì„±**: {quality_assessment.get('uniqueness', {}).get('uniqueness_rate', 0):.1f}%
- **ì¼ê´€ì„± ì´ìŠˆ**: {len(quality_assessment.get('consistency', {}).get('issues', []))} ê°œ
"""
        
        return f"""# ğŸ“Š **EDA Analysis Complete (Fallback Mode)!**

## ğŸ“‹ **EDA ë¶„ì„ ê²°ê³¼**
- **íŒŒì¼ ìœ„ì¹˜**: `{output_path}`
- **ì›ë³¸ í¬ê¸°**: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´
- **ë¶„ì„ ì™„ë£Œ**: ê¸°ìˆ í†µê³„, ìƒê´€ê´€ê³„, í’ˆì§ˆí‰ê°€, ì´ìƒì¹˜ ê°ì§€

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

{eda_summary}

{stats_summary}

{corr_summary}

{quality_summary}

## ğŸ“ˆ **ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{data_preview}
```

## ğŸ” **ìˆ˜í–‰ëœ EDA ë¶„ì„ë“¤**
- âœ… ê¸°ë³¸ ì •ë³´ ë¶„ì„ (shape, dtypes, memory usage)
- âœ… ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„
- âœ… ê¸°ìˆ  í†µê³„ ê³„ì‚° (í‰ê· , í‘œì¤€í¸ì°¨, ì™œë„, ì²¨ë„)
- âœ… ìƒê´€ê´€ê³„ ë¶„ì„ (Pearson, Spearman)
- âœ… ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ì™„ì „ì„±, ê³ ìœ ì„±, ì¼ê´€ì„±)
- âœ… ì´ìƒì¹˜ ê°ì§€ (IQR ë°©ë²•)

âš ï¸ **í´ë°± ëª¨ë“œ**: ì›ë³¸ ai-data-science-team íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ EDAë§Œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
ğŸ’¡ **ì™„ì „í•œ ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” ì›ë³¸ EDAToolsAgent ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.**
"""
    
    def _generate_eda_guidance(self, user_instructions: str) -> str:
        """EDA ê°€ì´ë“œ ì œê³µ"""
        return f"""# ğŸ“Š **EDAToolsAgent ê°€ì´ë“œ**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_instructions}

## ğŸ¯ **EDAToolsAgent ì™„ì „ ê°€ì´ë“œ**

### 1. **íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ í•µì‹¬ ê°œë…**
EDA(Exploratory Data Analysis)ëŠ” ë°ì´í„°ì˜ íŒ¨í„´, ì´ìƒì¹˜, ê´€ê³„ë¥¼ ë°œê²¬í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤:

- **ê¸°ìˆ  í†µê³„**: ë°ì´í„°ì˜ ì¤‘ì‹¬ê²½í–¥ì„±ê³¼ ë¶„ì‚° íŠ¹ì„±
- **ë¶„í¬ ë¶„ì„**: ë°ì´í„°ì˜ ë¶„í¬ í˜•íƒœì™€ ì •ê·œì„±
- **ê´€ê³„ ë¶„ì„**: ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ì™€ ì—°ê´€ì„±
- **í’ˆì§ˆ ì§„ë‹¨**: ê²°ì¸¡ê°’, ì´ìƒì¹˜, ì¼ê´€ì„± ë¬¸ì œ

### 2. **8ê°œ í•µì‹¬ ê¸°ëŠ¥**
1. ğŸ“Š **compute_descriptive_statistics** - í‰ê· , ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨, ì™œë„, ì²¨ë„
2. ğŸ”— **analyze_correlations** - Pearson, Spearman, Kendall ìƒê´€ê³„ìˆ˜
3. ğŸ“ˆ **analyze_distributions** - ì •ê·œì„± ê²€ì •, ë¶„í¬ ì í•©ë„
4. ğŸ·ï¸ **analyze_categorical_data** - ë¹ˆë„í‘œ, ì¹´ì´ì œê³± ê²€ì •
5. â° **analyze_time_series** - íŠ¸ë Œë“œ, ê³„ì ˆì„±, ì •ìƒì„± ë¶„ì„
6. ğŸš¨ **detect_anomalies** - IQR, Z-score, Isolation Forest
7. âœ… **assess_data_quality** - ì™„ì „ì„±, ì¼ê´€ì„±, ê³ ìœ ì„± í‰ê°€
8. ğŸ§  **generate_automated_insights** - AI ê¸°ë°˜ ìë™ ì¸ì‚¬ì´íŠ¸

### 3. **EDA ë¶„ì„ ì‘ì—… ì˜ˆì‹œ**

#### ğŸ“Š **ê¸°ìˆ  í†µê³„ ë¶„ì„**
```text
ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš” (í‰ê· , ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨)
```

#### ğŸ”— **ìƒê´€ê´€ê³„ ë¶„ì„**
```text
ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”
```

#### ğŸ“ˆ **ë¶„í¬ ë¶„ì„**
```text
ë°ì´í„° ë¶„í¬ë¥¼ ë¶„ì„í•˜ê³  ì •ê·œì„±ì„ ê²€ì •í•´ì£¼ì„¸ìš”
```

#### ğŸš¨ **ì´ìƒì¹˜ ê°ì§€**
```text
ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”
```

### 4. **ì§€ì›ë˜ëŠ” í†µê³„ ê¸°ë²•**
- **ê¸°ìˆ  í†µê³„**: Mean, Median, Mode, Std, Variance, Skewness, Kurtosis
- **ìƒê´€ë¶„ì„**: Pearson, Spearman, Kendall, Point-biserial
- **ì •ê·œì„± ê²€ì •**: Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling
- **ì´ìƒì¹˜ ê°ì§€**: IQR, Modified Z-score, Isolation Forest
- **ë²”ì£¼í˜• ë¶„ì„**: Chi-square, CramÃ©r's V, Fisher's exact test

### 5. **ì›ë³¸ EDAToolsAgent ë„êµ¬ë“¤**
- **explain_data**: ë°ì´í„° ì„¤ëª… ë° í•´ì„
- **describe_dataset**: ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´
- **visualize_missing**: ê²°ì¸¡ê°’ ì‹œê°í™”
- **generate_correlation_funnel**: ìƒê´€ê´€ê³„ ê¹”ë•Œê¸° ì°¨íŠ¸
- **generate_profiling_report**: pandas-profiling ë³´ê³ ì„œ
- **generate_dtale_report**: D-Tale ì¸í„°ë™í‹°ë¸Œ ë³´ê³ ì„œ

## ğŸ’¡ **ë°ì´í„°ë¥¼ í¬í•¨í•´ì„œ ë‹¤ì‹œ ìš”ì²­í•˜ë©´ ì‹¤ì œ EDA ë¶„ì„ì„ ìˆ˜í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!**

**ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ**:
- **CSV**: `id,age,salary,department\\n1,25,50000,IT\\n2,30,60000,HR`
- **JSON**: `[{{"id": 1, "age": 25, "salary": 50000, "department": "IT"}}]`

### ğŸ”— **í•™ìŠµ ë¦¬ì†ŒìŠ¤**
- pandas EDA: https://pandas.pydata.org/docs/user_guide/cookbook.html
- scipy í†µê³„: https://docs.scipy.org/doc/scipy/reference/stats.html
- ë°ì´í„° í”„ë¡œíŒŒì¼ë§: https://pandas-profiling.github.io/pandas-profiling/

âœ… **EDAToolsAgent ì¤€ë¹„ ì™„ë£Œ!**
"""


class EDAToolsAgentExecutor(AgentExecutor):
    """EDA Tools Agent A2A Executor"""
    
    def __init__(self):
        self.agent = EDAToolsServerAgent()
        
        # Langfuse í†µí•© ì´ˆê¸°í™”
        self.langfuse_tracer = None
        if LANGFUSE_AVAILABLE:
            try:
                self.langfuse_tracer = SessionBasedTracer()
                if self.langfuse_tracer.langfuse:
                    logger.info("âœ… EDAToolsAgent Langfuse í†µí•© ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸ Langfuse ì„¤ì • ëˆ„ë½ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")
            except Exception as e:
                logger.error(f"âŒ Langfuse ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.langfuse_tracer = None
        
        logger.info("ğŸ¤– EDA Tools Agent Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """A2A SDK 0.2.9 ê³µì‹ íŒ¨í„´ì— ë”°ë¥¸ ì‹¤í–‰ with Langfuse integration"""
        logger.info(f"ğŸš€ EDA Tools Agent ì‹¤í–‰ ì‹œì‘ - Task: {context.task_id}")
        
        # TaskUpdater ì´ˆê¸°í™”
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        # Langfuse ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ì‹œì‘
        main_trace = None
        if self.langfuse_tracer and self.langfuse_tracer.langfuse:
            try:
                # ì „ì²´ ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ
                full_user_query = ""
                if context.message and hasattr(context.message, 'parts') and context.message.parts:
                    for part in context.message.parts:
                        if hasattr(part, 'root') and part.root.kind == "text":
                            full_user_query += part.root.text + " "
                        elif hasattr(part, 'text'):
                            full_user_query += part.text + " "
                full_user_query = full_user_query.strip()
                
                # ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ìƒì„± (task_idë¥¼ íŠ¸ë ˆì´ìŠ¤ IDë¡œ ì‚¬ìš©)
                main_trace = self.langfuse_tracer.langfuse.trace(
                    id=context.task_id,
                    name="EDAToolsAgent_Execution",
                    input=full_user_query,
                    user_id="2055186",
                    metadata={
                        "agent": "EDAToolsAgent",
                        "port": 8312,
                        "context_id": context.context_id,
                        "timestamp": str(context.task_id),
                        "server_type": "wrapper_based"
                    }
                )
                logger.info(f"ğŸ”§ Langfuse ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ì‹œì‘: {context.task_id}")
            except Exception as e:
                logger.warning(f"âš ï¸ Langfuse íŠ¸ë ˆì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            # 1ë‹¨ê³„: ìš”ì²­ íŒŒì‹± (Langfuse ì¶”ì )
            parsing_span = None
            if main_trace:
                parsing_span = self.langfuse_tracer.langfuse.span(
                    trace_id=context.task_id,
                    name="request_parsing",
                    input={"user_request": full_user_query[:500]},
                    metadata={"step": "1", "description": "Parse EDA analysis request"}
                )
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ¤– EDAToolsAgent ì‹œì‘...")
            )
            
            # A2A SDK 0.2.9 ê³µì‹ íŒ¨í„´ì— ë”°ë¥¸ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
            user_instructions = ""
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if part.root.kind == "text":
                        user_instructions += part.root.text + " "
                
                user_instructions = user_instructions.strip()
                logger.info(f"ğŸ“ ì‚¬ìš©ì ìš”ì²­: {user_instructions}")
                
                # íŒŒì‹± ê²°ê³¼ ì—…ë°ì´íŠ¸
                if parsing_span:
                    parsing_span.update(
                        output={
                            "success": True,
                            "query_extracted": user_instructions[:200],
                            "request_length": len(user_instructions),
                            "analysis_type": "exploratory_data_analysis"
                        }
                    )
                
                if not user_instructions:
                    await task_updater.update_status(
                        TaskState.completed,
                        message=new_agent_text_message("âŒ EDA ë¶„ì„ ìš”ì²­ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    )
                    return
                
                # 2ë‹¨ê³„: EDA ë¶„ì„ ì‹¤í–‰ (Langfuse ì¶”ì )
                eda_span = None
                if main_trace:
                    eda_span = self.langfuse_tracer.langfuse.span(
                        trace_id=context.task_id,
                        name="eda_analysis",
                        input={
                            "query": user_instructions[:200],
                            "analysis_type": "wrapper_based_processing"
                        },
                        metadata={"step": "2", "description": "Execute EDA analysis with optimized wrapper"}
                    )
                
                # EDA ë¶„ì„ ì²˜ë¦¬ ì‹¤í–‰
                result = await self.agent.process_eda_analysis(user_instructions)
                
                # EDA ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸
                if eda_span:
                    eda_span.update(
                        output={
                            "success": True,
                            "result_length": len(result),
                            "analysis_completed": True,
                            "insights_generated": True,
                            "execution_method": "optimized_wrapper"
                        }
                    )
                
                # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥/ë°˜í™˜ (Langfuse ì¶”ì )
                save_span = None
                if main_trace:
                    save_span = self.langfuse_tracer.langfuse.span(
                        trace_id=context.task_id,
                        name="save_results",
                        input={
                            "result_size": len(result),
                            "analysis_success": True
                        },
                        metadata={"step": "3", "description": "Prepare EDA analysis results"}
                    )
                
                # ì €ì¥ ê²°ê³¼ ì—…ë°ì´íŠ¸
                if save_span:
                    save_span.update(
                        output={
                            "response_prepared": True,
                            "insights_delivered": True,
                            "final_status": "completed",
                            "analysis_included": True
                        }
                    )
                
                # ì‘ì—… ì™„ë£Œ
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(result)
                )
                
            else:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message("âŒ ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                )
            
            # Langfuse ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ì™„ë£Œ
            if main_trace:
                try:
                    # Outputì„ ìš”ì•½ëœ í˜•íƒœë¡œ ì œê³µ
                    output_summary = {
                        "status": "completed",
                        "result_preview": result[:1000] + "..." if len(result) > 1000 else result,
                        "full_result_length": len(result)
                    }
                    
                    main_trace.update(
                        output=output_summary,
                        metadata={
                            "status": "completed",
                            "result_length": len(result),
                            "success": True,
                            "completion_timestamp": str(context.task_id),
                            "agent": "EDAToolsAgent",
                            "port": 8312,
                            "server_type": "wrapper_based",
                            "analysis_type": "exploratory_data_analysis"
                        }
                    )
                    logger.info(f"ğŸ”§ Langfuse íŠ¸ë ˆì´ìŠ¤ ì™„ë£Œ: {context.task_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Langfuse íŠ¸ë ˆì´ìŠ¤ ì™„ë£Œ ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            logger.error(f"âŒ EDA Tools Agent ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # Langfuse ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ì˜¤ë¥˜ ê¸°ë¡
            if main_trace:
                try:
                    main_trace.update(
                        output=f"Error: {str(e)}",
                        metadata={
                            "status": "failed",
                            "error": str(e),
                            "error_type": type(e).__name__,
                            "success": False,
                            "agent": "EDAToolsAgent",
                            "port": 8312,
                            "server_type": "wrapper_based"
                        }
                    )
                except Exception as langfuse_error:
                    logger.warning(f"âš ï¸ Langfuse ì˜¤ë¥˜ ê¸°ë¡ ì‹¤íŒ¨: {langfuse_error}")
            
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(f"âŒ EDA ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            )
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        logger.info(f"ğŸš« EDA Tools Agent ì‘ì—… ì·¨ì†Œ - Task: {context.task_id}")


def main():
    """EDA Tools Agent ì„œë²„ ìƒì„± ë° ì‹¤í–‰"""
    
    # AgentSkill ì •ì˜
    skill = AgentSkill(
        id="eda_tools",
        name="Exploratory Data Analysis Tools",
        description="ì›ë³¸ ai-data-science-team EDAToolsAgentë¥¼ í™œìš©í•œ ì™„ì „í•œ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. 8ê°œ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ í†µê³„ ë¶„ì„, ìƒê´€ê´€ê³„, í’ˆì§ˆ í‰ê°€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        tags=["eda", "statistics", "correlation", "data-quality", "outliers", "distribution", "ai-data-science-team"],
        examples=[
            "ë°ì´í„°ì˜ ê¸°ìˆ  í†µê³„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”",
            "ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",  
            "ë°ì´í„° ë¶„í¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
            "ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
            "ì‹œê³„ì—´ íŒ¨í„´ì„ ë¶„ì„í•´ì£¼ì„¸ìš”",
            "ì´ìƒì¹˜ë¥¼ ê°ì§€í•´ì£¼ì„¸ìš”",
            "ë°ì´í„° í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”",
            "ìë™ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”"
        ]
    )
    
    # Agent Card ì •ì˜
    agent_card = AgentCard(
        name="EDA Tools Agent",
        description="ì›ë³¸ ai-data-science-team EDAToolsAgentë¥¼ A2A SDKë¡œ ë˜í•‘í•œ ì™„ì „í•œ íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ì„œë¹„ìŠ¤. 8ê°œ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ í†µê³„ ë¶„ì„, ìƒê´€ê´€ê³„ ë¶„ì„, í’ˆì§ˆ í‰ê°€, ì´ìƒì¹˜ ê°ì§€ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.",
        url="http://localhost:8312/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=False),
        skills=[skill],
        supportsAuthenticatedExtendedCard=False
    )
    
    # Request Handler ìƒì„±
    request_handler = DefaultRequestHandler(
        agent_executor=EDAToolsAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )
    
    # A2A Server ìƒì„±
    server = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler,
    )
    
    print("ğŸ“Š Starting EDA Tools Agent Server")
    print("ğŸŒ Server starting on http://localhost:8312")
    print("ğŸ“‹ Agent card: http://localhost:8312/.well-known/agent.json")
    print("ğŸ¯ Features: ì›ë³¸ ai-data-science-team EDAToolsAgent 8ê°œ ê¸°ëŠ¥ 100% ë˜í•‘")
    print("ğŸ’¡ EDA Analysis: í†µê³„ ë¶„ì„, ìƒê´€ê´€ê³„, í’ˆì§ˆ í‰ê°€, ì´ìƒì¹˜ ê°ì§€, ë¶„í¬ ë¶„ì„")
    
    uvicorn.run(server.build(), host="0.0.0.0", port=8312, log_level="info")


if __name__ == "__main__":
    main()