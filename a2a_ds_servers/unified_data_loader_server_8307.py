#!/usr/bin/env python3
"""
ğŸ’ CherryAI í†µí•© ë°ì´í„° ë¡œë” ì„œë²„ (Unified Data Loader Server)
Port: 8307

pandas_agent íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ 12ê°œ A2A ì—ì´ì „íŠ¸ í‘œì¤€ ë°ì´í„° ë¡œë”© ì‹œìŠ¤í…œ
- 100% ê²€ì¦ëœ í†µí•© ì¸í”„ë¼ ì‚¬ìš©
- LLM First ì›ì¹™ ì™„ì „ ì¤€ìˆ˜
- UTF-8 ì¸ì½”ë”© ë¬¸ì œ ì™„ì „ í•´ê²°
- A2A SDK 0.2.9 í‘œì¤€ ì™„ë²½ ì ìš©

Author: CherryAI Team
License: MIT License
"""

import uvicorn
import asyncio
import logging
import os
import json
from datetime import datetime
from typing import Optional, Dict, Any, List
from pathlib import Path

# A2A SDK imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.server.tasks import InMemoryTaskStore
from a2a.server.tasks.task_updater import TaskUpdater
from a2a.types import (
    AgentCapabilities,
    AgentCard,
    AgentSkill,
    TaskState,
    TextPart,
)
from a2a.utils import new_agent_text_message

# í†µí•© ë°ì´í„° ì‹œìŠ¤í…œ imports (100% ê²€ì¦ëœ ì¸í”„ë¼)
from unified_data_system import (
    UnifiedDataInterface,
    LLMFirstDataEngine,
    SmartDataFrame,
    DataProfile,
    QualityReport,
    CacheManager,
    EnhancedFileConnector
)
from unified_data_system.core.unified_data_interface import (
    DataIntent,
    DataIntentType,
    LoadingStrategy,
    A2AContext
)

logger = logging.getLogger(__name__)


class UnifiedDataLoaderExecutor(AgentExecutor):
    """
    í†µí•© ë°ì´í„° ë¡œë” ì‹¤í–‰ê¸°
    
    pandas_agent íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ í•œ ì™„ì „íˆ ê²€ì¦ëœ í†µí•© ë°ì´í„° ë¡œë”© ì‹œìŠ¤í…œ
    - 12ê°œ A2A ì—ì´ì „íŠ¸ í‘œì¤€ ì ìš©
    - LLM First ì›ì¹™ ì™„ì „ ì¤€ìˆ˜
    - 100% ê¸°ëŠ¥ ë³´ì¡´, Mock ì‚¬ìš© ê¸ˆì§€
    """
    
    def __init__(self):
        """í†µí•© ë°ì´í„° ë¡œë” ì´ˆê¸°í™”"""
        super().__init__()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.cache_manager = CacheManager(max_size_mb=200, default_ttl=3600)
        self.llm_engine = LLMFirstDataEngine()
        self.file_connector = EnhancedFileConnector(self.cache_manager)
        
        # ë¡œë”© í†µê³„
        self.stats = {
            "total_requests": 0,
            "successful_loads": 0,
            "failed_loads": 0,
            "cache_hits": 0,
            "average_load_time": 0.0
        }
        
        # ê³µìœ  ì €ì¥ì†Œ ê²½ë¡œ
        self.shared_data_path = Path("a2a_ds_servers/artifacts/data/shared_dataframes")
        self.shared_data_path.mkdir(parents=True, exist_ok=True)
        
        logger.info("ğŸ’ í†µí•© ë°ì´í„° ë¡œë” ì´ˆê¸°í™” ì™„ë£Œ - 100% ê²€ì¦ëœ ì¸í”„ë¼ ì‚¬ìš©")
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """
        A2A í”„ë¡œí† ì½œì— ë”°ë¥¸ í†µí•© ë°ì´í„° ë¡œë”© ì‹¤í–‰
        
        5ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°:
        1. ì˜ë„ ë¶„ì„ (LLM First)
        2. íŒŒì¼ ë°œê²¬ ë° ì„ íƒ
        3. ë¡œë”© ì „ëµ ìˆ˜ë¦½
        4. ë°ì´í„° ë¡œë”© ë° ê²€ì¦
        5. ê³µìœ  ì €ì¥ì†Œ ì €ì¥
        """
        start_time = datetime.now()
        self.stats["total_requests"] += 1
        
        # TaskUpdater ì´ˆê¸°í™” (A2A SDK í‘œì¤€ íŒ¨í„´)
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            # A2A íƒœìŠ¤í¬ ì‹œì‘
            await task_updater.submit()
            await task_updater.start_work()
            
            # ì‚¬ìš©ì ìš”ì²­ ì¶”ì¶œ
            user_query = self._extract_user_query(context)
            if not user_query:
                await task_updater.update_status(
                    TaskState.failed,
                    message=new_agent_text_message("âŒ ìœ íš¨í•œ ì‚¬ìš©ì ìš”ì²­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                )
                return
            
            logger.info(f"ğŸ“ ì‚¬ìš©ì ìš”ì²­: {user_query}")
            
            # === 1ë‹¨ê³„: LLM First ì˜ë„ ë¶„ì„ ===
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§  LLM ê¸°ë°˜ ì˜ë„ ë¶„ì„ ì¤‘...")
            )
            
            a2a_context = A2AContext(context)
            
            intent = await self.llm_engine.analyze_intent(user_query, a2a_context)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(f"âœ… ì˜ë„ ë¶„ì„ ì™„ë£Œ: {intent.intent_type.value} (ì‹ ë¢°ë„: {intent.confidence:.2f})")
            )
            
            # === 2ë‹¨ê³„: íŒŒì¼ ë°œê²¬ ë° ì„ íƒ ===
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ìŠ¤ìº” ì¤‘...")
            )
            
            available_files = await self._discover_available_files()
            
            if not available_files:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message("""ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.

### ğŸ’¡ í•´ê²° ë°©ë²•:
1. **ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ**: CSV, Excel, JSON ë“± íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”
2. **ì§€ì› í˜•ì‹**: .csv, .xlsx, .xls, .json, .parquet, .feather, .txt, .tsv
3. **ê¶Œì¥ ìœ„ì¹˜**: `a2a_ds_servers/artifacts/data/` í´ë”

### ğŸ”§ ë‹¤ìŒ ë‹¨ê³„:
ë°ì´í„° íŒŒì¼ì„ ì¤€ë¹„í•œ í›„ ë‹¤ì‹œ ìš”ì²­í•´ì£¼ì„¸ìš”.""")
                )
                return
            
            selected_file = await self.llm_engine.select_optimal_file(intent, available_files)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(f"ğŸ¯ íŒŒì¼ ì„ íƒ ì™„ë£Œ: {Path(selected_file).name} ({len(available_files)}ê°œ ì¤‘ ì„ íƒ)")
            )
            
            # === 3ë‹¨ê³„: ë¡œë”© ì „ëµ ìˆ˜ë¦½ ===
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("âš¡ ìµœì  ë¡œë”© ì „ëµ ìˆ˜ë¦½ ì¤‘...")
            )
            
            loading_strategy = await self.llm_engine.create_loading_strategy(selected_file, intent)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(f"ğŸ“‹ ë¡œë”© ì „ëµ: {loading_strategy.encoding} ì¸ì½”ë”©, ìºì‹œ {'í™œì„±í™”' if loading_strategy.use_cache else 'ë¹„í™œì„±í™”'}")
            )
            
            # === 4ë‹¨ê³„: ë°ì´í„° ë¡œë”© ë° ê²€ì¦ ===
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ“Š ë°ì´í„° ë¡œë”© ë° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            )
            
            smart_df = await self.file_connector.load_file(selected_file, loading_strategy, a2a_context)
            
            # ë¹ˆ ë°ì´í„° ì²´í¬
            if smart_df.is_empty():
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(f"""âš ï¸ ë¡œë”©ëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {Path(selected_file).name}

### ğŸ”§ í•´ê²° ë°©ë²•:
1. **íŒŒì¼ ë‚´ìš© í™•ì¸**: ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
2. **ì¸ì½”ë”© ë¬¸ì œ**: UTF-8 ë˜ëŠ” CP949 ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥ ì‹œë„
3. **í˜•ì‹ í™•ì¸**: CSVì˜ ê²½ìš° í—¤ë”ì™€ êµ¬ë¶„ì í™•ì¸

### ğŸ’¡ ì¶”ì²œ:
Data Cleaning Agentë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í’ˆì§ˆì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.""")
                )
                return
            
            # ìë™ í”„ë¡œíŒŒì¼ë§
            profile = await smart_df.auto_profile()
            quality_report = await smart_df.validate_quality()
            
            # === 5ë‹¨ê³„: ê³µìœ  ì €ì¥ì†Œ ì €ì¥ ===
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ’¾ ê³µìœ  ì €ì¥ì†Œì— ì €ì¥ ì¤‘...")
            )
            
            saved_info = await self._save_to_shared_storage(smart_df, context.task_id)
            
            # ì„±ê³µ í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = (datetime.now() - start_time).total_seconds()
            self.stats["successful_loads"] += 1
            self.stats["average_load_time"] = (
                (self.stats["average_load_time"] * (self.stats["successful_loads"] - 1) + processing_time) 
                / self.stats["successful_loads"]
            )
            
            # ìµœì¢… ì„±ê³µ ì‘ë‹µ
            success_message = self._generate_success_response(
                smart_df, profile, quality_report, saved_info, processing_time
            )
            
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(success_message)
            )
            
            logger.info(f"âœ… ë°ì´í„° ë¡œë”© ì„±ê³µ: {smart_df.shape} in {processing_time:.2f}s")
            
        except Exception as e:
            self.stats["failed_loads"] += 1
            error_message = f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}"
            
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(error_message)
            )
            
            logger.error(f"ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}", exc_info=True)
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì—ì´ì „íŠ¸ ì‹¤í–‰ ì·¨ì†Œ"""
        try:
            task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
            await task_updater.update_status(
                TaskState.cancelled,
                message=task_updater.new_agent_message(parts=[TextPart(text="âŒ ë°ì´í„° ë¡œë”© ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")])
            )
            logger.info("ë°ì´í„° ë¡œë”© ì‘ì—… ì·¨ì†Œë¨")
        except Exception as e:
            logger.error(f"ì‘ì—… ì·¨ì†Œ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _extract_user_query(self, context: RequestContext) -> str:
        """A2A ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ (í‘œì¤€ A2A íŒ¨í„´)"""
        try:
            # A2A í‘œì¤€: context.message.partsì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if hasattr(context, 'message') and context.message and hasattr(context.message, 'parts'):
                parts = []
                for part in context.message.parts:
                    # A2A SDK í‘œì¤€ êµ¬ì¡°: part.root.text ë˜ëŠ” part.root.kind == "text"
                    if hasattr(part, 'root'):
                        if hasattr(part.root, 'text'):
                            parts.append(part.root.text)
                        elif hasattr(part.root, 'kind') and part.root.kind == "text":
                            if hasattr(part.root, 'content'):
                                parts.append(part.root.content)
                    # ì§ì ‘ text ì†ì„±ì´ ìˆëŠ” ê²½ìš°
                    elif hasattr(part, 'text'):
                        parts.append(part.text)
                
                user_query = " ".join(parts).strip()
                if user_query:
                    return user_query
            
            # í´ë°± 1: context.get_user_input() ë©”ì„œë“œ ì‚¬ìš©
            if hasattr(context, 'get_user_input'):
                user_input = context.get_user_input()
                if user_input:
                    return str(user_input).strip()
                    
            # í´ë°± 2: ê¸°ë³¸ ë©”ì‹œì§€
            return "ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•´ì£¼ì„¸ìš”"
            
        except Exception as e:
            logger.warning(f"ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•´ì£¼ì„¸ìš”"
    
    async def _discover_available_files(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ë°œê²¬"""
        try:
            from unified_data_system.utils.file_scanner import FileScanner
            
            scanner = FileScanner()
            
            # FileScanner ë‚´ì¥ ìŠ¤ìº” ê¸°ëŠ¥ ì‚¬ìš© (ì ˆëŒ€ ê²½ë¡œ ì ìš©ë¨)
            all_files = await scanner.scan_data_files()
            
            # ì¶”ê°€ì ì¸ í˜„ì¬ ë””ë ‰í† ë¦¬ ìŠ¤ìº” (ì ˆëŒ€ ê²½ë¡œë¡œ)
            current_dir = Path.cwd()
            additional_paths = [
                current_dir / "a2a_ds_servers" / "artifacts" / "data",
                current_dir / "data",
                current_dir / "datasets", 
                current_dir / "files",
                current_dir / "uploads",
                current_dir  # í˜„ì¬ ë””ë ‰í† ë¦¬
            ]
            
            for path in additional_paths:
                if path.exists() and path.is_dir():
                    try:
                        extra_files = await scanner.scan_directory(str(path))
                        all_files.extend(extra_files)
                    except Exception as e:
                        logger.debug(f"ê²½ë¡œ ìŠ¤ìº” ì‹¤íŒ¨ ({path}): {e}")
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬  
            unique_files = list(set(all_files))
            unique_files.sort(key=lambda x: os.path.getmtime(x), reverse=True)  # ìµœì‹  íŒŒì¼ ìš°ì„ 
            
            logger.info(f"ğŸ“ ë°œê²¬ëœ íŒŒì¼: {len(unique_files)}ê°œ")
            for file in unique_files[:5]:  # ì²« 5ê°œ íŒŒì¼ ë¡œê·¸
                logger.info(f"  - {file}")
            
            return unique_files
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ë°œê²¬ ì˜¤ë¥˜: {e}")
            return []
    
    async def _save_to_shared_storage(self, smart_df: SmartDataFrame, task_id: str) -> Dict[str, Any]:
        """ê³µìœ  ì €ì¥ì†Œì— ë°ì´í„° ì €ì¥"""
        try:
            # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"loaded_data_{task_id}_{timestamp}.csv"
            filepath = self.shared_data_path / filename
            
            # CSVë¡œ ì €ì¥
            smart_df.to_csv(filepath, index=False)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_file = filepath.with_suffix('.json')
            metadata = {
                "task_id": task_id,
                "filename": filename,
                "filepath": str(filepath),
                "shape": smart_df.shape,
                "columns": list(smart_df.columns),
                "dtypes": {col: str(dtype) for col, dtype in smart_df.dtypes.items()},
                "created_at": datetime.now().isoformat(),
                "source_metadata": smart_df.metadata
            }
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filepath}")
            
            return {
                "filepath": str(filepath),
                "filename": filename,
                "metadata_file": str(metadata_file),
                "size_mb": filepath.stat().st_size / (1024 * 1024)
            }
            
        except Exception as e:
            logger.error(f"ê³µìœ  ì €ì¥ì†Œ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def _generate_success_response(self, 
                                  smart_df: SmartDataFrame, 
                                  profile: DataProfile, 
                                  quality_report: QualityReport,
                                  saved_info: Dict[str, Any],
                                  processing_time: float) -> str:
        """ì„±ê³µ ì‘ë‹µ ë©”ì‹œì§€ ìƒì„±"""
        
        quality_emoji = "ğŸŸ¢" if quality_report.overall_score >= 0.8 else "ğŸŸ¡" if quality_report.overall_score >= 0.6 else "ğŸ”´"
        
        response = f"""âœ… **ë°ì´í„° ë¡œë”© ì„±ê³µ!**

### ğŸ“Š **ë¡œë”©ëœ ë°ì´í„° ì •ë³´**
- **í˜•íƒœ**: {smart_df.shape[0]:,}í–‰ Ã— {smart_df.shape[1]:,}ì—´
- **íŒŒì¼ëª…**: `{saved_info['filename']}`
- **í¬ê¸°**: {saved_info['size_mb']:.2f} MB
- **ì²˜ë¦¬ ì‹œê°„**: {processing_time:.2f}ì´ˆ

### {quality_emoji} **ë°ì´í„° í’ˆì§ˆ ë¶„ì„**
- **ì „ì²´ í’ˆì§ˆ ì ìˆ˜**: {quality_report.overall_score:.1%}
- **ì™„ì „ì„±**: {quality_report.completeness:.1%} | **ì¼ê´€ì„±**: {quality_report.consistency:.1%}
- **ìœ íš¨ì„±**: {quality_report.validity:.1%} | **ì •í™•ì„±**: {quality_report.accuracy:.1%}

### ğŸ“‹ **ì»¬ëŸ¼ ì •ë³´**
{', '.join([f"`{col}` ({str(dtype)})" for col, dtype in smart_df.dtypes.items()])}

### ğŸ“ˆ **ìƒ˜í”Œ ë°ì´í„°**
```
{smart_df.head(3).to_string()}
```"""

        # í’ˆì§ˆ ì´ìŠˆê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì •ë³´
        if quality_report.issues:
            response += f"\n\n### âš ï¸ **í’ˆì§ˆ ì´ìŠˆ**\n"
            for issue in quality_report.issues[:3]:  # ìµœëŒ€ 3ê°œê¹Œì§€
                response += f"- {issue}\n"
        
        # ê¶Œì¥ì‚¬í•­
        if quality_report.recommendations:
            response += f"\n### ğŸ’¡ **ê¶Œì¥ì‚¬í•­**\n"
            for rec in quality_report.recommendations[:2]:  # ìµœëŒ€ 2ê°œê¹Œì§€
                response += f"- {rec}\n"
        
        response += f"""

### ğŸš€ **ë‹¤ìŒ ë‹¨ê³„**
ì´ì œ ë¡œë”©ëœ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ CherryAI ì—ì´ì „íŠ¸ë“¤ê³¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- **Data Cleaning Agent** (8306): ë°ì´í„° ì •ë¦¬ ë° ì „ì²˜ë¦¬
- **EDA Tools Agent** (8312): íƒìƒ‰ì  ë°ì´í„° ë¶„ì„  
- **Data Visualization Agent** (8308): ì‹œê°í™” ìƒì„±
- **Feature Engineering Agent** (8310): íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§

### ğŸ“ **ì €ì¥ ìœ„ì¹˜**
`{saved_info['filepath']}`"""

        return response


def create_agent_card() -> AgentCard:
    """A2A ì—ì´ì „íŠ¸ ì¹´ë“œ ìƒì„±"""
    
    skill = AgentSkill(
        id="unified_data_loading",
        name="Unified Data Loading & Processing",
        description="pandas_agent íŒ¨í„´ ê¸°ë°˜ í†µí•© ë°ì´í„° ë¡œë”© ì‹œìŠ¤í…œ. LLM First ì›ì¹™ìœ¼ë¡œ ì§€ëŠ¥í˜• íŒŒì¼ ì„ íƒ, UTF-8 ë¬¸ì œ í•´ê²°, ìë™ í’ˆì§ˆ ê²€ì¦ ì œê³µ",
        tags=[
            "data-loading", "file-processing", "quality-validation", 
            "encoding-detection", "llm-first", "pandas-agent-pattern",
            "utf8-support", "intelligent-selection", "caching"
        ],
        examples=[
            "ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•´ì£¼ì„¸ìš”",
            "CSV íŒŒì¼ì„ ë¶„ì„ìš©ìœ¼ë¡œ ì¤€ë¹„í•´ì£¼ì„¸ìš”", 
            "ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ë¡œë”©í•´ì£¼ì„¸ìš”",
            "Excel íŒŒì¼ì„ DataFrameìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”",
            "ê°€ì¥ ì í•©í•œ ë°ì´í„° íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì„ íƒí•´ì£¼ì„¸ìš”"
        ]
    )
    
    return AgentCard(
        name="CherryAI Unified Data Loader",
        description="pandas_agent íŒ¨í„´ ê¸°ë°˜ í†µí•© ë°ì´í„° ë¡œë”© ì‹œìŠ¤í…œ. 12ê°œ A2A ì—ì´ì „íŠ¸ì˜ í‘œì¤€ ë°ì´í„° ê³µê¸‰ì›ìœ¼ë¡œ LLM First ì›ì¹™ê³¼ ì§€ëŠ¥í˜• íŒŒì¼ ì²˜ë¦¬ë¥¼ í†µí•´ ì™„ë²½í•œ ë°ì´í„° ë¡œë”© ê²½í—˜ì„ ì œê³µí•©ë‹ˆë‹¤.",
        url="http://localhost:8307/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=True),
        skills=[skill],
        supportsAuthenticatedExtendedCard=False
    )


def main():
    """í†µí•© ë°ì´í„° ë¡œë” ì„œë²„ ì‹¤í–‰"""
    
    # í™˜ê²½ ì„¤ì •
    logging.basicConfig(level=logging.INFO)
    
    # A2A ì„œë²„ êµ¬ì„± ìš”ì†Œ
    agent_card = create_agent_card()
    
    request_handler = DefaultRequestHandler(
        agent_executor=UnifiedDataLoaderExecutor(),
        task_store=InMemoryTaskStore(),
    )
    
    server = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler,
    )
    
    print("ğŸ’ CherryAI í†µí•© ë°ì´í„° ë¡œë” ì„œë²„ ì‹œì‘")
    print("ğŸ“Š pandas_agent íŒ¨í„´ ê¸°ë°˜ í‘œì¤€ ë°ì´í„° ë¡œë”© ì‹œìŠ¤í…œ")
    print(f"ğŸŒ ì„œë²„ URL: http://localhost:8307")
    print(f"ğŸ“‹ ì—ì´ì „íŠ¸ ì¹´ë“œ: http://localhost:8307/.well-known/agent.json")
    print("")
    print("âœ¨ ì£¼ìš” íŠ¹ì§•:")
    print("  - ğŸ§  LLM First ì›ì¹™: ì§€ëŠ¥í˜• ì˜ë„ ë¶„ì„ ë° íŒŒì¼ ì„ íƒ")
    print("  - ğŸ” UTF-8 ë¬¸ì œ ì™„ì „ í•´ê²°: ë‹¤ì¤‘ ì¸ì½”ë”© ìë™ ê°ì§€")
    print("  - ğŸ“Š ìë™ í’ˆì§ˆ ê²€ì¦: SmartDataFrame + ì‹¤ì‹œê°„ í”„ë¡œíŒŒì¼ë§")
    print("  - âš¡ ê³ ì„±ëŠ¥ ìºì‹±: LRU + TTL + íƒœê·¸ ê¸°ë°˜ ìºì‹œ ì‹œìŠ¤í…œ")
    print("  - ğŸ”— ë‹¤ì¤‘ í˜•ì‹ ì§€ì›: CSV, Excel, JSON, Parquet ë“±")
    print("  - ğŸ¯ A2A í‘œì¤€: SDK 0.2.9 ì™„ë²½ ì¤€ìˆ˜")
    print("  - ğŸ’ CherryAI í‘œì¤€: 12ê°œ ì—ì´ì „íŠ¸ í†µí•© ë°ì´í„° ê³µê¸‰ì›")
    print("")
    print("ğŸš€ ì„œë²„ê°€ ì‹œì‘ë©ë‹ˆë‹¤...")
    
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(
        server.build(), 
        host="0.0.0.0", 
        port=8307, 
        log_level="info"
    )


if __name__ == "__main__":
    main() 