#!/usr/bin/env python3
"""
ğŸ§  LLM First Shared Knowledge Bank - A2A SDK 0.2.9 í‘œì¤€ ì„œë²„
Port: 8602

CherryAI LLM First Architecture ì™„ì „ ì¤€ìˆ˜ ì„¤ê³„
- ëª¨ë“  ì§€ì‹ ê´€ë ¨ ê²°ì •ì„ LLMì´ ë™ì ìœ¼ë¡œ ìˆ˜í–‰
- No Hardcoded Workflows
- 100% LLM-driven Knowledge Orchestration
- Context-Aware Dynamic Knowledge Management
- Real-time LLM-powered Knowledge Discovery
"""

import asyncio
import json
import logging
import os
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass, asdict
from enum import Enum

import pandas as pd
import networkx as nx
import uvicorn
from openai import AsyncOpenAI

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart,
    Part
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
from dotenv import load_dotenv
load_dotenv()

# LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
llm_client = AsyncOpenAI(
    api_key=os.getenv("OPENAI_API_KEY", "your-api-key-here"),
    base_url=os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
)

# ì§€ì‹ ì €ì¥ì†Œ ê²½ë¡œ
KNOWLEDGE_BASE_DIR = Path("a2a_ds_servers/artifacts/knowledge_base")
KNOWLEDGE_BASE_DIR.mkdir(parents=True, exist_ok=True)

@dataclass
class KnowledgeItem:
    """ë™ì  ì§€ì‹ í•­ëª©"""
    id: str
    content: str
    metadata: Dict[str, Any]
    created_at: datetime
    updated_at: datetime
    relevance_score: float = 0.0
    usage_count: int = 0

@dataclass
class CollaborationMemory:
    """í˜‘ì—… ë©”ëª¨ë¦¬"""
    id: str
    agents_involved: List[str]
    user_query: str
    workflow_executed: List[str]
    success: bool
    execution_time: float
    insights: str  # LLMì´ ìƒì„±í•œ ì¸ì‚¬ì´íŠ¸
    created_at: datetime

class LLMFirstKnowledgeBank:
    """LLM First ì§€ì‹ ì€í–‰"""
    
    def __init__(self):
        self.knowledge_items: Dict[str, KnowledgeItem] = {}
        self.collaboration_memories: Dict[str, CollaborationMemory] = {}
        self.knowledge_graph = nx.DiGraph()
        
        # ì§€ì‹ ë¡œë“œ
        self._load_existing_knowledge()
        
        logger.info("ğŸ§  LLM First Knowledge Bank initialized")
    
    def _load_existing_knowledge(self):
        """ê¸°ì¡´ ì§€ì‹ ë¡œë“œ"""
        try:
            # ì§€ì‹ í•­ëª© ë¡œë“œ
            knowledge_file = KNOWLEDGE_BASE_DIR / "llm_knowledge_items.json"
            if knowledge_file.exists():
                with open(knowledge_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for item_data in data:
                        item = KnowledgeItem(
                            id=item_data['id'],
                            content=item_data['content'],
                            metadata=item_data['metadata'],
                            created_at=datetime.fromisoformat(item_data['created_at']),
                            updated_at=datetime.fromisoformat(item_data['updated_at']),
                            relevance_score=item_data.get('relevance_score', 0.0),
                            usage_count=item_data.get('usage_count', 0)
                        )
                        self.knowledge_items[item.id] = item
            
            # í˜‘ì—… ë©”ëª¨ë¦¬ ë¡œë“œ
            memory_file = KNOWLEDGE_BASE_DIR / "collaboration_memories.json"
            if memory_file.exists():
                with open(memory_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for memory_data in data:
                        memory = CollaborationMemory(
                            id=memory_data['id'],
                            agents_involved=memory_data['agents_involved'],
                            user_query=memory_data['user_query'],
                            workflow_executed=memory_data['workflow_executed'],
                            success=memory_data['success'],
                            execution_time=memory_data['execution_time'],
                            insights=memory_data['insights'],
                            created_at=datetime.fromisoformat(memory_data['created_at'])
                        )
                        self.collaboration_memories[memory.id] = memory
            
            # ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ
            graph_file = KNOWLEDGE_BASE_DIR / "llm_knowledge_graph.json"
            if graph_file.exists():
                with open(graph_file, 'r', encoding='utf-8') as f:
                    graph_data = json.load(f)
                    self.knowledge_graph = nx.node_link_graph(graph_data)
            
            logger.info(f"ğŸ“š Loaded {len(self.knowledge_items)} knowledge items")
            logger.info(f"ğŸ¤ Loaded {len(self.collaboration_memories)} collaboration memories")
            
        except Exception as e:
            logger.error(f"âŒ Error loading knowledge: {e}")
    
    def _save_knowledge(self):
        """ì§€ì‹ ì €ì¥"""
        try:
            # ì§€ì‹ í•­ëª© ì €ì¥
            knowledge_data = []
            for item in self.knowledge_items.values():
                knowledge_data.append({
                    'id': item.id,
                    'content': item.content,
                    'metadata': item.metadata,
                    'created_at': item.created_at.isoformat(),
                    'updated_at': item.updated_at.isoformat(),
                    'relevance_score': item.relevance_score,
                    'usage_count': item.usage_count
                })
            
            with open(KNOWLEDGE_BASE_DIR / "llm_knowledge_items.json", 'w', encoding='utf-8') as f:
                json.dump(knowledge_data, f, indent=2, ensure_ascii=False)
            
            # í˜‘ì—… ë©”ëª¨ë¦¬ ì €ì¥
            memory_data = []
            for memory in self.collaboration_memories.values():
                memory_data.append({
                    'id': memory.id,
                    'agents_involved': memory.agents_involved,
                    'user_query': memory.user_query,
                    'workflow_executed': memory.workflow_executed,
                    'success': memory.success,
                    'execution_time': memory.execution_time,
                    'insights': memory.insights,
                    'created_at': memory.created_at.isoformat()
                })
            
            with open(KNOWLEDGE_BASE_DIR / "collaboration_memories.json", 'w', encoding='utf-8') as f:
                json.dump(memory_data, f, indent=2, ensure_ascii=False)
            
            # ì§€ì‹ ê·¸ë˜í”„ ì €ì¥
            graph_data = nx.node_link_data(self.knowledge_graph)
            with open(KNOWLEDGE_BASE_DIR / "llm_knowledge_graph.json", 'w', encoding='utf-8') as f:
                json.dump(graph_data, f, indent=2, ensure_ascii=False)
            
            logger.info("ğŸ’¾ Knowledge base saved successfully")
            
        except Exception as e:
            logger.error(f"âŒ Error saving knowledge: {e}")
    
    async def llm_analyze_knowledge_request(self, user_query: str) -> Dict[str, Any]:
        """LLMì´ ì§€ì‹ ìš”ì²­ì„ ë¶„ì„"""
        try:
            analysis_prompt = f"""
ë‹¹ì‹ ì€ CherryAIì˜ ì§€ì‹ ì€í–‰ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ì¢…ë¥˜ì˜ ì§€ì‹ ì‘ì—…ì´ í•„ìš”í•œì§€ íŒë‹¨í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{user_query}"

ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "request_type": "knowledge_search|collaboration_recommendation|pattern_learning|insight_generation|knowledge_creation",
    "intent": "ì‚¬ìš©ìì˜ êµ¬ì²´ì ì¸ ì˜ë„",
    "context": "ìš”ì²­ì˜ ë§¥ë½ê³¼ ë°°ê²½",
    "knowledge_domains": ["ê´€ë ¨ ì§€ì‹ ë„ë©”ì¸ë“¤"],
    "expected_outcome": "ì˜ˆìƒë˜ëŠ” ê²°ê³¼",
    "complexity_level": "simple|medium|complex",
    "urgency": "low|medium|high",
    "reasoning": "ë¶„ì„ ê³¼ì •ê³¼ ê·¼ê±°"
}}
"""
            
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ CherryAIì˜ ì§€ëŠ¥í˜• ì§€ì‹ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": analysis_prompt}
                ],
                temperature=0.3
            )
            
            analysis_text = response.choices[0].message.content
            
            # JSON ì¶”ì¶œ
            try:
                import re
                json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
            except:
                pass
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "request_type": "knowledge_search",
                "intent": user_query,
                "context": "ì¼ë°˜ì ì¸ ì§€ì‹ ìš”ì²­",
                "knowledge_domains": ["general"],
                "expected_outcome": "ê´€ë ¨ ì •ë³´ ì œê³µ",
                "complexity_level": "medium",
                "urgency": "medium",
                "reasoning": "LLM ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨"
            }
            
        except Exception as e:
            logger.error(f"âŒ LLM analysis error: {e}")
            return {
                "request_type": "knowledge_search",
                "intent": user_query,
                "context": "ì˜¤ë¥˜ ë°œìƒ",
                "knowledge_domains": ["general"],
                "expected_outcome": "ê¸°ë³¸ ì‘ë‹µ",
                "complexity_level": "medium",
                "urgency": "medium",
                "reasoning": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    async def llm_search_knowledge(self, query: str, analysis: Dict[str, Any]) -> List[KnowledgeItem]:
        """LLMì´ ë§¥ë½ì„ ì´í•´í•˜ì—¬ ì§€ì‹ ê²€ìƒ‰"""
        try:
            # í˜„ì¬ ì§€ì‹ í•­ëª©ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
            knowledge_context = []
            for item in self.knowledge_items.values():
                knowledge_context.append({
                    "id": item.id,
                    "content": item.content[:200] + "..." if len(item.content) > 200 else item.content,
                    "metadata": item.metadata,
                    "relevance_score": item.relevance_score,
                    "usage_count": item.usage_count
                })
            
            search_prompt = f"""
ë‹¹ì‹ ì€ CherryAIì˜ ì§€ëŠ¥í˜• ì§€ì‹ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì§€ì‹ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{query}"
ë¶„ì„ ê²°ê³¼: {json.dumps(analysis, ensure_ascii=False, indent=2)}

ì‚¬ìš© ê°€ëŠ¥í•œ ì§€ì‹ í•­ëª©ë“¤:
{json.dumps(knowledge_context, ensure_ascii=False, indent=2)}

ë‹¤ìŒ í˜•íƒœë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "relevant_knowledge_ids": ["ê´€ë ¨ì„± ë†’ì€ ì§€ì‹ IDë“¤ (ìµœëŒ€ 5ê°œ)"],
    "search_reasoning": "ê²€ìƒ‰ ê³¼ì •ê³¼ ê·¼ê±°",
    "relevance_explanation": "ê° ì§€ì‹ì´ ê´€ë ¨ì„± ìˆëŠ” ì´ìœ ",
    "missing_knowledge": "ë¶€ì¡±í•œ ì§€ì‹ ì˜ì—­ì´ ìˆë‹¤ë©´ ì„¤ëª…"
}}
"""
            
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ CherryAIì˜ ì§€ëŠ¥í˜• ì§€ì‹ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": search_prompt}
                ],
                temperature=0.3
            )
            
            search_result = response.choices[0].message.content
            
            # ê²°ê³¼ íŒŒì‹±
            try:
                import re
                json_match = re.search(r'\{.*\}', search_result, re.DOTALL)
                if json_match:
                    result_data = json.loads(json_match.group())
                    relevant_ids = result_data.get("relevant_knowledge_ids", [])
                    
                    # ê´€ë ¨ ì§€ì‹ í•­ëª©ë“¤ ë°˜í™˜
                    relevant_items = []
                    for item_id in relevant_ids:
                        if item_id in self.knowledge_items:
                            item = self.knowledge_items[item_id]
                            item.usage_count += 1  # ì‚¬ìš© íšŸìˆ˜ ì¦ê°€
                            relevant_items.append(item)
                    
                    return relevant_items
            except:
                pass
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            return []
            
        except Exception as e:
            logger.error(f"âŒ LLM search error: {e}")
            return []
    
    async def llm_generate_collaboration_recommendation(self, query: str, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """LLMì´ í˜‘ì—… ì¶”ì²œ ìƒì„±"""
        try:
            # í˜‘ì—… ë©”ëª¨ë¦¬ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ì œê³µ
            memory_context = []
            for memory in self.collaboration_memories.values():
                memory_context.append({
                    "agents_involved": memory.agents_involved,
                    "user_query": memory.user_query,
                    "workflow_executed": memory.workflow_executed,
                    "success": memory.success,
                    "execution_time": memory.execution_time,
                    "insights": memory.insights
                })
            
            recommendation_prompt = f"""
ë‹¹ì‹ ì€ CherryAIì˜ í˜‘ì—… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ê³¼ ê³¼ê±° í˜‘ì—… ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ í˜‘ì—… ë°©ì•ˆì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{query}"
ë¶„ì„ ê²°ê³¼: {json.dumps(analysis, ensure_ascii=False, indent=2)}

ê³¼ê±° í˜‘ì—… ê²½í—˜:
{json.dumps(memory_context, ensure_ascii=False, indent=2)}

ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë“¤:
- pandas_agent: ë°ì´í„° ë¶„ì„ ë° ì²˜ë¦¬
- visualization_agent: ë°ì´í„° ì‹œê°í™”
- ml_modeling_agent: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§
- data_cleaning_agent: ë°ì´í„° ì •ì œ
- eda_agent: íƒìƒ‰ì  ë°ì´í„° ë¶„ì„
- sql_agent: SQL ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„

ë‹¤ìŒ í˜•íƒœë¡œ ì¶”ì²œì„ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "recommended_agents": ["ì¶”ì²œ ì—ì´ì „íŠ¸ ëª©ë¡"],
    "collaboration_strategy": "í˜‘ì—… ì „ëµ ì„¤ëª…",
    "workflow_steps": ["ì˜ˆìƒ ì›Œí¬í”Œë¡œìš° ë‹¨ê³„ë“¤"],
    "success_probability": "ì„±ê³µ í™•ë¥  (0-1)",
    "estimated_time": "ì˜ˆìƒ ì‹¤í–‰ ì‹œê°„ (ë¶„)",
    "potential_challenges": ["ì˜ˆìƒ ë„ì „ ê³¼ì œë“¤"],
    "optimization_tips": ["ìµœì í™” íŒë“¤"],
    "reasoning": "ì¶”ì²œ ê·¼ê±°ì™€ ê³¼ì •"
}}
"""
            
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ CherryAIì˜ í˜‘ì—… ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": recommendation_prompt}
                ],
                temperature=0.3
            )
            
            recommendation_text = response.choices[0].message.content
            
            # JSON ì¶”ì¶œ
            try:
                import re
                json_match = re.search(r'\{.*\}', recommendation_text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group())
            except:
                pass
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¶”ì²œ ë°˜í™˜
            return {
                "recommended_agents": ["pandas_agent"],
                "collaboration_strategy": "ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ì „ëµ",
                "workflow_steps": ["ë°ì´í„° ë¡œë“œ", "ë¶„ì„ ìˆ˜í–‰", "ê²°ê³¼ ì œê³µ"],
                "success_probability": 0.7,
                "estimated_time": 10,
                "potential_challenges": ["ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ"],
                "optimization_tips": ["ë°ì´í„° ì „ì²˜ë¦¬ ìš°ì„  ìˆ˜í–‰"],
                "reasoning": "LLM ì¶”ì²œ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì¶”ì²œ ì œê³µ"
            }
            
        except Exception as e:
            logger.error(f"âŒ LLM recommendation error: {e}")
            return {
                "recommended_agents": ["pandas_agent"],
                "collaboration_strategy": "ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ê¸°ë³¸ ì „ëµ ì œê³µ",
                "workflow_steps": ["ê¸°ë³¸ ë¶„ì„"],
                "success_probability": 0.5,
                "estimated_time": 15,
                "potential_challenges": ["ì‹œìŠ¤í…œ ì˜¤ë¥˜"],
                "optimization_tips": ["ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"],
                "reasoning": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    async def llm_learn_collaboration_pattern(self, agents: List[str], user_query: str, 
                                           workflow: List[str], success: bool, 
                                           execution_time: float) -> str:
        """LLMì´ í˜‘ì—… íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            learning_prompt = f"""
ë‹¹ì‹ ì€ CherryAIì˜ í˜‘ì—… íŒ¨í„´ í•™ìŠµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ìƒˆë¡œìš´ í˜‘ì—… ê²½í—˜ì„ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

í˜‘ì—… ì •ë³´:
- ì°¸ì—¬ ì—ì´ì „íŠ¸: {agents}
- ì‚¬ìš©ì ìš”ì²­: "{user_query}"
- ì‹¤í–‰ ì›Œí¬í”Œë¡œìš°: {workflow}
- ì„±ê³µ ì—¬ë¶€: {success}
- ì‹¤í–‰ ì‹œê°„: {execution_time}ë¶„

ê¸°ì¡´ í˜‘ì—… ê²½í—˜ë“¤ê³¼ ë¹„êµí•˜ì—¬ ë‹¤ìŒì„ ë¶„ì„í•´ì£¼ì„¸ìš”:
{{
    "pattern_insights": "ì´ í˜‘ì—…ì—ì„œ ë°œê²¬í•œ íŒ¨í„´ê³¼ ì¸ì‚¬ì´íŠ¸",
    "success_factors": "ì„±ê³µ/ì‹¤íŒ¨ ìš”ì¸ ë¶„ì„",
    "optimization_opportunities": "ìµœì í™” ê¸°íšŒ",
    "lessons_learned": "í•™ìŠµí•œ êµí›ˆë“¤",
    "future_improvements": "í–¥í›„ ê°œì„  ë°©ì•ˆ",
    "collaboration_effectiveness": "í˜‘ì—… íš¨ê³¼ì„± í‰ê°€",
    "knowledge_gaps": "ë°œê²¬ëœ ì§€ì‹ ê²©ì°¨",
    "recommendations": "í–¥í›„ ìœ ì‚¬ ìƒí™©ì— ëŒ€í•œ ì¶”ì²œ"
}}
"""
            
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ CherryAIì˜ í˜‘ì—… íŒ¨í„´ í•™ìŠµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": learning_prompt}
                ],
                temperature=0.3
            )
            
            insights_text = response.choices[0].message.content
            
            # í˜‘ì—… ë©”ëª¨ë¦¬ ìƒì„±
            memory_id = str(uuid.uuid4())
            memory = CollaborationMemory(
                id=memory_id,
                agents_involved=agents,
                user_query=user_query,
                workflow_executed=workflow,
                success=success,
                execution_time=execution_time,
                insights=insights_text,
                created_at=datetime.now()
            )
            
            self.collaboration_memories[memory_id] = memory
            self._save_knowledge()
            
            return insights_text
            
        except Exception as e:
            logger.error(f"âŒ LLM learning error: {e}")
            return f"í˜‘ì—… íŒ¨í„´ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    async def llm_generate_knowledge_insights(self, query: str) -> Dict[str, Any]:
        """LLMì´ ì§€ì‹ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            # ì „ì²´ ì§€ì‹ ë² ì´ìŠ¤ ìš”ì•½
            knowledge_summary = {
                "total_knowledge_items": len(self.knowledge_items),
                "total_collaboration_memories": len(self.collaboration_memories),
                "knowledge_domains": list(set([
                    domain for item in self.knowledge_items.values()
                    for domain in item.metadata.get("domains", [])
                ])),
                "most_used_knowledge": sorted(
                    self.knowledge_items.values(),
                    key=lambda x: x.usage_count,
                    reverse=True
                )[:5],
                "recent_collaborations": sorted(
                    self.collaboration_memories.values(),
                    key=lambda x: x.created_at,
                    reverse=True
                )[:5]
            }
            
            insights_prompt = f"""
ë‹¹ì‹ ì€ CherryAIì˜ ì§€ì‹ ì¸ì‚¬ì´íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜„ì¬ ì§€ì‹ ë² ì´ìŠ¤ ìƒíƒœë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{query}"

ì§€ì‹ ë² ì´ìŠ¤ ìš”ì•½:
{json.dumps({
    "total_knowledge_items": knowledge_summary["total_knowledge_items"],
    "total_collaboration_memories": knowledge_summary["total_collaboration_memories"],
    "knowledge_domains": knowledge_summary["knowledge_domains"]
}, ensure_ascii=False, indent=2)}

ë‹¤ìŒ í˜•íƒœë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
{{
    "knowledge_health": "ì§€ì‹ ë² ì´ìŠ¤ì˜ ê±´ê°• ìƒíƒœ",
    "usage_patterns": "ì§€ì‹ ì‚¬ìš© íŒ¨í„´ ë¶„ì„",
    "collaboration_trends": "í˜‘ì—… íŠ¸ë Œë“œ ë¶„ì„",
    "knowledge_gaps": "ë°œê²¬ëœ ì§€ì‹ ê²©ì°¨",
    "recommendations": "ì§€ì‹ ë² ì´ìŠ¤ ê°œì„  ì¶”ì²œ",
    "growth_opportunities": "ì„±ì¥ ê¸°íšŒ",
    "efficiency_metrics": "íš¨ìœ¨ì„± ì§€í‘œ",
    "strategic_insights": "ì „ëµì  ì¸ì‚¬ì´íŠ¸"
}}
"""
            
            response = await llm_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ CherryAIì˜ ì§€ì‹ ì¸ì‚¬ì´íŠ¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": insights_prompt}
                ],
                temperature=0.3
            )
            
            insights_text = response.choices[0].message.content
            
            # JSON ì¶”ì¶œ
            try:
                import re
                json_match = re.search(r'\{.*\}', insights_text, re.DOTALL)
                if json_match:
                    insights_data = json.loads(json_match.group())
                    insights_data["raw_insights"] = insights_text
                    return insights_data
            except:
                pass
            
            return {
                "knowledge_health": "ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ",
                "raw_insights": insights_text,
                "error": "JSON íŒŒì‹± ì‹¤íŒ¨"
            }
            
        except Exception as e:
            logger.error(f"âŒ LLM insights error: {e}")
            return {
                "error": f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }

# A2A ì„œë²„ êµ¬í˜„
class LLMFirstKnowledgeBankExecutor(AgentExecutor):
    """LLM First Knowledge Bank A2A ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.knowledge_bank = LLMFirstKnowledgeBank()
        logger.info("ğŸ§  LLM First Knowledge Bank Executor initialized")
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> Any:
        """A2A ìš”ì²­ ì‹¤í–‰ - ì™„ì „í•œ LLM ê¸°ë°˜ ì²˜ë¦¬"""
        try:
            # ì‹œì‘ ì—…ë°ì´íŠ¸
            await task_updater.update_status(
                TaskState.working,
                message="ğŸ§  LLM First Knowledge Bank ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
            )
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ íŒŒì‹±
            user_message = None
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if hasattr(part, 'root') and hasattr(part.root, 'text'):
                        user_message = part.root.text
                        break
            
            if not user_message:
                await task_updater.update_status(
                    TaskState.failed,
                    message="âŒ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
                return
            
            # 1ë‹¨ê³„: LLMì´ ìš”ì²­ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message="ğŸ” LLMì´ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
            )
            
            analysis = await self.knowledge_bank.llm_analyze_knowledge_request(user_message)
            
            # 2ë‹¨ê³„: ë¶„ì„ ê²°ê³¼ì— ë”°ë¼ LLMì´ ì ì ˆí•œ ì²˜ë¦¬ ìˆ˜í–‰
            await task_updater.update_status(
                TaskState.working,
                message=f"ğŸ¯ {analysis['intent']} ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤..."
            )
            
            result = await self._llm_process_request(user_message, analysis, task_updater)
            
            # ê²°ê³¼ ì „ì†¡
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(result, ensure_ascii=False, indent=2))],
                name="llm_knowledge_result",
                metadata={"content_type": "application/json"}
            )
            
            await task_updater.update_status(
                TaskState.completed,
                message="âœ… LLM First Knowledge Bank ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            )
            
        except Exception as e:
            logger.error(f"âŒ LLM First Knowledge Bank error: {e}")
            await task_updater.update_status(
                TaskState.failed,
                message=f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
    
    async def _llm_process_request(self, user_message: str, analysis: Dict[str, Any], task_updater: TaskUpdater) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ìš”ì²­ ì²˜ë¦¬"""
        try:
            request_type = analysis.get("request_type", "knowledge_search")
            
            if request_type == "knowledge_search":
                # LLM ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ“š LLMì´ ë§¥ë½ì„ ì´í•´í•˜ì—¬ ì§€ì‹ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤..."
                )
                
                relevant_knowledge = await self.knowledge_bank.llm_search_knowledge(user_message, analysis)
                
                return {
                    "type": "llm_knowledge_search",
                    "analysis": analysis,
                    "relevant_knowledge": [
                        {
                            "id": item.id,
                            "content": item.content,
                            "metadata": item.metadata,
                            "relevance_score": item.relevance_score,
                            "usage_count": item.usage_count
                        } for item in relevant_knowledge
                    ],
                    "llm_insights": "LLMì´ ë§¥ë½ì„ ì´í•´í•˜ì—¬ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ì§€ì‹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                }
            
            elif request_type == "collaboration_recommendation":
                # LLM ê¸°ë°˜ í˜‘ì—… ì¶”ì²œ
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ¤ LLMì´ í˜‘ì—… ì „ëµì„ ë¶„ì„í•˜ê³  ì¶”ì²œí•©ë‹ˆë‹¤..."
                )
                
                recommendation = await self.knowledge_bank.llm_generate_collaboration_recommendation(user_message, analysis)
                
                return {
                    "type": "llm_collaboration_recommendation",
                    "analysis": analysis,
                    "recommendation": recommendation,
                    "llm_insights": "LLMì´ ê³¼ê±° ê²½í—˜ê³¼ í˜„ì¬ ìƒí™©ì„ ì¢…í•©í•˜ì—¬ ìµœì ì˜ í˜‘ì—… ì „ëµì„ ì¶”ì²œí–ˆìŠµë‹ˆë‹¤."
                }
            
            elif request_type == "pattern_learning":
                # LLM ê¸°ë°˜ íŒ¨í„´ í•™ìŠµ (ë°ëª¨ìš©)
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ“ˆ LLMì´ í˜‘ì—… íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤..."
                )
                
                insights = await self.knowledge_bank.llm_learn_collaboration_pattern(
                    agents=["pandas_agent", "visualization_agent"],
                    user_query=user_message,
                    workflow=["ë°ì´í„° ë¡œë“œ", "ë¶„ì„", "ì‹œê°í™”"],
                    success=True,
                    execution_time=12.5
                )
                
                return {
                    "type": "llm_pattern_learning",
                    "analysis": analysis,
                    "insights": insights,
                    "llm_insights": "LLMì´ ìƒˆë¡œìš´ í˜‘ì—… íŒ¨í„´ì„ í•™ìŠµí•˜ê³  í–¥í›„ ê°œì„  ë°©ì•ˆì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤."
                }
            
            elif request_type == "insight_generation":
                # LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ’¡ LLMì´ ì§€ì‹ ë² ì´ìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤..."
                )
                
                insights = await self.knowledge_bank.llm_generate_knowledge_insights(user_message)
                
                return {
                    "type": "llm_knowledge_insights",
                    "analysis": analysis,
                    "insights": insights,
                    "llm_insights": "LLMì´ ì „ì²´ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì „ëµì  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí–ˆìŠµë‹ˆë‹¤."
                }
            
            else:
                # ê¸°ë³¸ LLM ê¸°ë°˜ ì²˜ë¦¬
                relevant_knowledge = await self.knowledge_bank.llm_search_knowledge(user_message, analysis)
                
                return {
                    "type": "llm_general_knowledge",
                    "analysis": analysis,
                    "relevant_knowledge": [
                        {
                            "content": item.content[:300] + "..." if len(item.content) > 300 else item.content,
                            "metadata": item.metadata
                        } for item in relevant_knowledge[:3]
                    ],
                    "llm_insights": "LLMì´ ìš”ì²­ì„ ì´í•´í•˜ê³  ê´€ë ¨ ì§€ì‹ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."
                }
            
        except Exception as e:
            logger.error(f"âŒ Error in LLM processing: {e}")
            return {
                "type": "error",
                "message": f"LLM ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "analysis": analysis
            }
    
    async def cancel(self, context: RequestContext, task_updater: TaskUpdater) -> Any:
        """ì‘ì—… ì·¨ì†Œ"""
        await task_updater.update_status(
            TaskState.cancelled,
            message="ğŸ›‘ LLM First Knowledge Bank ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
        )

# Agent Card ì •ì˜
AGENT_CARD = AgentCard(
    name="LLM First Shared Knowledge Bank",
    description="CherryAI LLM First Architectureë¥¼ ì™„ì „íˆ ì¤€ìˆ˜í•˜ëŠ” ì§€ëŠ¥í˜• ê³µìœ  ì§€ì‹ ì€í–‰ - ëª¨ë“  ì§€ì‹ ì‘ì—…ì„ LLMì´ ë™ì ìœ¼ë¡œ ìˆ˜í–‰",
    skills=[
        AgentSkill(
            name="llm_knowledge_analysis",
            description="LLMì´ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì§€ì‹ ì‘ì—… ìœ í˜•ì„ ë™ì ìœ¼ë¡œ ê²°ì •"
        ),
        AgentSkill(
            name="llm_contextual_search",
            description="LLMì´ ë§¥ë½ì„ ì´í•´í•˜ì—¬ ê´€ë ¨ ì§€ì‹ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ê²€ìƒ‰"
        ),
        AgentSkill(
            name="llm_collaboration_strategy",
            description="LLMì´ ê³¼ê±° ê²½í—˜ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ í˜‘ì—… ì „ëµì„ ì¶”ì²œ"
        ),
        AgentSkill(
            name="llm_pattern_learning",
            description="LLMì´ í˜‘ì—… íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±"
        ),
        AgentSkill(
            name="llm_knowledge_insights",
            description="LLMì´ ì§€ì‹ ë² ì´ìŠ¤ë¥¼ ë¶„ì„í•˜ì—¬ ì „ëµì  ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µ"
        )
    ],
    capabilities=AgentCapabilities(
        supports_streaming=True,
        supports_cancellation=True,
        supports_artifacts=True
    )
)

# ë©”ì¸ ì‹¤í–‰
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # A2A ì„œë²„ ì„¤ì •
    task_store = InMemoryTaskStore()
    request_handler = DefaultRequestHandler(task_store)
    
    # ì—ì´ì „íŠ¸ ë“±ë¡
    executor = LLMFirstKnowledgeBankExecutor()
    request_handler.register_agent(AGENT_CARD, executor)
    
    # ì•± ìƒì„±
    app = A2AStarletteApplication(
        request_handler=request_handler,
        agent_card=AGENT_CARD
    )
    
    # ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Starting LLM First Shared Knowledge Bank Server on port 8602")
    
    config = uvicorn.Config(
        app=app,
        host="0.0.0.0",
        port=8602,
        log_level="info"
    )
    
    server = uvicorn.Server(config)
    await server.serve()

if __name__ == "__main__":
    asyncio.run(main()) 