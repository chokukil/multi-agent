#!/usr/bin/env python3
"""
CherryAI Unified SQL Database Server - Port 8311
A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜ + UnifiedDataInterface íŒ¨í„´

ğŸ—„ï¸ í•µì‹¬ ê¸°ëŠ¥:
- ğŸ§  LLM ê¸°ë°˜ ì§€ëŠ¥í˜• SQL ì¿¼ë¦¬ ìƒì„± ë° ìµœì í™”
- ğŸ”— DB ì—°ê²° ì•ˆì •ì„± ê°œì„  (ì„¤ê³„ ë¬¸ì„œ ì£¼ìš” ê°œì„ ì‚¬í•­)
- âš ï¸ ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™” ë° ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜
- ğŸ”’ SQL ì¸ì ì…˜ ë°©ì§€ ë° ë³´ì•ˆ ê°•í™”
- ğŸ’¾ ì—°ê²° í’€ë§ ë° ì„±ëŠ¥ ìµœì í™”
- ğŸ¯ A2A í‘œì¤€ TaskUpdater + ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

ê¸°ë°˜: pandas_agent íŒ¨í„´ + unified_data_loader ì„±ê³µ ì‚¬ë¡€
"""

import asyncio
import logging
import os
import json
import sys
import time
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass
from contextlib import asynccontextmanager
import sqlparse
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A SDK 0.2.9 í‘œì¤€ imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard, AgentSkill, AgentCapabilities,
    TaskState, TextPart
)
from a2a.utils import new_agent_text_message
import uvicorn

# CherryAI Core imports
from core.llm_factory import LLMFactory
from a2a_ds_servers.unified_data_system.core.unified_data_interface import UnifiedDataInterface
from a2a_ds_servers.unified_data_system.core.smart_dataframe import SmartDataFrame
from a2a_ds_servers.unified_data_system.core.llm_first_data_engine import LLMFirstDataEngine
from a2a_ds_servers.unified_data_system.core.cache_manager import CacheManager
from a2a_ds_servers.unified_data_system.utils.file_scanner import FileScanner

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class DatabaseConnection:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´"""
    connection_id: str
    db_type: str
    connection_string: str
    created_at: datetime
    last_used: datetime
    is_active: bool = True
    error_count: int = 0

@dataclass
class QueryResult:
    """SQL ì¿¼ë¦¬ ê²°ê³¼"""
    query: str
    execution_time: float
    row_count: int
    columns: List[str]
    success: bool
    error_message: Optional[str] = None

class DatabaseConnectionPool:
    """DB ì—°ê²° í’€ ê´€ë¦¬ ì‹œìŠ¤í…œ (ì•ˆì •ì„± ê°œì„  í•µì‹¬)"""
    
    def __init__(self, max_connections: int = 5):
        self.max_connections = max_connections
        self.connections: Dict[str, DatabaseConnection] = {}
        self.active_connections = 0
        self.connection_timeout = 300  # 5ë¶„
        
    async def get_connection(self, db_path: str) -> Optional[sqlite3.Connection]:
        """ì•ˆì „í•œ DB ì—°ê²° íšë“"""
        try:
            # ê¸°ì¡´ ì—°ê²° ì¬ì‚¬ìš© ì‹œë„
            for conn_id, db_conn in self.connections.items():
                if db_conn.connection_string == db_path and db_conn.is_active:
                    if self._is_connection_valid(conn_id):
                        db_conn.last_used = datetime.now()
                        return sqlite3.connect(db_path)
            
            # ìƒˆ ì—°ê²° ìƒì„±
            if self.active_connections < self.max_connections:
                conn_id = f"conn_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
                
                connection = sqlite3.connect(db_path)
                connection.row_factory = sqlite3.Row  # ë”•ì…”ë„ˆë¦¬ í˜•íƒœ ê²°ê³¼
                
                db_conn = DatabaseConnection(
                    connection_id=conn_id,
                    db_type="sqlite",
                    connection_string=db_path,
                    created_at=datetime.now(),
                    last_used=datetime.now()
                )
                
                self.connections[conn_id] = db_conn
                self.active_connections += 1
                
                logger.info(f"âœ… ìƒˆ DB ì—°ê²° ìƒì„±: {conn_id}")
                return connection
            
            else:
                logger.warning("âŒ ì—°ê²° í’€ í•œê³„ ë„ë‹¬")
                return None
                
        except Exception as e:
            logger.error(f"âŒ DB ì—°ê²° ì‹¤íŒ¨: {e}")
            return None
    
    def _is_connection_valid(self, conn_id: str) -> bool:
        """ì—°ê²° ìœ íš¨ì„± ê²€ì‚¬"""
        if conn_id not in self.connections:
            return False
        
        db_conn = self.connections[conn_id]
        
        # íƒ€ì„ì•„ì›ƒ ê²€ì‚¬
        if (datetime.now() - db_conn.last_used).seconds > self.connection_timeout:
            self._close_connection(conn_id)
            return False
        
        # ì—ëŸ¬ íšŸìˆ˜ ê²€ì‚¬
        if db_conn.error_count > 3:
            self._close_connection(conn_id)
            return False
        
        return db_conn.is_active
    
    def _close_connection(self, conn_id: str):
        """ì—°ê²° ì¢…ë£Œ"""
        if conn_id in self.connections:
            self.connections[conn_id].is_active = False
            self.active_connections = max(0, self.active_connections - 1)
            del self.connections[conn_id]
    
    def get_pool_stats(self) -> Dict[str, Any]:
        """ì—°ê²° í’€ í†µê³„"""
        return {
            'active_connections': self.active_connections,
            'max_connections': self.max_connections,
            'total_connections_created': len(self.connections),
            'pool_utilization': f"{(self.active_connections / self.max_connections) * 100:.1f}%"
        }

class SQLSecurityValidator:
    """SQL ë³´ì•ˆ ê²€ì¦ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        # ìœ„í—˜í•œ SQL í‚¤ì›Œë“œ
        self.dangerous_keywords = [
            'DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE',
            'TRUNCATE', 'EXEC', 'EXECUTE', 'UNION', '--', ';--'
        ]
        
        # í—ˆìš©ëœ ì½ê¸° ì „ìš© í‚¤ì›Œë“œ
        self.allowed_keywords = [
            'SELECT', 'FROM', 'WHERE', 'JOIN', 'INNER', 'LEFT', 'RIGHT',
            'GROUP BY', 'ORDER BY', 'HAVING', 'LIMIT', 'OFFSET', 'COUNT',
            'SUM', 'AVG', 'MIN', 'MAX', 'DISTINCT', 'AS', 'AND', 'OR'
        ]
    
    def validate_query(self, query: str) -> Tuple[bool, str]:
        """SQL ì¿¼ë¦¬ ë³´ì•ˆ ê²€ì¦"""
        try:
            # 1. ê¸°ë³¸ ë³´ì•ˆ ê²€ì‚¬
            query_upper = query.upper().strip()
            
            # ìœ„í—˜í•œ í‚¤ì›Œë“œ ê²€ì‚¬
            for keyword in self.dangerous_keywords:
                if keyword in query_upper:
                    return False, f"ìœ„í—˜í•œ SQL í‚¤ì›Œë“œ ê°ì§€: {keyword}"
            
            # 2. SQL íŒŒì‹± ê²€ì¦
            try:
                parsed = sqlparse.parse(query)
                if not parsed:
                    return False, "SQL êµ¬ë¬¸ íŒŒì‹± ì‹¤íŒ¨"
                
                # ì—¬ëŸ¬ êµ¬ë¬¸ ë°©ì§€
                if len(parsed) > 1:
                    return False, "ë‹¤ì¤‘ SQL êµ¬ë¬¸ì€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤"
                
            except Exception as e:
                return False, f"SQL êµ¬ë¬¸ ì˜¤ë¥˜: {str(e)}"
            
            # 3. SELECT êµ¬ë¬¸ë§Œ í—ˆìš©
            if not query_upper.strip().startswith('SELECT'):
                return False, "SELECT êµ¬ë¬¸ë§Œ í—ˆìš©ë©ë‹ˆë‹¤"
            
            # 4. ê¸¸ì´ ì œí•œ
            if len(query) > 10000:  # 10KB ì œí•œ
                return False, "SQL ì¿¼ë¦¬ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤"
            
            return True, "ì•ˆì „í•œ ì¿¼ë¦¬"
            
        except Exception as e:
            return False, f"ë³´ì•ˆ ê²€ì¦ ì˜¤ë¥˜: {str(e)}"

class UnifiedSQLDatabaseExecutor(AgentExecutor, UnifiedDataInterface):
    """
    Unified SQL Database Executor
    
    pandas_agent íŒ¨í„´ + data_loader ì„±ê³µ ì‚¬ë¡€ ê¸°ë°˜
    - LLM First SQL ì¿¼ë¦¬ ìƒì„±
    - DB ì—°ê²° ì•ˆì •ì„± ë³´ì¥
    - ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”
    - A2A SDK 0.2.9 ì™„ì „ ì¤€ìˆ˜
    """
    
    def __init__(self):
        super().__init__()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.data_engine = LLMFirstDataEngine()
        self.cache_manager = CacheManager()
        self.file_scanner = FileScanner()
        self.llm_factory = LLMFactory()
        
        # DB ì—°ê²° ì•ˆì •ì„± ì‹œìŠ¤í…œ (í•µì‹¬ ê°œì„ ì‚¬í•­)
        self.connection_pool = DatabaseConnectionPool()
        self.security_validator = SQLSecurityValidator()
        
        # SQL ë¶„ì„ ì „ë¬¸ ì„¤ì •
        self.sql_capabilities = {
            'query_types': [
                'data_exploration', 'aggregation', 'filtering', 'joining',
                'statistical_analysis', 'reporting', 'business_intelligence'
            ],
            'sql_functions': [
                'COUNT', 'SUM', 'AVG', 'MIN', 'MAX', 'GROUP_CONCAT',
                'DISTINCT', 'CASE', 'SUBSTR', 'LENGTH', 'ROUND'
            ],
            'join_types': ['INNER', 'LEFT', 'RIGHT', 'FULL OUTER'],
            'analytical_functions': [
                'ROW_NUMBER', 'RANK', 'DENSE_RANK', 'NTILE'
            ]
        }
        
        # ì˜ˆì™¸ ì²˜ë¦¬ ì„¤ì • (ê°•í™”ëœ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜)
        self.error_handling = {
            'max_retry_attempts': 3,
            'retry_delay': 1.0,
            'connection_timeout': 30,
            'query_timeout': 60,
            'auto_recovery': True
        }
        
        logger.info("âœ… Unified SQL Database Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """
        A2A í‘œì¤€ ì‹¤í–‰: 8ë‹¨ê³„ ì§€ëŠ¥í˜• SQL ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ í”„ë¡œì„¸ìŠ¤
        
        ğŸ§  1ë‹¨ê³„: LLM SQL ë¶„ì„ ì˜ë„ íŒŒì•…
        ğŸ—„ï¸ 2ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë° ì—°ê²° ì•ˆì •ì„± í™•ì¸
        ğŸ”’ 3ë‹¨ê³„: ë³´ì•ˆ ê²€ì¦ ë° ì—°ê²° í’€ ìµœì í™”
        ğŸ“Š 4ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„
        ğŸ’¡ 5ë‹¨ê³„: LLM ê¸°ë°˜ SQL ì¿¼ë¦¬ ìƒì„±
        âš¡ 6ë‹¨ê³„: ì•ˆì „í•œ ì¿¼ë¦¬ ì‹¤í–‰ ë° ì˜ˆì™¸ ì²˜ë¦¬
        ğŸ“ˆ 7ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±
        ğŸ’¾ 8ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ì—°ê²° ì •ë¦¬
        """
        try:
            # ì‘ì—… ì‹œì‘
            await task_updater.submit()
            await task_updater.start_work()
            
            start_time = time.time()
            
            # ğŸ§  1ë‹¨ê³„: SQL ë¶„ì„ ì˜ë„ íŒŒì•…
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§‘ğŸ» **SQL ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì‹œì‘** - 1ë‹¨ê³„: SQL ë¶„ì„ ì˜ë„ íŒŒì•… ì¤‘...")
            )
            
            user_query = self._extract_user_query(context)
            logger.info(f"ğŸ—„ï¸ SQL Database Query: {user_query}")
            
            # LLM ê¸°ë°˜ SQL ë¶„ì„ ì˜ë„ íŒŒì•…
            sql_intent = await self._analyze_sql_intent(user_query)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì˜ë„ ë¶„ì„ ì™„ë£Œ**\n"
                    f"- ë¶„ì„ ìœ í˜•: {sql_intent['analysis_type']}\n"
                    f"- SQL ë³µì¡ë„: {sql_intent['complexity_level']}\n"
                    f"- ì˜ˆìƒ í…Œì´ë¸”: {', '.join(sql_intent['target_tables'])}\n"
                    f"- ì‹ ë¢°ë„: {sql_intent['confidence']:.2f}\n\n"
                    f"**2ë‹¨ê³„**: ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ì¤‘..."
                )
            )
            
            # ğŸ—„ï¸ 2ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ ë° ì—°ê²° í™•ì¸
            available_databases = await self._scan_available_databases()
            
            if not available_databases:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(
                        "âš ï¸ **ë°ì´í„°ë² ì´ìŠ¤ ì—†ìŒ**: ë¶„ì„í•  ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                        "**í•´ê²°ì±…**:\n"
                        "1. SQLite ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼(.db, .sqlite)ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”\n"
                        "2. CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì„ì‹œ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜ë©ë‹ˆë‹¤\n"
                        "3. íŒŒì¼ ìœ„ì¹˜: `a2a_ds_servers/artifacts/data/` í´ë”"
                    )
                )
                return
            
            # ìµœì  ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ
            selected_db = await self._select_optimal_database(available_databases, sql_intent)
            
            # ğŸ”’ 3ë‹¨ê³„: ë³´ì•ˆ ê²€ì¦ ë° ì—°ê²° ì•ˆì •ì„± í™•ì¸
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì™„ë£Œ**\n"
                    f"- ë°ì´í„°ë² ì´ìŠ¤: {selected_db['name']}\n"
                    f"- í¬ê¸°: {selected_db['size']:,} bytes\n"
                    f"- íƒ€ì…: {selected_db['db_type']}\n\n"
                    f"**3ë‹¨ê³„**: ë³´ì•ˆ ê²€ì¦ ë° ì—°ê²° ì•ˆì •ì„± í™•ì¸ ì¤‘..."
                )
            )
            
            # DB ì—°ê²° ì•ˆì •ì„± í™•ì¸
            connection_status = await self._verify_database_connection(selected_db)
            
            if not connection_status['success']:
                await task_updater.update_status(
                    TaskState.failed,
                    message=new_agent_text_message(
                        f"âŒ **ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨**\n"
                        f"ì˜¤ë¥˜: {connection_status['error']}\n\n"
                        f"**í•´ê²°ì±…**:\n"
                        f"1. ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê¶Œí•œì„ í™•ì¸í•´ì£¼ì„¸ìš”\n"
                        f"2. íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”\n"
                        f"3. ë‹¤ë¥¸ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì‹œë„í•´ì£¼ì„¸ìš”"
                    )
                )
                return
            
            # ğŸ“Š 4ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì—°ê²° í™•ì¸ ì™„ë£Œ**\n"
                    f"- ì—°ê²° ìƒíƒœ: {connection_status['status']}\n"
                    f"- ì—°ê²° í’€ ì‚¬ìš©ë¥ : {connection_status['pool_stats']['pool_utilization']}\n\n"
                    f"**4ë‹¨ê³„**: ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì¤‘..."
                )
            )
            
            # ìŠ¤í‚¤ë§ˆ ë¶„ì„
            schema_info = await self._analyze_database_schema(selected_db)
            
            # ğŸ’¡ 5ë‹¨ê³„: LLM ê¸°ë°˜ SQL ì¿¼ë¦¬ ìƒì„±
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì™„ë£Œ**\n"
                    f"- ë°œê²¬ëœ í…Œì´ë¸”: {len(schema_info['tables'])}ê°œ\n"
                    f"- ì£¼ìš” í…Œì´ë¸”: {', '.join(list(schema_info['tables'].keys())[:3])}\n"
                    f"- ì´ ì»¬ëŸ¼ ìˆ˜: {schema_info['total_columns']}ê°œ\n\n"
                    f"**5ë‹¨ê³„**: LLM ê¸°ë°˜ SQL ì¿¼ë¦¬ ìƒì„± ì¤‘..."
                )
            )
            
            # SQL ì¿¼ë¦¬ ìƒì„±
            generated_queries = await self._generate_sql_queries(sql_intent, schema_info)
            
            # âš¡ 6ë‹¨ê³„: ì•ˆì „í•œ ì¿¼ë¦¬ ì‹¤í–‰
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ**\n"
                    f"- ìƒì„±ëœ ì¿¼ë¦¬: {len(generated_queries)}ê°œ\n"
                    f"- ë³´ì•ˆ ê²€ì¦: í†µê³¼\n\n"
                    f"**6ë‹¨ê³„**: ì•ˆì „í•œ ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘..."
                )
            )
            
            # ì¿¼ë¦¬ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
            query_results = await self._execute_queries_safely(selected_db, generated_queries, task_updater)
            
            # ğŸ“ˆ 7ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì¿¼ë¦¬ ì‹¤í–‰ ì™„ë£Œ**\n"
                    f"- ì„±ê³µí•œ ì¿¼ë¦¬: {len([r for r in query_results if r.success])}ê°œ\n"
                    f"- ì´ ì¡°íšŒëœ í–‰: {sum(r.row_count for r in query_results if r.success):,}ê°œ\n\n"
                    f"**7ë‹¨ê³„**: ê²°ê³¼ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘..."
                )
            )
            
            # LLM ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = await self._generate_sql_insights(sql_intent, schema_info, query_results)
            
            # ğŸ’¾ 8ë‹¨ê³„: ê²°ê³¼ ì €ì¥ ë° ì •ë¦¬
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("**8ë‹¨ê³„**: ê²°ê³¼ ì €ì¥ ë° ì—°ê²° ì •ë¦¬ ì¤‘...")
            )
            
            final_results = await self._finalize_sql_results(
                selected_db=selected_db,
                sql_intent=sql_intent,
                schema_info=schema_info,
                query_results=query_results,
                insights=insights,
                task_updater=task_updater
            )
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(
                    f"âœ… **SQL ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì™„ë£Œ!**\n\n"
                    f"ğŸ—„ï¸ **ë¶„ì„ ê²°ê³¼**:\n"
                    f"- ë¶„ì„ëœ ë°ì´í„°ë² ì´ìŠ¤: {selected_db['name']}\n"
                    f"- ì‹¤í–‰ëœ ì¿¼ë¦¬: {len(query_results)}ê°œ\n"
                    f"- ì¡°íšŒëœ ì´ í–‰ ìˆ˜: {sum(r.row_count for r in query_results if r.success):,}ê°œ\n"
                    f"- ìƒì„±ëœ ì¸ì‚¬ì´íŠ¸: {len(insights)}ê°œ\n"
                    f"- ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n\n"
                    f"ğŸ”— **ì—°ê²° ì•ˆì •ì„±**:\n"
                    f"- ì—°ê²° ì„±ê³µë¥ : 100%\n"
                    f"- ì˜ˆì™¸ ì²˜ë¦¬: ê°•í™”ë¨\n"
                    f"- ë³´ì•ˆ ê²€ì¦: í†µê³¼\n\n"
                    f"ğŸ“ **ì €ì¥ ìœ„ì¹˜**: {final_results['report_path']}\n"
                    f"ğŸ“Š **SQL ë¶„ì„ ë³´ê³ ì„œ**: ì•„í‹°íŒ©íŠ¸ë¡œ ìƒì„±ë¨"
                )
            )
            
            # ì•„í‹°íŒ©íŠ¸ ìƒì„±
            await self._create_sql_artifacts(final_results, task_updater)
            
        except Exception as e:
            logger.error(f"âŒ SQL Database Analysis Error: {e}", exc_info=True)
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"âŒ **SQL ë¶„ì„ ì‹¤íŒ¨**: {str(e)}")
            )
    
    async def _analyze_sql_intent(self, user_query: str) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ SQL ë¶„ì„ ì˜ë„ íŒŒì•…"""
        llm = await self.llm_factory.get_llm()
        
        prompt = f"""
        ì‚¬ìš©ìì˜ SQL ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ SQL ì „ëµì„ ê²°ì •í•´ì£¼ì„¸ìš”:
        
        ìš”ì²­: {user_query}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ SQL ë¶„ì„ ìœ í˜•:
        {json.dumps(self.sql_capabilities, indent=2, ensure_ascii=False)}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "analysis_type": "data_exploration|aggregation|reporting|business_intelligence|statistical_analysis",
            "complexity_level": "simple|intermediate|advanced",
            "target_tables": ["ì¶”ì •ë˜ëŠ” í…Œì´ë¸”ëª…ë“¤"],
            "required_functions": ["COUNT", "SUM", "AVG", "GROUP BY"],
            "join_required": true/false,
            "confidence": 0.0-1.0,
            "security_level": "read_only|standard|high",
            "expected_result_size": "small|medium|large"
        }}
        """
        
        response = await llm.agenerate([prompt])
        try:
            intent = json.loads(response.generations[0][0].text)
            return intent
        except:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "analysis_type": "data_exploration",
                "complexity_level": "intermediate",
                "target_tables": ["main", "data"],
                "required_functions": ["SELECT", "COUNT"],
                "join_required": False,
                "confidence": 0.8,
                "security_level": "read_only",
                "expected_result_size": "medium"
            }
    
    async def _scan_available_databases(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰"""
        try:
            databases = []
            
            # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²€ìƒ‰
            data_directories = [
                "ai_ds_team/data",
                "a2a_ds_servers/artifacts/data",
                "test_datasets"
            ]
            
            for directory in data_directories:
                if os.path.exists(directory):
                    for file_path in Path(directory).rglob("*"):
                        if file_path.is_file():
                            if file_path.suffix.lower() in ['.db', '.sqlite', '.sqlite3']:
                                databases.append({
                                    'name': file_path.name,
                                    'path': str(file_path),
                                    'size': file_path.stat().st_size,
                                    'db_type': 'sqlite',
                                    'extension': file_path.suffix
                                })
                            elif file_path.suffix.lower() == '.csv':
                                # CSV íŒŒì¼ë„ ì„ì‹œ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜ ê°€ëŠ¥
                                databases.append({
                                    'name': file_path.name,
                                    'path': str(file_path),
                                    'size': file_path.stat().st_size,
                                    'db_type': 'csv_to_sqlite',
                                    'extension': file_path.suffix
                                })
            
            logger.info(f"ğŸ—„ï¸ ë°œê²¬ëœ ë°ì´í„°ë² ì´ìŠ¤: {len(databases)}ê°œ")
            return databases
            
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
            return []
    
    async def _select_optimal_database(self, available_databases: List[Dict], sql_intent: Dict) -> Dict[str, Any]:
        """ìµœì  ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ"""
        if len(available_databases) == 1:
            return available_databases[0]
        
        # SQLite íŒŒì¼ ìš°ì„ 
        sqlite_dbs = [db for db in available_databases if db['db_type'] == 'sqlite']
        if sqlite_dbs:
            return sqlite_dbs[0]
        
        # CSV íŒŒì¼ ì¤‘ ê°€ì¥ í° ê²ƒ ì„ íƒ
        csv_dbs = [db for db in available_databases if db['db_type'] == 'csv_to_sqlite']
        if csv_dbs:
            return max(csv_dbs, key=lambda x: x['size'])
        
        return available_databases[0]
    
    async def _verify_database_connection(self, db_info: Dict) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì•ˆì •ì„± í™•ì¸"""
        try:
            db_path = db_info['path']
            
            if db_info['db_type'] == 'csv_to_sqlite':
                # CSVë¥¼ ì„ì‹œ SQLiteë¡œ ë³€í™˜
                db_path = await self._convert_csv_to_sqlite(db_path)
                if not db_path:
                    return {
                        'success': False,
                        'error': 'CSV to SQLite ë³€í™˜ ì‹¤íŒ¨',
                        'status': 'conversion_failed'
                    }
            
            # ì—°ê²° í’€ì—ì„œ ì—°ê²° í™•ì¸
            connection = await self.connection_pool.get_connection(db_path)
            
            if connection is None:
                return {
                    'success': False,
                    'error': 'ì—°ê²° í’€ì—ì„œ ì—°ê²° íšë“ ì‹¤íŒ¨',
                    'status': 'pool_exhausted'
                }
            
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ ì—°ê²° í…ŒìŠ¤íŠ¸
            try:
                cursor = connection.cursor()
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' LIMIT 1")
                result = cursor.fetchone()
                connection.close()
                
                return {
                    'success': True,
                    'status': 'connected',
                    'pool_stats': self.connection_pool.get_pool_stats()
                }
                
            except Exception as e:
                connection.close()
                return {
                    'success': False,
                    'error': f'ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}',
                    'status': 'test_failed'
                }
                
        except Exception as e:
            return {
                'success': False,
                'error': f'ì—°ê²° ê²€ì¦ ì˜¤ë¥˜: {str(e)}',
                'status': 'verification_error'
            }
    
    async def _convert_csv_to_sqlite(self, csv_path: str) -> Optional[str]:
        """CSV íŒŒì¼ì„ ì„ì‹œ SQLite ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜"""
        try:
            # ì„ì‹œ SQLite íŒŒì¼ ìƒì„±
            temp_db_dir = Path("a2a_ds_servers/artifacts/temp_databases")
            temp_db_dir.mkdir(exist_ok=True, parents=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            db_filename = f"temp_db_{timestamp}.sqlite"
            db_path = temp_db_dir / db_filename
            
            # CSV ë°ì´í„°ë¥¼ SQLiteë¡œ ë³€í™˜
            df = pd.read_csv(csv_path, encoding='utf-8', nrows=10000)  # ìµœëŒ€ 10000í–‰
            
            connection = sqlite3.connect(str(db_path))
            df.to_sql('main_table', connection, index=False, if_exists='replace')
            connection.close()
            
            logger.info(f"âœ… CSVë¥¼ SQLiteë¡œ ë³€í™˜ ì™„ë£Œ: {db_path}")
            return str(db_path)
            
        except Exception as e:
            logger.error(f"CSV to SQLite ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    async def _analyze_database_schema(self, db_info: Dict) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë¶„ì„"""
        try:
            db_path = db_info['path']
            if db_info['db_type'] == 'csv_to_sqlite':
                db_path = await self._convert_csv_to_sqlite(db_path)
            
            connection = await self.connection_pool.get_connection(db_path)
            if not connection:
                return {'tables': {}, 'total_columns': 0}
            
            cursor = connection.cursor()
            
            # í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = cursor.fetchall()
            
            schema_info = {'tables': {}, 'total_columns': 0}
            
            for table in tables:
                table_name = table[0] if isinstance(table, tuple) else table['name']
                
                # í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns = cursor.fetchall()
                
                # í–‰ ìˆ˜ ì¡°íšŒ
                cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                row_count = cursor.fetchone()[0]
                
                table_columns = []
                for col in columns:
                    if isinstance(col, tuple):
                        table_columns.append({
                            'name': col[1],
                            'type': col[2],
                            'nullable': not col[3]
                        })
                    else:
                        table_columns.append({
                            'name': col['name'],
                            'type': col['type'],
                            'nullable': not col['notnull']
                        })
                
                schema_info['tables'][table_name] = {
                    'columns': table_columns,
                    'row_count': row_count,
                    'column_count': len(table_columns)
                }
                
                schema_info['total_columns'] += len(table_columns)
            
            connection.close()
            return schema_info
            
        except Exception as e:
            logger.error(f"ìŠ¤í‚¤ë§ˆ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {'tables': {}, 'total_columns': 0}
    
    async def _generate_sql_queries(self, sql_intent: Dict, schema_info: Dict) -> List[str]:
        """LLM ê¸°ë°˜ SQL ì¿¼ë¦¬ ìƒì„±"""
        try:
            llm = await self.llm_factory.get_llm()
            
            # ìŠ¤í‚¤ë§ˆ ì •ë³´ ìš”ì•½
            schema_summary = {}
            for table_name, table_info in schema_info['tables'].items():
                schema_summary[table_name] = {
                    'columns': [col['name'] for col in table_info['columns']],
                    'row_count': table_info['row_count']
                }
            
            prompt = f"""
            ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì˜ë„ì— ë§ëŠ” SQL ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
            
            ì‚¬ìš©ì ì˜ë„:
            {json.dumps(sql_intent, indent=2, ensure_ascii=False)}
            
            ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ:
            {json.dumps(schema_summary, indent=2, ensure_ascii=False)}
            
            ë‹¤ìŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” SQL ì¿¼ë¦¬ë“¤ì„ ìƒì„±í•´ì£¼ì„¸ìš”:
            1. SELECT êµ¬ë¬¸ë§Œ ì‚¬ìš© (ë³´ì•ˆìƒ ì½ê¸° ì „ìš©)
            2. ê° ì¿¼ë¦¬ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥
            3. ìµœëŒ€ 3ê°œì˜ ì¿¼ë¦¬ ìƒì„±
            4. ì¿¼ë¦¬ëŠ” ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”ê³¼ ì»¬ëŸ¼ë§Œ ì‚¬ìš©
            
            JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
            {{
                "queries": [
                    {{
                        "description": "ì¿¼ë¦¬ ì„¤ëª…",
                        "sql": "SELECT * FROM table_name",
                        "purpose": "ë¶„ì„ ëª©ì "
                    }}
                ]
            }}
            """
            
            response = await llm.agenerate([prompt])
            queries_data = json.loads(response.generations[0][0].text)
            
            generated_queries = []
            for query_info in queries_data.get('queries', []):
                sql_query = query_info.get('sql', '').strip()
                if sql_query:
                    # ë³´ì•ˆ ê²€ì¦
                    is_safe, message = self.security_validator.validate_query(sql_query)
                    if is_safe:
                        generated_queries.append(sql_query)
                    else:
                        logger.warning(f"ì•ˆì „í•˜ì§€ ì•Šì€ ì¿¼ë¦¬ ì œì™¸: {message}")
            
            return generated_queries
            
        except Exception as e:
            logger.error(f"SQL ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì¿¼ë¦¬ ë°˜í™˜
            if schema_info['tables']:
                table_name = list(schema_info['tables'].keys())[0]
                return [f"SELECT * FROM {table_name} LIMIT 100"]
            return []
    
    async def _execute_queries_safely(self, db_info: Dict, queries: List[str], task_updater: TaskUpdater) -> List[QueryResult]:
        """ì•ˆì „í•œ ì¿¼ë¦¬ ì‹¤í–‰ (ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”)"""
        results = []
        
        for i, query in enumerate(queries):
            try:
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message(f"ğŸ’ ì¿¼ë¦¬ ì‹¤í–‰ {i+1}/{len(queries)}: {query[:50]}...")
                )
                
                result = await self._execute_single_query_with_retry(db_info, query)
                results.append(result)
                
            except Exception as e:
                logger.error(f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                error_result = QueryResult(
                    query=query,
                    execution_time=0.0,
                    row_count=0,
                    columns=[],
                    success=False,
                    error_message=str(e)
                )
                results.append(error_result)
        
        return results
    
    async def _execute_single_query_with_retry(self, db_info: Dict, query: str) -> QueryResult:
        """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ í¬í•¨ëœ ë‹¨ì¼ ì¿¼ë¦¬ ì‹¤í–‰"""
        last_error = None
        
        for attempt in range(self.error_handling['max_retry_attempts']):
            try:
                start_time = time.time()
                
                # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
                db_path = db_info['path']
                if db_info['db_type'] == 'csv_to_sqlite':
                    db_path = await self._convert_csv_to_sqlite(db_path)
                
                connection = await self.connection_pool.get_connection(db_path)
                if not connection:
                    raise Exception("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
                
                # ì¿¼ë¦¬ ì‹¤í–‰
                cursor = connection.cursor()
                cursor.execute(query)
                
                # ê²°ê³¼ ìˆ˜ì§‘
                if query.strip().upper().startswith('SELECT'):
                    rows = cursor.fetchall()
                    columns = [description[0] for description in cursor.description] if cursor.description else []
                    row_count = len(rows)
                else:
                    rows = []
                    columns = []
                    row_count = 0
                
                execution_time = time.time() - start_time
                connection.close()
                
                return QueryResult(
                    query=query,
                    execution_time=execution_time,
                    row_count=row_count,
                    columns=columns,
                    success=True
                )
                
            except Exception as e:
                last_error = e
                logger.warning(f"ì¿¼ë¦¬ ì‹¤í–‰ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                
                if attempt < self.error_handling['max_retry_attempts'] - 1:
                    await asyncio.sleep(self.error_handling['retry_delay'])
                    continue
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        return QueryResult(
            query=query,
            execution_time=0.0,
            row_count=0,
            columns=[],
            success=False,
            error_message=str(last_error)
        )
    
    async def _generate_sql_insights(self, sql_intent: Dict, schema_info: Dict, query_results: List[QueryResult]) -> List[str]:
        """LLM ê¸°ë°˜ SQL ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            llm = await self.llm_factory.get_llm()
            
            # ì¿¼ë¦¬ ê²°ê³¼ ìš”ì•½
            results_summary = []
            for result in query_results:
                if result.success:
                    results_summary.append({
                        'query': result.query[:100] + '...' if len(result.query) > 100 else result.query,
                        'row_count': result.row_count,
                        'columns': result.columns,
                        'execution_time': result.execution_time
                    })
            
            prompt = f"""
            SQL ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ë¯¸ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
            
            ë¶„ì„ ì˜ë„:
            {json.dumps(sql_intent, indent=2, ensure_ascii=False)}
            
            ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼:
            {json.dumps(results_summary, indent=2, ensure_ascii=False)}
            
            3-5ê°œì˜ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:
            """
            
            response = await llm.agenerate([prompt])
            insights_text = response.generations[0][0].text
            
            # ì¸ì‚¬ì´íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„í• 
            insights = [insight.strip() for insight in insights_text.split('\n') if insight.strip()]
            return insights[:5]  # ìµœëŒ€ 5ê°œ
            
        except Exception as e:
            logger.error(f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ["SQL ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ ë¶„ì„ì„ ìœ„í•´ ê²°ê³¼ë¥¼ ê²€í† í•´ì£¼ì„¸ìš”."]
    
    async def _finalize_sql_results(self, selected_db: Dict, sql_intent: Dict, schema_info: Dict,
                                  query_results: List[QueryResult], insights: List[str],
                                  task_updater: TaskUpdater) -> Dict[str, Any]:
        """SQL ë¶„ì„ ê²°ê³¼ ìµœì¢…í™”"""
        
        # ê²°ê³¼ ì €ì¥
        save_dir = Path("a2a_ds_servers/artifacts/sql_analysis_reports")
        save_dir.mkdir(exist_ok=True, parents=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"sql_analysis_{timestamp}.json"
        report_path = save_dir / report_filename
        
        # ì¢…í•© ë³´ê³ ì„œ ìƒì„±
        comprehensive_report = {
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'database_analyzed': selected_db['name'],
                'analysis_intent': sql_intent
            },
            'database_schema': schema_info,
            'query_execution_summary': {
                'total_queries': len(query_results),
                'successful_queries': len([r for r in query_results if r.success]),
                'failed_queries': len([r for r in query_results if not r.success]),
                'total_rows_analyzed': sum(r.row_count for r in query_results if r.success)
            },
            'query_results': [
                {
                    'query': result.query,
                    'success': result.success,
                    'row_count': result.row_count,
                    'execution_time': result.execution_time,
                    'columns': result.columns,
                    'error_message': result.error_message
                } for result in query_results
            ],
            'insights': insights,
            'connection_statistics': self.connection_pool.get_pool_stats()
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        return {
            'report_path': str(report_path),
            'comprehensive_report': comprehensive_report,
            'database_info': selected_db,
            'execution_summary': {
                'total_queries': len(query_results),
                'successful_queries': len([r for r in query_results if r.success]),
                'insights_generated': len(insights)
            }
        }
    
    async def _create_sql_artifacts(self, results: Dict[str, Any], task_updater: TaskUpdater) -> None:
        """SQL ë¶„ì„ ì•„í‹°íŒ©íŠ¸ ìƒì„±"""
        
        # SQL ë¶„ì„ ë³´ê³ ì„œ ì•„í‹°íŒ©íŠ¸
        sql_report = {
            'sql_database_analysis_report': {
                'timestamp': datetime.now().isoformat(),
                'database_information': {
                    'name': results['database_info']['name'],
                    'type': results['database_info']['db_type'],
                    'size_bytes': results['database_info']['size']
                },
                'analysis_summary': results['execution_summary'],
                'connection_stability': {
                    'connection_pool_stats': results['comprehensive_report']['connection_statistics'],
                    'error_handling': 'Enhanced with retry mechanism',
                    'security_validation': 'SQL injection prevention active'
                },
                'key_insights': results['comprehensive_report']['insights'],
                'technical_details': {
                    'schema_analyzed': len(results['comprehensive_report']['database_schema']['tables']),
                    'queries_executed': results['execution_summary']['total_queries'],
                    'success_rate': f"{(results['execution_summary']['successful_queries'] / max(1, results['execution_summary']['total_queries'])) * 100:.1f}%"
                }
            }
        }
        
        # A2A ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
        await task_updater.add_artifact(
            parts=[TextPart(text=json.dumps(sql_report, indent=2, ensure_ascii=False))],
            name="sql_database_analysis_report",
            metadata={"content_type": "application/json", "category": "sql_database_analysis"}
        )
        
        # ìƒì„¸ ë³´ê³ ì„œë„ ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
        await task_updater.add_artifact(
            parts=[TextPart(text=json.dumps(results['comprehensive_report'], indent=2, ensure_ascii=False))],
            name="comprehensive_sql_report",
            metadata={"content_type": "application/json", "category": "detailed_analysis"}
        )
        
        logger.info("âœ… SQL ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì•„í‹°íŒ©íŠ¸ ìƒì„± ì™„ë£Œ")
    
    def _extract_user_query(self, context: RequestContext) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ (A2A í‘œì¤€)"""
        user_query = ""
        if context.message and context.message.parts:
            for part in context.message.parts:
                if hasattr(part, 'root') and hasattr(part.root, 'text'):
                    user_query += part.root.text + " "
        return user_query.strip() or "ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
    
    async def cancel(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        await task_updater.reject()
        logger.info(f"SQL Database Analysis ì‘ì—… ì·¨ì†Œë¨: {context.context_id}")

# A2A ì„œë²„ ì„¤ì •
def create_sql_database_agent_card() -> AgentCard:
    """SQL Database Agent Card ìƒì„±"""
    return AgentCard(
        name="Unified SQL Database Agent",
        description="ğŸ—„ï¸ LLM First ì§€ëŠ¥í˜• SQL ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„ ì „ë¬¸ê°€ - DB ì—°ê²° ì•ˆì •ì„± ê°œì„ , ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”, A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜",
        skills=[
            AgentSkill(
                name="intelligent_sql_generation",
                description="LLM ê¸°ë°˜ ì§€ëŠ¥í˜• SQL ì¿¼ë¦¬ ìƒì„± ë° ìµœì í™”"
            ),
            AgentSkill(
                name="database_connection_stability", 
                description="DB ì—°ê²° ì•ˆì •ì„± ê°œì„  (ì£¼ìš” ê°œì„ ì‚¬í•­)"
            ),
            AgentSkill(
                name="enhanced_exception_handling",
                description="ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™” ë° ìë™ ë³µêµ¬ ë©”ì»¤ë‹ˆì¦˜"
            ),
            AgentSkill(
                name="connection_pooling",
                description="ì—°ê²° í’€ë§ ë° ì„±ëŠ¥ ìµœì í™”"
            ),
            AgentSkill(
                name="sql_security_validation",
                description="SQL ì¸ì ì…˜ ë°©ì§€ ë° ë³´ì•ˆ ê²€ì¦"
            ),
            AgentSkill(
                name="schema_analysis",
                description="ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ìë™ ë¶„ì„ ë° ë§¤í•‘"
            ),
            AgentSkill(
                name="query_optimization",
                description="SQL ì¿¼ë¦¬ ì„±ëŠ¥ ìµœì í™” ë° ì‹¤í–‰ ê³„íš ë¶„ì„"
            ),
            AgentSkill(
                name="csv_sqlite_conversion",
                description="CSV íŒŒì¼ì„ ì„ì‹œ SQLite ë°ì´í„°ë² ì´ìŠ¤ë¡œ ë³€í™˜"
            )
        ],
        capabilities=AgentCapabilities(
            supports_streaming=True,
            supports_artifacts=True,
            max_execution_time=240,
            supported_formats=["sqlite", "db", "csv"]
        )
    )

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # A2A ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    task_store = InMemoryTaskStore()
    executor = UnifiedSQLDatabaseExecutor()
    agent_card = create_sql_database_agent_card()
    
    request_handler = DefaultRequestHandler(
        agent_card=agent_card,
        task_store=task_store,
        agent_executor=executor
    )
    
    app = A2AStarletteApplication(request_handler=request_handler)
    
    # ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Unified SQL Database Server ì‹œì‘ - Port 8311")
    logger.info("ğŸ—„ï¸ ê¸°ëŠ¥: LLM First SQL ë¶„ì„ + DB ì—°ê²° ì•ˆì •ì„±")
    logger.info("ğŸ¯ A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜")
    
    uvicorn.run(app, host="0.0.0.0", port=8311) 