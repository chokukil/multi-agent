#!/usr/bin/env python3
"""
ü§ù Collaboration Rules Engine - Context Engineering ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í¥ÄÎ¶¨

A2A Í∏∞Î∞ò Context Engineering ÌîåÎû´ÌèºÏóêÏÑú ÏóêÏù¥Ï†ÑÌä∏ Í∞Ñ ÌòëÏóÖ Í∑úÏπô Î∞è ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º Ï†ïÏùòÌïòÍ≥† Í¥ÄÎ¶¨ÌïòÎäî ÌïµÏã¨ ÏãúÏä§ÌÖú
Agent Persona ManagerÏôÄ Ïó∞Í≥ÑÌïòÏó¨ ÏµúÏ†ÅÌôîÎêú Î©ÄÌã∞ÏóêÏù¥Ï†ÑÌä∏ ÌòëÏóÖ ÌôòÍ≤Ω Ï†úÍ≥µ

Key Features:
- ÌòëÏóÖ Ìå®ÌÑ¥ ÌïôÏäµ - ÏÑ±Í≥µÏ†ÅÏù∏ ÌòëÏóÖ Ìå®ÌÑ¥ ÏûêÎèô ÌïôÏäµ
- Ï∂©Îèå Ìï¥Í≤∞ - ÏóêÏù¥Ï†ÑÌä∏ Í∞Ñ Ï∂©Îèå ÏÉÅÌô© ÏûêÎèô Í∞êÏßÄ Î∞è Ìï¥Í≤∞
- Ìö®Ïú®ÏÑ± ÏµúÏ†ÅÌôî - ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÑ±Îä• ÏµúÏ†ÅÌôî
- ÎèôÏ†Å Í∑úÏπô ÏÉùÏÑ± - ÏÉÅÌô©Ïóê Îî∞Î•∏ ÎèôÏ†Å ÌòëÏóÖ Í∑úÏπô ÏÉùÏÑ±
- Ïã§ÏãúÍ∞Ñ Î™®ÎãàÌÑ∞ÎßÅ - ÌòëÏóÖ ÏÉÅÌÉú Ïã§ÏãúÍ∞Ñ Ï∂îÏ†Å Î∞è Ï°∞Ï†ï

Architecture:
- Rule Registry: ÌòëÏóÖ Í∑úÏπô Ï†ÄÏû•ÏÜå Î∞è Í¥ÄÎ¶¨
- Pattern Learning Engine: ÌòëÏóÖ Ìå®ÌÑ¥ ÌïôÏäµ Î∞è Î∂ÑÏÑù
- Conflict Resolution System: Ï∂©Îèå Í∞êÏßÄ Î∞è Ìï¥Í≤∞
- Workflow Optimizer: ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏÑ±Îä• ÏµúÏ†ÅÌôî
- Real-time Monitor: Ïã§ÏãúÍ∞Ñ ÌòëÏóÖ Î™®ÎãàÌÑ∞ÎßÅ
"""

import asyncio
import json
import logging
import os
import time
import uuid
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Tuple, Union
from dataclasses import dataclass, asdict, field
from enum import Enum
from collections import defaultdict, deque
import statistics

import aiofiles
from openai import AsyncOpenAI

# Context Engineering Í¥ÄÎ†® ÏûÑÌè¨Ìä∏
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class RuleType(Enum):
    """ÌòëÏóÖ Í∑úÏπô ÌÉÄÏûÖ"""
    WORKFLOW = "workflow"               # ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í∑úÏπô
    PRIORITY = "priority"               # Ïö∞ÏÑ†ÏàúÏúÑ Í∑úÏπô
    RESOURCE = "resource"               # Î¶¨ÏÜåÏä§ Ìï†Îãπ Í∑úÏπô
    DEPENDENCY = "dependency"           # ÏùòÏ°¥ÏÑ± Í∑úÏπô
    CONFLICT_RESOLUTION = "conflict"    # Ï∂©Îèå Ìï¥Í≤∞ Í∑úÏπô
    PERFORMANCE = "performance"         # ÏÑ±Îä• ÏµúÏ†ÅÌôî Í∑úÏπô
    COMMUNICATION = "communication"     # ÏÜåÌÜµ Í∑úÏπô
    QUALITY = "quality"                 # ÌíàÏßà Î≥¥Ï¶ù Í∑úÏπô

class CollaborationStatus(Enum):
    """ÌòëÏóÖ ÏÉÅÌÉú"""
    PENDING = "pending"
    ACTIVE = "active"
    PAUSED = "paused"
    COMPLETED = "completed"
    FAILED = "failed"
    CONFLICT = "conflict"
    OPTIMIZING = "optimizing"

class ConflictType(Enum):
    """Ï∂©Îèå ÌÉÄÏûÖ"""
    RESOURCE_CONTENTION = "resource_contention"
    PRIORITY_CONFLICT = "priority_conflict"
    DEPENDENCY_CYCLE = "dependency_cycle"
    COMMUNICATION_BREAKDOWN = "communication_breakdown"
    PERFORMANCE_DEGRADATION = "performance_degradation"
    QUALITY_MISMATCH = "quality_mismatch"

@dataclass
class CollaborationRule:
    """ÌòëÏóÖ Í∑úÏπô Ï†ïÏùò"""
    rule_id: str
    rule_type: RuleType
    name: str
    description: str
    conditions: Dict[str, Any]
    actions: List[Dict[str, Any]]
    priority: int
    scope: str  # "global", "domain", "session"
    applicable_agents: List[str]
    success_rate: float
    usage_count: int
    created_at: datetime
    updated_at: datetime
    is_active: bool = True
    learned_pattern: bool = False

@dataclass
class CollaborationEvent:
    """ÌòëÏóÖ Ïù¥Î≤§Ìä∏"""
    event_id: str
    event_type: str
    timestamp: datetime
    agents_involved: List[str]
    event_data: Dict[str, Any]
    duration: float
    success: bool
    performance_metrics: Dict[str, float]
    rule_applied: Optional[str] = None
    conflict_detected: Optional[str] = None

@dataclass
class WorkflowPattern:
    """ÏõåÌÅ¨ÌîåÎ°úÏö∞ Ìå®ÌÑ¥"""
    pattern_id: str
    pattern_name: str
    agent_sequence: List[str]
    typical_duration: float
    success_rate: float
    usage_frequency: int
    performance_score: float
    context_requirements: Dict[str, Any]
    learned_rules: List[str]
    last_updated: datetime

@dataclass
class ConflictSituation:
    """Ï∂©Îèå ÏÉÅÌô©"""
    conflict_id: str
    conflict_type: ConflictType
    involved_agents: List[str]
    description: str
    detected_at: datetime
    resolution_strategy: str
    resolution_actions: List[Dict[str, Any]]
    resolution_time: Optional[float] = None
    resolved: bool = False
    success: bool = False

class RuleRegistry:
    """ÌòëÏóÖ Í∑úÏπô Ï†ÄÏû•ÏÜå"""
    
    def __init__(self, registry_path: str = "collaboration_rules.json"):
        self.registry_path = registry_path
        self.rules: Dict[str, CollaborationRule] = {}
        self.rule_templates: Dict[str, Dict[str, Any]] = {}
        
        # Í∏∞Î≥∏ Í∑úÏπô ÌÖúÌîåÎ¶ø Î°úÎìú
        self._load_default_rule_templates()
        
        logger.info("ü§ù Rule Registry Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    def _load_default_rule_templates(self):
        """Í∏∞Î≥∏ Í∑úÏπô ÌÖúÌîåÎ¶ø Î°úÎìú"""
        self.rule_templates = {
            "sequential_data_processing": {
                "rule_type": RuleType.WORKFLOW,
                "name": "Sequential Data Processing",
                "description": "Îç∞Ïù¥ÌÑ∞ Ï≤òÎ¶¨Î•º ÏúÑÌïú ÏàúÏ∞®Ï†Å ÏõåÌÅ¨ÌîåÎ°úÏö∞",
                "conditions": {
                    "task_type": "data_processing",
                    "data_size": "large",
                    "complexity": "high"
                },
                "actions": [
                    {"action": "assign_agent", "agent": "data_loader", "phase": 1},
                    {"action": "assign_agent", "agent": "data_cleaning", "phase": 2},
                    {"action": "assign_agent", "agent": "eda_tools", "phase": 3},
                    {"action": "assign_agent", "agent": "data_visualization", "phase": 4}
                ],
                "priority": 8,
                "scope": "domain"
            },
            
            "parallel_analysis": {
                "rule_type": RuleType.WORKFLOW,
                "name": "Parallel Analysis Workflow",
                "description": "Î≥ëÎ†¨ Î∂ÑÏÑùÏùÑ ÏúÑÌïú ÏõåÌÅ¨ÌîåÎ°úÏö∞",
                "conditions": {
                    "task_type": "analysis",
                    "urgency": "high",
                    "agents_available": ">=3"
                },
                "actions": [
                    {"action": "parallel_assign", "agents": ["eda_tools", "data_visualization", "feature_engineering"]},
                    {"action": "sync_point", "wait_for": "all"},
                    {"action": "aggregate_results", "coordinator": "pandas_collaboration_hub"}
                ],
                "priority": 7,
                "scope": "session"
            },
            
            "resource_priority": {
                "rule_type": RuleType.PRIORITY,
                "name": "Resource Priority Management",
                "description": "Î¶¨ÏÜåÏä§ Ïö∞ÏÑ†ÏàúÏúÑ Í¥ÄÎ¶¨",
                "conditions": {
                    "resource_contention": True,
                    "priority_conflict": True
                },
                "actions": [
                    {"action": "evaluate_priority", "criteria": ["urgency", "complexity", "user_importance"]},
                    {"action": "allocate_resource", "method": "highest_priority_first"},
                    {"action": "queue_remaining", "strategy": "fifo"}
                ],
                "priority": 9,
                "scope": "global"
            },
            
            "dependency_resolution": {
                "rule_type": RuleType.DEPENDENCY,
                "name": "Dependency Resolution",
                "description": "ÏùòÏ°¥ÏÑ± Ìï¥Í≤∞ Í∑úÏπô",
                "conditions": {
                    "dependency_conflict": True,
                    "circular_dependency": True
                },
                "actions": [
                    {"action": "detect_cycle", "algorithm": "dfs"},
                    {"action": "break_cycle", "strategy": "lowest_priority_edge"},
                    {"action": "reorder_workflow", "optimization": "topological_sort"}
                ],
                "priority": 10,
                "scope": "session"
            },
            
            "performance_optimization": {
                "rule_type": RuleType.PERFORMANCE,
                "name": "Performance Optimization",
                "description": "ÏÑ±Îä• ÏµúÏ†ÅÌôî Í∑úÏπô",
                "conditions": {
                    "performance_degradation": True,
                    "response_time": ">threshold"
                },
                "actions": [
                    {"action": "identify_bottleneck", "method": "performance_profiling"},
                    {"action": "optimize_agent_load", "strategy": "load_balancing"},
                    {"action": "adjust_parallelism", "target": "optimal_throughput"}
                ],
                "priority": 6,
                "scope": "global"
            },
            
            "conflict_mediation": {
                "rule_type": RuleType.CONFLICT_RESOLUTION,
                "name": "Conflict Mediation",
                "description": "ÏóêÏù¥Ï†ÑÌä∏ Í∞Ñ Ï∂©Îèå Ï°∞Ï†ï",
                "conditions": {
                    "communication_conflict": True,
                    "result_inconsistency": True
                },
                "actions": [
                    {"action": "pause_conflicting_agents", "duration": "5s"},
                    {"action": "analyze_conflict", "method": "context_comparison"},
                    {"action": "mediate_resolution", "mediator": "orchestrator"},
                    {"action": "resume_with_agreement", "verification": True}
                ],
                "priority": 9,
                "scope": "session"
            },
            
            "quality_assurance": {
                "rule_type": RuleType.QUALITY,
                "name": "Quality Assurance Protocol",
                "description": "ÌíàÏßà Î≥¥Ï¶ù ÌîÑÎ°úÌÜ†ÏΩú",
                "conditions": {
                    "quality_check": "required",
                    "critical_task": True
                },
                "actions": [
                    {"action": "validate_input", "validator": "data_cleaning"},
                    {"action": "cross_validate", "method": "peer_review"},
                    {"action": "quality_gate", "threshold": 0.95},
                    {"action": "approve_or_retry", "max_retries": 3}
                ],
                "priority": 8,
                "scope": "domain"
            }
        }
    
    async def load_rules(self) -> Dict[str, CollaborationRule]:
        """Í∑úÏπô Î°úÎìú"""
        try:
            if os.path.exists(self.registry_path):
                async with aiofiles.open(self.registry_path, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    data = json.loads(content)
                    
                    for rule_data in data.get('rules', []):
                        rule = CollaborationRule(
                            rule_id=rule_data['rule_id'],
                            rule_type=RuleType(rule_data['rule_type']),
                            name=rule_data['name'],
                            description=rule_data['description'],
                            conditions=rule_data['conditions'],
                            actions=rule_data['actions'],
                            priority=rule_data['priority'],
                            scope=rule_data['scope'],
                            applicable_agents=rule_data['applicable_agents'],
                            success_rate=rule_data.get('success_rate', 0.0),
                            usage_count=rule_data.get('usage_count', 0),
                            created_at=datetime.fromisoformat(rule_data['created_at']),
                            updated_at=datetime.fromisoformat(rule_data['updated_at']),
                            is_active=rule_data.get('is_active', True),
                            learned_pattern=rule_data.get('learned_pattern', False)
                        )
                        self.rules[rule.rule_id] = rule
                        
                logger.info(f"üìö {len(self.rules)}Í∞ú ÌòëÏóÖ Í∑úÏπô Î°úÎìú ÏôÑÎ£å")
            else:
                logger.info("üìö Í∏∞Î≥∏ ÌòëÏóÖ Í∑úÏπô ÏÉùÏÑ± Ï§ë...")
                await self._create_default_rules()
                
        except Exception as e:
            logger.error(f"‚ùå ÌòëÏóÖ Í∑úÏπô Î°úÎìú Ïã§Ìå®: {e}")
            await self._create_default_rules()
        
        return self.rules
    
    async def _create_default_rules(self):
        """Í∏∞Î≥∏ ÌòëÏóÖ Í∑úÏπô ÏÉùÏÑ±"""
        for template_id, template in self.rule_templates.items():
            rule = CollaborationRule(
                rule_id=f"default_{template_id}",
                rule_type=template["rule_type"],
                name=template["name"],
                description=template["description"],
                conditions=template["conditions"],
                actions=template["actions"],
                priority=template["priority"],
                scope=template["scope"],
                applicable_agents=[],  # Î™®Îì† ÏóêÏù¥Ï†ÑÌä∏Ïóê Ï†ÅÏö© Í∞ÄÎä•
                success_rate=0.0,
                usage_count=0,
                created_at=datetime.now(),
                updated_at=datetime.now(),
                is_active=True,
                learned_pattern=False
            )
            
            self.rules[rule.rule_id] = rule
        
        await self.save_rules()
        logger.info(f"ü§ù {len(self.rules)}Í∞ú Í∏∞Î≥∏ ÌòëÏóÖ Í∑úÏπô ÏÉùÏÑ± ÏôÑÎ£å")
    
    async def save_rules(self):
        """Í∑úÏπô Ï†ÄÏû•"""
        try:
            data = {
                "rules": [
                    {
                        "rule_id": rule.rule_id,
                        "rule_type": rule.rule_type.value,
                        "name": rule.name,
                        "description": rule.description,
                        "conditions": rule.conditions,
                        "actions": rule.actions,
                        "priority": rule.priority,
                        "scope": rule.scope,
                        "applicable_agents": rule.applicable_agents,
                        "success_rate": rule.success_rate,
                        "usage_count": rule.usage_count,
                        "created_at": rule.created_at.isoformat(),
                        "updated_at": rule.updated_at.isoformat(),
                        "is_active": rule.is_active,
                        "learned_pattern": rule.learned_pattern
                    }
                    for rule in self.rules.values()
                ],
                "updated_at": datetime.now().isoformat()
            }
            
            async with aiofiles.open(self.registry_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(data, ensure_ascii=False, indent=2))
                
            logger.info(f"üíæ {len(self.rules)}Í∞ú ÌòëÏóÖ Í∑úÏπô Ï†ÄÏû• ÏôÑÎ£å")
            
        except Exception as e:
            logger.error(f"‚ùå ÌòëÏóÖ Í∑úÏπô Ï†ÄÏû• Ïã§Ìå®: {e}")
    
    async def add_rule(self, rule: CollaborationRule):
        """Í∑úÏπô Ï∂îÍ∞Ä"""
        self.rules[rule.rule_id] = rule
        await self.save_rules()
    
    async def update_rule_performance(self, rule_id: str, success: bool):
        """Í∑úÏπô ÏÑ±Îä• ÏóÖÎç∞Ïù¥Ìä∏"""
        if rule_id in self.rules:
            rule = self.rules[rule_id]
            rule.usage_count += 1
            
            if success:
                current_success_count = rule.success_rate * (rule.usage_count - 1)
                new_success_count = current_success_count + 1
                rule.success_rate = new_success_count / rule.usage_count
            
            rule.updated_at = datetime.now()
            await self.save_rules()
    
    async def get_applicable_rules(self, context: Dict[str, Any], agents: List[str]) -> List[CollaborationRule]:
        """Ï†ÅÏö© Í∞ÄÎä•Ìïú Í∑úÏπô Ï°∞Ìöå"""
        applicable_rules = []
        
        for rule in self.rules.values():
            if not rule.is_active:
                continue
            
            # Ï°∞Í±¥ Îß§Ïπ≠ ÌôïÏù∏
            if self._matches_conditions(rule.conditions, context):
                # ÏóêÏù¥Ï†ÑÌä∏ Ï†ÅÏö© Î≤îÏúÑ ÌôïÏù∏
                if not rule.applicable_agents or any(agent in rule.applicable_agents for agent in agents):
                    applicable_rules.append(rule)
        
        # Ïö∞ÏÑ†ÏàúÏúÑÎ°ú Ï†ïÎ†¨
        applicable_rules.sort(key=lambda r: r.priority, reverse=True)
        
        return applicable_rules
    
    def _matches_conditions(self, rule_conditions: Dict[str, Any], context: Dict[str, Any]) -> bool:
        """Ï°∞Í±¥ Îß§Ïπ≠ ÌôïÏù∏"""
        for key, expected_value in rule_conditions.items():
            if key not in context:
                continue
            
            actual_value = context[key]
            
            # Î¨∏ÏûêÏó¥ ÎπÑÍµê
            if isinstance(expected_value, str):
                if isinstance(actual_value, str) and expected_value != actual_value:
                    return False
            
            # Î∂àÎ¶∞ ÎπÑÍµê
            elif isinstance(expected_value, bool):
                if isinstance(actual_value, bool) and expected_value != actual_value:
                    return False
            
            # Ïà´Ïûê ÎπÑÍµê (Î¨∏ÏûêÏó¥Î°ú ÌëúÌòÑÎêú Ï°∞Í±¥ Ìè¨Ìï®)
            elif isinstance(expected_value, str) and expected_value.startswith((">=", "<=", ">", "<", "==")):
                try:
                    operator = expected_value[:2] if expected_value.startswith((">=", "<=")) else expected_value[0]
                    threshold = float(expected_value[len(operator):])
                    
                    if operator == ">=":
                        if actual_value < threshold:
                            return False
                    elif operator == "<=":
                        if actual_value > threshold:
                            return False
                    elif operator == ">":
                        if actual_value <= threshold:
                            return False
                    elif operator == "<":
                        if actual_value >= threshold:
                            return False
                    elif operator == "==":
                        if actual_value != threshold:
                            return False
                except (ValueError, TypeError):
                    continue
        
        return True

class PatternLearningEngine:
    """ÌòëÏóÖ Ìå®ÌÑ¥ ÌïôÏäµ ÏóîÏßÑ"""
    
    def __init__(self, rule_registry: RuleRegistry):
        self.rule_registry = rule_registry
        self.collaboration_history: List[CollaborationEvent] = []
        self.learned_patterns: Dict[str, WorkflowPattern] = {}
        self.learning_threshold = 5  # Ìå®ÌÑ¥ ÌïôÏäµÏùÑ ÏúÑÌïú ÏµúÏÜå Í¥ÄÏ∞∞ ÌöüÏàò
        
    async def record_collaboration_event(self, event: CollaborationEvent):
        """ÌòëÏóÖ Ïù¥Î≤§Ìä∏ Í∏∞Î°ù"""
        self.collaboration_history.append(event)
        
        # ÏµúÍ∑º 1000Í∞ú Ïù¥Î≤§Ìä∏Îßå Ïú†ÏßÄ
        if len(self.collaboration_history) > 1000:
            self.collaboration_history = self.collaboration_history[-1000:]
        
        # Ìå®ÌÑ¥ ÌïôÏäµ Ìä∏Î¶¨Í±∞
        if len(self.collaboration_history) % 10 == 0:  # 10Í∞ú Ïù¥Î≤§Ìä∏ÎßàÎã§ ÌïôÏäµ
            await self._analyze_patterns()
    
    async def _analyze_patterns(self):
        """Ìå®ÌÑ¥ Î∂ÑÏÑù Î∞è ÌïôÏäµ"""
        logger.info("üß† ÌòëÏóÖ Ìå®ÌÑ¥ Î∂ÑÏÑù ÏãúÏûë")
        
        # ÏµúÍ∑º Ïù¥Î≤§Ìä∏Îì§ÏùÑ Î∂ÑÏÑùÌïòÏó¨ Ìå®ÌÑ¥ Ï∂îÏ∂ú
        recent_events = self.collaboration_history[-50:]  # ÏµúÍ∑º 50Í∞ú Ïù¥Î≤§Ìä∏
        
        # ÏóêÏù¥Ï†ÑÌä∏ ÏãúÌÄÄÏä§ Ìå®ÌÑ¥ Î∂ÑÏÑù
        sequence_patterns = self._extract_sequence_patterns(recent_events)
        
        # ÏÑ±Í≥µÏ†ÅÏù∏ Ìå®ÌÑ¥ ÏãùÎ≥Ñ
        successful_patterns = self._identify_successful_patterns(sequence_patterns)
        
        # ÏÉàÎ°úÏö¥ Í∑úÏπô ÏÉùÏÑ±
        for pattern in successful_patterns:
            if pattern.usage_frequency >= self.learning_threshold and pattern.success_rate > 0.8:
                await self._create_learned_rule(pattern)
        
        logger.info(f"‚úÖ Ìå®ÌÑ¥ Î∂ÑÏÑù ÏôÑÎ£å: {len(successful_patterns)}Í∞ú ÏÑ±Í≥µ Ìå®ÌÑ¥ Î∞úÍ≤¨")
    
    def _extract_sequence_patterns(self, events: List[CollaborationEvent]) -> Dict[str, WorkflowPattern]:
        """ÏãúÌÄÄÏä§ Ìå®ÌÑ¥ Ï∂îÏ∂ú"""
        patterns = {}
        
        # Ïó∞ÏÜçÎêú Ïù¥Î≤§Ìä∏Îì§ÏùÑ Í∑∏Î£πÌôî
        for i in range(len(events) - 2):
            sequence = []
            for j in range(i, min(i + 5, len(events))):  # ÏµúÎåÄ 5Í∞ú ÏóêÏù¥Ï†ÑÌä∏ ÏãúÌÄÄÏä§
                sequence.extend(events[j].agents_involved)
            
            if len(sequence) >= 2:
                pattern_key = "_".join(sequence[:4])  # ÏµúÎåÄ 4Í∞ú ÏóêÏù¥Ï†ÑÌä∏
                
                if pattern_key not in patterns:
                    patterns[pattern_key] = WorkflowPattern(
                        pattern_id=f"learned_{pattern_key}_{int(time.time())}",
                        pattern_name=f"Learned Pattern: {pattern_key}",
                        agent_sequence=sequence[:4],
                        typical_duration=0.0,
                        success_rate=0.0,
                        usage_frequency=0,
                        performance_score=0.0,
                        context_requirements={},
                        learned_rules=[],
                        last_updated=datetime.now()
                    )
                
                patterns[pattern_key].usage_frequency += 1
        
        return patterns
    
    def _identify_successful_patterns(self, patterns: Dict[str, WorkflowPattern]) -> List[WorkflowPattern]:
        """ÏÑ±Í≥µÏ†ÅÏù∏ Ìå®ÌÑ¥ ÏãùÎ≥Ñ"""
        successful_patterns = []
        
        for pattern in patterns.values():
            # Ìï¥Îãπ Ìå®ÌÑ¥Í≥º Í¥ÄÎ†®Îêú Ïù¥Î≤§Ìä∏Îì§ Ï∞æÍ∏∞
            related_events = [
                event for event in self.collaboration_history
                if any(agent in pattern.agent_sequence for agent in event.agents_involved)
            ]
            
            if related_events:
                # ÏÑ±Í≥µÎ•† Í≥ÑÏÇ∞
                successful_events = [e for e in related_events if e.success]
                pattern.success_rate = len(successful_events) / len(related_events)
                
                # ÌèâÍ∑† ÏßÄÏÜç ÏãúÍ∞Ñ Í≥ÑÏÇ∞
                pattern.typical_duration = statistics.mean([e.duration for e in related_events])
                
                # ÏÑ±Îä• Ï†êÏàò Í≥ÑÏÇ∞
                if successful_events:
                    avg_performance = statistics.mean([
                        statistics.mean(list(e.performance_metrics.values()))
                        for e in successful_events
                        if e.performance_metrics
                    ])
                    pattern.performance_score = avg_performance
                
                if pattern.success_rate > 0.6 and pattern.usage_frequency >= 3:
                    successful_patterns.append(pattern)
        
        return successful_patterns
    
    async def _create_learned_rule(self, pattern: WorkflowPattern):
        """ÌïôÏäµÎêú Ìå®ÌÑ¥ÏúºÎ°úÎ∂ÄÌÑ∞ Í∑úÏπô ÏÉùÏÑ±"""
        rule_id = f"learned_pattern_{pattern.pattern_id}"
        
        # Ïù¥ÎØ∏ Ï°¥Ïû¨ÌïòÎäî Í∑úÏπôÏù∏ÏßÄ ÌôïÏù∏
        if rule_id in self.rule_registry.rules:
            return
        
        # ÏÉàÎ°úÏö¥ Í∑úÏπô ÏÉùÏÑ±
        learned_rule = CollaborationRule(
            rule_id=rule_id,
            rule_type=RuleType.WORKFLOW,
            name=f"Learned: {pattern.pattern_name}",
            description=f"Ìå®ÌÑ¥ ÌïôÏäµÏúºÎ°ú ÏÉùÏÑ±Îêú Í∑úÏπô (ÏÑ±Í≥µÎ•†: {pattern.success_rate:.1%})",
            conditions={
                "pattern_match": True,
                "agents_available": pattern.agent_sequence,
                "performance_requirement": "high" if pattern.performance_score > 0.8 else "medium"
            },
            actions=[
                {
                    "action": "apply_learned_sequence",
                    "sequence": pattern.agent_sequence,
                    "expected_duration": pattern.typical_duration,
                    "confidence": pattern.success_rate
                }
            ],
            priority=int(pattern.success_rate * 10),  # ÏÑ±Í≥µÎ•†Ïóê ÎπÑÎ°ÄÌïú Ïö∞ÏÑ†ÏàúÏúÑ
            scope="session",
            applicable_agents=pattern.agent_sequence,
            success_rate=pattern.success_rate,
            usage_count=pattern.usage_frequency,
            created_at=datetime.now(),
            updated_at=datetime.now(),
            is_active=True,
            learned_pattern=True
        )
        
        await self.rule_registry.add_rule(learned_rule)
        pattern.learned_rules.append(rule_id)
        
        logger.info(f"üéì ÏÉàÎ°úÏö¥ ÌïôÏäµ Í∑úÏπô ÏÉùÏÑ±: {learned_rule.name}")

class ConflictResolutionSystem:
    """Ï∂©Îèå Ìï¥Í≤∞ ÏãúÏä§ÌÖú"""
    
    def __init__(self, rule_registry: RuleRegistry):
        self.rule_registry = rule_registry
        self.active_conflicts: Dict[str, ConflictSituation] = {}
        self.resolution_strategies: Dict[ConflictType, List[str]] = {}
        self.conflict_history: List[ConflictSituation] = []
        
        self._initialize_resolution_strategies()
    
    def _initialize_resolution_strategies(self):
        """Ìï¥Í≤∞ Ï†ÑÎûµ Ï¥àÍ∏∞Ìôî"""
        self.resolution_strategies = {
            ConflictType.RESOURCE_CONTENTION: [
                "priority_based_allocation",
                "time_slicing",
                "resource_scaling",
                "queue_management"
            ],
            ConflictType.PRIORITY_CONFLICT: [
                "priority_reevaluation",
                "escalation_to_orchestrator",
                "user_intervention",
                "automatic_resolution"
            ],
            ConflictType.DEPENDENCY_CYCLE: [
                "cycle_detection",
                "dependency_breaking",
                "workflow_reordering",
                "parallel_execution"
            ],
            ConflictType.COMMUNICATION_BREAKDOWN: [
                "communication_retry",
                "alternative_channel",
                "mediator_intervention",
                "isolation_and_restart"
            ],
            ConflictType.PERFORMANCE_DEGRADATION: [
                "load_balancing",
                "resource_optimization",
                "caching_strategy",
                "algorithm_switching"
            ],
            ConflictType.QUALITY_MISMATCH: [
                "quality_validation",
                "result_reconciliation",
                "expert_review",
                "quality_threshold_adjustment"
            ]
        }
    
    async def detect_conflict(self, context: Dict[str, Any], agents: List[str]) -> Optional[ConflictSituation]:
        """Ï∂©Îèå Í∞êÏßÄ"""
        # Î¶¨ÏÜåÏä§ Í≤ΩÌï© Í∞êÏßÄ
        if context.get("resource_contention", False):
            return await self._create_conflict(
                ConflictType.RESOURCE_CONTENTION,
                agents,
                "Multiple agents competing for the same resource"
            )
        
        # Ïö∞ÏÑ†ÏàúÏúÑ Ï∂©Îèå Í∞êÏßÄ
        if context.get("priority_conflict", False):
            return await self._create_conflict(
                ConflictType.PRIORITY_CONFLICT,
                agents,
                "Conflicting priority assignments detected"
            )
        
        # ÏùòÏ°¥ÏÑ± ÏÇ¨Ïù¥ÌÅ¥ Í∞êÏßÄ
        if context.get("dependency_cycle", False):
            return await self._create_conflict(
                ConflictType.DEPENDENCY_CYCLE,
                agents,
                "Circular dependency detected in workflow"
            )
        
        # ÏÑ±Îä• Ï†ÄÌïò Í∞êÏßÄ
        if context.get("performance_degradation", False):
            return await self._create_conflict(
                ConflictType.PERFORMANCE_DEGRADATION,
                agents,
                "Performance degradation detected"
            )
        
        return None
    
    async def _create_conflict(self, conflict_type: ConflictType, agents: List[str], description: str) -> ConflictSituation:
        """Ï∂©Îèå ÏÉÅÌô© ÏÉùÏÑ±"""
        conflict_id = f"conflict_{uuid.uuid4().hex[:8]}"
        
        conflict = ConflictSituation(
            conflict_id=conflict_id,
            conflict_type=conflict_type,
            involved_agents=agents,
            description=description,
            detected_at=datetime.now(),
            resolution_strategy="",
            resolution_actions=[]
        )
        
        self.active_conflicts[conflict_id] = conflict
        
        logger.warning(f"‚ö†Ô∏è Ï∂©Îèå Í∞êÏßÄ: {conflict_type.value} (ÏóêÏù¥Ï†ÑÌä∏: {', '.join(agents)})")
        
        return conflict
    
    async def resolve_conflict(self, conflict: ConflictSituation) -> bool:
        """Ï∂©Îèå Ìï¥Í≤∞"""
        logger.info(f"üîß Ï∂©Îèå Ìï¥Í≤∞ ÏãúÏûë: {conflict.conflict_id}")
        
        resolution_start = time.time()
        
        # Ìï¥Í≤∞ Ï†ÑÎûµ ÏÑ†ÌÉù
        strategies = self.resolution_strategies.get(conflict.conflict_type, ["default_resolution"])
        
        for strategy in strategies:
            try:
                success = await self._apply_resolution_strategy(conflict, strategy)
                if success:
                    conflict.resolution_strategy = strategy
                    conflict.resolution_time = time.time() - resolution_start
                    conflict.resolved = True
                    conflict.success = True
                    
                    # Ï∂©Îèå Ìï¥Í≤∞ ÏôÑÎ£å
                    if conflict.conflict_id in self.active_conflicts:
                        del self.active_conflicts[conflict.conflict_id]
                    
                    self.conflict_history.append(conflict)
                    
                    logger.info(f"‚úÖ Ï∂©Îèå Ìï¥Í≤∞ ÏôÑÎ£å: {conflict.conflict_id} ({strategy})")
                    return True
                    
            except Exception as e:
                logger.error(f"‚ùå Ìï¥Í≤∞ Ï†ÑÎûµ Ïã§Ìå®: {strategy} - {e}")
                continue
        
        # Î™®Îì† Ï†ÑÎûµ Ïã§Ìå®
        conflict.resolution_time = time.time() - resolution_start
        conflict.resolved = True
        conflict.success = False
        
        logger.error(f"‚ùå Ï∂©Îèå Ìï¥Í≤∞ Ïã§Ìå®: {conflict.conflict_id}")
        return False
    
    async def _apply_resolution_strategy(self, conflict: ConflictSituation, strategy: str) -> bool:
        """Ìï¥Í≤∞ Ï†ÑÎûµ Ï†ÅÏö©"""
        # Mock Íµ¨ÌòÑ - Ïã§Ï†ú ÌôòÍ≤ΩÏóêÏÑúÎäî Íµ¨Ï≤¥Ï†ÅÏù∏ Ìï¥Í≤∞ Î°úÏßÅ Íµ¨ÌòÑ
        if strategy == "priority_based_allocation":
            conflict.resolution_actions.append({
                "action": "reallocate_resources",
                "method": "priority_based",
                "agents": conflict.involved_agents
            })
            return True
        
        elif strategy == "communication_retry":
            conflict.resolution_actions.append({
                "action": "retry_communication",
                "attempts": 3,
                "agents": conflict.involved_agents
            })
            return True
        
        elif strategy == "load_balancing":
            conflict.resolution_actions.append({
                "action": "redistribute_load",
                "method": "even_distribution",
                "agents": conflict.involved_agents
            })
            return True
        
        elif strategy == "dependency_breaking":
            conflict.resolution_actions.append({
                "action": "break_dependency_cycle",
                "method": "topological_sort",
                "agents": conflict.involved_agents
            })
            return True
        
        else:
            # Í∏∞Î≥∏ Ìï¥Í≤∞ Ï†ÑÎûµ
            conflict.resolution_actions.append({
                "action": "default_resolution",
                "method": "restart_agents",
                "agents": conflict.involved_agents
            })
            return True
    
    async def get_conflict_analytics(self) -> Dict[str, Any]:
        """Ï∂©Îèå Î∂ÑÏÑù Ï†ïÎ≥¥"""
        total_conflicts = len(self.conflict_history) + len(self.active_conflicts)
        resolved_conflicts = len([c for c in self.conflict_history if c.resolved])
        successful_resolutions = len([c for c in self.conflict_history if c.success])
        
        analytics = {
            "total_conflicts": total_conflicts,
            "active_conflicts": len(self.active_conflicts),
            "resolved_conflicts": resolved_conflicts,
            "resolution_rate": resolved_conflicts / total_conflicts if total_conflicts > 0 else 0,
            "success_rate": successful_resolutions / resolved_conflicts if resolved_conflicts > 0 else 0,
            "conflict_types": {},
            "resolution_strategies": {},
            "average_resolution_time": 0.0
        }
        
        # Ï∂©Îèå ÌÉÄÏûÖÎ≥Ñ Î∂ÑÏÑù
        for conflict in self.conflict_history:
            conflict_type = conflict.conflict_type.value
            if conflict_type not in analytics["conflict_types"]:
                analytics["conflict_types"][conflict_type] = {"count": 0, "success_rate": 0.0}
            
            analytics["conflict_types"][conflict_type]["count"] += 1
            if conflict.success:
                analytics["conflict_types"][conflict_type]["success_rate"] += 1
        
        # ÏÑ±Í≥µÎ•† Í≥ÑÏÇ∞
        for conflict_type in analytics["conflict_types"]:
            count = analytics["conflict_types"][conflict_type]["count"]
            success_count = analytics["conflict_types"][conflict_type]["success_rate"]
            analytics["conflict_types"][conflict_type]["success_rate"] = success_count / count if count > 0 else 0
        
        # ÌèâÍ∑† Ìï¥Í≤∞ ÏãúÍ∞Ñ Í≥ÑÏÇ∞
        resolution_times = [c.resolution_time for c in self.conflict_history if c.resolution_time is not None]
        if resolution_times:
            analytics["average_resolution_time"] = statistics.mean(resolution_times)
        
        return analytics

class WorkflowOptimizer:
    """ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏµúÏ†ÅÌôîÍ∏∞"""
    
    def __init__(self, rule_registry: RuleRegistry, pattern_learning: PatternLearningEngine):
        self.rule_registry = rule_registry
        self.pattern_learning = pattern_learning
        self.optimization_history: List[Dict[str, Any]] = []
        
    async def optimize_workflow(self, workflow_context: Dict[str, Any], agents: List[str]) -> Dict[str, Any]:
        """ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏµúÏ†ÅÌôî"""
        logger.info(f"‚ö° ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏµúÏ†ÅÌôî ÏãúÏûë: {len(agents)}Í∞ú ÏóêÏù¥Ï†ÑÌä∏")
        
        optimization_start = time.time()
        
        # ÌòÑÏû¨ ÏÉÅÌÉú Î∂ÑÏÑù
        current_state = await self._analyze_current_state(workflow_context, agents)
        
        # ÏµúÏ†ÅÌôî Í∏∞Ìöå ÏãùÎ≥Ñ
        optimization_opportunities = await self._identify_optimization_opportunities(current_state)
        
        # ÏµúÏ†ÅÌôî Ï†ÅÏö©
        optimized_workflow = await self._apply_optimizations(workflow_context, optimization_opportunities)
        
        optimization_time = time.time() - optimization_start
        
        # ÏµúÏ†ÅÌôî Í≤∞Í≥º Í∏∞Î°ù
        optimization_record = {
            "timestamp": datetime.now().isoformat(),
            "original_workflow": workflow_context,
            "optimized_workflow": optimized_workflow,
            "optimization_opportunities": optimization_opportunities,
            "optimization_time": optimization_time,
            "expected_improvement": optimization_opportunities.get("expected_improvement", 0.0)
        }
        
        self.optimization_history.append(optimization_record)
        
        logger.info(f"‚úÖ ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏµúÏ†ÅÌôî ÏôÑÎ£å ({optimization_time:.2f}Ï¥à)")
        
        return optimized_workflow
    
    async def _analyze_current_state(self, context: Dict[str, Any], agents: List[str]) -> Dict[str, Any]:
        """ÌòÑÏû¨ ÏÉÅÌÉú Î∂ÑÏÑù"""
        return {
            "agent_count": len(agents),
            "complexity": context.get("complexity", "medium"),
            "estimated_duration": context.get("estimated_duration", 10.0),
            "resource_usage": context.get("resource_usage", "medium"),
            "parallel_potential": self._calculate_parallel_potential(agents),
            "bottlenecks": self._identify_bottlenecks(context, agents)
        }
    
    async def _identify_optimization_opportunities(self, current_state: Dict[str, Any]) -> Dict[str, Any]:
        """ÏµúÏ†ÅÌôî Í∏∞Ìöå ÏãùÎ≥Ñ"""
        opportunities = {
            "parallelization": 0.0,
            "load_balancing": 0.0,
            "resource_optimization": 0.0,
            "workflow_reordering": 0.0,
            "expected_improvement": 0.0
        }
        
        # Î≥ëÎ†¨Ìôî Í∏∞Ìöå
        if current_state["parallel_potential"] > 0.7:
            opportunities["parallelization"] = 0.3  # 30% Í∞úÏÑ† Í∏∞ÎåÄ
        
        # Î°úÎìú Î∞∏Îü∞Ïã± Í∏∞Ìöå
        if current_state.get("bottlenecks"):
            opportunities["load_balancing"] = 0.2  # 20% Í∞úÏÑ† Í∏∞ÎåÄ
        
        # Î¶¨ÏÜåÏä§ ÏµúÏ†ÅÌôî Í∏∞Ìöå
        if current_state["resource_usage"] == "high":
            opportunities["resource_optimization"] = 0.15  # 15% Í∞úÏÑ† Í∏∞ÎåÄ
        
        # Ï†ÑÏ≤¥ ÏòàÏÉÅ Í∞úÏÑ†ÎèÑ Í≥ÑÏÇ∞
        opportunities["expected_improvement"] = sum([
            opportunities["parallelization"],
            opportunities["load_balancing"],
            opportunities["resource_optimization"]
        ])
        
        return opportunities
    
    async def _apply_optimizations(self, context: Dict[str, Any], opportunities: Dict[str, Any]) -> Dict[str, Any]:
        """ÏµúÏ†ÅÌôî Ï†ÅÏö©"""
        optimized_context = context.copy()
        
        # Î≥ëÎ†¨Ìôî Ï†ÅÏö©
        if opportunities["parallelization"] > 0:
            optimized_context["parallel_execution"] = True
            optimized_context["parallel_degree"] = min(4, optimized_context.get("agent_count", 1))
        
        # Î°úÎìú Î∞∏Îü∞Ïã± Ï†ÅÏö©
        if opportunities["load_balancing"] > 0:
            optimized_context["load_balancing"] = True
            optimized_context["load_distribution"] = "even"
        
        # Î¶¨ÏÜåÏä§ ÏµúÏ†ÅÌôî Ï†ÅÏö©
        if opportunities["resource_optimization"] > 0:
            optimized_context["resource_optimization"] = True
            optimized_context["resource_allocation"] = "dynamic"
        
        # ÏòàÏÉÅ ÏÑ±Îä• Í∞úÏÑ† Ï†ÅÏö©
        if "estimated_duration" in optimized_context:
            improvement_factor = 1 - opportunities["expected_improvement"]
            optimized_context["estimated_duration"] *= improvement_factor
        
        return optimized_context
    
    def _calculate_parallel_potential(self, agents: List[str]) -> float:
        """Î≥ëÎ†¨ Ï≤òÎ¶¨ Ïû†Ïû¨Î†• Í≥ÑÏÇ∞"""
        # ÏóêÏù¥Ï†ÑÌä∏ Í∞Ñ ÎèÖÎ¶ΩÏÑ± ÌèâÍ∞Ä (Í∞ÑÎã®Ìïú Ìú¥Î¶¨Ïä§Ìã±)
        if len(agents) <= 1:
            return 0.0
        elif len(agents) <= 3:
            return 0.6
        else:
            return 0.8
    
    def _identify_bottlenecks(self, context: Dict[str, Any], agents: List[str]) -> List[str]:
        """Î≥ëÎ™© ÏßÄÏ†ê ÏãùÎ≥Ñ"""
        bottlenecks = []
        
        # Î≥µÏû°ÎèÑ Í∏∞Î∞ò Î≥ëÎ™© ÏãùÎ≥Ñ
        if context.get("complexity") == "high":
            bottlenecks.append("high_complexity_processing")
        
        # Î¶¨ÏÜåÏä§ ÏÇ¨Ïö©Îüâ Í∏∞Î∞ò Î≥ëÎ™© ÏãùÎ≥Ñ
        if context.get("resource_usage") == "high":
            bottlenecks.append("resource_contention")
        
        # ÏóêÏù¥Ï†ÑÌä∏ Ïàò Í∏∞Î∞ò Î≥ëÎ™© ÏãùÎ≥Ñ
        if len(agents) > 5:
            bottlenecks.append("coordination_overhead")
        
        return bottlenecks

class CollaborationRulesEngine:
    """Collaboration Rules Engine - Î©îÏù∏ Í¥ÄÎ¶¨ ÌÅ¥ÎûòÏä§"""
    
    def __init__(self, rules_path: str = "collaboration_rules.json"):
        # ÌïµÏã¨ Ïª¥Ìè¨ÎÑåÌä∏ Ï¥àÍ∏∞Ìôî
        self.rule_registry = RuleRegistry(rules_path)
        self.pattern_learning = PatternLearningEngine(self.rule_registry)
        self.conflict_resolution = ConflictResolutionSystem(self.rule_registry)
        self.workflow_optimizer = WorkflowOptimizer(self.rule_registry, self.pattern_learning)
        
        # ÏÉÅÌÉú Í¥ÄÎ¶¨
        self.active_collaborations: Dict[str, Dict[str, Any]] = {}
        self.performance_metrics: Dict[str, float] = {}
        
        logger.info("ü§ù Collaboration Rules Engine Ï¥àÍ∏∞Ìôî ÏôÑÎ£å")
    
    async def initialize(self) -> Dict[str, Any]:
        """Collaboration Rules Engine Ï¥àÍ∏∞Ìôî"""
        logger.info("üöÄ Collaboration Rules Engine Ï¥àÍ∏∞Ìôî Ï§ë...")
        
        # Í∑úÏπô Î°úÎìú
        rules = await self.rule_registry.load_rules()
        
        initialization_result = {
            "total_rules": len(rules),
            "rule_types": list(set(rule.rule_type.value for rule in rules.values())),
            "learned_rules": len([rule for rule in rules.values() if rule.learned_pattern]),
            "initialization_status": "completed",
            "features": [
                "pattern_learning",
                "conflict_resolution",
                "workflow_optimization",
                "real_time_monitoring"
            ]
        }
        
        logger.info(f"‚úÖ Collaboration Rules Engine Ï¥àÍ∏∞Ìôî ÏôÑÎ£å: {initialization_result['total_rules']}Í∞ú Í∑úÏπô")
        
        return initialization_result
    
    async def start_collaboration(self, collaboration_id: str, context: Dict[str, Any], 
                                agents: List[str]) -> Dict[str, Any]:
        """ÌòëÏóÖ ÏãúÏûë"""
        logger.info(f"üöÄ ÌòëÏóÖ ÏãúÏûë: {collaboration_id} (ÏóêÏù¥Ï†ÑÌä∏: {len(agents)}Í∞ú)")
        
        collaboration_start = time.time()
        
        # Ï∂©Îèå Í∞êÏßÄ
        conflict = await self.conflict_resolution.detect_conflict(context, agents)
        
        if conflict:
            # Ï∂©Îèå Ìï¥Í≤∞ ÏãúÎèÑ
            resolution_success = await self.conflict_resolution.resolve_conflict(conflict)
            if not resolution_success:
                return {
                    "collaboration_id": collaboration_id,
                    "status": CollaborationStatus.CONFLICT.value,
                    "error": "Unresolved conflict detected",
                    "conflict": asdict(conflict)
                }
        
        # Ï†ÅÏö© Í∞ÄÎä•Ìïú Í∑úÏπô Ï°∞Ìöå
        applicable_rules = await self.rule_registry.get_applicable_rules(context, agents)
        
        # ÏõåÌÅ¨ÌîåÎ°úÏö∞ ÏµúÏ†ÅÌôî
        optimized_context = await self.workflow_optimizer.optimize_workflow(context, agents)
        
        # ÌòëÏóÖ ÏÑ∏ÏÖò ÏÉùÏÑ±
        collaboration_session = {
            "collaboration_id": collaboration_id,
            "start_time": collaboration_start,
            "context": optimized_context,
            "agents": agents,
            "applicable_rules": [rule.rule_id for rule in applicable_rules],
            "status": CollaborationStatus.ACTIVE.value,
            "conflict_resolved": conflict is not None and conflict.success,
            "optimization_applied": True
        }
        
        self.active_collaborations[collaboration_id] = collaboration_session
        
        result = {
            "collaboration_id": collaboration_id,
            "status": CollaborationStatus.ACTIVE.value,
            "agents": agents,
            "applied_rules": len(applicable_rules),
            "optimization_improvement": optimized_context.get("expected_improvement", 0.0),
            "conflict_detected": conflict is not None,
            "conflict_resolved": conflict is not None and conflict.success,
            "estimated_duration": optimized_context.get("estimated_duration", context.get("estimated_duration", 0.0)),
            "collaboration_features": {
                "rules_applied": [rule.name for rule in applicable_rules[:3]],
                "optimization_enabled": True,
                "conflict_resolution": True,
                "pattern_learning": True
            }
        }
        
        logger.info(f"‚úÖ ÌòëÏóÖ ÏãúÏûë ÏôÑÎ£å: {collaboration_id}")
        
        return result
    
    async def update_collaboration_progress(self, collaboration_id: str, 
                                          progress_data: Dict[str, Any]) -> Dict[str, Any]:
        """ÌòëÏóÖ ÏßÑÌñâ ÏÉÅÌô© ÏóÖÎç∞Ïù¥Ìä∏"""
        if collaboration_id not in self.active_collaborations:
            return {"error": "Collaboration not found"}
        
        collaboration = self.active_collaborations[collaboration_id]
        
        # ÏßÑÌñâ ÏÉÅÌô© Í∏∞Î°ù
        if "events" not in collaboration:
            collaboration["events"] = []
        
        # Ïù¥Î≤§Ìä∏ ÏÉùÏÑ± Î∞è Í∏∞Î°ù
        event = CollaborationEvent(
            event_id=f"event_{uuid.uuid4().hex[:8]}",
            event_type=progress_data.get("event_type", "progress_update"),
            timestamp=datetime.now(),
            agents_involved=progress_data.get("agents", collaboration["agents"]),
            event_data=progress_data,
            duration=progress_data.get("duration", 0.0),
            success=progress_data.get("success", True),
            performance_metrics=progress_data.get("performance_metrics", {})
        )
        
        collaboration["events"].append(asdict(event))
        
        # Ìå®ÌÑ¥ ÌïôÏäµÏùÑ ÏúÑÌïú Ïù¥Î≤§Ìä∏ Í∏∞Î°ù
        await self.pattern_learning.record_collaboration_event(event)
        
        # ÏÑ±Îä• Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏
        if event.performance_metrics:
            self._update_performance_metrics(event.performance_metrics)
        
        return {
            "collaboration_id": collaboration_id,
            "event_recorded": True,
            "total_events": len(collaboration["events"]),
            "learning_active": True
        }
    
    async def complete_collaboration(self, collaboration_id: str, 
                                   success: bool, final_metrics: Dict[str, Any] = None) -> Dict[str, Any]:
        """ÌòëÏóÖ ÏôÑÎ£å"""
        if collaboration_id not in self.active_collaborations:
            return {"error": "Collaboration not found"}
        
        collaboration = self.active_collaborations[collaboration_id]
        
        # ÏôÑÎ£å ÏãúÍ∞Ñ Í∏∞Î°ù
        completion_time = time.time()
        total_duration = completion_time - collaboration["start_time"]
        
        collaboration["status"] = CollaborationStatus.COMPLETED.value if success else CollaborationStatus.FAILED.value
        collaboration["completion_time"] = completion_time
        collaboration["total_duration"] = total_duration
        collaboration["success"] = success
        collaboration["final_metrics"] = final_metrics or {}
        
        # Ï†ÅÏö©Îêú Í∑úÏπôÎì§Ïùò ÏÑ±Îä• ÏóÖÎç∞Ïù¥Ìä∏
        for rule_id in collaboration["applicable_rules"]:
            await self.rule_registry.update_rule_performance(rule_id, success)
        
        # ÌòëÏóÖ ÏÑ∏ÏÖòÏùÑ ÌôúÏÑ± Î™©Î°ùÏóêÏÑú Ï†úÍ±∞
        completed_collaboration = self.active_collaborations.pop(collaboration_id)
        
        logger.info(f"üèÅ ÌòëÏóÖ ÏôÑÎ£å: {collaboration_id} ({'ÏÑ±Í≥µ' if success else 'Ïã§Ìå®'}, {total_duration:.2f}Ï¥à)")
        
        return {
            "collaboration_id": collaboration_id,
            "status": completed_collaboration["status"],
            "total_duration": total_duration,
            "success": success,
            "events_count": len(completed_collaboration.get("events", [])),
            "rules_updated": len(collaboration["applicable_rules"]),
            "performance_learned": True
        }
    
    def _update_performance_metrics(self, new_metrics: Dict[str, float]):
        """ÏÑ±Îä• Î©îÌä∏Î¶≠ ÏóÖÎç∞Ïù¥Ìä∏"""
        for metric_name, value in new_metrics.items():
            if metric_name not in self.performance_metrics:
                self.performance_metrics[metric_name] = value
            else:
                # ÏßÄÏàò Ïù¥Îèô ÌèâÍ∑†ÏúºÎ°ú ÏóÖÎç∞Ïù¥Ìä∏
                alpha = 0.3  # ÌïôÏäµÎ•†
                self.performance_metrics[metric_name] = (
                    alpha * value + (1 - alpha) * self.performance_metrics[metric_name]
                )
    
    async def get_collaboration_analytics(self) -> Dict[str, Any]:
        """ÌòëÏóÖ Î∂ÑÏÑù Ï†ïÎ≥¥"""
        rules = await self.rule_registry.load_rules()
        conflict_analytics = await self.conflict_resolution.get_conflict_analytics()
        
        analytics = {
            "rules_summary": {
                "total_rules": len(rules),
                "active_rules": len([r for r in rules.values() if r.is_active]),
                "learned_rules": len([r for r in rules.values() if r.learned_pattern]),
                "average_success_rate": statistics.mean([r.success_rate for r in rules.values() if r.usage_count > 0]) if rules else 0.0
            },
            "collaboration_summary": {
                "active_collaborations": len(self.active_collaborations),
                "total_patterns_learned": len(self.pattern_learning.learned_patterns),
                "pattern_learning_threshold": self.pattern_learning.learning_threshold
            },
            "conflict_analytics": conflict_analytics,
            "performance_metrics": self.performance_metrics,
            "optimization_history": len(self.workflow_optimizer.optimization_history)
        }
        
        return analytics
    
    async def close(self):
        """Î¶¨ÏÜåÏä§ Ï†ïÎ¶¨"""
        # ÌôúÏÑ± ÌòëÏóÖ Í∞ïÏ†ú ÏôÑÎ£å
        for collaboration_id in list(self.active_collaborations.keys()):
            await self.complete_collaboration(collaboration_id, success=False)
        
        logger.info("üîö Collaboration Rules Engine Ï¢ÖÎ£å")

# Ï†ÑÏó≠ Collaboration Rules Engine Ïù∏Ïä§ÌÑ¥Ïä§
_collaboration_rules_engine = None

def get_collaboration_rules_engine() -> CollaborationRulesEngine:
    """Collaboration Rules Engine Ïù∏Ïä§ÌÑ¥Ïä§ Î∞òÌôò (Ïã±Í∏ÄÌÜ§ Ìå®ÌÑ¥)"""
    global _collaboration_rules_engine
    if _collaboration_rules_engine is None:
        _collaboration_rules_engine = CollaborationRulesEngine()
    return _collaboration_rules_engine

async def initialize_collaboration_rules_engine():
    """Collaboration Rules Engine Ï¥àÍ∏∞Ìôî (Ìé∏Ïùò Ìï®Ïàò)"""
    engine = get_collaboration_rules_engine()
    return await engine.initialize()

async def start_collaboration(collaboration_id: str, context: Dict[str, Any], agents: List[str]):
    """ÌòëÏóÖ ÏãúÏûë (Ìé∏Ïùò Ìï®Ïàò)"""
    engine = get_collaboration_rules_engine()
    return await engine.start_collaboration(collaboration_id, context, agents) 