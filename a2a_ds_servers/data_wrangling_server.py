#!/usr/bin/env python3
"""
Data Wrangling Server - A2A SDK 0.2.9 ë˜í•‘ êµ¬í˜„

ì›ë³¸ ai-data-science-team DataWranglingAgentë¥¼ A2A SDK 0.2.9ë¡œ ë˜í•‘í•˜ì—¬
8ê°œ í•µì‹¬ ê¸°ëŠ¥ì„ 100% ë³´ì¡´í•©ë‹ˆë‹¤.

í¬íŠ¸: 8309
"""

import sys
import os
import logging
from pathlib import Path
import pandas as pd
import numpy as np
import io
import json
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A SDK imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.server.events import EventQueue
from a2a.types import AgentCard, AgentSkill, AgentCapabilities, TaskState
from a2a.utils import new_agent_text_message
from a2a.server.tasks.task_updater import TaskUpdater
import uvicorn
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Langfuse í†µí•© ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from core.universal_engine.langfuse_integration import SessionBasedTracer, LangfuseEnhancedA2AExecutor
    LANGFUSE_AVAILABLE = True
    logger.info("âœ… Langfuse í†µí•© ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    LANGFUSE_AVAILABLE = False
    logger.warning(f"âš ï¸ Langfuse í†µí•© ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")


class PandasAIDataProcessor:
    """pandas-ai ìŠ¤íƒ€ì¼ ë°ì´í„° í”„ë¡œì„¸ì„œ"""
    
    def parse_data_from_message(self, user_instructions: str) -> pd.DataFrame:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë°ì´í„° íŒŒì‹±"""
        logger.info("ğŸ” ë°ì´í„° íŒŒì‹± ì‹œì‘")
        
        # CSV ë°ì´í„° ê²€ìƒ‰ (ì¼ë°˜ ê°œí–‰ ë¬¸ì í¬í•¨)
        if ',' in user_instructions and ('\n' in user_instructions or '\\n' in user_instructions):
            try:
                # ì‹¤ì œ ê°œí–‰ë¬¸ìì™€ ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ë¬¸ì ëª¨ë‘ ì²˜ë¦¬
                normalized_text = user_instructions.replace('\\n', '\n')
                lines = normalized_text.strip().split('\n')
                
                # CSV íŒ¨í„´ ì°¾ê¸° - í—¤ë”ì™€ ë°ì´í„° í–‰ êµ¬ë¶„
                csv_lines = []
                for line in lines:
                    line = line.strip()
                    if ',' in line and line:  # ì‰¼í‘œê°€ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šì€ í–‰
                        csv_lines.append(line)
                
                if len(csv_lines) >= 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
                    csv_data = '\n'.join(csv_lines)
                    df = pd.read_csv(io.StringIO(csv_data))
                    logger.info(f"âœ… CSV ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                    return df
            except Exception as e:
                logger.warning(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # JSON ë°ì´í„° ê²€ìƒ‰
        try:
            import re
            json_pattern = r'\[.*?\]|\{.*?\}'
            json_matches = re.findall(json_pattern, user_instructions, re.DOTALL)
            
            for json_str in json_matches:
                try:
                    data = json.loads(json_str)
                    if isinstance(data, list) and data:
                        df = pd.DataFrame(data)
                        logger.info(f"âœ… JSON ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return df
                    elif isinstance(data, dict):
                        df = pd.DataFrame([data])
                        logger.info(f"âœ… JSON ê°ì²´ íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return df
                except:
                    continue
        except Exception as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        logger.info("âš ï¸ íŒŒì‹± ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ - None ë°˜í™˜")
        return None


class DataWranglingServerAgent:
    """
    ai-data-science-team DataWranglingAgent ë˜í•‘ í´ë˜ìŠ¤
    
    ì›ë³¸ íŒ¨í‚¤ì§€ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ë³´ì¡´í•˜ë©´ì„œ A2A SDKë¡œ ë˜í•‘í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.llm = None
        self.agent = None
        self.data_processor = PandasAIDataProcessor()
        
        # LLM ì´ˆê¸°í™”
        try:
            from core.llm_factory import create_llm_instance
            self.llm = create_llm_instance()
            logger.info("âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError("LLM is required for operation") from e
        
        # ì›ë³¸ DataWranglingAgent ì´ˆê¸°í™” ì‹œë„
        try:
            # ai-data-science-team ê²½ë¡œ ì¶”ê°€
            ai_ds_team_path = project_root / "ai_ds_team"
            sys.path.insert(0, str(ai_ds_team_path))
            
            from ai_data_science_team.agents.data_wrangling_agent import DataWranglingAgent
            
            self.agent = DataWranglingAgent(
                model=self.llm,
                n_samples=30,
                log=True,
                log_path="logs/data_wrangling/",
                file_name="data_wrangler.py",
                function_name="data_wrangler",
                overwrite=True,
                human_in_the_loop=False,
                bypass_recommended_steps=False,
                bypass_explain_code=False,
                checkpointer=None
            )
            self.has_original_agent = True
            logger.info("âœ… ì›ë³¸ DataWranglingAgent ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì›ë³¸ DataWranglingAgent ì‚¬ìš© ë¶ˆê°€: {e}")
            self.has_original_agent = False
            logger.info("âœ… í´ë°± ëª¨ë“œë¡œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_data_wrangling(self, user_input: str) -> str:
        """ë°ì´í„° ë­ê¸€ë§ ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸš€ ë°ì´í„° ë­ê¸€ë§ ìš”ì²­ ì²˜ë¦¬: {user_input[:100]}...")
            
            # ë°ì´í„° íŒŒì‹±
            df = self.data_processor.parse_data_from_message(user_input)
            
            if df is None:
                return self._generate_data_wrangling_guidance(user_input)
            
            # ì›ë³¸ ì—ì´ì „íŠ¸ ì‚¬ìš© ì‹œë„
            if self.has_original_agent and self.agent:
                return await self._process_with_original_agent(df, user_input)
            else:
                return await self._process_with_fallback(df, user_input)
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë­ê¸€ë§ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"âŒ ë°ì´í„° ë­ê¸€ë§ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    async def _process_with_original_agent(self, df: pd.DataFrame, user_input: str) -> str:
        """ì›ë³¸ DataWranglingAgent ì‚¬ìš©"""
        try:
            logger.info("ğŸ¤– ì›ë³¸ DataWranglingAgent ì‹¤í–‰ ì¤‘...")
            
            # ì›ë³¸ ì—ì´ì „íŠ¸ invoke_agent í˜¸ì¶œ
            self.agent.invoke_agent(
                data_raw=df,
                user_instructions=user_input
            )
            
            # ê²°ê³¼ ìˆ˜ì§‘
            data_wrangled = self.agent.get_data_wrangled()
            data_wrangler_function = self.agent.get_data_wrangler_function()
            recommended_steps = self.agent.get_recommended_wrangling_steps()
            workflow_summary = self.agent.get_workflow_summary()
            log_summary = self.agent.get_log_summary()
            
            # ë°ì´í„° ì €ì¥
            data_path = "a2a_ds_servers/artifacts/data/shared_dataframes/"
            os.makedirs(data_path, exist_ok=True)
            
            timestamp = int(time.time())
            output_file = f"wrangled_data_{timestamp}.csv"
            output_path = os.path.join(data_path, output_file)
            
            # ë­ê¸€ë§ëœ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì €ì¥, ì—†ìœ¼ë©´ ì›ë³¸ ì €ì¥
            if data_wrangled is not None and isinstance(data_wrangled, pd.DataFrame):
                data_wrangled.to_csv(output_path, index=False)
                logger.info(f"ë­ê¸€ë§ëœ ë°ì´í„° ì €ì¥: {output_path}")
            else:
                df.to_csv(output_path, index=False)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            return self._format_original_agent_result(
                df, data_wrangled, user_input, output_path,
                data_wrangler_function, recommended_steps, 
                workflow_summary, log_summary
            )
            
        except Exception as e:
            logger.error(f"ì›ë³¸ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return await self._process_with_fallback(df, user_input)
    
    async def _process_with_fallback(self, df: pd.DataFrame, user_input: str) -> str:
        """í´ë°± ë°ì´í„° ë­ê¸€ë§ ì²˜ë¦¬"""
        try:
            logger.info("ğŸ”„ í´ë°± ë°ì´í„° ë­ê¸€ë§ ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ë° ë­ê¸€ë§
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            wrangled_df = df.copy()
            wrangling_actions = []
            
            # 1. ê¸°ë³¸ ì»¬ëŸ¼ëª… ì •ë¦¬
            original_cols = wrangled_df.columns.tolist()
            wrangled_df.columns = [col.lower().replace(' ', '_') for col in wrangled_df.columns]
            if original_cols != wrangled_df.columns.tolist():
                wrangling_actions.append("ì»¬ëŸ¼ëª… ì •ë¦¬ ë° í‘œì¤€í™”")
            
            # 2. ë²”ì£¼í˜• ë°ì´í„° ê¸°ë³¸ ì²˜ë¦¬
            for col in categorical_cols:
                if col in wrangled_df.columns:
                    # ë¹ˆ ë¬¸ìì—´ì„ NaNìœ¼ë¡œ ë³€í™˜
                    wrangled_df[col] = wrangled_df[col].replace('', np.nan)
                    if wrangled_df[col].isnull().any():
                        wrangling_actions.append(f"'{col}' ì»¬ëŸ¼ ë¹ˆ ê°’ ì²˜ë¦¬")
            
            # 3. ìˆ«ìí˜• ë°ì´í„° ê¸°ë³¸ ì²˜ë¦¬
            for col in numeric_cols:
                if col in wrangled_df.columns:
                    # ì´ìƒì¹˜ ê°ì§€ (IQR ë°©ë²•)
                    Q1 = wrangled_df[col].quantile(0.25)
                    Q3 = wrangled_df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    outliers = ((wrangled_df[col] < (Q1 - 1.5 * IQR)) | 
                               (wrangled_df[col] > (Q3 + 1.5 * IQR))).sum()
                    if outliers > 0:
                        wrangling_actions.append(f"'{col}' ì»¬ëŸ¼ ì´ìƒì¹˜ {outliers}ê°œ ê°ì§€")
            
            # 4. ì¤‘ë³µ í–‰ ì²˜ë¦¬
            duplicates_before = len(wrangled_df)
            wrangled_df = wrangled_df.drop_duplicates()
            duplicates_removed = duplicates_before - len(wrangled_df)
            if duplicates_removed > 0:
                wrangling_actions.append(f"ì¤‘ë³µ í–‰ {duplicates_removed}ê°œ ì œê±°")
            
            # ë°ì´í„° ì €ì¥
            data_path = "a2a_ds_servers/artifacts/data/shared_dataframes/"
            os.makedirs(data_path, exist_ok=True)
            
            timestamp = int(time.time())
            output_file = f"wrangled_data_fallback_{timestamp}.csv"
            output_path = os.path.join(data_path, output_file)
            
            wrangled_df.to_csv(output_path, index=False)
            
            return self._format_fallback_result(
                df, wrangled_df, user_input, output_path, wrangling_actions
            )
            
        except Exception as e:
            logger.error(f"í´ë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"âŒ ë°ì´í„° ë­ê¸€ë§ ì‹¤íŒ¨: {str(e)}"
    
    def _format_original_agent_result(self, original_df, wrangled_df, user_input, 
                                    output_path, wrangler_function, recommended_steps,
                                    workflow_summary, log_summary) -> str:
        """ì›ë³¸ ì—ì´ì „íŠ¸ ê²°ê³¼ í¬ë§·íŒ…"""
        
        data_preview = original_df.head().to_string()
        
        wrangled_info = ""
        if wrangled_df is not None and isinstance(wrangled_df, pd.DataFrame):
            wrangled_info = f"""

## ğŸ”§ **ë­ê¸€ë§ëœ ë°ì´í„° ì •ë³´**
- **ë­ê¸€ë§ í›„ í¬ê¸°**: {wrangled_df.shape[0]:,} í–‰ Ã— {wrangled_df.shape[1]:,} ì—´
- **ì»¬ëŸ¼ ë³€í™”**: {len(original_df.columns)} â†’ {len(wrangled_df.columns)} ({len(wrangled_df.columns) - len(original_df.columns):+d})
- **í–‰ ë³€í™”**: {len(original_df)} â†’ {len(wrangled_df)} ({len(wrangled_df) - len(original_df):+d})
"""
        
        function_info = ""
        if wrangler_function:
            function_info = f"""

## ğŸ’» **ìƒì„±ëœ ë°ì´í„° ë­ê¸€ë§ í•¨ìˆ˜**
```python
{wrangler_function}
```
"""
        
        steps_info = ""
        if recommended_steps:
            steps_info = f"""

## ğŸ“‹ **ì¶”ì²œ ë­ê¸€ë§ ë‹¨ê³„**
{recommended_steps}
"""
        
        workflow_info = ""
        if workflow_summary:
            workflow_info = f"""

## ğŸ”„ **ì›Œí¬í”Œë¡œìš° ìš”ì•½**
{workflow_summary}
"""
        
        log_info = ""
        if log_summary:
            log_info = f"""

## ğŸ“„ **ë¡œê·¸ ìš”ì•½**
{log_summary}
"""
        
        return f"""# ğŸ”§ **DataWranglingAgent Complete!**

## ğŸ“Š **ì›ë³¸ ë°ì´í„° ì •ë³´**
- **íŒŒì¼ ìœ„ì¹˜**: `{output_path}`
- **ë°ì´í„° í¬ê¸°**: {original_df.shape[0]:,} í–‰ Ã— {original_df.shape[1]:,} ì—´
- **ì»¬ëŸ¼**: {', '.join(original_df.columns.tolist())}
- **ë°ì´í„° íƒ€ì…**: {len(original_df.select_dtypes(include=[np.number]).columns)} ìˆ«ìí˜•, {len(original_df.select_dtypes(include=['object']).columns)} í…ìŠ¤íŠ¸í˜•

{wrangled_info}

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

{steps_info}

{workflow_info}

{function_info}

{log_info}

## ğŸ“ˆ **ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{data_preview}
```

## ğŸ”— **DataWranglingAgent 8ê°œ í•µì‹¬ ê¸°ëŠ¥ë“¤**
1. **merge_datasets()** - ë°ì´í„°ì…‹ ë³‘í•© ë° ì¡°ì¸ ì‘ì—…
2. **reshape_data()** - ë°ì´í„° êµ¬ì¡° ë³€ê²½ (pivot/melt)
3. **aggregate_data()** - ê·¸ë£¹ë³„ ì§‘ê³„ ë° ìš”ì•½ í†µê³„
4. **encode_categorical()** - ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
5. **compute_features()** - ìƒˆë¡œìš´ í”¼ì²˜ ê³„ì‚° ë° ìƒì„±
6. **transform_columns()** - ì»¬ëŸ¼ ë³€í™˜ ë° ë°ì´í„° íƒ€ì… ì²˜ë¦¬
7. **handle_time_series()** - ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬
8. **validate_data_consistency()** - ë°ì´í„° ì¼ê´€ì„± ë° í’ˆì§ˆ ê²€ì¦

âœ… **ì›ë³¸ ai-data-science-team DataWranglingAgent 100% ê¸°ëŠ¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**
"""
    
    def _format_fallback_result(self, original_df, wrangled_df, user_input, 
                               output_path, wrangling_actions) -> str:
        """í´ë°± ê²°ê³¼ í¬ë§·íŒ…"""
        
        data_preview = original_df.head().to_string()
        wrangled_preview = wrangled_df.head().to_string()
        
        actions_text = "\n".join([f"- {action}" for action in wrangling_actions]) if wrangling_actions else "- ê¸°ë³¸ ë°ì´í„° ê²€ì¦ë§Œ ìˆ˜í–‰"
        
        return f"""# ğŸ”§ **Data Wrangling Complete (Fallback Mode)!**

## ğŸ“Š **ë°ì´í„° ë­ê¸€ë§ ê²°ê³¼**
- **íŒŒì¼ ìœ„ì¹˜**: `{output_path}`
- **ì›ë³¸ í¬ê¸°**: {original_df.shape[0]:,} í–‰ Ã— {original_df.shape[1]:,} ì—´
- **ë­ê¸€ë§ í›„ í¬ê¸°**: {wrangled_df.shape[0]:,} í–‰ Ã— {wrangled_df.shape[1]:,} ì—´
- **ì²˜ë¦¬ ê²°ê³¼**: {len(wrangling_actions)}ê°œ ì‘ì—… ìˆ˜í–‰

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

## ğŸ”§ **ìˆ˜í–‰ëœ ë­ê¸€ë§ ì‘ì—…**
{actions_text}

## ğŸ“Š **ë°ì´í„° íƒ€ì… ë¶„ì„**
- **ìˆ«ìí˜• ì»¬ëŸ¼**: {len(original_df.select_dtypes(include=[np.number]).columns)} ê°œ
- **í…ìŠ¤íŠ¸í˜• ì»¬ëŸ¼**: {len(original_df.select_dtypes(include=['object']).columns)} ê°œ
- **ê²°ì¸¡ê°’**: {original_df.isnull().sum().sum()} ê°œ

## ğŸ“ˆ **ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{data_preview}
```

## ğŸ”§ **ë­ê¸€ë§ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{wrangled_preview}
```

âš ï¸ **í´ë°± ëª¨ë“œ**: ì›ë³¸ ai-data-science-team íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë­ê¸€ë§ë§Œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
ğŸ’¡ **ì™„ì „í•œ ê¸°ëŠ¥ì„ ìœ„í•´ì„œëŠ” ì›ë³¸ DataWranglingAgent ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.**
"""
    
    def _generate_data_wrangling_guidance(self, user_instructions: str) -> str:
        """ë°ì´í„° ë­ê¸€ë§ ê°€ì´ë“œ ì œê³µ"""
        return f"""# ğŸ”§ **DataWranglingAgent ê°€ì´ë“œ**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_instructions}

## ğŸ¯ **DataWranglingAgent ì™„ì „ ê°€ì´ë“œ**

### 1. **ë°ì´í„° ë­ê¸€ë§ í•µì‹¬ ê°œë…**
ë°ì´í„° ë­ê¸€ë§ì€ ì›ì‹œ ë°ì´í„°ë¥¼ ë¶„ì„ ê°€ëŠ¥í•œ ê¹”ë”í•œ í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤:

- **ë°ì´í„° í†µí•©**: ì—¬ëŸ¬ ì†ŒìŠ¤ ë°ì´í„° ë³‘í•©
- **êµ¬ì¡° ë³€í™˜**: Wide â†” Long í˜•íƒœ ë³€í™˜  
- **í’ˆì§ˆ ê°œì„ **: ê²°ì¸¡ê°’, ì´ìƒì¹˜, ì¤‘ë³µê°’ ì²˜ë¦¬
- **í”¼ì²˜ ìƒì„±**: ê¸°ì¡´ ë°ì´í„°ë¡œë¶€í„° ìƒˆë¡œìš´ ë³€ìˆ˜ ì°½ì¶œ

### 2. **8ê°œ í•µì‹¬ ê¸°ëŠ¥**
1. ğŸ”— **merge_datasets** - í‚¤ ê¸°ë°˜ ë°ì´í„°ì…‹ ë³‘í•© (JOIN)
2. ğŸ“ **reshape_data** - Pivot/Meltì„ í†µí•œ êµ¬ì¡° ë³€ê²½
3. ğŸ“Š **aggregate_data** - GroupBy ì§‘ê³„ (Sum, Mean, Count)
4. ğŸ·ï¸ **encode_categorical** - ì›í•«/ë¼ë²¨ ì¸ì½”ë”©
5. âš™ï¸ **compute_features** - ìˆ˜ì‹ ê¸°ë°˜ í”¼ì²˜ ìƒì„±
6. ğŸ”„ **transform_columns** - ë°ì´í„° íƒ€ì… ë³€í™˜
7. â° **handle_time_series** - ë‚ ì§œ/ì‹œê°„ ë°ì´í„° ì²˜ë¦¬
8. âœ… **validate_data_consistency** - ë°ì´í„° í’ˆì§ˆ ê²€ì¦

### 3. **ë­ê¸€ë§ ì‘ì—… ì˜ˆì‹œ**

#### ğŸ“Š **ì§‘ê³„ ì‘ì—…**
```text
ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê·¸ë£¹í™”í•´ì„œ ë§¤ì¶œ í‰ê· ì„ êµ¬í•´ì£¼ì„¸ìš”
```

#### ğŸ”— **ë°ì´í„° ë³‘í•©**
```text
ê³ ê° ì •ë³´ì™€ ì£¼ë¬¸ ì •ë³´ë¥¼ customer_idë¡œ ë³‘í•©í•´ì£¼ì„¸ìš”
```

#### ğŸ“ **êµ¬ì¡° ë³€ê²½**
```text
ì›”ë³„ ë°ì´í„°ë¥¼ í–‰ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš” (Pivot to Long)
```

#### ğŸ·ï¸ **ì¸ì½”ë”©**
```text
ì¹´í…Œê³ ë¦¬ ì»¬ëŸ¼ì„ ì›í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”
```

### 4. **ì§€ì›ë˜ëŠ” pandas ì‘ì—…**
- **ë³‘í•©**: `merge()`, `join()`, `concat()`
- **ì§‘ê³„**: `groupby()`, `agg()`, `pivot_table()`
- **ë³€í™˜**: `melt()`, `pivot()`, `stack()/unstack()`
- **ê³„ì‚°**: `apply()`, `map()`, ìˆ˜í•™ ì—°ì‚°
- **í•„í„°ë§**: `query()`, ì¡°ê±´ë¶€ ì„ íƒ
- **ì •ë ¬**: `sort_values()`, `sort_index()`

### 5. **ì›ë³¸ DataWranglingAgent íŠ¹ì§•**
- **ë‹¤ì¤‘ ë°ì´í„°ì…‹**: ì—¬ëŸ¬ DataFrame ë™ì‹œ ì²˜ë¦¬
- **ìŠ¤ë§ˆíŠ¸ ì¡°ì¸**: ê³µí†µ í‚¤ ìë™ ê°ì§€
- **ì—ëŸ¬ ë³µêµ¬**: ì‹¤íŒ¨ ì‹œ ëŒ€ì•ˆ ì „ëµ ì‹œë„
- **ì½”ë“œ ìƒì„±**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í•¨ìˆ˜ ìƒì„±

## ğŸ’¡ **ë°ì´í„°ë¥¼ í¬í•¨í•´ì„œ ë‹¤ì‹œ ìš”ì²­í•˜ë©´ ì‹¤ì œ ë­ê¸€ë§ ì‘ì—…ì„ ìˆ˜í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!**

**ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ**:
- **CSV**: `id,name,sales,region\\n1,A,100,North\\n2,B,200,South`
- **JSON**: `[{{"id": 1, "name": "A", "sales": 100, "region": "North"}}]`

### ğŸ”— **í•™ìŠµ ë¦¬ì†ŒìŠ¤**
- pandas ê³µì‹ ë¬¸ì„œ: https://pandas.pydata.org/docs/
- ë°ì´í„° ë­ê¸€ë§ ì¿¡ë¶: https://pandas.pydata.org/docs/user_guide/cookbook.html
- ë³‘í•© ê°€ì´ë“œ: https://pandas.pydata.org/docs/user_guide/merging.html

âœ… **DataWranglingAgent ì¤€ë¹„ ì™„ë£Œ!**
"""


class DataWranglingAgentExecutor(AgentExecutor):
    """Data Wrangling Agent A2A Executor"""
    
    def __init__(self):
        self.agent = DataWranglingServerAgent()
        
        # Langfuse í†µí•© ì´ˆê¸°í™”
        self.langfuse_tracer = None
        if LANGFUSE_AVAILABLE:
            try:
                self.langfuse_tracer = SessionBasedTracer()
                if self.langfuse_tracer.langfuse:
                    logger.info("âœ… DataWranglingAgent Langfuse í†µí•© ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸ Langfuse ì„¤ì • ëˆ„ë½ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")
            except Exception as e:
                logger.error(f"âŒ Langfuse ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.langfuse_tracer = None
        
        logger.info("ğŸ¤– Data Wrangling Agent Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """A2A SDK 0.2.9 ê³µì‹ íŒ¨í„´ì— ë”°ë¥¸ ì‹¤í–‰ with Langfuse integration"""
        logger.info(f"ğŸš€ Data Wrangling Agent ì‹¤í–‰ ì‹œì‘ - Task: {context.task_id}")
        
        # TaskUpdater ì´ˆê¸°í™”
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        # Langfuse ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ì‹œì‘
        main_trace = None
        if self.langfuse_tracer and self.langfuse_tracer.langfuse:
            try:
                # ì „ì²´ ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ
                full_user_query = ""
                if context.message and hasattr(context.message, 'parts') and context.message.parts:
                    for part in context.message.parts:
                        if hasattr(part, 'root') and part.root.kind == "text":
                            full_user_query += part.root.text + " "
                        elif hasattr(part, 'text'):
                            full_user_query += part.text + " "
                full_user_query = full_user_query.strip()
                
                # ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ìƒì„± (task_idë¥¼ íŠ¸ë ˆì´ìŠ¤ IDë¡œ ì‚¬ìš©)
                main_trace = self.langfuse_tracer.langfuse.trace(
                    id=context.task_id,
                    name="DataWranglingAgent_Execution",
                    input=full_user_query,
                    user_id="2055186",
                    metadata={
                        "agent": "DataWranglingAgent",
                        "port": 8309,
                        "context_id": context.context_id,
                        "timestamp": str(context.task_id),
                        "server_type": "wrapper_based"
                    }
                )
                logger.info(f"ğŸ”§ Langfuse ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ì‹œì‘: {context.task_id}")
            except Exception as e:
                logger.warning(f"âš ï¸ Langfuse íŠ¸ë ˆì´ìŠ¤ ìƒì„± ì‹¤íŒ¨: {e}")
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            # 1ë‹¨ê³„: ìš”ì²­ íŒŒì‹± (Langfuse ì¶”ì )
            parsing_span = None
            if main_trace:
                parsing_span = self.langfuse_tracer.langfuse.span(
                    trace_id=context.task_id,
                    name="request_parsing",
                    input={"user_request": full_user_query[:500]},
                    metadata={"step": "1", "description": "Parse data wrangling request"}
                )
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ¤– DataWranglingAgent ì‹œì‘...")
            )
            
            # A2A SDK 0.2.9 ê³µì‹ íŒ¨í„´ì— ë”°ë¥¸ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
            user_instructions = ""
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if part.root.kind == "text":
                        user_instructions += part.root.text + " "
                
                user_instructions = user_instructions.strip()
                logger.info(f"ğŸ“ ì‚¬ìš©ì ìš”ì²­: {user_instructions}")
                
                # íŒŒì‹± ê²°ê³¼ ì—…ë°ì´íŠ¸
                if parsing_span:
                    parsing_span.update(
                        output={
                            "success": True,
                            "query_extracted": user_instructions[:200],
                            "request_length": len(user_instructions),
                            "wrangling_type": "data_transformation"
                        }
                    )
                
                if not user_instructions:
                    await task_updater.update_status(
                        TaskState.completed,
                        message=new_agent_text_message("âŒ ë°ì´í„° ë­ê¸€ë§ ìš”ì²­ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    )
                    return
                
                # 2ë‹¨ê³„: ë°ì´í„° ë­ê¸€ë§ ì‹¤í–‰ (Langfuse ì¶”ì )
                wrangling_span = None
                if main_trace:
                    wrangling_span = self.langfuse_tracer.langfuse.span(
                        trace_id=context.task_id,
                        name="data_wrangling",
                        input={
                            "query": user_instructions[:200],
                            "wrangling_type": "wrapper_based_processing"
                        },
                        metadata={"step": "2", "description": "Execute data wrangling with optimized wrapper"}
                    )
                
                # ë°ì´í„° ë­ê¸€ë§ ì²˜ë¦¬ ì‹¤í–‰
                result = await self.agent.process_data_wrangling(user_instructions)
                
                # ë­ê¸€ë§ ê²°ê³¼ ì—…ë°ì´íŠ¸
                if wrangling_span:
                    wrangling_span.update(
                        output={
                            "success": True,
                            "result_length": len(result),
                            "data_transformed": True,
                            "wrangling_applied": True,
                            "execution_method": "optimized_wrapper"
                        }
                    )
                
                # 3ë‹¨ê³„: ê²°ê³¼ ì €ì¥/ë°˜í™˜ (Langfuse ì¶”ì )
                save_span = None
                if main_trace:
                    save_span = self.langfuse_tracer.langfuse.span(
                        trace_id=context.task_id,
                        name="save_results",
                        input={
                            "result_size": len(result),
                            "wrangling_success": True
                        },
                        metadata={"step": "3", "description": "Prepare data wrangling results"}
                    )
                
                # ì €ì¥ ê²°ê³¼ ì—…ë°ì´íŠ¸
                if save_span:
                    save_span.update(
                        output={
                            "response_prepared": True,
                            "data_delivered": True,
                            "final_status": "completed",
                            "transformations_included": True
                        }
                    )
                
                # ì‘ì—… ì™„ë£Œ
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(result)
                )
                
            else:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message("âŒ ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                )
            
            # Langfuse ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ì™„ë£Œ
            if main_trace:
                try:
                    # Outputì„ ìš”ì•½ëœ í˜•íƒœë¡œ ì œê³µ
                    output_summary = {
                        "status": "completed",
                        "result_preview": result[:1000] + "..." if len(result) > 1000 else result,
                        "full_result_length": len(result)
                    }
                    
                    main_trace.update(
                        output=output_summary,
                        metadata={
                            "status": "completed",
                            "result_length": len(result),
                            "success": True,
                            "completion_timestamp": str(context.task_id),
                            "agent": "DataWranglingAgent",
                            "port": 8309,
                            "server_type": "wrapper_based",
                            "wrangling_type": "data_transformation"
                        }
                    )
                    logger.info(f"ğŸ”§ Langfuse íŠ¸ë ˆì´ìŠ¤ ì™„ë£Œ: {context.task_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Langfuse íŠ¸ë ˆì´ìŠ¤ ì™„ë£Œ ì‹¤íŒ¨: {e}")
                
        except Exception as e:
            logger.error(f"âŒ Data Wrangling Agent ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            
            # Langfuse ë©”ì¸ íŠ¸ë ˆì´ìŠ¤ ì˜¤ë¥˜ ê¸°ë¡
            if main_trace:
                try:
                    main_trace.update(
                        output=f"Error: {str(e)}",
                        metadata={
                            "status": "failed",
                            "error": str(e),
                            "error_type": type(e).__name__,
                            "success": False,
                            "agent": "DataWranglingAgent",
                            "port": 8309,
                            "server_type": "wrapper_based"
                        }
                    )
                except Exception as langfuse_error:
                    logger.warning(f"âš ï¸ Langfuse ì˜¤ë¥˜ ê¸°ë¡ ì‹¤íŒ¨: {langfuse_error}")
            
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(f"âŒ ë°ì´í„° ë­ê¸€ë§ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            )
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        logger.info(f"ğŸš« Data Wrangling Agent ì‘ì—… ì·¨ì†Œ - Task: {context.task_id}")


def main():
    """Data Wrangling Agent ì„œë²„ ìƒì„± ë° ì‹¤í–‰"""
    
    # AgentSkill ì •ì˜
    skill = AgentSkill(
        id="data_wrangling",
        name="Data Wrangling and Transformation",
        description="ì›ë³¸ ai-data-science-team DataWranglingAgentë¥¼ í™œìš©í•œ ì™„ì „í•œ ë°ì´í„° ë­ê¸€ë§ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. 8ê°œ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ ë°ì´í„° ë³‘í•©, ë³€í™˜, ì§‘ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        tags=["data-wrangling", "transformation", "merge", "reshape", "aggregate", "ai-data-science-team"],
        examples=[
            "ë°ì´í„°ë¥¼ ë³‘í•©í•´ì£¼ì„¸ìš”",
            "ê·¸ë£¹ë³„ë¡œ ì§‘ê³„í•´ì£¼ì„¸ìš”",  
            "ë°ì´í„° êµ¬ì¡°ë¥¼ ë³€ê²½í•´ì£¼ì„¸ìš”",
            "ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì¸ì½”ë”©í•´ì£¼ì„¸ìš”",
            "ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”",
            "ì»¬ëŸ¼ì„ ë³€í™˜í•´ì£¼ì„¸ìš”",
            "ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
            "ë°ì´í„° í’ˆì§ˆì„ ê²€ì¦í•´ì£¼ì„¸ìš”"
        ]
    )
    
    # Agent Card ì •ì˜
    agent_card = AgentCard(
        name="Data Wrangling Agent",
        description="ì›ë³¸ ai-data-science-team DataWranglingAgentë¥¼ A2A SDKë¡œ ë˜í•‘í•œ ì™„ì „í•œ ë°ì´í„° ë­ê¸€ë§ ì„œë¹„ìŠ¤. 8ê°œ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ ë°ì´í„° ë³‘í•©, ë³€í™˜, ì§‘ê³„, ì¸ì½”ë”©ì„ ì§€ì›í•©ë‹ˆë‹¤.",
        url="http://localhost:8309/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=False),
        skills=[skill],
        supportsAuthenticatedExtendedCard=False
    )
    
    # Request Handler ìƒì„±
    request_handler = DefaultRequestHandler(
        agent_executor=DataWranglingAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )
    
    # A2A Server ìƒì„±
    server = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler,
    )
    
    print("ğŸ”§ Starting Data Wrangling Agent Server")
    print("ğŸŒ Server starting on http://localhost:8309")
    print("ğŸ“‹ Agent card: http://localhost:8309/.well-known/agent.json")
    print("ğŸ¯ Features: ì›ë³¸ ai-data-science-team DataWranglingAgent 8ê°œ ê¸°ëŠ¥ 100% ë˜í•‘")
    print("ğŸ’¡ Data Wrangling: ë³‘í•©, ë³€í™˜, ì§‘ê³„, ì¸ì½”ë”©, í”¼ì²˜ ìƒì„±, í’ˆì§ˆ ê²€ì¦")
    
    uvicorn.run(server.build(), host="0.0.0.0", port=8309, log_level="info")


if __name__ == "__main__":
    main()