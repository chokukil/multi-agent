#!/usr/bin/env python3
"""
PandasAnalyst Server - A2A SDK Complete Implementation with Langfuse Integration

Following the same pattern as SQL Database and MLflow Tools agents:
- AgentExecutor inheritance
- Complete Langfuse integration with 3-stage span structure
- TaskUpdater pattern
- A2A standard server initialization

í¬íŠ¸: 8315
"""

import sys
import os
import logging
from pathlib import Path
import pandas as pd
import numpy as np
import io
import json
import time
from typing import Dict, Any
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai_ds_team"))

# A2A SDK imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.server.events import EventQueue
from a2a.types import AgentCard, AgentSkill, AgentCapabilities, TaskState, TextPart
from a2a.utils import new_agent_text_message
from a2a.server.tasks.task_updater import TaskUpdater
import uvicorn
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Langfuse í†µí•© ëª¨ë“ˆ ì„í¬íŠ¸
try:
    from core.universal_engine.langfuse_integration import SessionBasedTracer, LangfuseEnhancedA2AExecutor
    LANGFUSE_AVAILABLE = True
    logger.info("âœ… Langfuse í†µí•© ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    LANGFUSE_AVAILABLE = False
    logger.warning(f"âš ï¸ Langfuse í†µí•© ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: {e}")


class PandasAIDataProcessor:
    """pandas-ai ìŠ¤íƒ€ì¼ ë°ì´í„° í”„ë¡œì„¸ì„œ"""
    
    def parse_data_from_message(self, user_instructions: str) -> pd.DataFrame:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë°ì´í„° íŒŒì‹±"""
        logger.info("ğŸ” ë°ì´í„° íŒŒì‹± ì‹œì‘")
        
        # CSV ë°ì´í„° ê²€ìƒ‰ (ì¼ë°˜ ê°œí–‰ ë¬¸ì í¬í•¨)
        if ',' in user_instructions and ('\n' in user_instructions or '\\n' in user_instructions):
            try:
                # ì‹¤ì œ ê°œí–‰ë¬¸ìì™€ ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ë¬¸ì ëª¨ë‘ ì²˜ë¦¬
                normalized_text = user_instructions.replace('\\n', '\n')
                lines = normalized_text.strip().split('\n')
                
                # CSV íŒ¨í„´ ì°¾ê¸° - í—¤ë”ì™€ ë°ì´í„° í–‰ êµ¬ë¶„
                csv_lines = []
                for line in lines:
                    line = line.strip()
                    if ',' in line and line:  # ì‰¼í‘œê°€ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šì€ í–‰
                        csv_lines.append(line)
                
                if len(csv_lines) >= 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
                    csv_data = '\n'.join(csv_lines)
                    df = pd.read_csv(io.StringIO(csv_data))
                    logger.info(f"âœ… CSV ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                    return df
            except Exception as e:
                logger.warning(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # JSON ë°ì´í„° ê²€ìƒ‰
        try:
            import re
            json_pattern = r'\[.*?\]|\{.*?\}'
            json_matches = re.findall(json_pattern, user_instructions, re.DOTALL)
            
            for json_str in json_matches:
                try:
                    data = json.loads(json_str)
                    if isinstance(data, list) and data:
                        df = pd.DataFrame(data)
                        logger.info(f"âœ… JSON ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return df
                    elif isinstance(data, dict):
                        df = pd.DataFrame([data])
                        logger.info(f"âœ… JSON ê°ì²´ íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return df
                except:
                    continue
        except Exception as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        logger.info("âš ï¸ íŒŒì‹± ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ")
        return None


class PandasAnalystAgentExecutor(AgentExecutor):
    """
    PandasAnalyst Agent Executor with complete A2A and Langfuse integration
    
    Following the same pattern as SQL Database and MLflow Tools agents:
    - Inherits from AgentExecutor
    - Complete Langfuse integration with 3-stage span structure
    - TaskUpdater pattern for execution management
    """
    
    def __init__(self):
        self.data_processor = PandasAIDataProcessor()
        
        # Initialize Langfuse tracer
        self.langfuse_tracer = None
        if LANGFUSE_AVAILABLE:
            try:
                self.langfuse_tracer = SessionBasedTracer()
                if self.langfuse_tracer.langfuse:
                    logger.info("âœ… PandasAnalyst Langfuse í†µí•© ì™„ë£Œ")
                else:
                    logger.warning("âš ï¸ Langfuse ì„¤ì • ëˆ„ë½ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")
            except Exception as e:
                logger.error(f"âŒ Langfuse ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.langfuse_tracer = None
        
        # Initialize PandasAnalyst wrapper
        try:
            from a2a_ds_servers.base.pandas_analyst_a2a_wrapper import PandasAnalystA2AWrapper
            self.pandas_wrapper = PandasAnalystA2AWrapper()
            logger.info("âœ… PandasAnalyst A2A Wrapper ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ PandasAnalyst A2A Wrapper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.pandas_wrapper = None
        
        logger.info("ğŸ¼ PandasAnalyst AgentExecutor ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("ğŸš€ LLM-First ë™ì  pandas ì½”ë“œ ìƒì„± ì‹œìŠ¤í…œ í™œì„±í™”")
        logger.info("ğŸ”§ 8ê°œ í•µì‹¬ ë°ì´í„° ì¡°ì‘ ê¸°ëŠ¥ ì¤€ë¹„ ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Execute method with TaskUpdater pattern and Langfuse integration"""
        # Initialize TaskUpdater
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            # Submit and start work
            await task_updater.submit()
            await task_updater.start_work()
            
            # Extract user message
            user_message = ""
            for part in context.message.parts:
                if part.root.kind == "text":
                    user_message += part.root.text
            
            logger.info(f"ğŸ“¥ Processing pandas analysis: {user_message[:100]}...")
            
            if not user_message:
                user_message = "Please provide pandas analysis instructions with data."
            
            # Create Langfuse session
            session_id = None
            if self.langfuse_tracer:
                session_id = self.langfuse_tracer.create_session(user_message)
            
            # Process with 3-stage Langfuse span structure
            result = await self._process_with_langfuse_spans(user_message, session_id)
            
            # Complete task with result
            await task_updater.update_status(
                TaskState.completed,
                message=task_updater.new_agent_message(parts=[TextPart(text=result)])
            )
            
        except Exception as e:
            logger.error(f"Error in execute: {e}", exc_info=True)
            # Report error through TaskUpdater
            await task_updater.update_status(
                TaskState.failed,
                message=task_updater.new_agent_message(parts=[TextPart(text=f"Pandas analysis failed: {str(e)}")]))
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Cancel the operation"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.reject()
        logger.info(f"Operation cancelled for context {context.context_id}")
    
    async def _process_with_langfuse_spans(self, user_message: str, session_id: str) -> str:
        """Process pandas analysis with 3-stage Langfuse span structure"""
        start_time = datetime.now()
        
        # Stage 1: Request Parsing
        if self.langfuse_tracer:
            self.langfuse_tracer.add_span(
                name="request_parsing",
                input_data={"user_message": user_message},
                metadata={"stage": "parsing", "agent": "PandasAnalyst"},
                start_time=start_time
            )
        
        # Parse data from message
        df = self.data_processor.parse_data_from_message(user_message)
        
        parsing_end_time = datetime.now()
        if self.langfuse_tracer:
            self.langfuse_tracer.add_span(
                name="request_parsing",
                output_data={
                    "data_found": df is not None,
                    "data_shape": df.shape if df is not None else None
                },
                end_time=parsing_end_time
            )
        
        # Stage 2: Pandas Operations
        operations_start_time = datetime.now()
        if self.langfuse_tracer:
            self.langfuse_tracer.add_span(
                name="pandas_operations",
                input_data={
                    "has_data": df is not None,
                    "data_shape": df.shape if df is not None else None,
                    "user_instructions": user_message
                },
                metadata={"stage": "operations", "agent": "PandasAnalyst"},
                start_time=operations_start_time
            )
        
        # Process pandas analysis
        if df is None or df.empty:
            logger.info("ğŸ“š ë°ì´í„° ì—†ìŒ - Pandas ê°€ì´ë“œ ì œê³µ")
            if self.pandas_wrapper:
                result = self.pandas_wrapper._generate_guidance(user_message)
            else:
                result = self._generate_fallback_guidance(user_message)
        else:
            logger.info(f"ğŸ¼ Processing pandas data: {df.shape}")
            if self.pandas_wrapper:
                wrapped_result = await self.pandas_wrapper.process_request(user_message)
                result = self._format_pandas_result(wrapped_result, df, user_message)
            else:
                result = self._generate_fallback_analysis(df, user_message)
        
        operations_end_time = datetime.now()
        if self.langfuse_tracer:
            self.langfuse_tracer.add_span(
                name="pandas_operations",
                output_data={
                    "result_length": len(result),
                    "processing_time": (operations_end_time - operations_start_time).total_seconds()
                },
                end_time=operations_end_time
            )
        
        # Stage 3: Save Results
        save_start_time = datetime.now()
        if self.langfuse_tracer:
            self.langfuse_tracer.add_span(
                name="save_results",
                input_data={"result_ready": True},
                metadata={"stage": "saving", "agent": "PandasAnalyst"},
                start_time=save_start_time
            )
        
        # Finalize result
        final_result = self._finalize_result(result, user_message, start_time)
        
        save_end_time = datetime.now()
        if self.langfuse_tracer:
            self.langfuse_tracer.add_span(
                name="save_results",
                output_data={
                    "final_result_length": len(final_result),
                    "total_time": (save_end_time - start_time).total_seconds()
                },
                end_time=save_end_time
            )
        
        return final_result
    
    def _format_pandas_result(self, wrapped_result: str, df: pd.DataFrame, user_message: str) -> str:
        """Format the pandas analysis result"""
        return f"""# ğŸ¼ **PandasAnalyst Complete Analysis**

## ğŸ“ **Request**
{user_message}

## ğŸ“Š **Data Information**
- **Shape**: {df.shape[0]:,} rows Ã— {df.shape[1]:,} columns
- **Columns**: {', '.join(df.columns.tolist())}
- **Memory Usage**: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB

## ğŸ§  **Analysis Results**
{wrapped_result}

## ğŸ¯ **Available Functions**
1. **load_data_formats()** - Load various data formats
2. **inspect_data()** - Data structure and quality inspection
3. **select_data()** - Advanced data selection and filtering
4. **manipulate_data()** - Complex data transformation
5. **aggregate_data()** - Grouping and aggregation operations
6. **merge_data()** - Data joining and merging
7. **clean_data()** - Data cleaning and preprocessing
8. **perform_statistical_analysis()** - Statistical analysis

âœ… **PandasAnalyst LLM-First analysis completed successfully!**
"""
    
    def _generate_fallback_guidance(self, user_message: str) -> str:
        """Generate fallback guidance when wrapper is not available"""
        return f"""# ğŸ¼ **PandasAnalyst Guide**

## ğŸ“ **Your Request**
{user_message}

## ğŸ§  **PandasAnalyst Capabilities**

I'm a specialized pandas data analysis agent that can help you with:

### ğŸ“Š **Data Operations**
- Load data from CSV, JSON, Excel formats
- Inspect data structure and quality
- Filter and select data with complex conditions
- Transform and manipulate data
- Perform grouping and aggregation
- Merge and join datasets
- Clean and preprocess data
- Generate statistical analysis

### ğŸ’¡ **Usage Examples**
```text
With CSV data:
name,age,city
John,25,Seoul
Jane,30,Busan

Ask me to:
- "Analyze the age distribution"
- "Filter data for age > 25"
- "Group by city and calculate average age"
- "Clean and optimize the data"
```

### ğŸš€ **How to Use**
1. Provide your data in CSV or JSON format
2. Describe what analysis you want
3. I'll generate custom pandas code and execute it
4. Get comprehensive results and insights

âœ… **Ready to analyze your data with advanced pandas operations!**
"""
    
    def _generate_fallback_analysis(self, df: pd.DataFrame, user_message: str) -> str:
        """Generate fallback analysis when wrapper is not available"""
        try:
            # Basic analysis
            basic_info = {
                "shape": df.shape,
                "columns": df.columns.tolist(),
                "dtypes": df.dtypes.to_dict(),
                "missing_values": df.isnull().sum().to_dict(),
                "memory_usage_mb": df.memory_usage(deep=True).sum() / 1024 / 1024
            }
            
            # Generate summary
            numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            
            return f"""# ğŸ¼ **PandasAnalyst Fallback Analysis**

## ğŸ“ **Request**
{user_message}

## ğŸ“Š **Data Overview**
- **Shape**: {basic_info['shape'][0]:,} rows Ã— {basic_info['shape'][1]:,} columns
- **Memory Usage**: {basic_info['memory_usage_mb']:.2f} MB
- **Numeric Columns**: {len(numeric_cols)} ({', '.join(numeric_cols[:5])}{'...' if len(numeric_cols) > 5 else ''})
- **Categorical Columns**: {len(categorical_cols)} ({', '.join(categorical_cols[:5])}{'...' if len(categorical_cols) > 5 else ''})

## ğŸ“ˆ **Basic Statistics**
{df.describe().to_string() if len(numeric_cols) > 0 else 'No numeric columns for statistics'}

## ğŸ” **Missing Values**
{', '.join([f"{col}: {count}" for col, count in basic_info['missing_values'].items() if count > 0]) or 'No missing values'}

## ğŸ’¡ **Recommendations**
- Use specific pandas operations for deeper analysis
- Consider data cleaning if missing values exist
- Explore correlations between numeric variables
- Analyze distributions and outliers

âœ… **Basic analysis completed. Provide specific instructions for advanced operations!**
"""
        except Exception as e:
            logger.error(f"Fallback analysis failed: {e}")
            return f"Data analysis failed: {str(e)}"
    
    def _finalize_result(self, result: str, user_message: str, start_time: datetime) -> str:
        """Finalize the result with timing information"""
        processing_time = (datetime.now() - start_time).total_seconds()
        
        return f"""{result}

---
**Processing Time**: {processing_time:.2f} seconds | **Agent**: PandasAnalyst | **Port**: 8315
"""


def create_agent_card() -> AgentCard:
    """Create Agent Card for PandasAnalyst"""
    
    # Pandas Analysis skill definition
    pandas_skill = AgentSkill(
        id="pandas_data_analysis",
        name="Pandas Data Analysis",
        description="ê³ ê¸‰ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•œ ì „ë¬¸ì ì¸ ë°ì´í„° ë¶„ì„, ì¡°ì‘, ë³€í™˜, ë° ì²˜ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë™ì  ì½”ë“œ ìƒì„±ì„ í†µí•´ ë§ì¶¤í˜• ë°ì´í„° ì†”ë£¨ì…˜ì„ ì œê³µí•©ë‹ˆë‹¤.",
        tags=["pandas", "data-analysis", "data-manipulation", "data-processing", "python", "dataframe"],
        examples=[
            "ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
            "íŠ¹ì • ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ë¥¼ í•„í„°ë§í•´ì£¼ì„¸ìš”",
            "ê·¸ë£¹ë³„ë¡œ ë°ì´í„°ë¥¼ ì§‘ê³„í•˜ê³  í†µê³„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”",
            "ë°ì´í„°ë¥¼ í”¼ë²— í…Œì´ë¸”ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”",
            "ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•˜ê³  ë°ì´í„°ë¥¼ ì •ì œí•´ì£¼ì„¸ìš”",
            "ìƒˆë¡œìš´ í”¼ì²˜ë¥¼ ìƒì„±í•˜ê³  ë°ì´í„°ë¥¼ ë³€í™˜í•´ì£¼ì„¸ìš”",
            "ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ì£¼ì„¸ìš”",
            "ë³µì¡í•œ ë°ì´í„° ì¡°ì‘ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”"
        ]
    )
    
    # Agent Card creation
    agent_card = AgentCard(
        name="PandasAnalyst",
        description="ê³ ê¸‰ pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. LLM-first ì ‘ê·¼ë°©ì‹ìœ¼ë¡œ ë™ì  pandas ì½”ë“œë¥¼ ìƒì„±í•˜ì—¬ ë³µì¡í•œ ë°ì´í„° ë¶„ì„, ì¡°ì‘, ë³€í™˜ ì‘ì—…ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤. ë§ì¶¤í˜• ë°ì´í„° ì†”ë£¨ì…˜ ì œê³µì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.",
        url="http://localhost:8315/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=False),
        skills=[pandas_skill],
        supportsAuthenticatedExtendedCard=False
    )
    
    return agent_card


def main():
    """Main function to start the PandasAnalyst server following A2A standard pattern"""
    logger.info("ğŸ¼ PandasAnalyst A2A Server ì‹œì‘ì¤‘...")
    logger.info("ğŸ“ í¬íŠ¸: 8315")
    logger.info("ğŸ”— URL: http://localhost:8315/")
    logger.info("ğŸš€ LLM-First ë™ì  pandas ì½”ë“œ ìƒì„± ì‹œìŠ¤í…œ")
    logger.info("ğŸ”§ 8ê°œ í•µì‹¬ ë°ì´í„° ì¡°ì‘ ê¸°ëŠ¥ ì¤€ë¹„ ì™„ë£Œ")
    logger.info("ğŸ¯ Langfuse í†µí•© ë° TaskUpdater íŒ¨í„´ ì ìš©")
    logger.info("="*80)
    
    # A2A application setup following standard pattern
    task_store = InMemoryTaskStore()
    request_handler = DefaultRequestHandler(
        agent_executor=PandasAnalystAgentExecutor(),
        agent_card=create_agent_card()
    )
    
    # Create Starlette application
    server = A2AStarletteApplication(
        request_handler=request_handler,
        task_store=task_store
    )
    
    # Build and run server
    server.build()
    
    uvicorn.run(
        server.app,
        host="0.0.0.0",
        port=8315,
        log_level="info"
    )


if __name__ == "__main__":
    main()