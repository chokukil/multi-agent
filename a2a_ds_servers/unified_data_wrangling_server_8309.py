#!/usr/bin/env python3
"""
CherryAI Unified Data Wrangling Server - Port 8309
A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜ + UnifiedDataInterface íŒ¨í„´

ğŸ”§ í•µì‹¬ ê¸°ëŠ¥:
- ğŸ§  LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ë°ì´í„° ë³€í™˜ ì „ëµ ë¶„ì„
- ğŸ“ ì•ˆì •ì  íŒŒì¼ ì„ íƒ ë° ë¡œë”© (ì„¤ê³„ ë¬¸ì„œ ì£¼ìš” ë¬¸ì œì  í•´ê²°)
- ğŸ”„ ì•ˆì „í•œ ë°ì´í„° ë³€í™˜ ë° ê²€ì¦ ì‹œìŠ¤í…œ
- ğŸ“Š ë‹¤ë‹¨ê³„ ë°ì´í„° í’ˆì§ˆ ë³´ì¥
- ğŸ’¾ ë³€í™˜ íˆìŠ¤í† ë¦¬ ë° ë¡¤ë°± ê¸°ëŠ¥
- ğŸ¯ A2A í‘œì¤€ TaskUpdater + ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

ê¸°ë°˜: pandas_agent íŒ¨í„´ + unified_data_loader ì„±ê³µ ì‚¬ë¡€
"""

import asyncio
import logging
import os
import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import pandas as pd
import numpy as np
from dataclasses import dataclass
import copy

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A SDK 0.2.9 í‘œì¤€ imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard, AgentSkill, AgentCapabilities,
    TaskState, TextPart
)
from a2a.utils import new_agent_text_message
import uvicorn

# CherryAI Core imports
from core.llm_factory import LLMFactory
from a2a_ds_servers.unified_data_system.core.unified_data_interface import UnifiedDataInterface
from a2a_ds_servers.unified_data_system.core.smart_dataframe import SmartDataFrame
from a2a_ds_servers.unified_data_system.core.llm_first_data_engine import LLMFirstDataEngine
from a2a_ds_servers.unified_data_system.core.cache_manager import CacheManager
from a2a_ds_servers.unified_data_system.utils.file_scanner import FileScanner

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class TransformationStep:
    """ë°ì´í„° ë³€í™˜ ë‹¨ê³„ ì •ì˜"""
    step_id: str
    operation: str
    target_columns: List[str]
    parameters: Dict[str, Any]
    description: str
    reversible: bool = True
    backup_required: bool = True

@dataclass 
class ValidationResult:
    """ë³€í™˜ ê²€ì¦ ê²°ê³¼"""
    is_valid: bool
    validation_score: float
    issues: List[str]
    warnings: List[str]
    recommendations: List[str]

class UnifiedDataWranglingExecutor(AgentExecutor, UnifiedDataInterface):
    """
    Unified Data Wrangling Executor
    
    pandas_agent íŒ¨í„´ + data_loader ì„±ê³µ ì‚¬ë¡€ ê¸°ë°˜
    - LLM First ë°ì´í„° ë³€í™˜ ì „ëµ
    - ì•ˆì •ì  íŒŒì¼ ì„ íƒ ì‹œìŠ¤í…œ
    - ì•ˆì „í•œ ë³€í™˜ ë° ê²€ì¦
    - A2A SDK 0.2.9 ì™„ì „ ì¤€ìˆ˜
    """
    
    def __init__(self):
        super().__init__()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.data_engine = LLMFirstDataEngine()
        self.cache_manager = CacheManager()
        self.file_scanner = FileScanner()
        self.llm_factory = LLMFactory()
        
        # ë°ì´í„° ë³€í™˜ ì „ë¬¸ ì„¤ì •
        self.transformation_operations = {
            'column_operations': [
                'rename_columns', 'drop_columns', 'add_columns', 'reorder_columns',
                'split_columns', 'merge_columns', 'extract_substrings'
            ],
            'data_type_operations': [
                'convert_types', 'parse_dates', 'categorize', 'numeric_conversion',
                'string_normalization', 'boolean_conversion'
            ],
            'filtering_operations': [
                'filter_rows', 'filter_by_condition', 'remove_duplicates',
                'sample_data', 'slice_data', 'query_filter'
            ],
            'aggregation_operations': [
                'group_by', 'pivot_table', 'melt_data', 'crosstab',
                'rolling_operations', 'cumulative_operations'
            ],
            'joining_operations': [
                'merge_dataframes', 'concat_dataframes', 'join_operations',
                'append_data', 'union_data'
            ],
            'transformation_operations': [
                'normalize_data', 'scale_features', 'encode_categorical',
                'create_features', 'apply_functions', 'mathematical_operations'
            ]
        }
        
        # ì•ˆì „ì„± ì„ê³„ê°’
        self.safety_thresholds = {
            'max_data_loss_percentage': 20,  # ìµœëŒ€ 20% ë°ì´í„° ì†ì‹¤ í—ˆìš©
            'min_data_quality_score': 0.7,   # ìµœì†Œ í’ˆì§ˆ ì ìˆ˜ 70%
            'max_processing_time': 300,      # ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„ 5ë¶„
            'backup_required_operations': [
                'drop_columns', 'filter_rows', 'remove_duplicates'
            ]
        }
        
        # ë³€í™˜ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
        self.transformation_history = []
        self.backup_dataframes = {}
        
        logger.info("âœ… Unified Data Wrangling Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """
        A2A í‘œì¤€ ì‹¤í–‰: 8ë‹¨ê³„ ì§€ëŠ¥í˜• ë°ì´í„° ë³€í™˜ í”„ë¡œì„¸ìŠ¤
        
        ğŸ§  1ë‹¨ê³„: LLM ë³€í™˜ ì˜ë„ ë¶„ì„
        ğŸ“‚ 2ë‹¨ê³„: ì•ˆì •ì  íŒŒì¼ ì„ íƒ ë° ë¡œë”©
        ğŸ” 3ë‹¨ê³„: ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë° ë³€í™˜ ê°€ëŠ¥ì„± í‰ê°€
        ğŸ“‹ 4ë‹¨ê³„: LLM ë³€í™˜ ê³„íš ìˆ˜ë¦½
        ğŸ’¾ 5ë‹¨ê³„: ë°±ì—… ìƒì„± ë° ì•ˆì „ì„± í™•ì¸
        ğŸ”„ 6ë‹¨ê³„: ë‹¨ê³„ë³„ ë³€í™˜ ì‹¤í–‰ ë° ê²€ì¦
        âœ… 7ë‹¨ê³„: ë³€í™˜ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
        ğŸ“ 8ë‹¨ê³„: ìµœì¢… ê²°ê³¼ ì €ì¥ ë° ë³´ê³ 
        """
        try:
            # ì‘ì—… ì‹œì‘
            await task_updater.submit()
            await task_updater.start_work()
            
            start_time = time.time()
            
            # ğŸ§  1ë‹¨ê³„: ë³€í™˜ ì˜ë„ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§‘ğŸ» **ë°ì´í„° ë³€í™˜ ì‹œì‘** - 1ë‹¨ê³„: ë³€í™˜ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘...")
            )
            
            user_query = self._extract_user_query(context)
            logger.info(f"ğŸ”§ Data Wrangling Query: {user_query}")
            
            # LLM ê¸°ë°˜ ë³€í™˜ ì˜ë„ ë¶„ì„
            wrangling_intent = await self._analyze_wrangling_intent(user_query)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì˜ë„ ë¶„ì„ ì™„ë£Œ**\n"
                    f"- ë³€í™˜ íƒ€ì…: {wrangling_intent['transformation_type']}\n"
                    f"- ì£¼ìš” ì‘ì—…: {', '.join(wrangling_intent['primary_operations'])}\n"
                    f"- ë³µì¡ë„: {wrangling_intent['complexity_level']}\n"
                    f"- ì‹ ë¢°ë„: {wrangling_intent['confidence']:.2f}\n\n"
                    f"**2ë‹¨ê³„**: ë°ì´í„° ê²€ìƒ‰ ì¤‘..."
                )
            )
            
            # ğŸ“‚ 2ë‹¨ê³„: ì•ˆì •ì  íŒŒì¼ ì„ íƒ (í•µì‹¬ ë¬¸ì œ í•´ê²°)
            available_files = await self._scan_available_files_with_stability_check()
            
            if not available_files:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(
                        "âš ï¸ **ë°ì´í„° ì—†ìŒ**: ë³€í™˜í•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                        "**í•´ê²°ì±…**:\n"
                        "1. `a2a_ds_servers/artifacts/data/` í´ë”ì— ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”\n"
                        "2. ì§€ì› í˜•ì‹: CSV, Excel (.xlsx/.xls), JSON, Parquet\n"
                        "3. íŒŒì¼ í¬ê¸°: ìµœëŒ€ 1GB (ìë™ ìµœì í™”)"
                    )
                )
                return
            
            # ì•ˆì •ì  íŒŒì¼ ì„ íƒ (ì‹ ë¢°ì„± ì ìˆ˜ ê¸°ë°˜)
            selected_file = await self._select_stable_file(available_files, wrangling_intent)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **íŒŒì¼ ì„ íƒ ì™„ë£Œ**\n"
                    f"- íŒŒì¼: {selected_file['name']}\n"
                    f"- í¬ê¸°: {selected_file['size']:,} bytes\n"
                    f"- ì•ˆì •ì„± ì ìˆ˜: {selected_file['stability_score']:.2f}\n\n"
                    f"**3ë‹¨ê³„**: ì•ˆì „í•œ ë°ì´í„° ë¡œë”© ì¤‘..."
                )
            )
            
            # ğŸ“Š 3ë‹¨ê³„: ì•ˆì „í•œ ë°ì´í„° ë¡œë”©
            smart_df = await self._load_data_safely_with_validation(selected_file)
            
            # ğŸ” 4ë‹¨ê³„: ë°ì´í„° í”„ë¡œíŒŒì¼ë§
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ë°ì´í„° ë¡œë”© ì™„ë£Œ**\n"
                    f"- í˜•íƒœ: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´\n"
                    f"- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {smart_df.data.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\n"
                    f"- ì»¬ëŸ¼: {list(smart_df.data.columns)}\n\n"
                    f"**4ë‹¨ê³„**: ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë° ë³€í™˜ ê°€ëŠ¥ì„± í‰ê°€ ì¤‘..."
                )
            )
            
            # ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë° ë³€í™˜ ê°€ëŠ¥ì„± í‰ê°€
            data_profile = await self._comprehensive_data_profiling(smart_df)
            transformation_feasibility = await self._assess_transformation_feasibility(smart_df, wrangling_intent)
            
            # ğŸ“‹ 5ë‹¨ê³„: LLM ë³€í™˜ ê³„íš ìˆ˜ë¦½
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **í”„ë¡œíŒŒì¼ë§ ì™„ë£Œ**\n"
                    f"- ë°ì´í„° í’ˆì§ˆ: {data_profile['quality_score']:.2f}/1.00\n"
                    f"- ë³€í™˜ ê°€ëŠ¥ì„±: {transformation_feasibility['feasibility_score']:.2f}\n"
                    f"- ê¶Œì¥ ì‘ì—…: {len(transformation_feasibility['recommended_operations'])}ê°œ\n\n"
                    f"**5ë‹¨ê³„**: LLM ë³€í™˜ ê³„íš ìˆ˜ë¦½ ì¤‘..."
                )
            )
            
            transformation_plan = await self._create_transformation_plan(smart_df, wrangling_intent, data_profile)
            
            # ğŸ’¾ 6ë‹¨ê³„: ë°±ì—… ë° ì•ˆì „ì„± í™•ì¸
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ë³€í™˜ ê³„íš ì™„ì„±**\n"
                    f"- ë³€í™˜ ë‹¨ê³„: {len(transformation_plan['steps'])}ê°œ\n"
                    f"- ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: {transformation_plan['estimated_time']}\n"
                    f"- ì•ˆì „ì„± ë“±ê¸‰: {transformation_plan['safety_level']}\n\n"
                    f"**6ë‹¨ê³„**: ë°±ì—… ìƒì„± ë° ì•ˆì „ì„± í™•ì¸ ì¤‘..."
                )
            )
            
            # ë°±ì—… ìƒì„±
            backup_info = await self._create_transformation_backup(smart_df, transformation_plan)
            
            # ğŸ”„ 7ë‹¨ê³„: ë‹¨ê³„ë³„ ë³€í™˜ ì‹¤í–‰
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ë°±ì—… ì™„ë£Œ**\n"
                    f"- ë°±ì—… ID: {backup_info['backup_id']}\n"
                    f"- ë¡¤ë°± ê°€ëŠ¥: {backup_info['rollback_enabled']}\n\n"
                    f"**7ë‹¨ê³„**: ë‹¨ê³„ë³„ ë³€í™˜ ì‹¤í–‰ ì¤‘..."
                )
            )
            
            transformed_smart_df = await self._execute_transformation_plan(smart_df, transformation_plan, task_updater)
            
            # âœ… 8ë‹¨ê³„: ë³€í™˜ ê²°ê³¼ ê²€ì¦ ë° ìµœì¢…í™”
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("**8ë‹¨ê³„**: ë³€í™˜ ê²°ê³¼ ê²€ì¦ ë° ìµœì¢…í™” ì¤‘...")
            )
            
            final_results = await self._finalize_wrangling_results(
                original_df=smart_df,
                transformed_df=transformed_smart_df,
                transformation_plan=transformation_plan,
                backup_info=backup_info,
                task_updater=task_updater
            )
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(
                    f"âœ… **ë°ì´í„° ë³€í™˜ ì™„ë£Œ!**\n\n"
                    f"ğŸ“Š **ë³€í™˜ ê²°ê³¼**:\n"
                    f"- ì›ë³¸: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´\n"
                    f"- ë³€í™˜ëœ ë°ì´í„°: {transformed_smart_df.shape[0]}í–‰ Ã— {transformed_smart_df.shape[1]}ì—´\n"
                    f"- í’ˆì§ˆ ë³€í™”: {final_results['quality_change']:.2f}\n"
                    f"- ë°ì´í„° ë³´ì¡´ìœ¨: {final_results['data_preservation_rate']:.1%}\n"
                    f"- ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n\n"
                    f"ğŸ“ **ì €ì¥ ìœ„ì¹˜**: {final_results['saved_path']}\n"
                    f"ğŸ”„ **ë¡¤ë°± ê°€ëŠ¥**: {final_results['rollback_available']}\n"
                    f"ğŸ“‹ **ë³€í™˜ ë³´ê³ ì„œ**: ì•„í‹°íŒ©íŠ¸ë¡œ ìƒì„±ë¨"
                )
            )
            
            # ì•„í‹°íŒ©íŠ¸ ìƒì„±
            await self._create_wrangling_artifacts(final_results, task_updater)
            
        except Exception as e:
            logger.error(f"âŒ Data Wrangling Error: {e}", exc_info=True)
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"âŒ **ë³€í™˜ ì‹¤íŒ¨**: {str(e)}")
            )
    
    async def _analyze_wrangling_intent(self, user_query: str) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ë°ì´í„° ë³€í™˜ ì˜ë„ ë¶„ì„"""
        llm = await self.llm_factory.get_llm()
        
        prompt = f"""
        ì‚¬ìš©ìì˜ ë°ì´í„° ë³€í™˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë³€í™˜ ì „ëµì„ ê²°ì •í•´ì£¼ì„¸ìš”:
        
        ìš”ì²­: {user_query}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ë³€í™˜ ì‘ì—…ë“¤:
        {json.dumps(self.transformation_operations, indent=2, ensure_ascii=False)}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "transformation_type": "structural|content|analytical|cleaning|aggregation",
            "primary_operations": ["ì£¼ìš” ë³€í™˜ ì‘ì—…ë“¤"],
            "target_columns": ["ëŒ€ìƒ ì»¬ëŸ¼ë“¤"],
            "complexity_level": "simple|moderate|complex",
            "confidence": 0.0-1.0,
            "data_size_sensitivity": "low|medium|high",
            "reversibility_required": true/false,
            "expected_outcomes": ["ì˜ˆìƒ ê²°ê³¼ë“¤"]
        }}
        """
        
        response = await llm.agenerate([prompt])
        try:
            intent = json.loads(response.generations[0][0].text)
            return intent
        except:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "transformation_type": "structural",
                "primary_operations": ["ë°ì´í„° ì •ë¦¬", "í˜•íƒœ ë³€í™˜"],
                "target_columns": [],
                "complexity_level": "moderate",
                "confidence": 0.8,
                "data_size_sensitivity": "medium",
                "reversibility_required": True,
                "expected_outcomes": ["ì •ë¦¬ëœ ë°ì´í„°", "êµ¬ì¡° ê°œì„ "]
            }
    
    async def _scan_available_files_with_stability_check(self) -> List[Dict[str, Any]]:
        """ì•ˆì •ì„± ê²€ì‚¬ê°€ í¬í•¨ëœ íŒŒì¼ ìŠ¤ìº” (í•µì‹¬ ë¬¸ì œ í•´ê²°)"""
        try:
            data_directories = [
                "ai_ds_team/data",
                "a2a_ds_servers/artifacts/data",
                "test_datasets"
            ]
            
            discovered_files = []
            for directory in data_directories:
                if os.path.exists(directory):
                    files = self.file_scanner.scan_data_files(directory)
                    
                    # ê° íŒŒì¼ì— ëŒ€í•œ ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚°
                    for file_info in files:
                        stability_score = await self._calculate_file_stability_score(file_info)
                        file_info['stability_score'] = stability_score
                        
                        # ì•ˆì •ì„± ì ìˆ˜ê°€ ì„ê³„ê°’ ì´ìƒì¸ íŒŒì¼ë§Œ í¬í•¨
                        if stability_score >= 0.5:  # 50% ì´ìƒ ì•ˆì •ì„±
                            discovered_files.append(file_info)
            
            # ì•ˆì •ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            discovered_files.sort(key=lambda x: x['stability_score'], reverse=True)
            
            logger.info(f"ğŸ“‚ ì•ˆì •ì„± ê²€ì¦ëœ íŒŒì¼: {len(discovered_files)}ê°œ")
            return discovered_files
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
            return []
    
    async def _calculate_file_stability_score(self, file_info: Dict) -> float:
        """íŒŒì¼ ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚° (ë³€í™˜ ê°€ëŠ¥ì„± í‰ê°€)"""
        try:
            stability_factors = []
            
            # 1. íŒŒì¼ í¬ê¸° ì•ˆì •ì„± (ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ë¶ˆì•ˆì •)
            size = file_info['size']
            if 1024 <= size <= 100_000_000:  # 1KB ~ 100MB
                stability_factors.append(1.0)
            elif size < 1024:
                stability_factors.append(0.3)  # ë„ˆë¬´ ì‘ìŒ
            elif size > 1_000_000_000:  # 1GB ì´ìƒ
                stability_factors.append(0.5)  # ë„ˆë¬´ í¼
            else:
                stability_factors.append(0.8)
            
            # 2. íŒŒì¼ í™•ì¥ì ì‹ ë¢°ì„±
            extension = file_info.get('extension', '').lower()
            if extension in ['.csv', '.xlsx', '.xls']:
                stability_factors.append(1.0)  # ë†’ì€ ì‹ ë¢°ì„±
            elif extension in ['.json', '.parquet']:
                stability_factors.append(0.9)  # ì¢‹ì€ ì‹ ë¢°ì„±
            else:
                stability_factors.append(0.6)  # ë³´í†µ ì‹ ë¢°ì„±
            
            # 3. íŒŒì¼ ì ‘ê·¼ ê°€ëŠ¥ì„±
            file_path = file_info['path']
            if os.path.exists(file_path) and os.access(file_path, os.R_OK):
                stability_factors.append(1.0)
            else:
                stability_factors.append(0.0)
            
            # 4. íŒŒì¼ëª… ëª…í™•ì„± (ëª…í™•í•œ íŒŒì¼ëª…ì¼ìˆ˜ë¡ ì•ˆì •)
            filename = file_info['name'].lower()
            if any(keyword in filename for keyword in ['test', 'sample', 'data', 'clean']):
                stability_factors.append(0.9)
            elif any(keyword in filename for keyword in ['temp', 'tmp', 'backup']):
                stability_factors.append(0.4)  # ì„ì‹œ íŒŒì¼
            else:
                stability_factors.append(0.7)
            
            # ì „ì²´ ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚°
            stability_score = sum(stability_factors) / len(stability_factors)
            return round(stability_score, 3)
            
        except Exception as e:
            logger.warning(f"ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5  # ê¸°ë³¸ê°’
    
    async def _select_stable_file(self, available_files: List[Dict], wrangling_intent: Dict) -> Dict[str, Any]:
        """ì•ˆì •ì„± ì ìˆ˜ ê¸°ë°˜ ìµœì  íŒŒì¼ ì„ íƒ"""
        if len(available_files) == 1:
            return available_files[0]
        
        # ì•ˆì •ì„± ì ìˆ˜ê°€ ê°€ì¥ ë†’ì€ íŒŒì¼ë“¤ ì„ ë³„ (ìƒìœ„ 30%)
        top_stable_files = available_files[:max(1, len(available_files) // 3)]
        
        # LLM ê¸°ë°˜ ìµœì¢… ì„ íƒ (ì•ˆì •ì„± + ì í•©ì„±)
        llm = await self.llm_factory.get_llm()
        
        files_info = "\n".join([
            f"- {f['name']} (í¬ê¸°: {f['size']} bytes, ì•ˆì •ì„±: {f['stability_score']:.2f})"
            for f in top_stable_files
        ])
        
        prompt = f"""
        ë°ì´í„° ë³€í™˜ ëª©ì ì— ê°€ì¥ ì í•©í•˜ë©´ì„œ ì•ˆì •ì ì¸ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”:
        
        ë³€í™˜ ì˜ë„: {wrangling_intent['transformation_type']}
        ì£¼ìš” ì‘ì—…: {wrangling_intent['primary_operations']}
        ë³µì¡ë„: {wrangling_intent['complexity_level']}
        
        ì•ˆì •ì„± ê²€ì¦ëœ íŒŒì¼ë“¤:
        {files_info}
        
        ê°€ì¥ ì í•©í•œ íŒŒì¼ëª…ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        """
        
        response = await llm.agenerate([prompt])
        selected_name = response.generations[0][0].text.strip()
        
        # íŒŒì¼ëª… ë§¤ì¹­
        for file_info in top_stable_files:
            if selected_name in file_info['name'] or file_info['name'] in selected_name:
                return file_info
        
        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ê°€ì¥ ì•ˆì •ì ì¸ íŒŒì¼ ë°˜í™˜
        return top_stable_files[0]
    
    async def _load_data_safely_with_validation(self, file_info: Dict[str, Any]) -> SmartDataFrame:
        """ê²€ì¦ì´ í¬í•¨ëœ ì•ˆì „í•œ ë°ì´í„° ë¡œë”©"""
        file_path = file_info['path']
        
        try:
            # ë‹¤ì¤‘ ì¸ì½”ë”© ì‹œë„ (unified pattern)
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1', 'utf-16']
            df = None
            used_encoding = None
            
            for encoding in encodings:
                try:
                    if file_path.endswith('.csv'):
                        df = pd.read_csv(file_path, encoding=encoding)
                    elif file_path.endswith(('.xlsx', '.xls')):
                        df = pd.read_excel(file_path)
                        used_encoding = 'excel_auto'
                    elif file_path.endswith('.json'):
                        df = pd.read_json(file_path, encoding=encoding)
                    elif file_path.endswith('.parquet'):
                        df = pd.read_parquet(file_path)
                        used_encoding = 'parquet_auto'
                    
                    if df is not None and not df.empty:
                        used_encoding = used_encoding or encoding
                        break
                        
                except (UnicodeDecodeError, Exception):
                    continue
            
            if df is None or df.empty:
                raise ValueError("ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: ì§€ì›ë˜ì§€ ì•ŠëŠ” í˜•ì‹ì´ê±°ë‚˜ ë¹ˆ íŒŒì¼")
            
            # ë¡œë”© í›„ ê¸°ë³¸ ê²€ì¦
            validation_results = await self._validate_loaded_data(df)
            
            if not validation_results['is_valid']:
                logger.warning(f"ë°ì´í„° ê²€ì¦ ê²½ê³ : {validation_results['issues']}")
            
            # SmartDataFrame ìƒì„±
            metadata = {
                'source_file': file_path,
                'encoding': used_encoding,
                'load_timestamp': datetime.now().isoformat(),
                'original_shape': df.shape,
                'stability_score': file_info['stability_score'],
                'validation_results': validation_results
            }
            
            smart_df = SmartDataFrame(df, metadata)
            logger.info(f"âœ… ì•ˆì „í•œ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {smart_df.shape}, ì•ˆì •ì„±: {file_info['stability_score']:.2f}")
            
            return smart_df
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _validate_loaded_data(self, df: pd.DataFrame) -> ValidationResult:
        """ë¡œë”©ëœ ë°ì´í„° ê¸°ë³¸ ê²€ì¦"""
        issues = []
        warnings = []
        recommendations = []
        
        try:
            # 1. ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
            if df.empty:
                issues.append("DataFrameì´ ë¹„ì–´ìˆìŒ")
            
            if df.shape[0] == 0:
                issues.append("ë°ì´í„° í–‰ì´ ì—†ìŒ")
            
            if df.shape[1] == 0:
                issues.append("ë°ì´í„° ì»¬ëŸ¼ì´ ì—†ìŒ")
            
            # 2. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            null_percentage = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
            if null_percentage > 50:
                warnings.append(f"ê²°ì¸¡ê°’ì´ ë§ìŒ: {null_percentage:.1f}%")
                recommendations.append("ë°ì´í„° ì •ë¦¬ ì—ì´ì „íŠ¸ ì‚¬ìš© ê¶Œì¥")
            
            # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²€ì¦
            memory_usage = df.memory_usage(deep=True).sum() / 1024 / 1024  # MB
            if memory_usage > 500:  # 500MB ì´ìƒ
                warnings.append(f"í° ë°ì´í„° í¬ê¸°: {memory_usage:.1f} MB")
                recommendations.append("ìƒ˜í”Œë§ ë˜ëŠ” ì²­í¬ ì²˜ë¦¬ ê³ ë ¤")
            
            # 4. ë°ì´í„° íƒ€ì… ê²€ì¦
            object_columns = df.select_dtypes(include=['object']).columns
            if len(object_columns) > df.shape[1] * 0.8:  # 80% ì´ìƒì´ object íƒ€ì…
                warnings.append("ëŒ€ë¶€ë¶„ì˜ ì»¬ëŸ¼ì´ ë¬¸ìì—´ íƒ€ì…")
                recommendations.append("ë°ì´í„° íƒ€ì… ìµœì í™” í•„ìš”")
            
            # ê²€ì¦ ì ìˆ˜ ê³„ì‚°
            validation_score = 1.0
            validation_score -= len(issues) * 0.3
            validation_score -= len(warnings) * 0.1
            validation_score = max(0.0, validation_score)
            
            return ValidationResult(
                is_valid=len(issues) == 0,
                validation_score=validation_score,
                issues=issues,
                warnings=warnings,
                recommendations=recommendations
            )
            
        except Exception as e:
            return ValidationResult(
                is_valid=False,
                validation_score=0.0,
                issues=[f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"],
                warnings=[],
                recommendations=["ë°ì´í„° í˜•ì‹ í™•ì¸ í•„ìš”"]
            )
    
    async def _comprehensive_data_profiling(self, smart_df: SmartDataFrame) -> Dict[str, Any]:
        """í¬ê´„ì  ë°ì´í„° í”„ë¡œíŒŒì¼ë§"""
        df = smart_df.data
        
        profile = {
            'basic_info': {
                'shape': df.shape,
                'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,
                'column_count': df.shape[1],
                'row_count': df.shape[0]
            },
            'data_types': {
                'numeric_columns': df.select_dtypes(include=[np.number]).columns.tolist(),
                'categorical_columns': df.select_dtypes(include=['object', 'category']).columns.tolist(),
                'datetime_columns': df.select_dtypes(include=['datetime64']).columns.tolist(),
                'boolean_columns': df.select_dtypes(include=['bool']).columns.tolist()
            },
            'quality_metrics': {
                'null_percentage': (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100,
                'duplicate_rows': df.duplicated().sum(),
                'unique_values_per_column': {col: df[col].nunique() for col in df.columns}
            }
        }
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_factors = []
        
        # ì™„ì „ì„±
        completeness = (1 - profile['quality_metrics']['null_percentage'] / 100)
        quality_factors.append(completeness)
        
        # ê³ ìœ ì„±
        uniqueness = 1 - (profile['quality_metrics']['duplicate_rows'] / len(df))
        quality_factors.append(uniqueness)
        
        # ë‹¤ì–‘ì„± (ì»¬ëŸ¼ë³„ ê³ ìœ ê°’ ë¹„ìœ¨)
        diversity_scores = []
        for col in df.columns:
            unique_ratio = profile['quality_metrics']['unique_values_per_column'][col] / len(df)
            diversity_scores.append(min(unique_ratio, 1.0))
        diversity = np.mean(diversity_scores) if diversity_scores else 0.5
        quality_factors.append(diversity)
        
        profile['quality_score'] = np.mean(quality_factors)
        
        return profile
    
    async def _assess_transformation_feasibility(self, smart_df: SmartDataFrame, wrangling_intent: Dict) -> Dict[str, Any]:
        """ë³€í™˜ ê°€ëŠ¥ì„± í‰ê°€"""
        df = smart_df.data
        
        feasibility_factors = []
        recommended_operations = []
        potential_issues = []
        
        # 1. ë°ì´í„° í¬ê¸° ê¸°ë°˜ ê°€ëŠ¥ì„±
        if df.shape[0] > 100000:  # 10ë§Œ í–‰ ì´ìƒ
            feasibility_factors.append(0.7)  # ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤
            recommended_operations.append("ë°ì´í„° ìƒ˜í”Œë§")
        else:
            feasibility_factors.append(1.0)
        
        # 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ê°€ëŠ¥ì„±
        memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024
        if memory_mb > 1000:  # 1GB ì´ìƒ
            feasibility_factors.append(0.6)
            potential_issues.append("ë©”ëª¨ë¦¬ ë¶€ì¡± ìœ„í—˜")
        else:
            feasibility_factors.append(1.0)
        
        # 3. ìš”ì²­ëœ ì‘ì—…ì˜ ë³µì¡ë„
        complexity = wrangling_intent.get('complexity_level', 'moderate')
        if complexity == 'simple':
            feasibility_factors.append(1.0)
        elif complexity == 'moderate':
            feasibility_factors.append(0.8)
        else:  # complex
            feasibility_factors.append(0.6)
            recommended_operations.append("ë‹¨ê³„ë³„ ì²˜ë¦¬")
        
        # 4. ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ê°€ëŠ¥ì„±
        null_percentage = (df.isnull().sum().sum() / (df.shape[0] * df.shape[1])) * 100
        if null_percentage > 30:
            feasibility_factors.append(0.7)
            potential_issues.append("ë†’ì€ ê²°ì¸¡ê°’ ë¹„ìœ¨")
            recommended_operations.append("ì‚¬ì „ ë°ì´í„° ì •ë¦¬")
        else:
            feasibility_factors.append(1.0)
        
        feasibility_score = np.mean(feasibility_factors)
        
        return {
            'feasibility_score': feasibility_score,
            'recommended_operations': recommended_operations,
            'potential_issues': potential_issues,
            'estimated_difficulty': complexity,
            'resource_requirements': {
                'memory_intensive': memory_mb > 500,
                'time_intensive': df.shape[0] > 50000,
                'cpu_intensive': complexity == 'complex'
            }
        }
    
    async def _create_transformation_plan(self, smart_df: SmartDataFrame, wrangling_intent: Dict, data_profile: Dict) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ë³€í™˜ ê³„íš ìˆ˜ë¦½"""
        llm = await self.llm_factory.get_llm()
        
        # ê³„íš ìˆ˜ë¦½ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = {
            'data_info': {
                'shape': smart_df.shape,
                'columns': list(smart_df.data.columns),
                'data_types': data_profile['data_types'],
                'quality_score': data_profile['quality_score']
            },
            'user_intent': wrangling_intent,
            'available_operations': self.transformation_operations
        }
        
        prompt = f"""
        ë°ì´í„° ë³€í™˜ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:
        
        ì»¨í…ìŠ¤íŠ¸:
        {json.dumps(context, indent=2, default=str, ensure_ascii=False)}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„¸í•œ ë³€í™˜ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        {{
            "steps": [
                {{
                    "step_number": 1,
                    "operation_category": "column_operations|data_type_operations|filtering_operations|...",
                    "specific_operation": "êµ¬ì²´ì  ì‘ì—…ëª…",
                    "target_columns": ["ëŒ€ìƒ ì»¬ëŸ¼ë“¤"],
                    "parameters": {{"ë§¤ê°œë³€ìˆ˜": "ê°’"}},
                    "description": "ì‘ì—… ì„¤ëª…",
                    "expected_result": "ì˜ˆìƒ ê²°ê³¼",
                    "risk_level": "low|medium|high",
                    "backup_required": true/false
                }}
            ],
            "estimated_time": "ì˜ˆìƒ ì‹œê°„",
            "safety_level": "safe|caution|risky",
            "rollback_plan": "ë¡¤ë°± ê³„íš",
            "validation_checks": ["ê²€ì¦ í•­ëª©ë“¤"]
        }}
        """
        
        response = await llm.agenerate([prompt])
        try:
            plan = json.loads(response.generations[0][0].text)
            
            # ê³„íš ê²€ì¦ ë° ë³´ê°•
            validated_plan = await self._validate_and_enhance_plan(plan, smart_df)
            return validated_plan
            
        except Exception as e:
            logger.warning(f"LLM ê³„íš ìƒì„± ì‹¤íŒ¨, ê¸°ë³¸ ê³„íš ì‚¬ìš©: {e}")
            # ê¸°ë³¸ ê³„íš ë°˜í™˜
            return {
                "steps": [
                    {
                        "step_number": 1,
                        "operation_category": "data_type_operations",
                        "specific_operation": "basic_optimization",
                        "target_columns": list(smart_df.data.columns),
                        "parameters": {"auto_optimize": True},
                        "description": "ê¸°ë³¸ ë°ì´í„° ìµœì í™”",
                        "expected_result": "ìµœì í™”ëœ ë°ì´í„°",
                        "risk_level": "low",
                        "backup_required": True
                    }
                ],
                "estimated_time": "1-2ë¶„",
                "safety_level": "safe",
                "rollback_plan": "ë°±ì—…ì—ì„œ ë³µì›",
                "validation_checks": ["ë°ì´í„° ë¬´ê²°ì„±", "ì»¬ëŸ¼ ì¼ê´€ì„±"]
            }
    
    async def _validate_and_enhance_plan(self, plan: Dict, smart_df: SmartDataFrame) -> Dict[str, Any]:
        """ë³€í™˜ ê³„íš ê²€ì¦ ë° ë³´ê°•"""
        try:
            # 1. ë‹¨ê³„ë³„ ê²€ì¦
            for step in plan.get('steps', []):
                # ëŒ€ìƒ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
                target_columns = step.get('target_columns', [])
                valid_columns = [col for col in target_columns if col in smart_df.data.columns]
                step['target_columns'] = valid_columns
                
                # ìœ„í—˜ë„ í‰ê°€
                if step.get('specific_operation') in ['drop_columns', 'filter_rows']:
                    step['risk_level'] = 'medium'
                    step['backup_required'] = True
            
            # 2. ì•ˆì „ì„± ë“±ê¸‰ ì¬í‰ê°€
            high_risk_steps = sum(1 for step in plan['steps'] if step.get('risk_level') == 'high')
            if high_risk_steps > 0:
                plan['safety_level'] = 'risky'
            elif any(step.get('risk_level') == 'medium' for step in plan['steps']):
                plan['safety_level'] = 'caution'
            else:
                plan['safety_level'] = 'safe'
            
            # 3. ë°±ì—… ìš”êµ¬ì‚¬í•­ í™•ì¸
            backup_required = any(step.get('backup_required', False) for step in plan['steps'])
            plan['backup_required'] = backup_required
            
            return plan
            
        except Exception as e:
            logger.error(f"ê³„íš ê²€ì¦ ì‹¤íŒ¨: {e}")
            return plan
    
    async def _create_transformation_backup(self, smart_df: SmartDataFrame, transformation_plan: Dict) -> Dict[str, Any]:
        """ë³€í™˜ ë°±ì—… ìƒì„±"""
        backup_id = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        try:
            # ë°±ì—…ì´ í•„ìš”í•œì§€ í™•ì¸
            backup_required = transformation_plan.get('backup_required', False)
            
            if not backup_required:
                return {
                    'backup_id': backup_id,
                    'backup_created': False,
                    'rollback_enabled': False,
                    'backup_path': None
                }
            
            # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
            backup_dir = Path("a2a_ds_servers/artifacts/data/backups")
            backup_dir.mkdir(exist_ok=True, parents=True)
            
            # ì›ë³¸ ë°ì´í„° ë°±ì—…
            backup_filename = f"original_data_{backup_id}.csv"
            backup_path = backup_dir / backup_filename
            smart_df.data.to_csv(backup_path, index=False, encoding='utf-8')
            
            # ë©”íƒ€ë°ì´í„° ë°±ì—…
            metadata_filename = f"metadata_{backup_id}.json"
            metadata_path = backup_dir / metadata_filename
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(smart_df.metadata, f, indent=2, ensure_ascii=False, default=str)
            
            # ë°±ì—… ì •ë³´ ì €ì¥
            self.backup_dataframes[backup_id] = {
                'original_data': smart_df.data.copy(),
                'metadata': smart_df.metadata.copy(),
                'creation_time': datetime.now().isoformat(),
                'backup_path': str(backup_path)
            }
            
            logger.info(f"âœ… ë³€í™˜ ë°±ì—… ìƒì„± ì™„ë£Œ: {backup_id}")
            
            return {
                'backup_id': backup_id,
                'backup_created': True,
                'rollback_enabled': True,
                'backup_path': str(backup_path),
                'metadata_path': str(metadata_path)
            }
            
        except Exception as e:
            logger.error(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                'backup_id': backup_id,
                'backup_created': False,
                'rollback_enabled': False,
                'backup_path': None,
                'error': str(e)
            }
    
    async def _execute_transformation_plan(self, smart_df: SmartDataFrame, transformation_plan: Dict, task_updater: TaskUpdater) -> SmartDataFrame:
        """ë³€í™˜ ê³„íš ì‹¤í–‰"""
        current_df = smart_df.data.copy()
        execution_log = []
        
        for step in transformation_plan.get('steps', []):
            step_num = step['step_number']
            operation = step['specific_operation']
            
            try:
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message(
                        f"ğŸ’ ë³€í™˜ ë‹¨ê³„ {step_num}/{len(transformation_plan['steps'])}: {step['description']} ì‹¤í–‰ ì¤‘..."
                    )
                )
                
                # ë‹¨ê³„ë³„ ë³€í™˜ ì‹¤í–‰
                transformed_df = await self._execute_single_transformation(current_df, step)
                
                # ë³€í™˜ ê²°ê³¼ ê²€ì¦
                validation = await self._validate_transformation_step(current_df, transformed_df, step)
                
                if validation['is_valid']:
                    current_df = transformed_df
                    execution_log.append(f"âœ… ë‹¨ê³„ {step_num}: {step['description']} ì™„ë£Œ")
                    logger.info(f"ë³€í™˜ ë‹¨ê³„ {step_num} ì„±ê³µ")
                else:
                    execution_log.append(f"âš ï¸ ë‹¨ê³„ {step_num}: {step['description']} ë¶€ë¶„ ì‹¤í–‰ - {validation['issues']}")
                    logger.warning(f"ë³€í™˜ ë‹¨ê³„ {step_num} ê²½ê³ : {validation['issues']}")
                
            except Exception as e:
                execution_log.append(f"âŒ ë‹¨ê³„ {step_num}: {step['description']} ì‹¤íŒ¨ - {str(e)}")
                logger.error(f"ë³€í™˜ ë‹¨ê³„ {step_num} ì‹¤íŒ¨: {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ í•´ë‹¹ ë‹¨ê³„ëŠ” ê±´ë„ˆë›°ê³  ê³„ì† ì§„í–‰
                continue
        
        # ë³€í™˜ëœ SmartDataFrame ìƒì„±
        transformed_metadata = smart_df.metadata.copy()
        transformed_metadata.update({
            'transformation_timestamp': datetime.now().isoformat(),
            'transformation_log': execution_log,
            'original_shape': smart_df.shape,
            'transformed_shape': current_df.shape,
            'transformation_plan': transformation_plan
        })
        
        return SmartDataFrame(current_df, transformed_metadata)
    
    async def _execute_single_transformation(self, df: pd.DataFrame, step: Dict) -> pd.DataFrame:
        """ë‹¨ì¼ ë³€í™˜ ë‹¨ê³„ ì‹¤í–‰"""
        operation = step['specific_operation']
        target_columns = step.get('target_columns', [])
        parameters = step.get('parameters', {})
        
        result_df = df.copy()
        
        try:
            # ê¸°ë³¸ì ì¸ ë³€í™˜ ì‘ì—…ë“¤ êµ¬í˜„
            if operation == 'basic_optimization':
                # ë°ì´í„° íƒ€ì… ìµœì í™”
                for col in result_df.columns:
                    if result_df[col].dtype == 'object':
                        # ìˆ«ìë¡œ ë³€í™˜ ê°€ëŠ¥í•œì§€ í™•ì¸
                        try:
                            numeric_series = pd.to_numeric(result_df[col], errors='coerce')
                            if not numeric_series.isna().all():
                                result_df[col] = numeric_series
                        except:
                            pass
                    elif result_df[col].dtype in ['int64', 'float64']:
                        # ë‹¤ìš´ìºìŠ¤íŒ…
                        result_df[col] = pd.to_numeric(result_df[col], downcast='integer' if 'int' in str(result_df[col].dtype) else 'float')
            
            elif operation == 'remove_duplicates':
                result_df = result_df.drop_duplicates()
            
            elif operation == 'drop_columns' and target_columns:
                valid_columns = [col for col in target_columns if col in result_df.columns]
                if valid_columns:
                    result_df = result_df.drop(columns=valid_columns)
            
            elif operation == 'rename_columns' and parameters.get('column_mapping'):
                column_mapping = parameters['column_mapping']
                result_df = result_df.rename(columns=column_mapping)
            
            elif operation == 'filter_rows' and parameters.get('condition'):
                # ê°„ë‹¨í•œ í•„í„°ë§ (ì•ˆì „í•œ eval ì‚¬ìš©)
                condition = parameters['condition']
                if 'query' in dir(result_df):
                    try:
                        result_df = result_df.query(condition)
                    except:
                        # ì¿¼ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìœ ì§€
                        pass
            
            elif operation == 'fill_missing_values':
                if target_columns:
                    for col in target_columns:
                        if col in result_df.columns:
                            fill_method = parameters.get('method', 'forward')
                            if fill_method == 'forward':
                                result_df[col] = result_df[col].fillna(method='ffill')
                            elif fill_method == 'mean' and result_df[col].dtype in ['int64', 'float64']:
                                result_df[col] = result_df[col].fillna(result_df[col].mean())
            
            # ë” ë§ì€ ë³€í™˜ ì‘ì—…ë“¤ì„ ì—¬ê¸°ì— ì¶”ê°€í•  ìˆ˜ ìˆìŒ
            
        except Exception as e:
            logger.warning(f"ë³€í™˜ ì‘ì—… ì‹¤íŒ¨ ({operation}): {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            return df
        
        return result_df
    
    async def _validate_transformation_step(self, original_df: pd.DataFrame, transformed_df: pd.DataFrame, step: Dict) -> ValidationResult:
        """ë³€í™˜ ë‹¨ê³„ ê²€ì¦"""
        issues = []
        warnings = []
        recommendations = []
        
        try:
            # 1. ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
            if transformed_df.empty and not original_df.empty:
                issues.append("ë³€í™˜ í›„ ë°ì´í„°ê°€ ë¹„ì–´ì§")
            
            # 2. ë°ì´í„° ì†ì‹¤ë¥  ê²€ì¦
            data_loss_rate = (original_df.shape[0] - transformed_df.shape[0]) / original_df.shape[0] * 100
            if data_loss_rate > self.safety_thresholds['max_data_loss_percentage']:
                issues.append(f"ê³¼ë„í•œ ë°ì´í„° ì†ì‹¤: {data_loss_rate:.1f}%")
            elif data_loss_rate > 5:
                warnings.append(f"ë°ì´í„° ì†ì‹¤ ê°ì§€: {data_loss_rate:.1f}%")
            
            # 3. ì»¬ëŸ¼ ë³€í™” ê²€ì¦
            original_columns = set(original_df.columns)
            transformed_columns = set(transformed_df.columns)
            
            removed_columns = original_columns - transformed_columns
            added_columns = transformed_columns - original_columns
            
            if removed_columns and step.get('operation_category') != 'column_operations':
                warnings.append(f"ì˜ˆìƒì¹˜ ëª»í•œ ì»¬ëŸ¼ ì œê±°: {list(removed_columns)}")
            
            if added_columns:
                recommendations.append(f"ìƒˆ ì»¬ëŸ¼ ì¶”ê°€ë¨: {list(added_columns)}")
            
            # 4. ë°ì´í„° íƒ€ì… ì¼ê´€ì„± ê²€ì¦
            for col in original_columns.intersection(transformed_columns):
                if original_df[col].dtype != transformed_df[col].dtype:
                    recommendations.append(f"ì»¬ëŸ¼ {col} íƒ€ì… ë³€ê²½: {original_df[col].dtype} â†’ {transformed_df[col].dtype}")
            
            # ê²€ì¦ ì ìˆ˜ ê³„ì‚°
            validation_score = 1.0
            validation_score -= len(issues) * 0.4
            validation_score -= len(warnings) * 0.2
            validation_score = max(0.0, validation_score)
            
            return ValidationResult(
                is_valid=len(issues) == 0,
                validation_score=validation_score,
                issues=issues,
                warnings=warnings,
                recommendations=recommendations
            )
            
        except Exception as e:
            return ValidationResult(
                is_valid=False,
                validation_score=0.0,
                issues=[f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {str(e)}"],
                warnings=[],
                recommendations=["ë³€í™˜ ê²°ê³¼ ìˆ˜ë™ í™•ì¸ í•„ìš”"]
            )
    
    async def _finalize_wrangling_results(self, original_df: SmartDataFrame, transformed_df: SmartDataFrame, 
                                        transformation_plan: Dict, backup_info: Dict, task_updater: TaskUpdater) -> Dict[str, Any]:
        """ë°ì´í„° ë³€í™˜ ê²°ê³¼ ìµœì¢…í™”"""
        
        # ë³€í™˜ í’ˆì§ˆ í‰ê°€
        original_quality = await self._comprehensive_data_profiling(original_df)
        transformed_quality = await self._comprehensive_data_profiling(transformed_df)
        
        quality_change = transformed_quality['quality_score'] - original_quality['quality_score']
        data_preservation_rate = transformed_df.shape[0] / original_df.shape[0]
        
        # ë³€í™˜ëœ ë°ì´í„° ì €ì¥
        save_dir = Path("a2a_ds_servers/artifacts/data/transformed_dataframes")
        save_dir.mkdir(exist_ok=True, parents=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"transformed_data_{timestamp}.csv"
        save_path = save_dir / filename
        
        transformed_df.data.to_csv(save_path, index=False, encoding='utf-8')
        
        return {
            'original_shape': original_df.shape,
            'transformed_shape': transformed_df.shape,
            'quality_change': round(quality_change, 3),
            'data_preservation_rate': data_preservation_rate,
            'saved_path': str(save_path),
            'backup_info': backup_info,
            'rollback_available': backup_info.get('rollback_enabled', False),
            'transformation_log': transformed_df.metadata.get('transformation_log', []),
            'execution_summary': {
                'total_steps': len(transformation_plan.get('steps', [])),
                'successful_steps': len([log for log in transformed_df.metadata.get('transformation_log', []) if 'âœ…' in log]),
                'failed_steps': len([log for log in transformed_df.metadata.get('transformation_log', []) if 'âŒ' in log]),
                'warning_steps': len([log for log in transformed_df.metadata.get('transformation_log', []) if 'âš ï¸' in log])
            }
        }
    
    async def _create_wrangling_artifacts(self, results: Dict[str, Any], task_updater: TaskUpdater) -> None:
        """ë°ì´í„° ë³€í™˜ ì•„í‹°íŒ©íŠ¸ ìƒì„±"""
        
        # ë³€í™˜ ë³´ê³ ì„œ ìƒì„±
        report = {
            'data_wrangling_report': {
                'timestamp': datetime.now().isoformat(),
                'transformation_summary': {
                    'original_data': {
                        'shape': results['original_shape'],
                    },
                    'transformed_data': {
                        'shape': results['transformed_shape'],
                        'saved_path': results['saved_path']
                    },
                    'quality_metrics': {
                        'quality_change': results['quality_change'],
                        'data_preservation_rate': results['data_preservation_rate']
                    }
                },
                'execution_details': results['execution_summary'],
                'transformation_log': results['transformation_log'],
                'backup_information': {
                    'backup_created': results['backup_info'].get('backup_created', False),
                    'rollback_available': results['rollback_available'],
                    'backup_path': results['backup_info'].get('backup_path')
                }
            }
        }
        
        # A2A ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
        await task_updater.add_artifact(
            parts=[TextPart(text=json.dumps(report, indent=2, ensure_ascii=False))],
            name="data_wrangling_report",
            metadata={"content_type": "application/json", "category": "data_transformation"}
        )
        
        logger.info("âœ… ë°ì´í„° ë³€í™˜ ì•„í‹°íŒ©íŠ¸ ìƒì„± ì™„ë£Œ")
    
    def _extract_user_query(self, context: RequestContext) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ (A2A í‘œì¤€)"""
        user_query = ""
        if context.message and context.message.parts:
            for part in context.message.parts:
                if hasattr(part, 'root') and hasattr(part.root, 'text'):
                    user_query += part.root.text + " "
        return user_query.strip() or "ë°ì´í„°ë¥¼ ë³€í™˜í•´ì£¼ì„¸ìš”"
    
    async def cancel(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        await task_updater.reject()
        logger.info(f"Data Wrangling ì‘ì—… ì·¨ì†Œë¨: {context.context_id}")

# A2A ì„œë²„ ì„¤ì •
def create_data_wrangling_agent_card() -> AgentCard:
    """Data Wrangling Agent Card ìƒì„±"""
    return AgentCard(
        name="Unified Data Wrangling Agent",
        description="ğŸ”§ LLM First ì§€ëŠ¥í˜• ë°ì´í„° ë³€í™˜ ì „ë¬¸ê°€ - ì•ˆì •ì  íŒŒì¼ ì„ íƒ, ì•ˆì „í•œ ë³€í™˜ ë° ê²€ì¦, A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜",
        skills=[
            AgentSkill(
                name="intelligent_transformation_planning",
                description="LLM ê¸°ë°˜ ë°ì´í„° ë³€í™˜ ì „ëµ ë¶„ì„ ë° ê³„íš ìˆ˜ë¦½"
            ),
            AgentSkill(
                name="stable_file_selection", 
                description="ì•ˆì •ì„± ì ìˆ˜ ê¸°ë°˜ íŒŒì¼ ì„ íƒ (ì£¼ìš” ë¬¸ì œì  í•´ê²°)"
            ),
            AgentSkill(
                name="safe_data_transformation",
                description="ë°±ì—… ë° ë¡¤ë°±ì´ í¬í•¨ëœ ì•ˆì „í•œ ë°ì´í„° ë³€í™˜"
            ),
            AgentSkill(
                name="transformation_validation",
                description="ë³€í™˜ ë‹¨ê³„ë³„ ê²€ì¦ ë° í’ˆì§ˆ ë³´ì¥"
            ),
            AgentSkill(
                name="column_operations",
                description="ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½, ì¶”ê°€, ì‚­ì œ, ì¬ë°°ì—´"
            ),
            AgentSkill(
                name="data_type_conversion",
                description="ë°ì´í„° íƒ€ì… ìµœì í™” ë° ë³€í™˜"
            ),
            AgentSkill(
                name="filtering_and_aggregation",
                description="ë°ì´í„° í•„í„°ë§, ê·¸ë£¹í™”, ì§‘ê³„ ì‘ì—…"
            ),
            AgentSkill(
                name="backup_and_rollback",
                description="ìë™ ë°±ì—… ìƒì„± ë° ë¡¤ë°± ê¸°ëŠ¥"
            )
        ],
        capabilities=AgentCapabilities(
            supports_streaming=True,
            supports_artifacts=True,
            max_execution_time=300,
            supported_formats=["csv", "excel", "json", "parquet"]
        )
    )

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # A2A ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    task_store = InMemoryTaskStore()
    executor = UnifiedDataWranglingExecutor()
    agent_card = create_data_wrangling_agent_card()
    
    request_handler = DefaultRequestHandler(
        agent_card=agent_card,
        task_store=task_store,
        agent_executor=executor
    )
    
    app = A2AStarletteApplication(request_handler=request_handler)
    
    # ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Unified Data Wrangling Server ì‹œì‘ - Port 8309")
    logger.info("ğŸ”§ ê¸°ëŠ¥: LLM First ë°ì´í„° ë³€í™˜ + ì•ˆì •ì  íŒŒì¼ ì„ íƒ")
    logger.info("ğŸ¯ A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜")
    
    uvicorn.run(app, host="0.0.0.0", port=8309) 