#!/usr/bin/env python3
"""
CherryAI Python REPL Agent - LangGraph ê¸°ë°˜ ì½”ë“œ ì‹¤í–‰ ì—ì´ì „íŠ¸
A2A SDK v0.2.9 í‘œì¤€ ì¤€ìˆ˜ + ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° + ì‹¤ì‹œê°„ ì½”ë“œ ì‹¤í–‰
"""

import asyncio
import json
import logging
import os
import sys
import traceback
import uuid
from datetime import datetime
from io import StringIO
from typing import Any, Dict, List, Optional, Union
import subprocess
import tempfile
import matplotlib
matplotlib.use('Agg')  # GUI ë°±ì—”ë“œ ì‚¬ìš© ì•ˆí•¨

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.offline import plot
import uvicorn
from openai import AsyncOpenAI

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart,
    Part
)
from a2a.utils import new_agent_text_message, new_task

# LangGraph ë¼ì´ë¸ŒëŸ¬ë¦¬ (ì„¤ì¹˜ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)
try:
    from langgraph.graph import StateGraph, END
    from langgraph.checkpoint.memory import MemorySaver
    from typing_extensions import TypedDict
    LANGGRAPH_AVAILABLE = True
except ImportError:
    # LangGraphê°€ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ êµ¬í˜„ ì‚¬ìš©
    LANGGRAPH_AVAILABLE = False
    from typing import Dict as TypedDict

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class CodeExecutionState(TypedDict):
    """ì½”ë“œ ì‹¤í–‰ ìƒíƒœ ê´€ë¦¬"""
    user_request: str
    generated_code: str
    execution_result: str
    execution_status: str
    artifacts: List[Dict]
    context_variables: Dict[str, Any]
    error_message: Optional[str]
    step_history: List[Dict]


class SafeCodeExecutor:
    """ì•ˆì „í•œ ì½”ë“œ ì‹¤í–‰ í™˜ê²½"""
    
    def __init__(self):
        self.allowed_modules = {
            'pandas', 'numpy', 'matplotlib', 'seaborn', 'plotly', 
            'scipy', 'sklearn', 'statsmodels', 'json', 'csv',
            'datetime', 'math', 'statistics', 're', 'os', 'sys'
        }
        self.execution_context = {
            'pd': pd,
            'np': np,
            'plt': plt,
            'sns': sns,
            'go': go,
            'px': px,
            'json': json
        }
        self.output_buffer = StringIO()
        self.artifacts = []
    
    def validate_code(self, code: str) -> bool:
        """ì½”ë“œ ì•ˆì „ì„± ê²€ì¦"""
        dangerous_patterns = [
            'import os', '__import__', 'eval(', 'exec(',
            'open(', 'file(', 'input(', 'raw_input(',
            'subprocess', 'system', 'popen', 'getattr',
            'setattr', 'delattr', 'globals(', 'locals(',
            'dir(', 'vars(', '__builtins__'
        ]
        
        # í—ˆìš©ëœ importë§Œ ì²´í¬
        lines = code.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('import ') or line.startswith('from '):
                # í—ˆìš©ëœ ëª¨ë“ˆì¸ì§€ í™•ì¸
                if not any(module in line for module in self.allowed_modules):
                    if not any(safe in line for safe in ['pandas', 'numpy', 'matplotlib', 'seaborn', 'plotly']):
                        return False
            
            # ìœ„í—˜í•œ íŒ¨í„´ ì²´í¬
            for pattern in dangerous_patterns:
                if pattern in line:
                    return False
        
        return True
    
    async def execute_code_chunk(self, code: str, context_vars: Dict = None) -> Dict:
        """ì½”ë“œ ì²­í¬ ì‹¤í–‰"""
        try:
            if not self.validate_code(code):
                return {
                    'status': 'error',
                    'output': '',
                    'error': 'ì•ˆì „í•˜ì§€ ì•Šì€ ì½”ë“œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.',
                    'artifacts': []
                }
        
            # ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            if context_vars:
                self.execution_context.update(context_vars)
        
            # stdout ìº¡ì²˜ ì„¤ì •
            old_stdout = sys.stdout
            old_stderr = sys.stderr
        
            captured_output = StringIO()
            sys.stdout = captured_output
            sys.stderr = captured_output
        
            try:
                # ì½”ë“œ ì‹¤í–‰
                exec(code, self.execution_context)
                output = captured_output.getvalue()
        
                # ì•„í‹°íŒ©íŠ¸ ìƒì„± (ê·¸ë˜í”„, ë°ì´í„° ë“±)
                artifacts = self._collect_artifacts()
        
                return {
                    'status': 'success',
                    'output': output,
                    'error': None,
                    'artifacts': artifacts,
                    'context_updated': True
                }
        
            except Exception as e:
                error_output = captured_output.getvalue()
                error_msg = f"{str(e)}\n{traceback.format_exc()}"
        
                return {
                    'status': 'error',
                    'output': error_output,
                    'error': error_msg,
                    'artifacts': [],
                    'context_updated': False
                }
        
            finally:
                sys.stdout = old_stdout
                sys.stderr = old_stderr
        
        except Exception as e:
            return {
                'status': 'error',
                'output': '',
                'error': f'ì½”ë“œ ì‹¤í–‰ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}',
                'artifacts': [],
                'context_updated': False
            }
    
    def _collect_artifacts(self) -> List[Dict]:
        """ì‹¤í–‰ ê²°ê³¼ ì•„í‹°íŒ©íŠ¸ ìˆ˜ì§‘"""
        artifacts = []
    
        # Matplotlib ê·¸ë˜í”„ ìˆ˜ì§‘
        if plt.get_fignums():
            for fig_num in plt.get_fignums():
                fig = plt.figure(fig_num)
        
                # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                temp_file = tempfile.NamedTemporaryFile(suffix='.png', delete=False)
                fig.savefig(temp_file.name, dpi=150, bbox_inches='tight')
        
                artifacts.append({
                    'type': 'matplotlib_plot',
                    'file_path': temp_file.name,
                    'figure_number': fig_num,
                    'timestamp': datetime.now().isoformat()
                })
        
                plt.close(fig)
    
        # DataFrame ê²°ê³¼ ìˆ˜ì§‘
        for var_name, var_value in self.execution_context.items():
            if isinstance(var_value, pd.DataFrame) and not var_name.startswith('_'):
                artifacts.append({
                    'type': 'dataframe',
                    'name': var_name,
                    'shape': var_value.shape,
                    'columns': list(var_value.columns),
                    'head': var_value.head().to_dict(),
                    'timestamp': datetime.now().isoformat()
                })
    
        return artifacts
    
    def get_context_summary(self) -> Dict:
        """ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸ ìš”ì•½"""
        summary = {
            'variables': [],
            'dataframes': [],
            'functions': []
        }
    
        for name, value in self.execution_context.items():
            if not name.startswith('_'):
                if isinstance(value, pd.DataFrame):
                    summary['dataframes'].append({
                        'name': name,
                        'type': 'DataFrame',
                        'shape': value.shape
                    })
                elif callable(value) and hasattr(value, '__name__'):
                    summary['functions'].append({
                        'name': name,
                        'type': 'function'
                    })
                else:
                    summary['variables'].append({
                        'name': name,
                        'type': type(value).__name__
                    })
       
        return summary


class LangGraphCodeInterpreter:
    """LangGraph ê¸°ë°˜ ì½”ë“œ í•´ì„ê¸°"""
    
    def __init__(self, openai_client: Optional[AsyncOpenAI] = None):
        self.openai_client = openai_client
        self.code_executor = SafeCodeExecutor()
        self.memory_saver = MemorySaver() if LANGGRAPH_AVAILABLE else None
        self.graph = self._build_graph() if LANGGRAPH_AVAILABLE else None
    
    def _build_graph(self) -> Optional[StateGraph]:
        """LangGraph ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""
        if not LANGGRAPH_AVAILABLE:
            return None
    
        workflow = StateGraph(CodeExecutionState)
    
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_request", self._analyze_request)
        workflow.add_node("generate_code", self._generate_code)
        workflow.add_node("execute_code", self._execute_code)
        workflow.add_node("validate_result", self._validate_result)
        workflow.add_node("create_response", self._create_response)
    
        # ì—£ì§€ ì¶”ê°€
        workflow.set_entry_point("analyze_request")
        workflow.add_edge("analyze_request", "generate_code")
        workflow.add_edge("generate_code", "execute_code")
        workflow.add_edge("execute_code", "validate_result")
        workflow.add_edge("validate_result", "create_response")
        workflow.add_edge("create_response", END)
    
        return workflow.compile(checkpointer=self.memory_saver)
    
    async def _analyze_request(self, state: CodeExecutionState) -> CodeExecutionState:
        """ì‚¬ìš©ì ìš”ì²­ ë¶„ì„"""
        user_request = state["user_request"]
    
        # ìš”ì²­ ë¶„ì„ ë¡œê¹…
        logger.info(f"ğŸ“ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„: {user_request[:100]}...")
    
        state["step_history"].append({
            "step": "analyze_request",
            "description": "ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ í•„ìš”í•œ ì½”ë“œ ìœ í˜•ì„ íŒŒì•…",
            "timestamp": datetime.now().isoformat()
        })
    
        return state
    
    async def _generate_code(self, state: CodeExecutionState) -> CodeExecutionState:
        """ì½”ë“œ ìƒì„±"""
        user_request = state["user_request"]
    
        try:
            if self.openai_client:
                code = await self._generate_code_with_llm(user_request)
            else:
                code = self._generate_basic_code(user_request)
    
            state["generated_code"] = code
            state["step_history"].append({
                "step": "generate_code", 
                "description": "ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” Python ì½”ë“œ ìƒì„±",
                "timestamp": datetime.now().isoformat(),
                "code_preview": code[:200] + "..." if len(code) > 200 else code
            })
    
        except Exception as e:
            state["error_message"] = f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            state["execution_status"] = "failed"
    
        return state
    
    async def _execute_code(self, state: CodeExecutionState) -> CodeExecutionState:
        """ì½”ë“œ ì‹¤í–‰"""
        code = state["generated_code"]
    
        try:
            result = await self.code_executor.execute_code_chunk(
                code, state["context_variables"]
            )
    
            state["execution_result"] = result["output"]
            state["execution_status"] = result["status"]
            state["artifacts"].extend(result["artifacts"])
    
            if result["error"]:
                state["error_message"] = result["error"]
    
            state["step_history"].append({
                "step": "execute_code",
                "description": "ìƒì„±ëœ ì½”ë“œë¥¼ ì•ˆì „í•œ í™˜ê²½ì—ì„œ ì‹¤í–‰",
                "timestamp": datetime.now().isoformat(),
                "status": result["status"],
                "artifacts_count": len(result["artifacts"])
            })
    
        except Exception as e:
            state["execution_status"] = "failed"
            state["error_message"] = f"ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
    
        return state
    
    async def _validate_result(self, state: CodeExecutionState) -> CodeExecutionState:
        """ì‹¤í–‰ ê²°ê³¼ ê²€ì¦"""
        if state["execution_status"] == "success" and state["execution_result"]:
            validation_status = "validated"
        elif state["artifacts"]:
            validation_status = "partial_success"  # ì¶œë ¥ì€ ì—†ì§€ë§Œ ì•„í‹°íŒ©íŠ¸ê°€ ìƒì„±ë¨
        else:
            validation_status = "failed"
    
        state["step_history"].append({
            "step": "validate_result",
            "description": "ì‹¤í–‰ ê²°ê³¼ì˜ ìœ íš¨ì„± ê²€ì¦",
            "timestamp": datetime.now().isoformat(),
            "validation_status": validation_status
        })
    
        return state
    
    async def _create_response(self, state: CodeExecutionState) -> CodeExecutionState:
        """ìµœì¢… ì‘ë‹µ ìƒì„±"""
        if self.openai_client and state["execution_status"] == "success":
            response = await self._create_intelligent_response(state)
        else:
            response = self._create_basic_response(state)
    
        state["final_response"] = response
        state["step_history"].append({
            "step": "create_response",
            "description": "ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì‘ë‹µ ìƒì„±",
            "timestamp": datetime.now().isoformat()
        })
    
        return state
    
    async def _generate_code_with_llm(self, user_request: str) -> str:
        """LLMì„ í™œìš©í•œ ì½”ë“œ ìƒì„±"""
        code_generation_prompt = f"""
ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì— ë§ëŠ” Python ì½”ë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ìš”ì²­**: {user_request}

**ì‚¬ìš© ê°€ëŠ¥í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬**: pandas, numpy, matplotlib, seaborn, plotly
**ì½”ë“œ ìš”êµ¬ì‚¬í•­**:
1. ì•ˆì „í•˜ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ
2. ê²°ê³¼ë¥¼ ëª…í™•íˆ ì¶œë ¥í•˜ëŠ” ì½”ë“œ
3. í•„ìš”ì‹œ ì‹œê°í™” í¬í•¨
4. ì£¼ì„ìœ¼ë¡œ ì„¤ëª… ì¶”ê°€

**ì‘ë‹µ í˜•ì‹**: Python ì½”ë“œë§Œ ë°˜í™˜ (```python íƒœê·¸ ì—†ì´)
"""
    
        response = await self.openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ë¬¸ Python ê°œë°œìì´ë©° ë°ì´í„° ë¶„ì„ ì½”ë“œ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                {"role": "user", "content": code_generation_prompt}
            ],
            temperature=0.3,
            max_tokens=2000
        )
    
        return response.choices[0].message.content.strip()
    
    def _generate_basic_code(self, user_request: str) -> str:
        """ê¸°ë³¸ ì½”ë“œ ìƒì„± (LLM ì—†ì„ ë•Œ)"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê¸°ë³¸ ì½”ë“œ ìƒì„±
        if "ì‹œê°í™”" in user_request or "ê·¸ë˜í”„" in user_request or "plot" in user_request:
            return """
# ìƒ˜í”Œ ë°ì´í„° ìƒì„± ë° ì‹œê°í™”
import matplotlib.pyplot as plt
import numpy as np

# ìƒ˜í”Œ ë°ì´í„°
x = np.linspace(0, 10, 100)
y = np.sin(x)

# ê·¸ë˜í”„ ìƒì„±
plt.figure(figsize=(10, 6))
plt.plot(x, y, label='sin(x)')
plt.title('ìƒ˜í”Œ ë°ì´í„° ì‹œê°í™”')
plt.xlabel('X')
plt.ylabel('Y')
plt.legend()
plt.grid(True)
plt.show()

print("ì‹œê°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
"""
        elif "ë°ì´í„°" in user_request or "ë¶„ì„" in user_request:
            return """
# ìƒ˜í”Œ ë°ì´í„° ë¶„ì„
import pandas as pd
import numpy as np

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
data = {
    'A': np.random.randn(100),
    'B': np.random.randn(100),
    'C': np.random.choice(['X', 'Y', 'Z'], 100)
}
df = pd.DataFrame(data)

# ê¸°ë³¸ ë¶„ì„
print("ë°ì´í„° í˜•íƒœ:", df.shape)
print("\nê¸°ë³¸ í†µê³„:")
print(df.describe())
print("\në²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬:")
print(df['C'].value_counts())

print("\në°ì´í„° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
"""
        else:
            return f"""
# ì‚¬ìš©ì ìš”ì²­: {user_request}
print("ì•ˆë…•í•˜ì„¸ìš”! Python REPL ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤.")
print("ìš”ì²­: {user_request}")
print("ë” êµ¬ì²´ì ì¸ ìš”ì²­ì„ í•´ì£¼ì‹œë©´ ë§ì¶¤í˜• ì½”ë“œë¥¼ ìƒì„±í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.")
"""
    
    async def _create_intelligent_response(self, state: CodeExecutionState) -> str:
        """ì§€ëŠ¥í˜• ì‘ë‹µ ìƒì„±"""
        response_prompt = f"""
Python ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•  ì‘ë‹µì„ ìƒì„±í•´ì£¼ì„¸ìš”.

**ì‚¬ìš©ì ìš”ì²­**: {state['user_request']}
**ì‹¤í–‰ëœ ì½”ë“œ**: {state['generated_code']}
**ì‹¤í–‰ ê²°ê³¼**: {state['execution_result']}
**ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸**: {len(state['artifacts'])}ê°œ

**ì‘ë‹µ ìš”êµ¬ì‚¬í•­**:
1. ì‹¤í–‰ ê²°ê³¼ ìš”ì•½
2. ìƒì„±ëœ ì‹œê°í™”ë‚˜ ë°ì´í„° ì„¤ëª…
3. ì¶”ê°€ ë¶„ì„ ì œì•ˆ
4. ì‚¬ìš©ì ì¹œí™”ì  ì„¤ëª…

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

# ğŸ Python ì½”ë“œ ì‹¤í–‰ ê²°ê³¼

## ğŸ“Š ì‹¤í–‰ ìš”ì•½
(ì‹¤í–‰ëœ ì‘ì—… ì„¤ëª…)

## ğŸ“ˆ ê²°ê³¼ ë¶„ì„
(êµ¬ì²´ì ì¸ ê²°ê³¼ í•´ì„)

## ğŸ’¡ ì¶”ê°€ ì œì•ˆ
(ì¶”ê°€ ë¶„ì„ì´ë‚˜ ê°œì„  ì œì•ˆ)
"""
    
        response = await self.openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ì „ë¬¸ì ì¸ Python ë°ì´í„° ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": response_prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )
    
        return response.choices[0].message.content
    
    def _create_basic_response(self, state: CodeExecutionState) -> str:
        """ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        if state["execution_status"] == "success":
            response = f"""# ğŸ Python ì½”ë“œ ì‹¤í–‰ ê²°ê³¼

## ğŸ“Š ì‹¤í–‰ ìš”ì•½
ì‚¬ìš©ì ìš”ì²­ '{state['user_request']}'ì— ëŒ€í•œ Python ì½”ë“œê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

## ğŸ“ˆ ê²°ê³¼ ë¶„ì„
**ì‹¤í–‰ ê²°ê³¼:**
```
{state['execution_result']}
```

**ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸:** {len(state['artifacts'])}ê°œ
{self._format_artifacts(state['artifacts'])}

## ğŸ’¡ ì¶”ê°€ ì œì•ˆ
- ì¶”ê°€ì ì¸ ë¶„ì„ì´ë‚˜ ì‹œê°í™”ê°€ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“ ì§€ ìš”ì²­í•´ì£¼ì„¸ìš”
- ì½”ë“œ ìˆ˜ì •ì´ë‚˜ ê°œì„ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì•Œë ¤ì£¼ì„¸ìš”
"""
        else:
            response = f"""# ğŸ Python ì½”ë“œ ì‹¤í–‰ ê²°ê³¼

## âš ï¸ ì‹¤í–‰ ì˜¤ë¥˜
ì½”ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ì˜¤ë¥˜ ë©”ì‹œì§€:**
```
{state.get('error_message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}
```

## ğŸ”§ í•´ê²° ë°©ì•ˆ
1. ì½”ë“œ ë¬¸ë²•ì„ í™•ì¸í•´ì£¼ì„¸ìš”
2. ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì§€ì›ë˜ëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”
3. ë” êµ¬ì²´ì ì¸ ìš”ì²­ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”
"""
       
        return response
    
    def _format_artifacts(self, artifacts: List[Dict]) -> str:
        """ì•„í‹°íŒ©íŠ¸ í¬ë§·íŒ…"""
        if not artifacts:
            return ""
    
        formatted = "\n\n**ìƒì„±ëœ ê²°ê³¼ë¬¼:**\n"
        for i, artifact in enumerate(artifacts, 1):
            if artifact['type'] == 'matplotlib_plot':
                formatted += f"- ğŸ“Š ì‹œê°í™” {i}: Matplotlib ê·¸ë˜í”„\n"
            elif artifact['type'] == 'dataframe':
                formatted += f"- ğŸ“‹ ë°ì´í„°í”„ë ˆì„ {i}: {artifact['name']} (í˜•íƒœ: {artifact['shape']})\n"
            else:
                formatted += f"- ğŸ“„ ê²°ê³¼ë¬¼ {i}: {artifact['type']}\n"
    
        return formatted
    
    async def process_request(self, user_request: str) -> Dict:
        """ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬"""
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state = {
            "user_request": user_request,
            "generated_code": "",
            "execution_result": "",
            "execution_status": "pending",
            "artifacts": [],
            "context_variables": {},
            "error_message": None,
            "step_history": [],
            "final_response": ""
        }
    
        if self.graph and LANGGRAPH_AVAILABLE:
            # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            thread_id = str(uuid.uuid4())
            config = {"configurable": {"thread_id": thread_id}}
    
            final_state = await self.graph.ainvoke(
                initial_state, 
                config=config
            )
        else:
            # ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            final_state = await self._execute_basic_workflow(initial_state)
    
        return final_state
    
    async def _execute_basic_workflow(self, state: CodeExecutionState) -> CodeExecutionState:
        """ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ (LangGraph ì—†ì„ ë•Œ)"""
        try:
            # 1. ìš”ì²­ ë¶„ì„
            state = await self._analyze_request(state)
    
            # 2. ì½”ë“œ ìƒì„±
            state = await self._generate_code(state)
    
            # 3. ì½”ë“œ ì‹¤í–‰
            state = await self._execute_code(state)
    
            # 4. ê²°ê³¼ ê²€ì¦
            state = await self._validate_result(state)
    
            # 5. ì‘ë‹µ ìƒì„±
            state = await self._create_response(state)
    
        except Exception as e:
            state["execution_status"] = "failed"
            state["error_message"] = f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
            state["final_response"] = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
        return state


class CherryAI_PythonREPL_Agent(AgentExecutor):
    """CherryAI Python REPL ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        super().__init__()
        self.openai_client = self._initialize_openai_client()
        self.code_interpreter = LangGraphCodeInterpreter(self.openai_client)
        logger.info("CherryAI Python REPL Agent ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _initialize_openai_client(self) -> Optional[AsyncOpenAI]:
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                logger.warning("âš ï¸ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")
                return None
            return AsyncOpenAI(api_key=api_key)
        except Exception as e:
            logger.error(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """A2A í‘œì¤€ í”„ë¡œí† ì½œ ê¸°ë°˜ ì‹¤í–‰"""
        updater = TaskUpdater(event_queue, context.task_id, context.context_id)
    
        try:
            # ì²« ë²ˆì§¸ íƒœìŠ¤í¬ì¸ ê²½ìš° submit í˜¸ì¶œ
            if not context.current_task:
                await updater.submit()
    
            # ì‘ì—… ì‹œì‘
            await updater.start_work()
    
            # ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ
            user_input = self._extract_user_input(context)
            logger.info(f"ğŸ Python REPL ìš”ì²­: {user_input[:100]}...")
    
            # ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
            await updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ” ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            )
    
            # LangGraph ì½”ë“œ í•´ì„ê¸°ë¡œ ì²˜ë¦¬
            result = await self.code_interpreter.process_request(user_input)
    
            # ì§„í–‰ ìƒí™© ìŠ¤íŠ¸ë¦¬ë°
            for step in result.get("step_history", []):
                await updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message(f"âš¡ {step['description']}")
                )
                await asyncio.sleep(0.5)  # ì‹œê°ì  íš¨ê³¼
    
            # ì½”ë“œ ì‹¤í–‰ ê²°ê³¼ ì „ì†¡
            await updater.update_status(
                TaskState.working,
                message=new_agent_text_message("âœ… Python ì½”ë“œ ì‹¤í–‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            )
    
            # ì•„í‹°íŒ©íŠ¸ ì „ì†¡
            artifacts = []
    
            # ìƒì„±ëœ ì½”ë“œ ì•„í‹°íŒ©íŠ¸
            if result.get("generated_code"):
                artifacts.append(TextPart(text=result["generated_code"]))
    
            # ì‹¤í–‰ ê²°ê³¼ ì•„í‹°íŒ©íŠ¸
            if result.get("execution_result"):
                artifacts.append(TextPart(text=f"ì‹¤í–‰ ê²°ê³¼:\n{result['execution_result']}"))
    
            # ìµœì¢… ì‘ë‹µ ì•„í‹°íŒ©íŠ¸
            if result.get("final_response"):
                artifacts.append(TextPart(text=result["final_response"]))
    
            await updater.add_artifact(
                artifacts,
                name="python_repl_execution",
                metadata={
                    "execution_status": result.get("execution_status", "unknown"),
                    "artifacts_count": len(result.get("artifacts", [])),
                    "step_count": len(result.get("step_history", [])),
                    "has_visualizations": any(a.get("type") == "matplotlib_plot" for a in result.get("artifacts", [])),
                    "code_length": len(result.get("generated_code", "")),
                    "version": "v1.0"
                }
            )
    
            await updater.complete()
    
        except Exception as e:
            logger.error(f"Python REPL ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            await updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            )
            raise
    
    def _extract_user_input(self, context: RequestContext) -> str:
        """ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ"""
        try:
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if hasattr(part.root, 'kind') and part.root.kind == 'text':
                        return part.root.text
                    elif hasattr(part.root, 'type') and part.root.type == 'text':
                        return part.root.text
            return ""
        except Exception as e:
            logger.error(f"ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        logger.info("âŒ Python REPL ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤")
        raise Exception('cancel not supported')


def create_agent_card() -> AgentCard:
    """Python REPL ì—ì´ì „íŠ¸ ì¹´ë“œ ìƒì„±"""
    return AgentCard(
        name="AI_DS_Team PythonREPLAgent",
        description="LangGraph ê¸°ë°˜ Python ì½”ë“œ ì‹¤í–‰ ì—ì´ì „íŠ¸ - ë¹„ë™ê¸° ìŠ¤íŠ¸ë¦¬ë° + ì‹¤ì‹œê°„ ê²°ê³¼ í‘œì‹œ",
        url="http://localhost:8315",
        version="1.0.0",
        provider={
            "organization": "CherryAI Team",
            "url": "https://github.com/CherryAI"
        },
        capabilities=AgentCapabilities(
            streaming=True,
            pushNotifications=False,
            stateTransitionHistory=True
        ),
        defaultInputModes=["text/plain"],
        defaultOutputModes=["text/plain"],
        skills=[
            AgentSkill(
                id="python_code_execution",
                name="Python Code Execution",
                description="ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¥¸ Python ì½”ë“œ ìƒì„±, ì‹¤í–‰, ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”",
                tags=["python", "code_execution", "data_analysis", "visualization", "langgraph"],
                examples=[
                    "ê°„ë‹¨í•œ ë°ì´í„° ë¶„ì„ì„ í•´ì£¼ì„¸ìš”",
                    "ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì£¼ì„¸ìš”",
                    "pandasë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
                    "í†µê³„ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”"
                ],
                inputModes=["text/plain"],
                outputModes=["text/plain"]
            )
        ],
        supportsAuthenticatedExtendedCard=False
    )


async def main():
    """Python REPL ì—ì´ì „íŠ¸ ì„œë²„ ì‹œì‘"""
    try:
        # ì—ì´ì „íŠ¸ ì¹´ë“œ ìƒì„±
        agent_card = create_agent_card()
    
        # íƒœìŠ¤í¬ ìŠ¤í† ì–´ ë° ì‹¤í–‰ì ì´ˆê¸°í™”
        task_store = InMemoryTaskStore()
        agent_executor = CherryAI_PythonREPL_Agent()
    
        # ìš”ì²­ í•¸ë“¤ëŸ¬ ìƒì„±
        request_handler = DefaultRequestHandler(
            agent_executor=agent_executor,
            task_store=task_store,
        )
    
        # A2A ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
        app_builder = A2AStarletteApplication(
            agent_card=agent_card,
            http_handler=request_handler
        )
        app = app_builder.build()
    
        # ì„œë²„ ì‹œì‘
        print("ğŸ CherryAI Python REPL Agent ì‹œì‘")
        print(f"ğŸ“ Agent Card: http://localhost:8315/.well-known/agent.json")
        print("ğŸ›‘ ì¢…ë£Œí•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”")
    
        config = uvicorn.Config(
            app=app,
            host="0.0.0.0",
            port=8315,
            log_level="info"
        )
        server = uvicorn.Server(config)
        await server.serve()
    
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main()) 