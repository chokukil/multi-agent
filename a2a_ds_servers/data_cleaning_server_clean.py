#!/usr/bin/env python3
"""
AI_DS_Team DataCleaningAgent A2A Server (Clean Implementation)
Port: 8316

ë³µì‚¬ëœ ì›ë³¸ ai_data_science_team ëª¨ë“ˆì„ ì‚¬ìš©í•˜ëŠ” ê¹”ë”í•œ êµ¬í˜„
"""

import asyncio
import sys
import os
from pathlib import Path
import json
import logging
import pandas as pd
import numpy as np
import io
from datetime import datetime
from typing import Union, List, Dict

# ë¡œê¹… ì„¤ì • (ê°€ì¥ ë¨¼ì €)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ë£¨íŠ¸ ì´ë™ í›„ ë‹¨ìˆœí™”)
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers.default_request_handler import DefaultRequestHandler
from a2a.server.tasks.inmemory_task_store import InMemoryTaskStore
from a2a.server.tasks.task_updater import TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events.event_queue import EventQueue
from a2a.types import TextPart, TaskState, AgentCard, AgentSkill, AgentCapabilities
from a2a.utils import new_agent_text_message
import uvicorn

# Core imports
from core.data_manager import DataManager
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# AI_DS_Team ì›ë³¸ í•¨ìˆ˜ ì§ì ‘ í¬í•¨ (ìƒëŒ€ import ë¬¸ì œ í•´ê²°)
def get_dataframe_summary(
    dataframes: Union[pd.DataFrame, List[pd.DataFrame], Dict[str, pd.DataFrame]],
    n_sample: int = 30,
    skip_stats: bool = False,
) -> List[str]:
    """
    ì›ë³¸ ai_data_science_teamì˜ get_dataframe_summary í•¨ìˆ˜
    ìƒëŒ€ import ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ì§ì ‘ í¬í•¨
    """
    summaries = []

    # --- Dictionary Case ---
    if isinstance(dataframes, dict):
        for dataset_name, df in dataframes.items():
            summaries.append(_summarize_dataframe(df, dataset_name, n_sample, skip_stats))

    # --- Single DataFrame Case ---
    elif isinstance(dataframes, pd.DataFrame):
        summaries.append(_summarize_dataframe(dataframes, "Single_Dataset", n_sample, skip_stats))

    # --- List of DataFrames Case ---
    elif isinstance(dataframes, list):
        for idx, df in enumerate(dataframes):
            dataset_name = f"Dataset_{idx}"
            summaries.append(_summarize_dataframe(df, dataset_name, n_sample, skip_stats))

    else:
        raise TypeError(
            "Input must be a single DataFrame, a list of DataFrames, or a dictionary of DataFrames."
        )

    return summaries

def _summarize_dataframe(
    df: pd.DataFrame, 
    dataset_name: str, 
    n_sample=30, 
    skip_stats=False
) -> str:
    """Generate a summary string for a single DataFrame."""
    # 1. Convert dictionary-type cells to strings
    df = df.apply(lambda col: col.map(lambda x: str(x) if isinstance(x, dict) else x))
    
    # 2. Capture df.info() output
    buffer = io.StringIO()
    df.info(buf=buffer)
    info_text = buffer.getvalue()

    # 3. Calculate missing value stats
    missing_stats = (df.isna().sum() / len(df) * 100).sort_values(ascending=False)
    missing_summary = "\n".join([f"{col}: {val:.2f}%" for col, val in missing_stats.items()])

    # 4. Get column data types
    column_types = "\n".join([f"{col}: {dtype}" for col, dtype in df.dtypes.items()])

    # 5. Get unique value counts
    unique_counts = df.nunique()
    unique_counts_summary = "\n".join([f"{col}: {count}" for col, count in unique_counts.items()])

    # 6. Generate the summary text
    if not skip_stats:
        summary_text = f"""
        Dataset Name: {dataset_name}
        ----------------------------
        Shape: {df.shape[0]} rows x {df.shape[1]} columns

        Column Data Types:
        {column_types}

        Missing Value Percentage:
        {missing_summary}

        Unique Value Counts:
        {unique_counts_summary}

        Data (first {n_sample} rows):
        {df.head(n_sample).to_string()}

        Data Description:
        {df.describe().to_string()}

        Data Info:
        {info_text}
        """
    else:
        summary_text = f"""
        Dataset Name: {dataset_name}
        ----------------------------
        Shape: {df.shape[0]} rows x {df.shape[1]} columns

        Column Data Types:
        {column_types}

        Data (first {n_sample} rows):
        {df.head(n_sample).to_string()}
        """
        
    return summary_text.strip()

# ì›ë³¸ í•¨ìˆ˜ ì‚¬ìš© ê°€ëŠ¥ í‘œì‹œ
AI_DS_TEAM_AVAILABLE = True
logger.info("âœ… AI DS Team ì›ë³¸ í•¨ìˆ˜ ì§ì ‘ í¬í•¨ ì™„ë£Œ")

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
data_manager = DataManager()

class DataProcessor:
    """ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.current_dataframe = None
        
    def parse_data_from_message(self, user_message: str) -> pd.DataFrame:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë°ì´í„°ë¥¼ íŒŒì‹±"""
        logger.info("ğŸ“Š ë©”ì‹œì§€ì—ì„œ ë°ì´í„° íŒŒì‹±...")
        
        # CSV ë°ì´í„° íŒŒì‹±
        df = self._parse_csv_data(user_message)
        if df is not None:
            return df
            
        # JSON ë°ì´í„° íŒŒì‹±
        df = self._parse_json_data(user_message)
        if df is not None:
            return df
        
        # ìƒ˜í”Œ ë°ì´í„° ìš”ì²­ í™•ì¸
        if self._is_sample_request(user_message):
            return self._create_sample_data()
        
        return None
    
    def _parse_csv_data(self, message: str) -> pd.DataFrame:
        """CSV í˜•íƒœ ë°ì´í„° íŒŒì‹±"""
        try:
            lines = message.split('\n')
            csv_lines = [line.strip() for line in lines if ',' in line and len(line.split(',')) >= 2]
            
            if len(csv_lines) >= 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
                csv_content = '\n'.join(csv_lines)
                df = pd.read_csv(io.StringIO(csv_content))
                logger.info(f"âœ… CSV ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                return df
        except Exception as e:
            logger.warning(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None
    
    def _parse_json_data(self, message: str) -> pd.DataFrame:
        """JSON í˜•íƒœ ë°ì´í„° íŒŒì‹±"""
        try:
            json_start = message.find('{')
            json_end = message.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_content = message[json_start:json_end]
                data = json.loads(json_content)
                
                if isinstance(data, list):
                    df = pd.DataFrame(data)
                elif isinstance(data, dict):
                    df = pd.DataFrame([data])
                else:
                    return None
                    
                logger.info(f"âœ… JSON ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                return df
        except Exception as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None
    
    def _is_sample_request(self, message: str) -> bool:
        """ìƒ˜í”Œ ë°ì´í„° ìš”ì²­ì¸ì§€ í™•ì¸"""
        keywords = ["ìƒ˜í”Œ", "í…ŒìŠ¤íŠ¸", "example", "demo", "sample", "test"]
        return any(keyword in message.lower() for keyword in keywords)
    
    def _create_sample_data(self) -> pd.DataFrame:
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        logger.info("ğŸ”§ ìƒ˜í”Œ ë°ì´í„° ìƒì„±...")
        
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        
        data = {
            'id': range(1, 101),
            'name': [f'User_{i}' for i in range(1, 101)],
            'age': np.random.randint(18, 80, 100),
            'income': np.random.randint(20000, 150000, 100),
            'category': np.random.choice(['A', 'B', 'C', 'D'], 100),
            'score': np.random.normal(75, 15, 100)
        }
        
        # ì˜ë„ì ìœ¼ë¡œ ê²°ì¸¡ê°’ê³¼ ì´ìƒê°’ ì¶”ê°€
        df = pd.DataFrame(data)
        
        # ê²°ì¸¡ê°’ ì¶”ê°€
        missing_indices = np.random.choice(df.index, 15, replace=False)
        df.loc[missing_indices[:5], 'age'] = np.nan
        df.loc[missing_indices[5:10], 'income'] = np.nan
        df.loc[missing_indices[10:], 'category'] = np.nan
        
        # ì´ìƒê°’ ì¶”ê°€
        df.loc[0, 'age'] = 200  # ì´ìƒê°’
        df.loc[1, 'income'] = 1000000  # ì´ìƒê°’
        df.loc[2, 'score'] = -50  # ì´ìƒê°’
        
        # ì¤‘ë³µ í–‰ ì¶”ê°€
        df = pd.concat([df, df.iloc[:3]], ignore_index=True)
        
        logger.info(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {df.shape}")
        return df

class DataCleaner:
    """ë°ì´í„° í´ë¦¬ë„ˆ"""
    
    def __init__(self):
        self.original_data = None
        self.cleaned_data = None
        self.cleaning_report = []
    
    def clean_data(self, df: pd.DataFrame, user_instructions: str = "") -> dict:
        """ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰"""
        self.original_data = df.copy()
        self.cleaned_data = df.copy()
        self.cleaning_report = []
        
        logger.info(f"ğŸ§¹ ë°ì´í„° í´ë¦¬ë‹ ì‹œì‘: {df.shape}")
        
        # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        original_shape = df.shape
        original_memory = df.memory_usage(deep=True).sum() / 1024**2  # MB
        
        # í´ë¦¬ë‹ ë‹¨ê³„ ì‹¤í–‰
        self._remove_high_missing_columns()
        self._handle_missing_values()
        self._optimize_data_types()
        self._remove_duplicates()
        
        # ì´ìƒê°’ ì²˜ë¦¬ (ì‚¬ìš©ìê°€ ê¸ˆì§€í•˜ì§€ ì•Šì€ ê²½ìš°)
        if "outlier" not in user_instructions.lower() and "ì´ìƒê°’" not in user_instructions:
            self._handle_outliers()
        
        # ìµœì¢… ê²°ê³¼ ê³„ì‚°
        final_shape = self.cleaned_data.shape
        final_memory = self.cleaned_data.memory_usage(deep=True).sum() / 1024**2  # MB
        quality_score = self._calculate_quality_score()
        
        return {
            'original_data': self.original_data,
            'cleaned_data': self.cleaned_data,
            'original_shape': original_shape,
            'final_shape': final_shape,
            'memory_saved': original_memory - final_memory,
            'cleaning_report': self.cleaning_report,
            'quality_score': quality_score
        }
    
    def _remove_high_missing_columns(self):
        """40% ì´ìƒ ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ ì œê±°"""
        missing_ratios = self.cleaned_data.isnull().mean()
        high_missing_cols = missing_ratios[missing_ratios > 0.4].index.tolist()
        
        if high_missing_cols:
            self.cleaned_data = self.cleaned_data.drop(columns=high_missing_cols)
            self.cleaning_report.append(f"âœ… 40% ì´ìƒ ê²°ì¸¡ê°’ ì»¬ëŸ¼ ì œê±°: {high_missing_cols}")
    
    def _handle_missing_values(self):
        """ê²°ì¸¡ê°’ ì²˜ë¦¬"""
        for col in self.cleaned_data.columns:
            missing_count = self.cleaned_data[col].isnull().sum()
            if missing_count > 0:
                if self.cleaned_data[col].dtype in ['int64', 'float64']:
                    # ìˆ«ìí˜•: í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
                    mean_val = self.cleaned_data[col].mean()
                    self.cleaned_data[col].fillna(mean_val, inplace=True)
                    self.cleaning_report.append(f"âœ… '{col}' ê²°ì¸¡ê°’ {missing_count}ê°œë¥¼ í‰ê· ê°’({mean_val:.2f})ìœ¼ë¡œ ëŒ€ì²´")
                else:
                    # ë²”ì£¼í˜•: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
                    mode_val = self.cleaned_data[col].mode()
                    if not mode_val.empty:
                        self.cleaned_data[col].fillna(mode_val.iloc[0], inplace=True)
                        self.cleaning_report.append(f"âœ… '{col}' ê²°ì¸¡ê°’ {missing_count}ê°œë¥¼ ìµœë¹ˆê°’('{mode_val.iloc[0]}')ìœ¼ë¡œ ëŒ€ì²´")
    
    def _optimize_data_types(self):
        """ë°ì´í„° íƒ€ì… ìµœì í™”"""
        optimized_count = 0
        
        for col in self.cleaned_data.columns:
            if self.cleaned_data[col].dtype == 'object':
                # ë¬¸ìì—´ ì •ê·œí™”
                self.cleaned_data[col] = self.cleaned_data[col].astype(str).str.strip()
            elif self.cleaned_data[col].dtype == 'int64':
                # ì •ìˆ˜í˜• ìµœì í™”
                col_min, col_max = self.cleaned_data[col].min(), self.cleaned_data[col].max()
                if col_min >= 0 and col_max < 255:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('uint8')
                    optimized_count += 1
                elif col_min >= -128 and col_max < 127:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('int8')
                    optimized_count += 1
                elif col_min >= -32768 and col_max < 32767:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('int16')
                    optimized_count += 1
        
        if optimized_count > 0:
            self.cleaning_report.append(f"âœ… ë°ì´í„° íƒ€ì… ìµœì í™”: {optimized_count}ê°œ ì»¬ëŸ¼")
    
    def _remove_duplicates(self):
        """ì¤‘ë³µ í–‰ ì œê±°"""
        duplicates_count = self.cleaned_data.duplicated().sum()
        if duplicates_count > 0:
            self.cleaned_data = self.cleaned_data.drop_duplicates()
            self.cleaning_report.append(f"âœ… ì¤‘ë³µ í–‰ {duplicates_count}ê°œ ì œê±°")
    
    def _handle_outliers(self):
        """IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒê°’ ì²˜ë¦¬"""
        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            Q1 = self.cleaned_data[col].quantile(0.25)
            Q3 = self.cleaned_data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers_mask = (self.cleaned_data[col] < lower_bound) | (self.cleaned_data[col] > upper_bound)
            outliers_count = outliers_mask.sum()
            
            if outliers_count > 0:
                # ì´ìƒê°’ì„ ê²½ê³„ê°’ìœ¼ë¡œ í´ë¦¬í•‘
                self.cleaned_data[col] = self.cleaned_data[col].clip(lower_bound, upper_bound)
                self.cleaning_report.append(f"âœ… '{col}' ì´ìƒê°’ {outliers_count}ê°œ ì²˜ë¦¬ (IQR 1.5ë°° ê¸°ì¤€)")
    
    def _calculate_quality_score(self) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        score = 100.0
        
        # ê²°ì¸¡ê°’ ë¹„ìœ¨
        total_cells = self.cleaned_data.shape[0] * self.cleaned_data.shape[1]
        missing_ratio = self.cleaned_data.isnull().sum().sum() / total_cells if total_cells > 0 else 0
        score -= missing_ratio * 40
        
        # ì¤‘ë³µ ë¹„ìœ¨
        duplicate_ratio = self.cleaned_data.duplicated().sum() / self.cleaned_data.shape[0] if self.cleaned_data.shape[0] > 0 else 0
        score -= duplicate_ratio * 30
        
        # ë°ì´í„° íƒ€ì… ì¼ê´€ì„±
        if len(self.cleaned_data.columns) > 0:
            numeric_ratio = len(self.cleaned_data.select_dtypes(include=[np.number]).columns) / len(self.cleaned_data.columns)
            score += numeric_ratio * 10
        
        return max(0, min(100, score))

class DataCleaningAgentExecutor(AgentExecutor):
    """ë°ì´í„° í´ë¦¬ë‹ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.data_processor = DataProcessor()
        self.data_cleaner = DataCleaner()
        logger.info("ğŸ§¹ Clean DataCleaningAgent ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰"""
        logger.info(f"ğŸš€ DataCleaningAgent ì‹¤í–‰ ì‹œì‘ - Task: {context.task_id}")
        
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§¹ ë³µì‚¬ëœ ì›ë³¸ ëª¨ë“ˆ ê¸°ë°˜ ë°ì´í„° í´ë¦¬ë‹ ì‹œì‘...")
            )
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
            user_instructions = ""
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if part.root.kind == "text":
                        user_instructions += part.root.text + " "
                
                user_instructions = user_instructions.strip()
                logger.info(f"ğŸ“ ì‚¬ìš©ì ìš”ì²­: {user_instructions}")
                
                # ë°ì´í„° íŒŒì‹±
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message("ğŸ“Š ë°ì´í„° ë¶„ì„ ì¤‘...")
                )
                
                df = self.data_processor.parse_data_from_message(user_instructions)
                
                if df is not None and not df.empty:
                    # ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰
                    await task_updater.update_status(
                        TaskState.working,
                        message=new_agent_text_message("ğŸ§¹ ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰ ì¤‘...")
                    )
                    
                    cleaning_results = self.data_cleaner.clean_data(df, user_instructions)
                    
                    # ê²°ê³¼ ì €ì¥
                    output_dir = Path("a2a_ds_servers/artifacts/cleaned_data")
                    output_dir.mkdir(parents=True, exist_ok=True)
                    output_path = output_dir / f"cleaned_data_{context.task_id}.csv"
                    
                    cleaning_results['cleaned_data'].to_csv(output_path, index=False)
                    
                    # ì›ë³¸ ëª¨ë“ˆ ì‚¬ìš© ì‹œ ì¶”ê°€ ì •ë³´
                    dataframe_summary = ""
                    if AI_DS_TEAM_AVAILABLE:
                        try:
                            summary_list = get_dataframe_summary(cleaning_results['cleaned_data'])
                            dataframe_summary = "\n".join(summary_list) if summary_list else ""
                        except Exception as e:
                            logger.warning(f"ì›ë³¸ ëª¨ë“ˆ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
                    
                    # ì‘ë‹µ ìƒì„±
                    result = self._generate_response(cleaning_results, user_instructions, str(output_path), dataframe_summary)
                    
                else:
                    result = self._generate_no_data_response(user_instructions)
                
                # ìµœì¢… ì‘ë‹µ
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(result)
                )
            else:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message("âŒ ë°ì´í„° í´ë¦¬ë‹ ìš”ì²­ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                )
                
        except Exception as e:
            logger.error(f"âŒ DataCleaningAgent ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"ë°ì´í„° í´ë¦¬ë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            )
    
    def _generate_response(self, results: dict, user_instructions: str, output_path: str, dataframe_summary: str = "") -> str:
        """í´ë¦¬ë‹ ê²°ê³¼ ì‘ë‹µ ìƒì„±"""
        ai_ds_status = "âœ… ì›ë³¸ AI DS Team ëª¨ë“ˆ ì‚¬ìš©" if AI_DS_TEAM_AVAILABLE else "âš ï¸ í´ë°± ëª¨ë“œ"
        
        response = f"""# ğŸ§¹ **AI DataCleaningAgent ì™„ë£Œ** (ë³µì‚¬ëœ ì›ë³¸ ëª¨ë“ˆ ê¸°ë°˜)

## ğŸ“Š **í´ë¦¬ë‹ ê²°ê³¼**
- **ì›ë³¸ ë°ì´í„°**: {results['original_shape'][0]:,}í–‰ Ã— {results['original_shape'][1]}ì—´
- **ì •ë¦¬ í›„**: {results['final_shape'][0]:,}í–‰ Ã— {results['final_shape'][1]}ì—´
- **ë©”ëª¨ë¦¬ ì ˆì•½**: {results['memory_saved']:.2f} MB
- **í’ˆì§ˆ ì ìˆ˜**: {results['quality_score']:.1f}/100
- **ëª¨ë“ˆ ìƒíƒœ**: {ai_ds_status}

## ğŸ”§ **ìˆ˜í–‰ëœ ì‘ì—…**
{chr(10).join(f"- {report}" for report in results['cleaning_report'])}

## ğŸ” **ì •ë¦¬ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{results['cleaned_data'].head().to_string()}
```

## ğŸ“ˆ **ë°ì´í„° í†µê³„ ìš”ì•½**
```
{results['cleaned_data'].describe().to_string()}
```
"""

        if dataframe_summary:
            response += f"""
## ğŸ¯ **AI DS Team ì›ë³¸ ëª¨ë“ˆ ë¶„ì„**
```
{dataframe_summary}
```
"""

        response += f"""
## ğŸ“ **ì €ì¥ ê²½ë¡œ**
`{output_path}`

---
**ğŸ’¬ ì‚¬ìš©ì ìš”ì²­**: {user_instructions}
**ğŸ¯ ì²˜ë¦¬ ë°©ì‹**: ë³µì‚¬ëœ ì›ë³¸ ai_data_science_team ëª¨ë“ˆ ê¸°ë°˜
**ğŸ•’ ì²˜ë¦¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        return response
    
    def _generate_no_data_response(self, user_instructions: str) -> str:
        """ë°ì´í„° ì—†ìŒ ì‘ë‹µ ìƒì„±"""
        return f"""# âŒ **ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤**

**í•´ê²° ë°©ë²•**:
1. **CSV í˜•íƒœë¡œ ë°ì´í„° í¬í•¨**:
   ```
   name,age,income
   John,25,50000
   Jane,30,60000
   ```

2. **JSON í˜•íƒœë¡œ ë°ì´í„° í¬í•¨**:
   ```json
   {{"name": "John", "age": 25, "income": 50000}}
   ```

3. **ìƒ˜í”Œ ë°ì´í„° ìš”ì²­**: "ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•´ì£¼ì„¸ìš”"

**ìš”ì²­**: {user_instructions}
"""
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.reject()
        logger.info(f"DataCleaningAgent ì‘ì—… ì·¨ì†Œ: {context.task_id}")

def main():
    """A2A ì„œë²„ ìƒì„± ë° ì‹¤í–‰"""
    
    # AgentSkill ì •ì˜
    skill = AgentSkill(
        id="clean_data_cleaning",
        name="Clean Data Cleaning with Original Modules",
        description="ë³µì‚¬ëœ ì›ë³¸ ai_data_science_team ëª¨ë“ˆì„ ì‚¬ìš©í•œ ì „ë¬¸ ë°ì´í„° í´ë¦¬ë‹ ì„œë¹„ìŠ¤",
        tags=["data-cleaning", "ai-ds-team", "preprocessing", "original-modules"],
        examples=[
            "ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•´ì£¼ì„¸ìš”",
            "ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
            "ì´ìƒê°’ ì œê±° ì—†ì´ ë°ì´í„°ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”",
            "ì¤‘ë³µ ë°ì´í„°ë¥¼ ì œê±°í•˜ê³  í’ˆì§ˆì„ ê°œì„ í•´ì£¼ì„¸ìš”"
        ]
    )
    
    # Agent Card ì •ì˜
    agent_card = AgentCard(
        name="Clean AI DataCleaningAgent",
        description="ë³µì‚¬ëœ ì›ë³¸ ai_data_science_team ëª¨ë“ˆ ê¸°ë°˜ ë°ì´í„° í´ë¦¬ë‹ ì „ë¬¸ê°€",
        url="http://localhost:8316/",
        version="3.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=False),
        skills=[skill],
        supportsAuthenticatedExtendedCard=False
    )
    
    # Request Handler ìƒì„±
    request_handler = DefaultRequestHandler(
        agent_executor=DataCleaningAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )
    
    # A2A Server ìƒì„±
    server = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler,
    )
    
    print("ğŸ§¹ Starting Clean AI DataCleaningAgent Server")
    print("ğŸŒ Server starting on http://localhost:8316")
    print("ğŸ“‹ Agent card: http://localhost:8316/.well-known/agent.json")
    print(f"âœ¨ Features: ë³µì‚¬ëœ ì›ë³¸ ëª¨ë“ˆ ê¸°ë°˜ ({AI_DS_TEAM_AVAILABLE and 'ì›ë³¸ ëª¨ë“ˆ ì‚¬ìš©' or 'í´ë°± ëª¨ë“œ'})")
    
    uvicorn.run(server.build(), host="0.0.0.0", port=8316, log_level="info")

if __name__ == "__main__":
    main()