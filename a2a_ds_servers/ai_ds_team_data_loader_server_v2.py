#!/usr/bin/env python3
"""
AI_DS_Team DataLoaderToolsAgent A2A Server V2 (Improved with Base Wrapper)
Port: 8307

AI_DS_Teamì˜ DataLoaderToolsAgentë¥¼ ìƒˆë¡œìš´ ë² ì´ìŠ¤ ë˜í¼ë¥¼ í†µí•´ A2A í”„ë¡œí† ì½œë¡œ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai_ds_team"))
sys.path.insert(0, str(project_root / "a2a_ds_servers"))

import uvicorn
import logging
import pandas as pd

from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers.default_request_handler import DefaultRequestHandler
from a2a.server.tasks.inmemory_task_store import InMemoryTaskStore
from a2a.types import AgentCard, AgentSkill, AgentCapabilities

# AI_DS_Team imports
from ai_data_science_team.agents import DataLoaderToolsAgent

# ìƒˆë¡œìš´ ë² ì´ìŠ¤ ë˜í¼ ì‚¬ìš©
from base import AIDataScienceTeamWrapper

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ ì„¤ì •
from dotenv import load_dotenv
load_dotenv()


class DataLoaderWrapper(AIDataScienceTeamWrapper):
    """Data Loader Agentë¥¼ ìœ„í•œ íŠ¹í™”ëœ ë˜í¼"""
    
    def __init__(self):
        # LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        from core.llm_factory import create_llm_instance
        llm = create_llm_instance()
        
        # ì—ì´ì „íŠ¸ ì„¤ì •
        agent_config = {
            "model": llm
        }
        
        super().__init__(
            agent_class=DataLoaderToolsAgent,
            agent_config=agent_config,
            agent_name="Data Loader Tools Agent"
        )
    
    async def _execute_agent(self, user_input: str) -> any:
        """Data Loader Agent íŠ¹í™” ì‹¤í–‰ ë¡œì§"""
        try:
            # Data Loader Agent ì‹¤í–‰
            result = self.agent.invoke_agent(
                user_instructions=user_input
            )
            
            # ë¡œë“œëœ ë°ì´í„° í™•ì¸
            loaded_data = None
            if hasattr(self.agent, 'data') and self.agent.data is not None:
                loaded_data = self.agent.data
                
                # ë°ì´í„°ë¥¼ ê³µìœ  í´ë”ì— ì €ì¥
                data_path = "a2a_ds_servers/artifacts/data/shared_dataframes/"
                os.makedirs(data_path, exist_ok=True)
                
                # ê³ ìœ í•œ íŒŒì¼ëª… ìƒì„±
                import time
                timestamp = int(time.time())
                output_file = f"loaded_data_{timestamp}.csv"
                output_path = os.path.join(data_path, output_file)
                
                loaded_data.to_csv(output_path, index=False)
                logger.info(f"Data saved to: {output_path}")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì†ŒìŠ¤ ê²€ìƒ‰
            available_sources = self._scan_available_data_sources()
            
            return {
                "result": result,
                "loaded_data": loaded_data,
                "data_file": output_file if loaded_data is not None else None,
                "available_sources": available_sources,
                "success": True
            }
            
        except Exception as e:
            logger.error(f"Data loading execution failed: {e}")
            return {
                "error": str(e),
                "content": f"ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "success": False
            }
    
    def _scan_available_data_sources(self) -> list:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ìŠ¤ìº”í•©ë‹ˆë‹¤."""
        available_sources = []
        
        # ìŠ¤ìº”í•  ë””ë ‰í† ë¦¬ë“¤
        data_dirs = [
            "ai_ds_team/data/",
            "a2a_ds_servers/artifacts/data/shared_dataframes/",
            "data/",
            "artifacts/data/shared_dataframes/"
        ]
        
        for data_dir in data_dirs:
            try:
                if os.path.exists(data_dir):
                    files = [f for f in os.listdir(data_dir) 
                            if f.endswith(('.csv', '.xlsx', '.json', '.parquet', '.pkl'))]
                    if files:
                        for file in files:
                            available_sources.append({
                                "path": os.path.join(data_dir, file),
                                "name": file,
                                "directory": data_dir,
                                "size": self._get_file_size(os.path.join(data_dir, file))
                            })
            except Exception as e:
                logger.warning(f"Error scanning directory {data_dir}: {e}")
        
        return available_sources
    
    def _get_file_size(self, file_path: str) -> str:
        """íŒŒì¼ í¬ê¸°ë¥¼ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        try:
            size_bytes = os.path.getsize(file_path)
            if size_bytes < 1024:
                return f"{size_bytes} B"
            elif size_bytes < 1024**2:
                return f"{size_bytes/1024:.1f} KB"
            elif size_bytes < 1024**3:
                return f"{size_bytes/(1024**2):.1f} MB"
            else:
                return f"{size_bytes/(1024**3):.1f} GB"
        except:
            return "Unknown"
    
    def _build_final_response(self, workflow_summary: str, a2a_response: dict, user_input: str) -> str:
        """Data Loading ê²°ê³¼ì— íŠ¹í™”ëœ ì‘ë‹µ êµ¬ì„±"""
        try:
            if not a2a_response.get("success"):
                return f"""## âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨

{a2a_response.get('content', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')}

ìš”ì²­: {user_input}

### ğŸ’¡ Data Loader Tools ì‚¬ìš©ë²•
1. **íŒŒì¼ ë¡œë”©**: "CSV íŒŒì¼ì„ ë¡œë“œí•´ì£¼ì„¸ìš”"
2. **ë°ì´í„° ê²€ìƒ‰**: "ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ë“¤ì„ ë³´ì—¬ì£¼ì„¸ìš”"
3. **í˜•ì‹ ë³€í™˜**: "JSONì„ DataFrameìœ¼ë¡œ ë³€í™˜í•´ì£¼ì„¸ìš”"
4. **ë°ì´í„°ë² ì´ìŠ¤**: "ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í…Œì´ë¸”ì„ ê°€ì ¸ì™€ì£¼ì„¸ìš”"
"""
            
            # ì„±ê³µì ì¸ ê²½ìš°
            loaded_data = a2a_response.get("loaded_data")
            data_file = a2a_response.get("data_file")
            available_sources = a2a_response.get("available_sources", [])
            
            response_parts = [
                "## ğŸ“ ë°ì´í„° ë¡œë”© ì™„ë£Œ\n",
                f"### ğŸ“‹ ì‘ì—… ìš”ì•½\n{workflow_summary}\n"
            ]
            
            # ë¡œë“œëœ ë°ì´í„° ì •ë³´
            if loaded_data is not None:
                response_parts.append(f"### ğŸ“Š ë¡œë“œëœ ë°ì´í„° ì •ë³´")
                response_parts.append(f"- **ì €ì¥ëœ íŒŒì¼**: `{data_file}`")
                response_parts.append(f"- **ë°ì´í„° í¬ê¸°**: {loaded_data.shape[0]:,} rows Ã— {loaded_data.shape[1]:,} columns")
                response_parts.append(f"- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {loaded_data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
                response_parts.append(f"- **ì»¬ëŸ¼**: {', '.join(loaded_data.columns.tolist()[:5])}{'...' if len(loaded_data.columns) > 5 else ''}")
                
                # ë°ì´í„° íƒ€ì… ì •ë³´
                if len(loaded_data.columns) <= 10:
                    response_parts.append(f"- **ë°ì´í„° íƒ€ì…**: {dict(loaded_data.dtypes.astype(str))}")
                else:
                    response_parts.append(f"- **ë°ì´í„° íƒ€ì…**: {len(loaded_data.select_dtypes('number').columns)} ìˆ«ìí˜•, {len(loaded_data.select_dtypes('object').columns)} ë¬¸ìí˜•")
                
                response_parts.append("")
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì†ŒìŠ¤ ì •ë³´
            if available_sources:
                response_parts.append("### ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° ì†ŒìŠ¤")
                for source in available_sources[:8]:  # ìµœëŒ€ 8ê°œê¹Œì§€ë§Œ í‘œì‹œ
                    response_parts.append(f"- **{source['name']}** ({source['size']}) - `{source['directory']}`")
                
                if len(available_sources) > 8:
                    response_parts.append(f"- ... ë° {len(available_sources) - 8}ê°œ ì¶”ê°€ íŒŒì¼")
                
                response_parts.append("")
            
            response_parts.append("### ğŸ› ï¸ Data Loader Tools ê¸°ëŠ¥")
            response_parts.append("- **íŒŒì¼ ë¡œë”©**: CSV, Excel, JSON, Parquet ë“± ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›")
            response_parts.append("- **ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°**: SQL ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì¿¼ë¦¬")
            response_parts.append("- **API í†µí•©**: REST APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘")
            response_parts.append("- **ë°ì´í„° ê²€ì¦**: ë¡œë“œëœ ë°ì´í„°ì˜ í’ˆì§ˆ ë° í˜•ì‹ ê²€ì¦")
            response_parts.append("- **ìë™ íƒ€ì… ì¶”ë¡ **: ì»¬ëŸ¼ íƒ€ì… ìë™ ê°ì§€ ë° ë³€í™˜")
            response_parts.append("- **ë°°ì¹˜ ì²˜ë¦¬**: ëŒ€ìš©ëŸ‰ íŒŒì¼ì˜ ì²­í¬ ë‹¨ìœ„ ë¡œë”©")
            
            return "\n".join(response_parts)
            
        except Exception as e:
            logger.error(f"Error building data loader response: {e}")
            return f"âœ… ë°ì´í„° ë¡œë”© ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\nìš”ì²­: {user_input}"


def main():
    """A2A ì„œë²„ ìƒì„± ë° ì‹¤í–‰"""
    
    # AgentSkill ì •ì˜
    skill = AgentSkill(
        id="data_loading",
        name="Data Loading & File Processing",
        description="Advanced data loading and file processing with support for multiple formats and sources",
        tags=["data-loading", "etl", "file-processing", "database", "api-integration"],
        examples=[
            "Load CSV file and convert to DataFrame",
            "Connect to database and fetch customer table",
            "Collect real-time data from API endpoints",
            "Read specific sheet from Excel file",
            "Show available data files in the system"
        ]
    )
    
    # AgentCard ìƒì„±
    agent_card = AgentCard(
        name="AI Data Science Team - Data Loader Tools Agent",
        description="Specialized agent for data loading and file processing with advanced ETL capabilities",
        url="http://localhost:8307/",
        version="2.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=False),
        skills=[skill],
        supportsAuthenticatedExtendedCard=False
    )
    
    # ìš”ì²­ í•¸ë“¤ëŸ¬ ì„¤ì •
    request_handler = DefaultRequestHandler(
        agent_executor=DataLoaderWrapper(),
        task_store=InMemoryTaskStore(),
    )
    
    # A2A ì„œë²„ ìƒì„±
    server = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler,
    )
    
    print("ğŸ“ Starting Data Loader Tools Agent Server V2")
    print("ğŸŒ Server starting on http://localhost:8307")
    print("ğŸ“‹ Agent card: http://localhost:8307/.well-known/agent.json")
    print("ğŸ”§ Using improved base wrapper architecture")
    
    # ì„œë²„ ì‹¤í–‰
    uvicorn.run(server.build(), host="0.0.0.0", port=8307, log_level="info")


if __name__ == "__main__":
    main() 