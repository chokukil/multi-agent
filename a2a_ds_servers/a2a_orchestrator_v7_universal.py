#!/usr/bin/env python3
"""
A2A Orchestrator v7.0 - Universal LLM Powered Dynamic System
ì™„ì „ ë²”ìš© LLM ê¸°ë°˜ ë™ì  ì‹œìŠ¤í…œ
- Universal Request Analyzer
- Adaptive Context Builder  
- Smart Question Expander
- Flexible Response Generator
- Rich Information Extraction Planning
- Dynamic Content Assessment
"""

import asyncio
import json
import logging
import os
import re
import time
from typing import Any, Dict, List, Optional

import httpx
import uvicorn
from openai import AsyncOpenAI

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# AI DS Team ì—ì´ì „íŠ¸ í¬íŠ¸ ë§¤í•‘
AGENT_PORTS = {
    "data_cleaning": 8306,
    "data_loader": 8307, 
    "data_visualization": 8308,
    "data_wrangling": 8309,
    "eda_tools": 8310,
    "feature_engineering": 8311,
    "h2o_modeling": 8312,
    "mlflow_tracking": 8313,
    "sql_database": 8314
}


class StreamingTaskUpdater(TaskUpdater):
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´íŠ¸ ì§€ì›"""
    
    async def stream_update(self, content: str):
        """ì¤‘ê°„ ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°"""
        await self.update_status(
            TaskState.working,
            message=self.new_agent_message(parts=[TextPart(text=content)])
        )
    
    async def stream_final_response(self, response: str):
        """ìµœì¢… ì‘ë‹µì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ìŠ¤íŠ¸ë¦¬ë°"""
        # Markdown ì„¹ì…˜ë³„ë¡œ ìŠ¤íŠ¸ë¦¬ë°
        sections = response.split('\n\n')
        
        for i, section in enumerate(sections):
            if section.strip():
                await self.update_status(
                    TaskState.working,
                    message=self.new_agent_message(parts=[TextPart(text=section)])
                )
                await asyncio.sleep(0.1)  # ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°
        
        # ì™„ë£Œ ìƒíƒœ ì—…ë°ì´íŠ¸
        await self.update_status(
            TaskState.completed,
            message=self.new_agent_message(parts=[TextPart(text="âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")])
        )


class LLMPoweredOrchestratorExecutor(AgentExecutor):
    """LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°"""
    
    def __init__(self):
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì˜µì…˜)
        try:
            api_key = os.getenv("OPENAI_API_KEY")
            if api_key and api_key.strip():
                self.openai_client = AsyncOpenAI(api_key=api_key)
                logger.info("ğŸ¤– LLM Powered Dynamic Orchestrator v6 with OpenAI integration")
            else:
                self.openai_client = None
                logger.info("ğŸ“Š Standard Orchestrator v6 (OpenAI API key not found)")
        except Exception as e:
            logger.warning(f"OpenAI client initialization failed: {e}")
            self.openai_client = None
        
        self.available_agents = {}
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ğŸ¯ Universal LLM Powered Dynamic System - ì™„ì „ ë²”ìš© ì‹¤í–‰"""
        task_updater = StreamingTaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            user_input = context.get_user_input()
            logger.info(f"ğŸ¯ Universal System Processing: {user_input}")
            
            if not user_input:
                user_input = "Please provide an analysis request."
            
            # ğŸ¯ Step 1: Universal Request Analyzer
            await task_updater.stream_update("ğŸ§  Universal Request Analyzer ì‹¤í–‰ ì¤‘...")
            request_analysis = await self._analyze_request_depth(user_input)
            
            # ğŸ¯ Step 2: Adaptive Context Builder
            await task_updater.stream_update("ğŸ­ Adaptive Context Builder ì‹¤í–‰ ì¤‘...")
            adaptive_context = await self._build_adaptive_context(user_input, request_analysis)
            
            # ğŸ¯ Step 3: Smart Question Expander
            await task_updater.stream_update("ğŸ“ˆ Smart Question Expander ì‹¤í–‰ ì¤‘...")
            expanded_request = await self._expand_simple_requests(user_input, request_analysis)
            
            # ì—ì´ì „íŠ¸ ë°œê²¬
            await task_updater.stream_update("ğŸ” AI DS Team ì—ì´ì „íŠ¸ë“¤ì„ ë°œê²¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
            self.available_agents = await self._discover_agents()
            
            if not self.available_agents:
                await task_updater.update_status(
                    TaskState.failed,
                    message=task_updater.new_agent_message(parts=[TextPart(text="âŒ ì‚¬ìš© ê°€ëŠ¥í•œ A2A ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")])
                )
                return
            
            await task_updater.stream_update(f"âœ… {len(self.available_agents)}ê°œ ì—ì´ì „íŠ¸ ë°œê²¬ ì™„ë£Œ")
            
            # ğŸ¯ Step 4: Rich Information Extraction Planning
            await task_updater.stream_update("ğŸ“‹ Rich Information Extraction Planning ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ë³¸ ìš”ì²­ ì´í•´
            request_understanding = await self._understand_request(expanded_request)
            
            # ì¢…í•©ì  ì‹¤í–‰ ê³„íš ìƒì„±
            execution_plan = await self._create_comprehensive_execution_plan(
                expanded_request,
                request_understanding,
                self.available_agents
            )
            
            if not execution_plan or not execution_plan.get('steps'):
                execution_plan = self._create_fallback_plan(self.available_agents)
            
            await task_updater.stream_update(f"âœ… ì¢…í•©ì  ì‹¤í–‰ ê³„íš ì™„ë£Œ: {len(execution_plan.get('steps', []))}ë‹¨ê³„")
            
            # ğŸ“‹ ê³„íš í‘œì‹œ
            plan_display = self._create_beautiful_plan_display(execution_plan, request_understanding)
            await task_updater.stream_update(plan_display)
            
            # ê³„íšì„ ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
            plan_artifact = {
                "execution_strategy": execution_plan.get('execution_strategy', 'comprehensive_value_extraction'),
                "plan_executed": [
                    {
                        "step": i + 1,
                        "agent": step.get('agent', 'unknown'),
                        "comprehensive_instructions": step.get('comprehensive_instructions', ''),
                        "expected_deliverables": step.get('expected_deliverables', {})
                    }
                    for i, step in enumerate(execution_plan.get('steps', []))
                ]
            }
            
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(plan_artifact, ensure_ascii=False, indent=2))],
                name="comprehensive_execution_plan.json",
                metadata={
                    "content_type": "application/json",
                    "plan_type": "universal_llm_orchestration",
                    "description": "Universal LLM ê¸°ë°˜ ì¢…í•©ì  ì‹¤í–‰ ê³„íš"
                }
            )
            
            await asyncio.sleep(2)
            await task_updater.stream_update("ğŸš€ Universal System ì‹¤í–‰ ì‹œì‘...")
            
            # ğŸ¯ Step 5: Execute Agents with Rich Context
            agent_results = {}
            for i, step in enumerate(execution_plan.get('steps', [])):
                step_num = i + 1
                agent_name = step.get('agent', 'unknown')
                
                step_info = f"""
ğŸ”„ **ë‹¨ê³„ {step_num}/{len(execution_plan.get('steps', []))}: {agent_name} ì‹¤í–‰**

ğŸ“ **ì¢…í•©ì  ì§€ì‹œì‚¬í•­**: {step.get('comprehensive_instructions', '')[:200]}...

ğŸ¯ **ê¸°ëŒ€ ì„±ê³¼**:
- ìµœì†Œ: {step.get('expected_deliverables', {}).get('minimum', 'ê¸°ë³¸ ë¶„ì„')}
- í‘œì¤€: {step.get('expected_deliverables', {}).get('standard', 'í’ˆì§ˆ ë¶„ì„')}
- íƒì›”: {step.get('expected_deliverables', {}).get('exceptional', 'ì¸ì‚¬ì´íŠ¸ ë„ì¶œ')}
"""
                await task_updater.stream_update(step_info)
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ì¢…í•©ì  ì§€ì‹œì‚¬í•­ ì‚¬ìš©)
                agent_result = await self._execute_agent_with_comprehensive_instructions(
                    agent_name, 
                    step,
                    adaptive_context,
                    agent_results
                )
                
                agent_results[agent_name] = agent_result
                
                if agent_result.get('status') == 'success':
                    await task_updater.stream_update(f"âœ… {agent_name} ì‹¤í–‰ ì™„ë£Œ")
                else:
                    await task_updater.stream_update(f"âš ï¸ {agent_name} ì‹¤í–‰ ì´ìŠˆ: {agent_result.get('error', 'Unknown error')}")
                
                await asyncio.sleep(1)
            
            # ğŸ¯ Step 6: Dynamic Content Assessment
            await task_updater.stream_update("ğŸ¨ Dynamic Content Assessment ì‹¤í–‰ ì¤‘...")
            content_assessment = await self._assess_content_richness(agent_results)
            
            # ğŸ¯ Step 7: Flexible Response Generation
            await task_updater.stream_update("ğŸ¯ Flexible Response Generation ì‹¤í–‰ ì¤‘...")
            
            # ì‹œê°í™” ì¶”ì¶œ
            visualizations = self._extract_visualizations(agent_results)
            
            # ìœ ì—°í•œ ì‘ë‹µ ìƒì„±
            base_response = await self._generate_flexible_response(
                user_input,  # ì›ë³¸ ìš”ì²­ ì‚¬ìš©
                request_analysis,
                adaptive_context,
                agent_results
            )
            
            # ğŸ¨ Rich Details Injection
            enriched_response = await self._inject_rich_details(
                base_response,
                content_assessment,
                agent_results,
                request_analysis
            )
            
            # ğŸ¨ Visualization Integration
            if visualizations and content_assessment.get('has_visualizations'):
                enriched_response = await self._integrate_visualizations(
                    enriched_response,
                    visualizations
                )
            
            # ğŸ¨ Smart Default Enrichment
            final_response = await self._enrich_unless_explicitly_simple(
                user_input,
                enriched_response,
                {
                    'visualizations': visualizations,
                    'metrics': content_assessment.get('key_metrics', {}),
                    'findings': content_assessment.get('critical_findings', [])
                }
            )
            
            # ğŸ¯ Final Delivery
            await task_updater.stream_update("ğŸ‰ Universal System ë¶„ì„ ì™„ë£Œ!")
            
            # ìµœì¢… ì‘ë‹µ ì „ë‹¬
            if request_analysis.get('detail_level', 5) < 3:
                # ê°„ë‹¨í•œ ì‘ë‹µì€ í•œ ë²ˆì—
                await task_updater.update_status(
                    TaskState.completed,
                    message=task_updater.new_agent_message(parts=[TextPart(text=final_response)])
                )
            else:
                # ìƒì„¸í•œ ì‘ë‹µì€ ìŠ¤íŠ¸ë¦¬ë°
                await task_updater.stream_final_response(final_response)
            
            # ì‹¤í–‰ ê²°ê³¼ ìš”ì•½ ì•„í‹°íŒ©íŠ¸
            execution_summary = {
                "request_analysis": request_analysis,
                "adaptive_context": adaptive_context,
                "content_assessment": content_assessment,
                "visualizations_found": len(visualizations),
                "agents_executed": len(agent_results),
                "response_length": len(final_response),
                "execution_strategy": "universal_llm_powered_dynamic_system"
            }
            
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(execution_summary, ensure_ascii=False, indent=2))],
                name="execution_summary.json",
                metadata={
                    "content_type": "application/json",
                    "summary_type": "universal_system_execution",
                    "description": "Universal System ì‹¤í–‰ ìš”ì•½"
                }
            )
            
        except Exception as e:
            error_msg = f"Universal System ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            logger.error(error_msg)
            await task_updater.update_status(
                TaskState.failed,
                message=task_updater.new_agent_message(parts=[TextPart(text=error_msg)])
            )
    
    async def _execute_agent_with_comprehensive_instructions(self, agent_name: str, step: Dict, 
                                                           adaptive_context: Dict,
                                                           previous_results: Dict) -> Dict:
        """ğŸ¯ NEW: ì¢…í•©ì  ì§€ì‹œì‚¬í•­ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì‹¤í–‰"""
        
        if agent_name not in self.available_agents:
            return {
                'status': 'failed',
                'error': f'Agent {agent_name} not available',
                'summary': f'ì—ì´ì „íŠ¸ {agent_name}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }
        
        # ì¢…í•©ì  ì§€ì‹œì‚¬í•­ ì‚¬ìš©
        comprehensive_instructions = step.get('comprehensive_instructions', f'{agent_name}ì— ëŒ€í•œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”.')
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        agent_url = self.available_agents[agent_name]['url']
        
        payload = {
            "jsonrpc": "2.0",
            "method": "message/send",
            "params": {
                "message": {
                    "messageId": f"universal_req_{agent_name}_{int(time.time())}",
                    "role": "user",
                    "parts": [{
                        "kind": "text",
                        "text": comprehensive_instructions
                    }]
                }
            },
            "id": f"universal_req_{agent_name}_{int(time.time())}"
        }
        
        try:
            async with httpx.AsyncClient(timeout=180.0) as client:
                response = await client.post(
                    agent_url,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return self._parse_agent_response(result, agent_name)
                else:
                    return {
                        'status': 'failed',
                        'error': f'HTTP {response.status_code}',
                        'summary': f'ì—ì´ì „íŠ¸ í˜¸ì¶œ ì‹¤íŒ¨ (HTTP {response.status_code})'
                    }
                    
        except Exception as e:
            logger.error(f"Agent {agent_name} execution error: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'summary': f'ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Cancel the operation"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.reject()
        logger.info(f"Operation cancelled for context {context.context_id}")

    async def _discover_agents(self) -> Dict[str, Dict[str, Any]]:
        """A2A í‘œì¤€ ì—ì´ì „íŠ¸ ë°œê²¬"""
        available_agents = {}
        
        async with httpx.AsyncClient(timeout=10.0) as client:  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            for agent_name, port in AGENT_PORTS.items():
                try:
                    response = await client.get(f"http://localhost:{port}/.well-known/agent.json")
                    if response.status_code == 200:
                        agent_card = response.json()
                        available_agents[agent_name] = {
                            "name": agent_card.get("name", agent_name),
                            "url": f"http://localhost:{port}",
                            "port": port,
                            "description": agent_card.get("description", ""),
                            "status": "available"
                        }
                        logger.info(f"âœ… {agent_name} agent discovered on port {port}")
                except Exception as e:
                    logger.warning(f"âš ï¸ {agent_name} agent on port {port} not available: {e}")
        
        logger.info(f"ğŸ” Total discovered agents: {len(available_agents)}")
        return available_agents

    async def _understand_request(self, user_input: str) -> Dict[str, Any]:
        """ê°•í™”ëœ LLM ê¸°ë°˜ ìš”ì²­ ì´í•´ - ì‚¬ìš©ì ì˜ë„ë¥¼ ì™„ì „íˆ íŒŒì•…"""
        
        if not self.openai_client:
            return {
                "domain": "general",
                "analysis_type": "exploratory",
                "analysis_depth": "intermediate",
                "tone": "technical",
                "intent_category": "exploratory_analysis",
                "specific_questions": [user_input],
                "business_context": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„ ìš”êµ¬"
            }

        understanding_prompt = f"""ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ê¹Šì´ ë¶„ì„í•˜ì—¬ ì™„ì „íˆ ì´í•´í•˜ì„¸ìš”:

"{user_input}"

ì‚¬ìš©ìì˜ ëª…ì‹œì /ì•”ì‹œì  ìš”êµ¬ì‚¬í•­ì„ ëª¨ë‘ íŒŒì•…í•˜ê³ , 
ì–´ë–¤ ì¢…ë¥˜ì˜ ë¶„ì„ê³¼ ê²°ê³¼ë¬¼ì„ ì›í•˜ëŠ”ì§€ ì •í™•íˆ íŒë‹¨í•˜ì„¸ìš”.

íŠ¹íˆ ë‹¤ìŒì„ ì¤‘ì ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”:
1. ì‚¬ìš©ìê°€ ì†í•œ ë„ë©”ì¸/ì—…ê³„ëŠ” ë¬´ì—‡ì¸ê°€?
2. ì–´ë–¤ ìˆ˜ì¤€ì˜ ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆëŠ”ê°€?
3. ì–´ë–¤ êµ¬ì²´ì ì¸ ë¬¸ì œë¥¼ í•´ê²°í•˜ë ¤ê³  í•˜ëŠ”ê°€?
4. ì–´ë–¤ í˜•íƒœì˜ ê²°ê³¼ë¬¼ì„ ê¸°ëŒ€í•˜ëŠ”ê°€?
5. ë¹„ì¦ˆë‹ˆìŠ¤/ì—…ë¬´ ì»¨í…ìŠ¤íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€?

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "domain": "ê°ì§€ëœ ë„ë©”ì¸ (ì˜ˆ: semiconductor, finance, healthcare, general)",
    "analysis_type": "ë¶„ì„ ìœ í˜• (descriptive/diagnostic/predictive/prescriptive)",
    "analysis_depth": "ë¶„ì„ ê¹Šì´ (basic/intermediate/expert)",
    "urgency": "ê¸´ê¸‰ë„ (low/medium/high)",
    "tone": "ì ì ˆí•œ ì‘ë‹µ í†¤ (casual/professional/technical/academic)",
    "intent_category": "ì˜ë„ ì¹´í…Œê³ ë¦¬",
    "specific_questions": ["ì‚¬ìš©ìê°€ ë‹µì„ ì›í•˜ëŠ” êµ¬ì²´ì  ì§ˆë¬¸ë“¤"],
    "business_context": "ë¹„ì¦ˆë‹ˆìŠ¤/ì—…ë¬´ ë§¥ë½",
    "expertise_claimed": "ì‚¬ìš©ìê°€ ì£¼ì¥í•˜ëŠ” ì „ë¬¸ì„± ìˆ˜ì¤€",
    "expected_deliverables": ["ê¸°ëŒ€í•˜ëŠ” ê²°ê³¼ë¬¼ ìœ í˜•ë“¤"],
    "stakeholder_considerations": ["ê³ ë ¤í•´ì•¼ í•  ì´í•´ê´€ê³„ìë“¤"]
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": understanding_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            understanding = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ“‹ Request understanding: {understanding.get('domain')} domain, {understanding.get('analysis_depth')} depth")
            return understanding
            
        except Exception as e:
            logger.warning(f"Request understanding failed: {e}")
            return {
                "domain": "general",
                "analysis_type": "exploratory", 
                "analysis_depth": "intermediate",
                "tone": "technical",
                "intent_category": "exploratory_analysis",
                "specific_questions": [user_input],
                "business_context": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„ ìš”êµ¬"
            }

    async def _analyze_request_depth(self, user_input: str) -> Dict:
        """ğŸ¯ NEW: ìš”ì²­ì˜ ê¹Šì´ì™€ íŠ¹ì„±ì„ LLMì´ ìë™ ë¶„ì„"""
        
        if not self.openai_client:
            return {
                "detail_level": 5,
                "has_role_description": False,
                "role_description": "",
                "explicit_requirements": ["ê¸°ë³¸ ë¶„ì„"],
                "implicit_needs": ["ë°ì´í„° ì´í•´"],
                "suggested_response_depth": "moderate",
                "needs_clarification": []
            }
        
        analysis_prompt = f"""
        ë‹¤ìŒ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì„¸ìš”:
        "{user_input}"
        
        ë¶„ì„í•  ë‚´ìš©:
        1. ìš”ì²­ì˜ êµ¬ì²´ì„± ìˆ˜ì¤€ (1-10)
        2. ì—­í• ì´ë‚˜ ì „ë¬¸ì„± ì–¸ê¸‰ ì—¬ë¶€
        3. ëª…ì‹œì  ìš”êµ¬ì‚¬í•­ vs ì•”ì‹œì  ë‹ˆì¦ˆ
        4. ì˜ˆìƒë˜ëŠ” ë‹µë³€ì˜ ê¹Šì´
        5. ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•œì§€ ì—¬ë¶€
        
        JSON ì‘ë‹µ:
        {{
            "detail_level": 1-10,
            "has_role_description": true/false,
            "role_description": "ìˆë‹¤ë©´ ì–´ë–¤ ì—­í• ì¸ì§€",
            "explicit_requirements": ["ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•œ ê²ƒë“¤"],
            "implicit_needs": ["ë§¥ë½ìƒ í•„ìš”í•  ê²ƒìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒë“¤"],
            "suggested_response_depth": "brief/moderate/comprehensive",
            "needs_clarification": ["ëª…í™•íˆ í•  í•„ìš”ê°€ ìˆëŠ” ë¶€ë¶„ë“¤"],
            "explicitly_wants_brief": true/false
        }}
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": analysis_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            analysis = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ“Š Request depth analysis: level {analysis.get('detail_level', 5)}/10")
            return analysis
            
        except Exception as e:
            logger.warning(f"Request depth analysis failed: {e}")
            return {
                "detail_level": 5,
                "has_role_description": False,
                "role_description": "",
                "explicit_requirements": ["ê¸°ë³¸ ë¶„ì„"],
                "implicit_needs": ["ë°ì´í„° ì´í•´"],
                "suggested_response_depth": "moderate",
                "needs_clarification": []
            }

    async def _build_adaptive_context(self, user_input: str, request_analysis: Dict) -> Dict:
        """ğŸ¯ NEW: ìš”ì²­ ë¶„ì„ì— ë”°ë¼ ì ì‘ì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•"""
        
        if not self.openai_client:
            return {
                "adopted_perspective": "ì¼ë°˜ ë°ì´í„° ë¶„ì„ê°€",
                "analysis_approach": "í‘œì¤€ ë¶„ì„",
                "response_style": "professional",
                "focus_areas": ["ê¸°ë³¸ í†µê³„"],
                "depth_strategy": "moderate"
            }
        
        context_prompt = f"""
        ì‚¬ìš©ì ìš”ì²­: "{user_input}"
        ìš”ì²­ ë¶„ì„: {json.dumps(request_analysis, ensure_ascii=False)}
        
        ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì ì ˆí•œ ë¶„ì„ ì»¨í…ìŠ¤íŠ¸ë¥¼ êµ¬ì¶•í•˜ì„¸ìš”:
        
        1. ì—­í• ì´ ëª…ì‹œë˜ì—ˆë‹¤ë©´: ê·¸ ì—­í• ì˜ ê´€ì  ì±„íƒ
        2. ì—­í• ì´ ì—†ë‹¤ë©´: ìš”ì²­ ë‚´ìš©ì—ì„œ ì ì ˆí•œ ì „ë¬¸ì„± ìˆ˜ì¤€ ì¶”ë¡ 
        3. ìƒì„¸í•œ ìš”ì²­ì´ë©´: ê¹Šì´ ìˆëŠ” ë¶„ì„ ì¤€ë¹„
        4. ê°„ë‹¨í•œ ìš”ì²­ì´ë©´: í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ
        
        ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë§Œë“œì„¸ìš”:
        {{
            "adopted_perspective": "ì±„íƒí•  ê´€ì ",
            "analysis_approach": "ë¶„ì„ ì ‘ê·¼ ë°©ì‹",
            "response_style": "ë‹µë³€ ìŠ¤íƒ€ì¼",
            "focus_areas": ["ì§‘ì¤‘í•  ì˜ì—­ë“¤"],
            "depth_strategy": "ì–¼ë§ˆë‚˜ ê¹Šì´ ë“¤ì–´ê°ˆì§€"
        }}
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": context_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            context = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ­ Adaptive context: {context.get('adopted_perspective', 'unknown')}")
            return context
            
        except Exception as e:
            logger.warning(f"Adaptive context building failed: {e}")
            return {
                "adopted_perspective": "ì¼ë°˜ ë°ì´í„° ë¶„ì„ê°€",
                "analysis_approach": "í‘œì¤€ ë¶„ì„",
                "response_style": "professional",
                "focus_areas": ["ê¸°ë³¸ í†µê³„"],
                "depth_strategy": "moderate"
            }

    async def _expand_simple_requests(self, user_input: str, request_analysis: Dict) -> str:
        """ğŸ¯ NEW: ê°„ë‹¨í•œ ìš”ì²­ì„ ì§€ëŠ¥ì ìœ¼ë¡œ í™•ì¥ (í•„ìš”í•œ ê²½ìš°ë§Œ)"""
        
        if request_analysis['detail_level'] >= 7:
            # ì´ë¯¸ ì¶©ë¶„íˆ ìƒì„¸í•¨
            return user_input
        
        if request_analysis['needs_clarification'] and not self.openai_client:
            return user_input
        
        if request_analysis['needs_clarification']:
            expansion_prompt = f"""
            ì‚¬ìš©ìì˜ ê°„ë‹¨í•œ ìš”ì²­: "{user_input}"
            
            ì´ ìš”ì²­ì—ì„œ ì‚¬ìš©ìê°€ ì•”ë¬µì ìœ¼ë¡œ ì•Œê³  ì‹¶ì–´í•  ìˆ˜ ìˆëŠ” ê²ƒë“¤ì„ ì¶”ë¡ í•˜ì„¸ìš”.
            í•˜ì§€ë§Œ ê³¼ë„í•˜ê²Œ í™•ì¥í•˜ì§€ ë§ê³ , ë§¥ë½ìƒ í•©ë¦¬ì ì¸ ìˆ˜ì¤€ì—ì„œë§Œ ë³´ì™„í•˜ì„¸ìš”.
            
            ì˜ˆì‹œ:
            - "ë°ì´í„° ë¶„ì„í•´ì¤˜" â†’ ê¸°ë³¸ í†µê³„, íŒ¨í„´, ì´ìƒì¹˜ ì •ë„
            - "ë¶ˆëŸ‰ ì›ì¸ ì°¾ì•„ì¤˜" â†’ ë°ì´í„° ê¸°ë°˜ ì›ì¸ ì¶”ì •, ê°€ëŠ¥ì„± ìˆœìœ„
            
            ì¶”ë¡ ëœ ìƒì„¸ ìš”êµ¬ì‚¬í•­:
            """
            
            try:
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=[{"role": "user", "content": expansion_prompt}],
                    temperature=0.3,
                    timeout=60.0
                )
                
                expanded = response.choices[0].message.content
                logger.info(f"ğŸ“ˆ Request expanded: {len(expanded)} chars")
                return expanded
                
            except Exception as e:
                logger.warning(f"Request expansion failed: {e}")
                return user_input
        
        return user_input

    async def _extract_answer_structure_from_question(self, user_input: str) -> Dict:
        """ğŸ¯ NEW: ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•„ìš”í•œ ë‹µë³€ êµ¬ì¡°ë¥¼ ë™ì ìœ¼ë¡œ ì¶”ì¶œ"""
        
        if not self.openai_client:
            return {
                "required_sections": [
                    {
                        "name": "ë¶„ì„ ê²°ê³¼",
                        "purpose": "ë°ì´í„° ê¸°ë°˜ ë¶„ì„ ìˆ˜í–‰",
                        "required_data": ["ê¸°ë³¸ í†µê³„", "íŒ¨í„´ ë¶„ì„"],
                        "expected_format": "í…ìŠ¤íŠ¸"
                    }
                ],
                "overall_structure": "ê¸°ë³¸ ë¶„ì„ ë³´ê³ ì„œ",
                "key_questions_to_answer": [user_input]
            }
        
        structure_extraction_prompt = f"""
        ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ êµ¬ì¡°ì˜ ë‹µë³€ì„ ì›í•˜ëŠ”ì§€ íŒŒì•…í•˜ì„¸ìš”.
        
        ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"
        
        ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•˜ëŠ” ê²ƒë“¤ì„ ì¶”ì¶œí•˜ì—¬ ë‹µë³€ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ì„¸ìš”.
        ì˜ˆë¥¼ ë“¤ì–´:
        - "ê³µì • ì´ìƒ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³ " â†’ ì´ìƒ ì—¬ë¶€ ì§„ë‹¨ ì„¹ì…˜ í•„ìš”
        - "ì›ì¸ì„ ì„¤ëª…í•˜ë©°" â†’ ì›ì¸ ë¶„ì„ ì„¹ì…˜ í•„ìš”  
        - "ì¡°ì¹˜ ë°©í–¥ì„ ì œì•ˆ" â†’ ì¡°ì¹˜ ë°©ì•ˆ ì„¹ì…˜ í•„ìš”
        
        í•˜ì§€ë§Œ ì´ê²ƒì€ ì˜ˆì‹œì¼ ë¿ì…ë‹ˆë‹¤. 
        ì‚¬ìš©ìê°€ "íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì¤˜"ë¼ê³  í•˜ë©´ íŠ¸ë Œë“œ ë¶„ì„ êµ¬ì¡°ê°€ í•„ìš”í•˜ê³ ,
        "Aì™€ Bë¥¼ ë¹„êµí•´ì¤˜"ë¼ê³  í•˜ë©´ ë¹„êµ ë¶„ì„ êµ¬ì¡°ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
        {{
            "required_sections": [
                {{
                    "name": "ì„¹ì…˜ ì´ë¦„",
                    "purpose": "ì´ ì„¹ì…˜ì˜ ëª©ì ",
                    "required_data": ["í•„ìš”í•œ ë°ì´í„° ìœ í˜•ë“¤"],
                    "expected_format": "í‘œ/ê·¸ë˜í”„/í…ìŠ¤íŠ¸/ëª©ë¡ ë“±"
                }}
            ],
            "overall_structure": "ì „ì²´ì ì¸ ë‹µë³€ íë¦„",
            "key_questions_to_answer": ["ë‹µí•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸ë“¤"]
        }}
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": structure_extraction_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            answer_structure = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ¯ Answer structure extracted: {len(answer_structure.get('required_sections', []))} sections")
            return answer_structure
            
        except Exception as e:
            logger.warning(f"Answer structure extraction failed: {e}")
            return {
                "required_sections": [
                    {
                        "name": "ë¶„ì„ ê²°ê³¼",
                        "purpose": "ë°ì´í„° ê¸°ë°˜ ë¶„ì„ ìˆ˜í–‰",
                        "required_data": ["ê¸°ë³¸ í†µê³„", "íŒ¨í„´ ë¶„ì„"],
                        "expected_format": "í…ìŠ¤íŠ¸"
                    }
                ],
                "overall_structure": "ê¸°ë³¸ ë¶„ì„ ë³´ê³ ì„œ",
                "key_questions_to_answer": [user_input]
            }

    async def _auto_adapt_to_domain(self, user_input: str) -> Dict:
        """ì–´ë–¤ ë„ë©”ì¸ì´ë“  ìë™ìœ¼ë¡œ ì ì‘"""
        
        if not self.openai_client:
            return {"adaptation": "ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ì ‘ê·¼ë²•"}
        
        adaptation_prompt = f"""
ë‹¤ìŒ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìë™ìœ¼ë¡œ ë„ë©”ì¸ê³¼ í•„ìš”í•œ ë¶„ì„ ë°©ë²•ì„ íŒŒì•…í•˜ì„¸ìš”:

{user_input}

ì´ ìš”ì²­ì´ ì–´ë–¤ ë„ë©”ì¸ì¸ì§€, ì–´ë–¤ ì¢…ë¥˜ì˜ ë¶„ì„ì´ í•„ìš”í•œì§€, 
ì–´ë–¤ ì „ë¬¸ ì§€ì‹ì´ í•„ìš”í•œì§€ íŒŒì•…í•˜ì—¬ ìµœì ì˜ ì ‘ê·¼ ë°©ë²•ì„ ì œì‹œí•˜ì„¸ìš”.

íŠ¹íˆ ì‚¬ìš©ìê°€ íŠ¹ì • ì—­í• ì´ë‚˜ ì „ë¬¸ì„±ì„ ì–¸ê¸‰í–ˆë‹¤ë©´, 
ê·¸ì— ë§ëŠ” ë¶„ì„ ê¹Šì´ì™€ ìš©ì–´ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "detected_domain": "ê°ì§€ëœ ë„ë©”ì¸",
    "required_expertise": "í•„ìš”í•œ ì „ë¬¸ì„±",
    "analysis_approach": "ê¶Œì¥ ë¶„ì„ ì ‘ê·¼ë²•",
    "terminology_level": "ìš©ì–´ ìˆ˜ì¤€ (basic/intermediate/expert)",
    "special_considerations": ["íŠ¹ë³„ ê³ ë ¤ì‚¬í•­ë“¤"]
}}
"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": adaptation_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.warning(f"Domain adaptation failed: {e}")
            return {"adaptation": "ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ì ‘ê·¼ë²•"}

    async def _create_comprehensive_execution_plan(self, expanded_request: str, 
                                                  request_understanding: Dict,
                                                  available_agents: Dict) -> Dict:
        """ì™„ì „ LLM ê¸°ë°˜ ë™ì  ê³„íš ìƒì„± - í•˜ë“œì½”ë”© ì œê±°, ë²”ìš©ì  ì ‘ê·¼"""
        
        if not self.openai_client:
            return self._create_fallback_plan(available_agents)
        
        # ì—ì´ì „íŠ¸ë“¤ì˜ ìƒì„¸ ì •ë³´ êµ¬ì¡°í™”
        agents_details = {}
        for name, info in available_agents.items():
            agents_details[name] = {
                "description": info.get('description', ''),
                "capabilities": info.get('capabilities', []),
                "typical_use_cases": info.get('use_cases', [])
            }
        
        planning_prompt = f"""ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³ , ê°€ì¥ íš¨ê³¼ì ì¸ ì—ì´ì „íŠ¸ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì•¼ í•©ë‹ˆë‹¤.

## ğŸ“‹ ì‚¬ìš©ì ìš”ì²­ ë¶„ì„ ê²°ê³¼
{json.dumps(request_understanding, ensure_ascii=False, indent=2)}

## ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë“¤
{json.dumps(agents_details, ensure_ascii=False, indent=2)}

## ğŸ¯ ê³„íš ìˆ˜ë¦½ ì§€ì¹¨
1. **ìš”ì²­ ì¤‘ì‹¬ ì ‘ê·¼**: ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ê²°ê³¼ì— ì§‘ì¤‘í•˜ì—¬ í•„ìš”í•œ ì—ì´ì „íŠ¸ë§Œ ì„ íƒ
2. **ë…¼ë¦¬ì  ìˆœì„œ**: ë°ì´í„° íë¦„ê³¼ ì˜ì¡´ì„±ì„ ê³ ë ¤í•œ ìˆœì„œ ê²°ì •
3. **íš¨ìœ¨ì„± ìµœì í™”**: ë¶ˆí•„ìš”í•œ ë‹¨ê³„ ì œê±°, í•µì‹¬ ë¶„ì„ì— ì§‘ì¤‘
4. **ë„ë©”ì¸ ì ì‘**: {request_understanding.get('domain', 'ì¼ë°˜')} ë„ë©”ì¸ íŠ¹ì„± ë°˜ì˜
5. **ì‚¬ìš©ì ìˆ˜ì¤€ ê³ ë ¤**: {request_understanding.get('expertise_claimed', 'ì¼ë°˜ ì‚¬ìš©ì')} ìˆ˜ì¤€ì— ë§ëŠ” ë¶„ì„ ê¹Šì´

## ğŸš€ ë™ì  ì—ì´ì „íŠ¸ ì„ íƒ ê¸°ì¤€
- ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ê°€ ë¬´ì—‡ì¸ê°€?
- ì–´ë–¤ ì¢…ë¥˜ì˜ ë¶„ì„ì´ ì‹¤ì œë¡œ í•„ìš”í•œê°€?
- ê° ì—ì´ì „íŠ¸ê°€ ì œê³µí•  ìˆ˜ ìˆëŠ” ê°€ì¹˜ëŠ” ë¬´ì—‡ì¸ê°€?
- ìµœì†Œí•œì˜ ë‹¨ê³„ë¡œ ìµœëŒ€í•œì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ì–»ìœ¼ë ¤ë©´?

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "analysis_strategy": "ì´ ìš”ì²­ì— ëŒ€í•œ ì „ì²´ì  ë¶„ì„ ì „ëµ",
    "agent_selection_reasoning": "ì„ íƒëœ ì—ì´ì „íŠ¸ë“¤ê³¼ ê·¸ ì´ìœ ",
    "steps": [
        {{
            "step_number": 1,
            "agent": "ì„ íƒëœ_ì—ì´ì „íŠ¸ëª…",
            "purpose": "ì´ ë‹¨ê³„ì˜ êµ¬ì²´ì  ëª©ì ",
            "enriched_task": "ì—ì´ì „íŠ¸ì—ê²Œ ì „ë‹¬í•  ìƒì„¸í•œ ì‘ì—… ì§€ì‹œ (ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)",
            "expected_output": "ì´ ë‹¨ê³„ì—ì„œ ê¸°ëŒ€í•˜ëŠ” êµ¬ì²´ì  ê²°ê³¼",
            "success_criteria": "ì„±ê³µ íŒë‹¨ ê¸°ì¤€",
            "context_for_next": "ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬í•  í•µì‹¬ ì •ë³´"
        }}
    ],
    "final_synthesis_strategy": "ëª¨ë“  ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ ì¢…í•©í•  ê²ƒì¸ê°€",
    "potential_insights": "ì´ ê³„íšìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆëŠ” ì˜ˆìƒ ì¸ì‚¬ì´íŠ¸ë“¤"
}}

ì¤‘ìš”: í…œí”Œë¦¿ì´ë‚˜ ê³ ì •ëœ íŒ¨í„´ì„ ì‚¬ìš©í•˜ì§€ ë§ê³ , ì´ íŠ¹ì • ìš”ì²­ì— ìµœì í™”ëœ ê³„íšì„ ìˆ˜ë¦½í•˜ì„¸ìš”."""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system", 
                        "content": "ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš° ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ê° ìš”ì²­ì˜ ê³ ìœ í•œ íŠ¹ì„±ì„ íŒŒì•…í•˜ê³ , ê°€ì¥ íš¨ìœ¨ì ì´ê³  íš¨ê³¼ì ì¸ ë¶„ì„ ê²½ë¡œë¥¼ ì„¤ê³„í•©ë‹ˆë‹¤. í•˜ë“œì½”ë”©ëœ í…œí”Œë¦¿ì´ ì•„ë‹Œ, ìš”ì²­ë³„ ë§ì¶¤í˜• ì ‘ê·¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤."
                    },
                    {"role": "user", "content": planning_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.4,  # ì°½ì˜ì  ê³„íš ìˆ˜ë¦½ì„ ìœ„í•´ ì•½ê°„ ë†’ì„
                max_tokens=3000,
                timeout=90.0
            )
            
            plan = json.loads(response.choices[0].message.content)
            
            # ê³„íš ê²€ì¦ ë° ë³´ì •
            validated_plan = self._validate_and_enhance_plan(plan, available_agents, request_understanding)
            
            return validated_plan
            
        except Exception as e:
            logger.warning(f"Dynamic planning failed: {e}")
            return self._create_fallback_plan(available_agents)

    def _validate_and_enhance_plan(self, plan: Dict, available_agents: Dict, understanding: Dict) -> Dict:
        """ìƒì„±ëœ ê³„íšì„ ê²€ì¦í•˜ê³  ë³´ê°•"""
        try:
            # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
            if not plan.get('steps'):
                logger.warning("Plan has no steps, using fallback")
                return self._create_fallback_plan(available_agents)
            
            # ì—ì´ì „íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ë³´ì •
            valid_steps = []
            for step in plan.get('steps', []):
                agent_name = step.get('agent', '')
                if agent_name in available_agents:
                    # í•„ìˆ˜ í•„ë“œ ë³´ì™„
                    enhanced_step = {
                        "agent": agent_name,
                        "purpose": step.get('purpose', f'{agent_name} ë¶„ì„ ìˆ˜í–‰'),
                        "enriched_task": step.get('enriched_task', step.get('purpose', f'{agent_name} ì‘ì—… ìˆ˜í–‰')),
                        "expected_output": step.get('expected_output', f'{agent_name} ë¶„ì„ ê²°ê³¼'),
                        "pass_to_next": step.get('context_for_next', step.get('pass_to_next', ['ë¶„ì„ ê²°ê³¼']))
                    }
                    valid_steps.append(enhanced_step)
                else:
                    logger.warning(f"Agent {agent_name} not available, skipping step")
            
            if not valid_steps:
                logger.warning("No valid steps after validation, using fallback")
                return self._create_fallback_plan(available_agents)
            
            # í–¥ìƒëœ ê³„íš ë°˜í™˜
            enhanced_plan = {
                "reasoning": plan.get('analysis_strategy', plan.get('reasoning', 'ì‚¬ìš©ì ìš”ì²­ì— ìµœì í™”ëœ ë¶„ì„ ì›Œí¬í”Œë¡œìš°')),
                "steps": valid_steps,
                "final_synthesis_guide": plan.get('final_synthesis_strategy', plan.get('final_synthesis_guide', 'ëª¨ë“  ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ')),
                "potential_insights": plan.get('potential_insights', ['ë°ì´í„° ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ']),
                "agent_selection_reasoning": plan.get('agent_selection_reasoning', 'ìš”ì²­ì— ìµœì í™”ëœ ì—ì´ì „íŠ¸ ì„ íƒ')
            }
            
            return enhanced_plan
            
        except Exception as e:
            logger.error(f"Plan validation failed: {e}")
            return self._create_fallback_plan(available_agents)

    async def _execute_agent_with_structure_context(self, agent_name: str, step: Dict, 
                                                    answer_structure: Dict,
                                                    full_context: Dict,
                                                    previous_results: Dict) -> Dict:
        """ê° ì—ì´ì „íŠ¸ì— í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸ì™€ í•¨ê»˜ ì‘ì—… ì „ë‹¬"""
        
        if agent_name not in self.available_agents:
            return {
                'status': 'failed',
                'error': f'Agent {agent_name} not available',
                'summary': f'ì—ì´ì „íŠ¸ {agent_name}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }
        
        # LLMì´ ì´ì „ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ë³´ê°•
        enriched_prompt = await self._enrich_agent_task(
            agent_name, step.get('enriched_task', step.get('purpose', '')),
            full_context, previous_results
        )
        
        # ì—ì´ì „íŠ¸ ì‹¤í–‰
        agent_url = self.available_agents[agent_name]['url']
        
        payload = {
            "jsonrpc": "2.0",
            "method": "message/send",
            "params": {
                "message": {
                    "messageId": f"req_{agent_name}_{int(time.time())}",
                    "role": "user",
                    "parts": [{
                        "kind": "text",
                        "text": enriched_prompt
                    }]
                }
            },
            "id": f"req_{agent_name}_{int(time.time())}"
        }
        
        try:
            async with httpx.AsyncClient(timeout=180.0) as client:  # 3ë¶„ìœ¼ë¡œ ëŒ€í­ ì¦ê°€
                response = await client.post(
                    agent_url,
                    json=payload,
                    headers={"Content-Type": "application/json"}
                )
                
                if response.status_code == 200:
                    result = response.json()
                    return self._parse_agent_response(result, agent_name)
                else:
                    return {
                        'status': 'failed',
                        'error': f'HTTP {response.status_code}',
                        'summary': f'ì—ì´ì „íŠ¸ í˜¸ì¶œ ì‹¤íŒ¨ (HTTP {response.status_code})'
                    }
                    
        except Exception as e:
            logger.error(f"Agent {agent_name} execution error: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'summary': f'ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}'
            }

    async def _assess_content_richness(self, agent_results: Dict) -> Dict:
        """ğŸ¨ NEW: ìƒì„±ëœ ì½˜í…ì¸ ì˜ í’ë¶€í•¨ì„ í‰ê°€í•˜ê³  í™œìš© ë°©ì•ˆ ê²°ì •"""
        
        if not self.openai_client:
            return {
                "has_visualizations": False,
                "visualization_details": [],
                "key_metrics": {},
                "critical_findings": ["ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"],
                "data_quality_score": 5,
                "recommended_inclusion": ["ë¶„ì„ ìš”ì•½"]
            }
        
        assessment_prompt = f"""
        ë‹¤ìŒ ë¶„ì„ ê²°ê³¼ë“¤ì„ í‰ê°€í•˜ì„¸ìš”:
        {json.dumps(agent_results, ensure_ascii=False, indent=2)}
        
        í‰ê°€í•  í•­ëª©:
        1. ì‹œê°í™” ìë£Œ (ì°¨íŠ¸, ê·¸ë˜í”„)ì˜ ì¡´ì¬ì™€ ì¤‘ìš”ë„
        2. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ í†µê³„ ë°ì´í„°
        3. ë°œê²¬ëœ íŒ¨í„´ì´ë‚˜ ì´ìƒì¹˜
        4. ì‹¤ë¬´ì  ì¸ì‚¬ì´íŠ¸ì˜ ê°€ì¹˜
        5. ì‚¬ìš©ìê°€ ë†“ì¹˜ë©´ ì•„ê¹Œìš¸ ì¤‘ìš” ì •ë³´
        
        {{
            "has_visualizations": true/false,
            "visualization_details": ["ì–´ë–¤ ì‹œê°í™”ê°€ ìˆëŠ”ì§€"],
            "key_metrics": {{"ë©”íŠ¸ë¦­ëª…": "ê°’"}},
            "critical_findings": ["ë†“ì¹˜ë©´ ì•ˆ ë˜ëŠ” ë°œê²¬ì‚¬í•­"],
            "data_quality_score": 1-10,
            "recommended_inclusion": ["ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•  ìš”ì†Œë“¤"]
        }}
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": assessment_prompt}],
                response_format={"type": "json_object"},
                temperature=0.3,
                timeout=60.0
            )
            
            assessment = json.loads(response.choices[0].message.content)
            logger.info(f"ğŸ¨ Content richness assessed: score {assessment.get('data_quality_score', 5)}/10")
            return assessment
            
        except Exception as e:
            logger.warning(f"Content richness assessment failed: {e}")
            return {
                "has_visualizations": False,
                "visualization_details": [],
                "key_metrics": {},
                "critical_findings": ["ê¸°ë³¸ ë¶„ì„ ê²°ê³¼"],
                "data_quality_score": 5,
                "recommended_inclusion": ["ë¶„ì„ ìš”ì•½"]
            }

    async def _inject_rich_details(self, 
                              base_response: str,
                              content_assessment: Dict,
                              agent_results: Dict,
                              user_request_analysis: Dict) -> str:
        """ğŸ¨ NEW: ê¸°ë³¸ ì‘ë‹µì— í’ë¶€í•œ ë””í…Œì¼ì„ ì ì‘ì ìœ¼ë¡œ ì£¼ì…"""
        
        if not self.openai_client:
            return base_response
        
        injection_prompt = f"""
        ê¸°ë³¸ ì‘ë‹µ: {base_response}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ í’ë¶€í•œ ì½˜í…ì¸ :
        - ì‹œê°í™”: {content_assessment['visualization_details']}
        - í•µì‹¬ ìˆ˜ì¹˜: {content_assessment['key_metrics']}
        - ì¤‘ìš” ë°œê²¬ì‚¬í•­: {content_assessment['critical_findings']}
        
        ì‚¬ìš©ì ìš”ì²­ íŠ¹ì„±:
        - ìƒì„¸ë„: {user_request_analysis['detail_level']}/10
        - ëª…ì‹œì  ê°„ë‹¨ ìš”ì²­ ì—¬ë¶€: {user_request_analysis.get('explicitly_wants_brief', False)}
        
        ì§€ì¹¨:
        1. ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ "ê°„ë‹¨íˆ"ë¥¼ ìš”ì²­í•˜ì§€ ì•Šì•˜ë‹¤ë©´, ì¤‘ìš”í•œ ë””í…Œì¼ í¬í•¨
        2. ì‹œê°í™”ê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ê³  ì£¼ìš” ì¸ì‚¬ì´íŠ¸ ì„¤ëª…
        3. êµ¬ì²´ì  ìˆ˜ì¹˜ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬í•¨ (ì˜ˆ: "TW í‰ê· ê°’ì´ 3,622ë¡œ ìƒí•œì„  4,080ì˜ 88.8%")
        4. ë°ì´í„°ê°€ í’ë¶€í•  ë•ŒëŠ” ì„¹ì…˜ì„ ë‚˜ëˆ„ì–´ ì²´ê³„ì ìœ¼ë¡œ ì œì‹œ
        5. ì¤‘ìš”í•œ ë°œê²¬ì€ ê°•ì¡° (ë³¼ë“œ, ë¶ˆë¦¿ í¬ì¸íŠ¸ ë“±)
        
        í–¥ìƒëœ ì‘ë‹µì„ ì‘ì„±í•˜ì„¸ìš”. ì›ë³¸ì˜ í†¤ì€ ìœ ì§€í•˜ë˜, ê°€ì¹˜ ìˆëŠ” ì •ë³´ëŠ” ë¹ ëœ¨ë¦¬ì§€ ë§ˆì„¸ìš”.
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": injection_prompt}],
                temperature=0.3,
                timeout=90.0
            )
            
            enhanced_response = response.choices[0].message.content
            logger.info(f"ğŸ’ Rich details injected: {len(enhanced_response)} chars")
            return enhanced_response
            
        except Exception as e:
            logger.warning(f"Rich detail injection failed: {e}")
            return base_response

    async def _integrate_visualizations(self, 
                                  text_response: str,
                                  visualizations: List[Dict]) -> str:
        """ğŸ¨ NEW: ì‹œê°í™”ë¥¼ í…ìŠ¤íŠ¸ ì‘ë‹µì— ìì—°ìŠ¤ëŸ½ê²Œ í†µí•©"""
        
        if not visualizations or not self.openai_client:
            return text_response
        
        integration_prompt = f"""
        í…ìŠ¤íŠ¸ ì‘ë‹µ: {text_response}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œê°í™”:
        {json.dumps(visualizations, ensure_ascii=False)}
        
        ê° ì‹œê°í™”ì— ëŒ€í•´:
        1. ì ì ˆí•œ ìœ„ì¹˜ì— ì°¸ì¡° ì¶”ê°€
        2. ì‹œê°í™”ê°€ ë³´ì—¬ì£¼ëŠ” í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì„¤ëª…
        3. ì¤‘ìš”í•œ ë°ì´í„° í¬ì¸íŠ¸ í…ìŠ¤íŠ¸ë¡œë„ ëª…ì‹œ
        
        ì˜ˆì‹œ:
        "ì•„ë˜ ì‹œê³„ì—´ ì°¨íŠ¸ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, HAE4026 ì¥ë¹„ì˜ TW ê°’ì´ 
        1ì›” 5ì¼ 3,706ì—ì„œ 1ì›” 7ì¼ 7,010ìœ¼ë¡œ 89% ì¦ê°€í–ˆìŠµë‹ˆë‹¤.
        íŠ¹íˆ IS CARBON IMP ê³µì •ì—ì„œ ê¸‰ê²©í•œ ìƒìŠ¹ì´ ê´€ì°°ë©ë‹ˆë‹¤."
        
        ì‹œê°í™”ì™€ í…ìŠ¤íŠ¸ê°€ ìƒí˜¸ë³´ì™„ì ì´ ë˜ë„ë¡ í†µí•©í•˜ì„¸ìš”.
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": integration_prompt}],
                temperature=0.2,
                timeout=90.0
            )
            
            integrated_response = response.choices[0].message.content
            logger.info(f"ğŸ“Š Visualizations integrated successfully")
            return integrated_response
            
        except Exception as e:
            logger.warning(f"Visualization integration failed: {e}")
            return text_response

    async def _enrich_unless_explicitly_simple(self,
                                         user_input: str,
                                         initial_response: str,
                                         available_content: Dict) -> str:
        """ğŸ¨ NEW: ëª…ì‹œì  ê°„ë‹¨ ìš”ì²­ì´ ì•„ë‹ˆë©´ ìë™ìœ¼ë¡œ í’ë¶€í•˜ê²Œ"""
        
        # ê°„ë‹¨í•¨ì„ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í–ˆëŠ”ì§€ í™•ì¸
        simplicity_indicators = ["ê°„ë‹¨íˆ", "ìš”ì•½ë§Œ", "briefly", "summary only", "í•œ ì¤„ë¡œ"]
        explicitly_simple = any(indicator in user_input.lower() for indicator in simplicity_indicators)
        
        if explicitly_simple:
            return initial_response
        
        if not self.openai_client:
            return initial_response
        
        # í’ë¶€í•œ ì½˜í…ì¸  ìë™ ì¶”ê°€
        enrichment_prompt = f"""
        ì‚¬ìš©ìê°€ íŠ¹ë³„íˆ ê°„ë‹¨í•¨ì„ ìš”êµ¬í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ, 
        ë¶„ì„ì˜ ê°€ì¹˜ë¥¼ ìµœëŒ€í•œ ì „ë‹¬í•˜ì„¸ìš”.
        
        í˜„ì¬ ì‘ë‹µ: {initial_response}
        
        ì¶”ê°€ ê°€ëŠ¥í•œ ì½˜í…ì¸ :
        {json.dumps(available_content, ensure_ascii=False)}
        
        ë‹¤ìŒì„ í¬í•¨í•˜ì—¬ ì‘ë‹µì„ í’ë¶€í•˜ê²Œ ë§Œë“œì„¸ìš”:
        1. ğŸ“Š ì‹œê°í™” ê²°ê³¼ì™€ ê·¸ ì˜ë¯¸
        2. ğŸ”¢ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë¹„ìœ¨
        3. ğŸ“ˆ íŠ¸ë Œë“œì™€ íŒ¨í„´
        4. âš ï¸ ì£¼ì˜ê°€ í•„ìš”í•œ ë°œê²¬ì‚¬í•­
        5. ğŸ’¡ ì‹¤ë¬´ì  ì¸ì‚¬ì´íŠ¸
        
        ë³´ê³ ì„œì²˜ëŸ¼ ì„¹ì…˜ì„ ë‚˜ëˆ„ì–´ë„ ì¢‹ìŠµë‹ˆë‹¤:
        - í•µì‹¬ ìš”ì•½
        - ìƒì„¸ ë¶„ì„ ê²°ê³¼
        - ì‹œê°í™” ì¸ì‚¬ì´íŠ¸
        - ê¶Œì¥ ì¡°ì¹˜ì‚¬í•­
        
        ì‚¬ìš©ìê°€ "ì–´ë µê²Œ ë¶„ì„í•œ" ê²°ê³¼ë¥¼ ì¶©ë¶„íˆ í™œìš©í•  ìˆ˜ ìˆê²Œ í•˜ì„¸ìš”.
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": enrichment_prompt}],
                temperature=0.4,
                timeout=120.0
            )
            
            enriched_response = response.choices[0].message.content
            logger.info(f"ğŸŒŸ Response enriched: {len(enriched_response)} chars")
            return enriched_response
            
        except Exception as e:
            logger.warning(f"Response enrichment failed: {e}")
            return initial_response

    def _extract_visualizations(self, agent_results: Dict) -> List[Dict]:
        """ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹œê°í™” ì •ë³´ ì¶”ì¶œ"""
        visualizations = []
        
        for agent_name, result in agent_results.items():
            if isinstance(result, dict):
                # ì•„í‹°íŒ©íŠ¸ì—ì„œ ì‹œê°í™” ì°¾ê¸°
                artifacts = result.get('artifacts', [])
                for artifact in artifacts:
                    if isinstance(artifact, dict):
                        name = artifact.get('name', '')
                        if any(ext in name.lower() for ext in ['.png', '.jpg', '.svg', '.html', 'chart', 'plot', 'graph']):
                            visualizations.append({
                                'agent': agent_name,
                                'name': name,
                                'type': self._infer_chart_type(name),
                                'description': artifact.get('description', ''),
                                'data_points': self._extract_data_points(artifact)
                            })
        
        return visualizations

    def _infer_chart_type(self, filename: str) -> str:
        """íŒŒì¼ëª…ì—ì„œ ì°¨íŠ¸ íƒ€ì… ì¶”ë¡ """
        filename_lower = filename.lower()
        if 'histogram' in filename_lower or 'hist' in filename_lower:
            return 'íˆìŠ¤í† ê·¸ë¨'
        elif 'scatter' in filename_lower:
            return 'ì‚°ì ë„'
        elif 'line' in filename_lower or 'time' in filename_lower:
            return 'ì‹œê³„ì—´ ì°¨íŠ¸'
        elif 'box' in filename_lower:
            return 'ë°•ìŠ¤í”Œë¡¯'
        elif 'bar' in filename_lower:
            return 'ë§‰ëŒ€ ì°¨íŠ¸'
        else:
            return 'ì°¨íŠ¸'

    def _extract_data_points(self, artifact: Dict) -> Dict:
        """ì•„í‹°íŒ©íŠ¸ì—ì„œ í•µì‹¬ ë°ì´í„° í¬ì¸íŠ¸ ì¶”ì¶œ"""
        # ë©”íƒ€ë°ì´í„°ë‚˜ ì„¤ëª…ì—ì„œ ìˆ˜ì¹˜ ì •ë³´ ì¶”ì¶œ ì‹œë„
        description = artifact.get('description', '')
        metadata = artifact.get('metadata', {})
        
        data_points = {}
        
        # ê°„ë‹¨í•œ íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ìˆ˜ì¹˜ ì¶”ì¶œ
        import re
        numbers = re.findall(r'(\w+):\s*([0-9,]+\.?[0-9]*)', description)
        for key, value in numbers:
            try:
                data_points[key] = float(value.replace(',', ''))
            except:
                data_points[key] = value
        
        return data_points

    async def _generate_flexible_response(self,
                                    user_input: str,
                                    request_analysis: Dict,
                                    context: Dict,
                                    agent_results: Dict) -> str:
        """ğŸ¯ NEW: ìš”ì²­ íŠ¹ì„±ì— ë§ëŠ” ìœ ì—°í•œ ì‘ë‹µ ìƒì„±"""
        
        if not self.openai_client:
            return self._create_fallback_synthesis(user_input, agent_results)
        
        # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        base_prompt = f"""
        ì‚¬ìš©ì ìš”ì²­: "{user_input}"
        
        ë¶„ì„ëœ ë°ì´í„°:
        {self._structure_agent_results(agent_results)}
        """
        
        # ì—­í• ì´ ìˆëŠ” ê²½ìš° ì¶”ê°€
        if request_analysis['has_role_description']:
            role_prompt = f"""
            ë‹¹ì‹ ì€ {request_analysis['role_description']}ì˜ ê´€ì ì—ì„œ ì‘ë‹µí•˜ì„¸ìš”.
            í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ ìš©ì–´ì™€ ê´€ì‹¬ì‚¬ë¥¼ ë°˜ì˜í•˜ì„¸ìš”.
            """
        else:
            role_prompt = """
            ì „ë¬¸ì ì´ì§€ë§Œ ì´í•´í•˜ê¸° ì‰¬ìš´ ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
            ê¸°ìˆ ì  ì •í™•ì„±ê³¼ ì‹¤ìš©ì„±ì˜ ê· í˜•ì„ ë§ì¶”ì„¸ìš”.
            """
        
        # ìƒì„¸ë„ì— ë”°ë¥¸ ì§€ì‹œ
        if request_analysis['detail_level'] < 3:
            depth_prompt = """
            í•µì‹¬ë§Œ ê°„ë‹¨ëª…ë£Œí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            ë¶ˆí•„ìš”í•œ ì„¸ë¶€ì‚¬í•­ì€ ì œì™¸í•˜ê³  ì¤‘ìš”í•œ ê²°ê³¼ë§Œ ì „ë‹¬í•˜ì„¸ìš”.
            """
        elif request_analysis['detail_level'] < 7:
            depth_prompt = """
            ì ì ˆí•œ ìˆ˜ì¤€ì˜ ìƒì„¸í•¨ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
            ì£¼ìš” ë°œê²¬ì‚¬í•­ê³¼ ê·¸ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ë˜, ê³¼ë„í•˜ê²Œ ê¸°ìˆ ì ì´ì§€ ì•Šê²Œ í•˜ì„¸ìš”.
            """
        else:
            depth_prompt = """
            í¬ê´„ì ì´ê³  ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.
            ëª¨ë“  ê´€ë ¨ ë°ì´í„°, íŒ¨í„´, ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•˜ì„¸ìš”.
            í•„ìš”í•˜ë‹¤ë©´ ê¸°ìˆ ì  ì„¸ë¶€ì‚¬í•­ë„ ì„¤ëª…í•˜ì„¸ìš”.
            """
        
        # ìµœì¢… í”„ë¡¬í”„íŠ¸ ì¡°í•©
        final_prompt = f"""
        {base_prompt}
        
        {role_prompt}
        
        {depth_prompt}
        
        ë‹µë³€ ì§€ì¹¨:
        - ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•œ ê²ƒ: {request_analysis['explicit_requirements']}
        - ì¶”ê°€ë¡œ ë„ì›€ë  ìˆ˜ ìˆëŠ” ì •ë³´: {request_analysis['implicit_needs']}
        
        í˜•ì‹ì— ì–½ë§¤ì´ì§€ ë§ê³ , ìƒí™©ì— ë§ëŠ” ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.
        """
        
        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": final_prompt}],
                temperature=0.4,
                timeout=120.0
            )
            
            flexible_response = response.choices[0].message.content
            logger.info(f"ğŸ¯ Flexible response generated: {len(flexible_response)} chars")
            return flexible_response
            
        except Exception as e:
            logger.warning(f"Flexible response generation failed: {e}")
            return self._create_fallback_synthesis(user_input, agent_results)

    async def _enrich_agent_task(self, agent_name: str, base_task: str, 
                                context: Dict, previous_results: Dict) -> str:
        """LLMì´ ê° ì—ì´ì „íŠ¸ì˜ ì‘ì—…ì„ ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ë³´ê°•"""
        
        if not self.openai_client:
            return base_task
        
        # ì´ì „ ì—ì´ì „íŠ¸ë“¤ì˜ í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
        previous_insights = self._extract_key_insights(previous_results)
        
        enrichment_prompt = f"""
ì—ì´ì „íŠ¸: {agent_name}
ê¸°ë³¸ ì‘ì—…: {base_task}

ì „ì²´ ë§¥ë½:
- ë„ë©”ì¸: {context['domain']}
- ì‚¬ìš©ì ì—­í• /ì „ë¬¸ì„±: {context.get('expertise_claimed', 'ì¼ë°˜ ì‚¬ìš©ì')}
- ìµœì¢… ëª©í‘œ: {context['key_objectives']}
- ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸: {context.get('domain_context', '')}

ì´ì „ ë¶„ì„ ê²°ê³¼:
{json.dumps(previous_insights, ensure_ascii=False)}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ {agent_name} ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•´ì•¼ í•  êµ¬ì²´ì ì¸ ì‘ì—…ì„ ì‘ì„±í•˜ì„¸ìš”.
ë„ë©”ì¸ ì§€ì‹ê³¼ ì´ì „ ê²°ê³¼ë¥¼ ë°˜ì˜í•˜ì—¬ ë” ì •í™•í•˜ê³  ìœ ìš©í•œ ë¶„ì„ì´ ë˜ë„ë¡ ì§€ì‹œí•˜ì„¸ìš”.
"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",  # ë¹ ë¥¸ ì‘ë‹µì„ ìœ„í•´
                messages=[{"role": "user", "content": enrichment_prompt}],
                temperature=0.2,
                max_tokens=1000,
                timeout=60.0  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.warning(f"Task enrichment failed: {e}")
            return base_task

    async def _synthesize_with_llm(self, original_request: str, 
                                  understanding: Dict, all_results: Dict,
                                  task_updater: StreamingTaskUpdater) -> str:
        """ğŸ¯ NEW: Question-Driven Dynamic Structure ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±"""
        
        logger.info("ğŸ¯ Question-Driven í•©ì„± ì‹œì‘")
        
        if not self.openai_client:
            logger.warning("âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ, fallback ì‚¬ìš©")
            return self._create_fallback_synthesis(original_request, all_results)
        
        try:
            # 1ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ ë‹µë³€ êµ¬ì¡° ì¶”ì¶œ
            logger.info("ğŸ“‹ 1ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ ë‹µë³€ êµ¬ì¡° ì¶”ì¶œ")
            answer_structure = await self._extract_answer_structure_from_question(original_request)
            logger.info(f"âœ… ë‹µë³€ êµ¬ì¡° ì¶”ì¶œ ì™„ë£Œ: {len(answer_structure.get('required_sections', []))} ì„¹ì…˜")
            
            # 2ë‹¨ê³„: ì‹¤ì œ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
            logger.info("ğŸ“Š 2ë‹¨ê³„: ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ")
            data_context = await self._extract_data_context(all_results)
            logger.info(f"âœ… ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: {data_context.get('data_quality', 'unknown')} í’ˆì§ˆ")
            
            # 3ë‹¨ê³„: ì—ì´ì „íŠ¸ ê²°ê³¼ êµ¬ì¡°í™”
            logger.info("ğŸ” 3ë‹¨ê³„: ì—ì´ì „íŠ¸ ê²°ê³¼ êµ¬ì¡°í™”")
            structured_results = self._structure_agent_results(all_results)
            logger.info(f"âœ… ê²°ê³¼ êµ¬ì¡°í™” ì™„ë£Œ: {len(structured_results)} ë¬¸ì")
            
            # 4ë‹¨ê³„: ğŸ¯ NEW - ë™ì  êµ¬ì¡° ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            logger.info("ğŸ¨ 4ë‹¨ê³„: ë™ì  êµ¬ì¡° ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±")
            synthesis_prompt = f"""ë‹¹ì‹ ì€ {understanding.get('domain', 'ë°ì´í„° ë¶„ì„')} ë¶„ì•¼ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

## ğŸ¯ ì‚¬ìš©ìì˜ ì›ë³¸ ì§ˆë¬¸
"{original_request}"

## ğŸ“‹ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë‹µë³€ êµ¬ì¡° (ì§ˆë¬¸ì—ì„œ ì¶”ì¶œ)
ì „ì²´ ë‹µë³€ íë¦„: {answer_structure.get('overall_structure', 'ì§ì ‘ ë‹µë³€')}

í•„ìš”í•œ ì„¹ì…˜ë“¤:
{json.dumps(answer_structure.get('required_sections', []), ensure_ascii=False, indent=2)}

ë‹µí•´ì•¼ í•  í•µì‹¬ ì§ˆë¬¸ë“¤:
{json.dumps(answer_structure.get('key_questions_to_answer', []), ensure_ascii=False, indent=2)}

## ğŸ“Š ì‹¤ì œ ë¶„ì„ëœ ë°ì´í„° ì •ë³´ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€)
- ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°: {len(data_context.get('available_data', []))}ê°œ ì†ŒìŠ¤
- ë°ì´í„° í’ˆì§ˆ: {data_context.get('data_quality', 'unknown')}
- í†µê³„ì  ì¦ê±°: {', '.join(data_context.get('statistical_evidence', [])[:10])}
- ë°ì´í„° ì œí•œì‚¬í•­: {', '.join(data_context.get('limitations', []))}

## ğŸ” ê° ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼
{structured_results}

## âœ… í•„ìˆ˜ ì¤€ìˆ˜ì‚¬í•­ (Question-Driven ë°©ì‹)
1. **ì§ˆë¬¸ êµ¬ì¡° ì™„ì „ ì¤€ìˆ˜**: ìœ„ì—ì„œ ì¶”ì¶œí•œ ë‹µë³€ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë”°ë¥´ì„¸ìš”
2. **ì„¹ì…˜ë³„ ë§ì¶¤ ì‘ì„±**: ê° required_sectionì˜ purposeì™€ expected_formatì— ë§ê²Œ ì‘ì„±
3. **ì‹¤ì œ ë°ì´í„°ë§Œ ì‚¬ìš©**: ìœ„ ë¶„ì„ ê²°ê³¼ë§Œ ì‚¬ìš©í•˜ê³ , ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”
4. **í•µì‹¬ ì§ˆë¬¸ ì™„ì „ ë‹µë³€**: key_questions_to_answerì˜ ëª¨ë“  ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”
5. **êµ¬ì²´ì  ê·¼ê±° ì œì‹œ**: ëª¨ë“  ì£¼ì¥ì— ëŒ€í•´ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê·¼ê±° ì œì‹œ

## âŒ ì ˆëŒ€ ê¸ˆì§€
- ë¯¸ë¦¬ ì •ì˜ëœ í…œí”Œë¦¿ ì‚¬ìš© (ì‚¬ìš©ì ì§ˆë¬¸ êµ¬ì¡°ì™€ ë‹¤ë¥¸ ê²½ìš°)
- ë¶„ì„ë˜ì§€ ì•Šì€ ë‚´ìš© ì¶”ì¸¡
- ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•˜ì§€ ì•Šì€ ì„¹ì…˜ ì¶”ê°€
- ë§‰ì—°í•œ í‘œí˜„ ("ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ", "ëŒ€ì²´ë¡œ" ë“±)

ğŸ¯ ì¤‘ìš”: ì‚¬ìš©ìê°€ ì§ˆë¬¸ì—ì„œ ìš”êµ¬í•œ êµ¬ì¡° ê·¸ëŒ€ë¡œ ë‹µë³€í•˜ì„¸ìš”. 
ì˜ˆë¥¼ ë“¤ì–´ "ì´ìƒ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ê³  ì›ì¸ì„ ì„¤ëª…í•˜ë©° ì¡°ì¹˜ë¥¼ ì œì•ˆ"ì´ë¼ê³  í–ˆë‹¤ë©´, 
ì •í™•íˆ ê·¸ 3ê°€ì§€ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”."""

            logger.info(f"âœ… í”„ë¡¬í”„íŠ¸ ìƒì„± ì™„ë£Œ: {len(synthesis_prompt)} ë¬¸ì")
            
            # 5ë‹¨ê³„: LLM í˜¸ì¶œ
            logger.info("ğŸ¤– 5ë‹¨ê³„: LLM í˜¸ì¶œ")
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": synthesis_prompt}],
                max_tokens=4000,
                temperature=0.3,
                timeout=180
            )
            
            llm_response = response.choices[0].message.content
            logger.info(f"âœ… LLM ì‘ë‹µ ìˆ˜ì‹ : {len(llm_response)} ë¬¸ì")
            
            # 6ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ (í• ë£¨ì‹œë„¤ì´ì…˜ ì²´í¬)
            logger.info("ğŸ” 6ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦")
            quality_ok = await self._validate_response_quality(llm_response, data_context, original_request)
            
            if quality_ok:
                logger.info("âœ… í’ˆì§ˆ ê²€ì¦ í†µê³¼, ìµœì¢… ì‘ë‹µ ë°˜í™˜")
                return llm_response
            else:
                logger.warning("âš ï¸ í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨, ê°•í™” í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„")
                # í’ˆì§ˆì´ ë¶€ì¡±í•˜ë©´ ë” ê°•í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
                retry_result = await self._retry_with_stronger_prompt(
                    original_request, understanding, structured_results, data_context, answer_structure
                )
                logger.info("âœ… ì¬ì‹œë„ ì™„ë£Œ")
                return retry_result
                                                           
        except Exception as e:
            logger.error(f"âŒ Question-Driven í•©ì„± ì‹¤íŒ¨: {e}", exc_info=True)
            logger.warning("ğŸ”„ fallback_synthesisë¡œ ì „í™˜")
            return self._create_fallback_synthesis(original_request, all_results)

    async def _validate_response_quality(self, response: str, data_context: Dict, 
                                       original_request: str) -> bool:
        """ğŸ” ì‘ë‹µ í’ˆì§ˆ ê²€ì¦ - í• ë£¨ì‹œë„¤ì´ì…˜ ë° í”¼ìƒì  ë‹µë³€ ê°ì§€"""
        
        # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
        if len(response) < 300:
            return False
            
        # í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ íŒ¨í„´
        hallucination_patterns = [
            r'ì¼ë°˜ì ìœ¼ë¡œ ì•Œë ¤ì§„', r'ë³´í†µ \w+ëŠ”', r'ëŒ€ì²´ë¡œ', r'í†µìƒì ìœ¼ë¡œ',
            r'ê²½í—˜ìƒ', r'ì¼ë°˜ì ì¸ ê²½ìš°', r'ë³´í¸ì ìœ¼ë¡œ'
        ]
        
        import re
        for pattern in hallucination_patterns:
            if re.search(pattern, response):
                logger.warning(f"í• ë£¨ì‹œë„¤ì´ì…˜ íŒ¨í„´ ê°ì§€: {pattern}")
                return False
        
        # í”¼ìƒì  ë‹µë³€ ê°ì§€
        superficial_patterns = [
            r'ë¶„ì„ì„ í†µí•´ í™•ì¸', r'ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ', r'ë°ì´í„°ë¥¼ í†µí•´',
            r'ì¶”ê°€ ë¶„ì„ì´ í•„ìš”', r'í–¥í›„ ì—°êµ¬ê°€ í•„ìš”'
        ]
        
        superficial_count = sum(1 for pattern in superficial_patterns 
                               if re.search(pattern, response))
        
        if superficial_count > 2:
            logger.warning(f"í”¼ìƒì  í‘œí˜„ ê³¼ë‹¤ ê°ì§€: {superficial_count}ê°œ")
            return False
        
        # ì‚¬ìš©ì ì§ˆë¬¸ ê´€ë ¨ì„± ì²´í¬
        key_terms = original_request.split()[:5]  # ì²« 5ê°œ ë‹¨ì–´
        relevance_score = sum(1 for term in key_terms if term in response)
        
        if relevance_score < 2:
            logger.warning(f"ì§ˆë¬¸ ê´€ë ¨ì„± ë¶€ì¡±: {relevance_score}")
            return False
            
        return True

    async def _retry_with_stronger_prompt(self, original_request: str, understanding: Dict,
                                        structured_results: str, data_context: Dict, answer_structure: Dict) -> str:
        """ğŸ”¥ í’ˆì§ˆ ë¶€ì¡± ì‹œ ë” ê°•í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„"""
        
        stronger_prompt = f"""ğŸš¨ ì¤‘ìš”: ì´ì „ ë‹µë³€ì´ í’ˆì§ˆ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{original_request}"

## ğŸ¯ ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•  ìš”êµ¬ì‚¬í•­
1. ì‚¬ìš©ì ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì—¬ ì§ì ‘ ë‹µë³€
2. ì•„ë˜ ì‹¤ì œ ë¶„ì„ ê²°ê³¼ë§Œ ì‚¬ìš© (ì¶”ì¸¡ ê¸ˆì§€)
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ë°œê²¬ì‚¬í•­ì´ ìˆë‹¤ë©´ ëª…ì‹œ
4. "ì¼ë°˜ì ìœ¼ë¡œ", "ë³´í†µ", "ëŒ€ì²´ë¡œ" ê°™ì€ ë§‰ì—°í•œ í‘œí˜„ ì‚¬ìš© ê¸ˆì§€
5. ìµœì†Œ 500ë‹¨ì–´ ì´ìƒì˜ ìƒì„¸í•œ ë‹µë³€

## ğŸ“Š ì‹¤ì œ ë¶„ì„ ë°ì´í„°
{structured_results}

## ğŸ” ë°ì´í„° ì»¨í…ìŠ¤íŠ¸
- í†µê³„ì  ì¦ê±°: {', '.join(data_context.get('statistical_evidence', []))}
- ë°ì´í„° ì œí•œì‚¬í•­: {', '.join(data_context.get('limitations', []))}

ì‚¬ìš©ìê°€ ì •í™•íˆ ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ íŒŒì•…í•˜ê³ , ë¶„ì„ëœ ì‹¤ì œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": stronger_prompt}],
                max_tokens=4000,
                temperature=0.2,
                timeout=180
            )
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"ì¬ì‹œë„ ì‹¤íŒ¨: {e}")
            return self._create_fallback_synthesis(original_request, {})

    async def _analyze_user_intent_and_structure(self, user_input: str, understanding: Dict) -> Dict:
        """ğŸ¯ ì‚¬ìš©ì ì˜ë„ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ë‹µë³€ êµ¬ì¡°ë¥¼ ë™ì  ìƒì„±"""
        
        if not self.openai_client:
            return {"structure_type": "direct_answer", "guidelines": []}
        
        try:
            structure_prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì í•©í•œ ë‹µë³€ êµ¬ì¡°ë¥¼ ê²°ì •í•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: "{user_input}"
ì§ˆë¬¸ ì˜ë„: {understanding}

ë‹¤ìŒ ì¤‘ì—ì„œ ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ë‹µë³€ í˜•íƒœë¥¼ ì„ íƒí•˜ê³  êµ¬ì¡°ë¥¼ ì œì•ˆí•˜ì„¸ìš”:

1. **ì§ì ‘ ë‹µë³€í˜•**: ì§ˆë¬¸ì— ë°”ë¡œ ë‹µí•˜ëŠ” í˜•íƒœ (ì˜ˆ: "ì–´ë–¤ íŒŒë¼ë¯¸í„°ê°€ ì¤‘ìš”í•œê°€?")
2. **ë¶„ì„ ë³´ê³ ì„œí˜•**: ì²´ê³„ì ì¸ ë¶„ì„ ê²°ê³¼ (ì˜ˆ: "ë°ì´í„°ë¥¼ ë¶„ì„í•´ì¤˜")
3. **ì‹¤í–‰ ê°€ì´ë“œí˜•**: êµ¬ì²´ì  í–‰ë™ ë°©ì•ˆ (ì˜ˆ: "ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì¤˜")
4. **ë¹„êµ ë¶„ì„í˜•**: ì—¬ëŸ¬ ì˜µì…˜ ë¹„êµ (ì˜ˆ: "ì–´ë–¤ ë°©ë²•ì´ ë” ì¢‹ì€ê°€?")
5. **ë¬¸ì œ í•´ê²°í˜•**: ë¬¸ì œ ì§„ë‹¨ ë° í•´ê²°ì±… (ì˜ˆ: "ë¶ˆëŸ‰ë¥ ì„ ì¤„ì´ê³  ì‹¶ì–´")
6. **íƒìƒ‰ì  ë¶„ì„í˜•**: íŒ¨í„´ ë°œê²¬ ë° ì¸ì‚¬ì´íŠ¸ (ì˜ˆ: "ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì°¾ì•„ì¤˜")

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "structure_type": "ì„ íƒëœ ë‹µë³€ í˜•íƒœ",
    "reasoning": "ì„ íƒ ì´ìœ ",
    "key_elements": ["ë‹µë³€ì— ë°˜ë“œì‹œ í¬í•¨í•´ì•¼ í•  í•µì‹¬ ìš”ì†Œë“¤"],
    "tone": "ë‹µë³€ í†¤ (professional/technical/conversational/urgent)",
    "focus_areas": ["ì§‘ì¤‘í•´ì•¼ í•  ì˜ì—­ë“¤"],
    "avoid": ["í”¼í•´ì•¼ í•  ë‚´ìš©ë“¤"]
}}"""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": structure_prompt}],
                max_tokens=800,
                temperature=0.3
            )
            
            # JSON íŒŒì‹±ì„ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
            response_content = response.choices[0].message.content.strip()
            logger.info(f"ğŸ” êµ¬ì¡° ë¶„ì„ ì‘ë‹µ: {response_content[:200]}...")
            
            # JSON ë¸”ë¡ ì¶”ì¶œ (```json...``` í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
            if "```json" in response_content:
                json_start = response_content.find("```json") + 7
                json_end = response_content.find("```", json_start)
                if json_end != -1:
                    response_content = response_content[json_start:json_end].strip()
            elif "```" in response_content:
                json_start = response_content.find("```") + 3
                json_end = response_content.find("```", json_start)
                if json_end != -1:
                    response_content = response_content[json_start:json_end].strip()
            
            try:
                structure_info = json.loads(response_content)
                logger.info(f"âœ… êµ¬ì¡° ë¶„ì„ ì„±ê³µ: {structure_info.get('structure_type', 'unknown')}")
                return structure_info
            except json.JSONDecodeError as json_error:
                logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {json_error}, ì›ë³¸ ì‘ë‹µ: {response_content[:200]}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                return {
                    "structure_type": "direct_answer",
                    "reasoning": "JSON íŒŒì‹± ì‹¤íŒ¨ë¡œ ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©",
                    "key_elements": ["êµ¬ì²´ì  ë‹µë³€", "ë°ì´í„° ê¸°ë°˜ ê·¼ê±°"],
                    "tone": "professional",
                    "focus_areas": ["ì‚¬ìš©ì ì§ˆë¬¸ ì§ì ‘ ë‹µë³€"],
                    "avoid": ["ë¶ˆí•„ìš”í•œ í˜•ì‹ì  êµ¬ì¡°"]
                }
            
        except Exception as e:
            logger.warning(f"êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨, ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©: {e}")
            return {
                "structure_type": "direct_answer",
                "reasoning": "ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©",
                "key_elements": ["êµ¬ì²´ì  ë‹µë³€", "ë°ì´í„° ê¸°ë°˜ ê·¼ê±°"],
                "tone": "professional",
                "focus_areas": ["ì‚¬ìš©ì ì§ˆë¬¸ ì§ì ‘ ë‹µë³€"],
                "avoid": ["ë¶ˆí•„ìš”í•œ í˜•ì‹ì  êµ¬ì¡°"]
            }

    async def _extract_data_context(self, all_results: Dict) -> Dict:
        """ğŸ“Š ë¶„ì„ ê²°ê³¼ì—ì„œ ì‹¤ì œ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€"""
        
        data_context = {
            "available_data": [],
            "key_findings": [],
            "statistical_evidence": [],
            "limitations": [],
            "data_quality": "unknown"
        }
        
        try:
            for agent_name, result in all_results.items():
                if isinstance(result, dict):
                    # ì‹¤ì œ ë°ì´í„° ì •ë³´ ì¶”ì¶œ
                    if 'artifacts' in result:
                        for artifact in result['artifacts']:
                            if isinstance(artifact, dict):
                                data_context["available_data"].append({
                                    "source": agent_name,
                                    "type": artifact.get('contentType', 'unknown'),
                                    "description": artifact.get('name', 'unnamed')
                                })
                    
                    # í•µì‹¬ ë°œê²¬ì‚¬í•­ ì¶”ì¶œ
                    if 'response' in result:
                        response_text = str(result['response'])
                        # í†µê³„ì  ì¦ê±°ë‚˜ êµ¬ì²´ì  ìˆ˜ì¹˜ ì¶”ì¶œ
                        numbers = re.findall(r'\d+\.?\d*%|\d+\.?\d*', response_text)
                        if numbers:
                            data_context["statistical_evidence"].extend(numbers[:5])  # ìµœëŒ€ 5ê°œ
                        
                        # í•µì‹¬ ë°œê²¬ì‚¬í•­ í‚¤ì›Œë“œ ì¶”ì¶œ
                        keywords = re.findall(r'(ì¤‘ìš”|í•µì‹¬|ì£¼ìš”|ë°œê²¬|ê²°ê³¼|ë¶„ì„|ìƒê´€ê´€ê³„|íŒ¨í„´)', response_text)
                        if keywords:
                            data_context["key_findings"].append(f"{agent_name}ì—ì„œ {len(keywords)}ê°œ í•µì‹¬ ë°œê²¬")
            
            # ë°ì´í„° í’ˆì§ˆ í‰ê°€
            if len(data_context["available_data"]) > 3:
                data_context["data_quality"] = "good"
            elif len(data_context["available_data"]) > 1:
                data_context["data_quality"] = "moderate"
            else:
                data_context["data_quality"] = "limited"
                data_context["limitations"].append("ì œí•œëœ ë°ì´í„° ì†ŒìŠ¤")
                
        except Exception as e:
            logger.warning(f"ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            data_context["limitations"].append("ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì œí•œ")
        
        return data_context

    def _structure_agent_results(self, all_results: Dict) -> str:
        """ì—ì´ì „íŠ¸ ê²°ê³¼ë¥¼ LLMì´ ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì¡°í™”"""
        structured = "### ì—ì´ì „íŠ¸ë³„ ìƒì„¸ ë¶„ì„ ê²°ê³¼\n\n"
        
        for agent_name, result in all_results.items():
            status = result.get('status', 'unknown')
            structured += f"#### ğŸ¤– {agent_name} ì—ì´ì „íŠ¸\n"
            structured += f"- **ì‹¤í–‰ ìƒíƒœ**: {status}\n"
            
            if status == 'success':
                # ì„±ê³µí•œ ê²½ìš° ê²°ê³¼ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
                agent_result = result.get('result', {})
                structured += f"- **ìš”ì•½**: {result.get('summary', 'ì‘ì—… ì™„ë£Œ')}\n"
                
                # ê²°ê³¼ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
                if isinstance(agent_result, dict):
                    if 'artifacts' in agent_result:
                        artifacts = agent_result['artifacts']
                        structured += f"- **ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸**: {len(artifacts)}ê°œ\n"
                        for artifact in artifacts[:3]:  # ìµœëŒ€ 3ê°œê¹Œì§€ë§Œ í‘œì‹œ
                            artifact_name = artifact.get('name', 'ì´ë¦„ ì—†ìŒ')
                            artifact_type = artifact.get('metadata', {}).get('content_type', 'íƒ€ì… ë¯¸ì§€ì •')
                            structured += f"  - {artifact_name} ({artifact_type})\n"
                    
                    if 'message' in agent_result:
                        message = agent_result['message']
                        if isinstance(message, dict) and 'parts' in message:
                            parts = message['parts']
                            if parts and len(parts) > 0:
                                first_part = parts[0]
                                if hasattr(first_part, 'text'):
                                    text_preview = first_part.text[:200] + "..." if len(first_part.text) > 200 else first_part.text
                                    structured += f"- **ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°**: {text_preview}\n"
                
                # ì›ì‹œ ê²°ê³¼ ë°ì´í„°ë„ í¬í•¨ (JSON í˜•íƒœ)
                structured += f"- **ìƒì„¸ ê²°ê³¼**: {json.dumps(agent_result, ensure_ascii=False, indent=2)[:500]}...\n"
                
            else:
                # ì‹¤íŒ¨í•œ ê²½ìš° ì˜¤ë¥˜ ì •ë³´
                error_msg = result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                structured += f"- **ì˜¤ë¥˜ ë‚´ìš©**: {error_msg}\n"
                structured += f"- **ì˜í–¥**: ì´ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ëŠ” ìµœì¢… ë¶„ì„ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤\n"
            
            structured += "\n"
        
        # ì „ì²´ ìš”ì•½ ì •ë³´
        total_agents = len(all_results)
        successful_agents = len([r for r in all_results.values() if r.get('status') == 'success'])
        failed_agents = total_agents - successful_agents
        
        structured += f"### ğŸ“Š ì „ì²´ ì‹¤í–‰ ìš”ì•½\n"
        structured += f"- **ì´ ì—ì´ì „íŠ¸ ìˆ˜**: {total_agents}ê°œ\n"
        structured += f"- **ì„±ê³µ**: {successful_agents}ê°œ\n"
        structured += f"- **ì‹¤íŒ¨**: {failed_agents}ê°œ\n"
        structured += f"- **ì„±ê³µë¥ **: {(successful_agents/total_agents*100):.1f}%\n\n"
        
        return structured

    def _extract_key_insights(self, previous_results: Dict) -> Dict:
        """ì´ì „ ê²°ê³¼ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        insights = {}
        for agent_name, result in previous_results.items():
            if result.get('status') == 'success':
                insights[agent_name] = result.get('summary', 'ì‘ì—… ì™„ë£Œ')
            else:
                insights[agent_name] = f"ì˜¤ë¥˜: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        return insights

    def _parse_agent_response(self, response: Dict, agent_name: str) -> Dict:
        """ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹±"""
        try:
            if 'result' in response:
                result = response['result']
                if isinstance(result, dict):
                    # A2A í‘œì¤€ ì‘ë‹µ ì²˜ë¦¬
                    status = result.get('status', {})
                    if isinstance(status, dict) and status.get('state') == 'completed':
                        return {
                            'status': 'success',
                            'result': result,
                            'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ì—… ì™„ë£Œ'
                        }
                    else:
                        return {
                            'status': 'partial',
                            'result': result,
                            'summary': f'{agent_name} ì—ì´ì „íŠ¸ ë¶€ë¶„ ì™„ë£Œ'
                        }
                else:
                    return {
                        'status': 'success',
                        'result': result,
                        'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ì—… ì™„ë£Œ'
                    }
            else:
                return {
                    'status': 'failed',
                    'error': 'No result in response',
                    'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ë‹µ ì˜¤ë¥˜'
                }
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e),
                'summary': f'{agent_name} ì—ì´ì „íŠ¸ ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜'
            }

    def _create_fallback_plan(self, available_agents: Dict[str, Dict[str, Any]]) -> Dict[str, Any]:
        """LLMì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•  ë•Œì˜ í´ë°± ê³„íš"""
        steps = []
        step_number = 1
        
        # ê¸°ë³¸ì ì¸ ë°ì´í„° ë¶„ì„ ì›Œí¬í”Œë¡œìš°
        basic_workflow = [
            ("data_loader", "ë°ì´í„° ë¡œë“œ ë° ê¸°ë³¸ ê²€ì¦", "ë°ì´í„° íŒŒì¼ì„ ë¡œë“œí•˜ê³  ê¸°ë³¸ì ì¸ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤"),
            ("data_cleaning", "ë°ì´í„° í’ˆì§ˆ í™•ì¸ ë° ì •ë¦¬", "ë°ì´í„°ì˜ í’ˆì§ˆì„ í™•ì¸í•˜ê³  í•„ìš”í•œ ì •ë¦¬ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤"),
            ("eda_tools", "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ ìˆ˜í–‰", "ë°ì´í„°ì˜ ë¶„í¬ì™€ íŒ¨í„´ì„ íƒìƒ‰í•˜ê³  ê¸°ì´ˆ í†µê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤"),
            ("data_visualization", "ë°ì´í„° ì‹œê°í™” ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ", "ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤")
        ]
        
        for agent_name, purpose, task_desc in basic_workflow:
            if agent_name in available_agents:
                steps.append({
                    "agent": agent_name,
                    "purpose": purpose,
                    "enriched_task": task_desc,
                    "expected_output": f"{purpose} ê²°ê³¼",
                    "pass_to_next": ["ë¶„ì„ ê²°ê³¼", "ë°ì´í„° ì •ë³´"]
                })
                step_number += 1
        
        return {
            "reasoning": "ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•œ í¬ê´„ì ì¸ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ í‘œì¤€ ì›Œí¬í”Œë¡œìš°",
            "steps": steps,
            "final_synthesis_guide": "ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì¸ì‚¬ì´íŠ¸ ì œê³µ"
        }

    def _create_fallback_synthesis(self, original_request: str, all_results: Dict) -> str:
        """í´ë°± ìµœì¢… ë‹µë³€ ìƒì„±"""
        
        successful_agents = [name for name, result in all_results.items() 
                           if result.get('status') == 'success']
        failed_agents = [name for name, result in all_results.items() 
                        if result.get('status') == 'failed']
        
        synthesis = f"""## ğŸ“Š ë°ì´í„° ë¶„ì„ ê²°ê³¼ ì¢…í•©

### ğŸ¯ ìš”ì²­ ì‚¬í•­
{original_request}

### âœ… ì™„ë£Œëœ ë¶„ì„ ë‹¨ê³„
"""
        
        for agent_name in successful_agents:
            result = all_results[agent_name]
            synthesis += f"- **{agent_name}**: {result.get('summary', 'ì‘ì—… ì™„ë£Œ')}\n"
        
        if failed_agents:
            synthesis += f"\n### âš ï¸ ì¼ë¶€ ì œí•œì‚¬í•­\n"
            for agent_name in failed_agents:
                result = all_results[agent_name]
                synthesis += f"- **{agent_name}**: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n"
        
        synthesis += f"""

### ğŸ‰ ê²°ë¡ 
ì´ {len(successful_agents)}ê°œì˜ ë¶„ì„ ë‹¨ê³„ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. 
ê° ì—ì´ì „íŠ¸ê°€ ìƒì„±í•œ ê²°ê³¼ë¬¼ê³¼ ì•„í‹°íŒ©íŠ¸ë¥¼ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì¶”ê°€ ì§ˆë¬¸ì´ë‚˜ ë” ìì„¸í•œ ë¶„ì„ì´ í•„ìš”í•˜ì‹œë©´ ì–¸ì œë“  ìš”ì²­í•´ ì£¼ì„¸ìš”.
"""
        
        return synthesis

    def _create_beautiful_plan_display(self, execution_plan: Dict, understanding: Dict) -> str:
        """ì˜ˆìœ ì‹¤í–‰ ê³„íš í‘œì‹œ ìƒì„±"""
        
        plan_display = f"""
## ğŸ“‹ LLM ê¸°ë°˜ ë™ì  ì‹¤í–‰ ê³„íš

### ğŸ¯ ë¶„ì„ ê°œìš”
- **ë„ë©”ì¸**: {understanding.get('domain', 'ë°ì´í„° ë¶„ì„')}
- **ëª©í‘œ**: {', '.join(understanding.get('key_objectives', ['ë°ì´í„° ë¶„ì„ ìˆ˜í–‰']))}
- **ë¶„ì„ ê¹Šì´**: {understanding.get('analysis_depth', 'intermediate')}
- **ì´ ë‹¨ê³„**: {len(execution_plan.get('steps', []))}ê°œ

### ğŸš€ ì‹¤í–‰ ë‹¨ê³„ë³„ ê³„íš

"""
        
        for i, step in enumerate(execution_plan.get('steps', [])):
            step_num = i + 1
            agent_name = step.get('agent', step.get('agent_name', 'unknown'))
            purpose = step.get('purpose', '')
            task = step.get('enriched_task', step.get('task_description', ''))
            expected = step.get('expected_output', '')
            
            plan_display += f"""**{step_num}. {agent_name} ì—ì´ì „íŠ¸**
   - ğŸ¯ **ëª©ì **: {purpose}
   - ğŸ“ **ì‘ì—…**: {task[:150]}{'...' if len(task) > 150 else ''}
   - ğŸ“Š **ì˜ˆìƒ ê²°ê³¼**: {expected}

"""
        
        plan_display += f"""
### ğŸ§  ê³„íš ê·¼ê±°
{execution_plan.get('reasoning', 'ì‚¬ìš©ì ìš”ì²­ì— ìµœì í™”ëœ ë¶„ì„ ì›Œí¬í”Œë¡œìš°')}

---
"""
        
        return plan_display

    def _create_beautiful_final_display(self, final_response: str, execution_plan: Dict, 
                                      agent_results: Dict, request_understanding: Dict) -> str:
        """ì˜ˆìœ ìµœì¢… ê²°ê³¼ í‘œì‹œ ìƒì„±"""
        
        successful_agents = [name for name, result in agent_results.items() 
                           if result.get('status') == 'success']
        failed_agents = [name for name, result in agent_results.items() 
                        if result.get('status') == 'failed']
        
        final_display = f"""
## ğŸ‰ LLM ê¸°ë°˜ ì¢…í•© ë¶„ì„ ê²°ê³¼

### ğŸ“ˆ ì‹¤í–‰ ì„±ê³¼
- âœ… **ì„±ê³µí•œ ë‹¨ê³„**: {len(successful_agents)}ê°œ
- âŒ **ì‹¤íŒ¨í•œ ë‹¨ê³„**: {len(failed_agents)}ê°œ
- ğŸ“Š **ì „ì²´ ì„±ê³µë¥ **: {(len(successful_agents) / len(agent_results) * 100):.1f}%

### ğŸ” ë‹¨ê³„ë³„ ì‹¤í–‰ ê²°ê³¼
"""
        
        for i, step in enumerate(execution_plan.get('steps', [])):
            step_num = i + 1
            agent_name = step.get('agent', step.get('agent_name', 'unknown'))
            result = agent_results.get(agent_name, {})
            status = result.get('status', 'unknown')
            summary = result.get('summary', 'ê²°ê³¼ ì—†ìŒ')
            
            status_icon = "âœ…" if status == 'success' else "âŒ" if status == 'failed' else "âš ï¸"
            
            final_display += f"""
**{step_num}. {agent_name}** {status_icon}
   - ğŸ“ **ê²°ê³¼**: {summary}
"""
        
        final_display += f"""

### ğŸ¯ ìµœì¢… ì¢…í•© ë¶„ì„

{final_response}

---
*ğŸ¤– AI DS Team LLM Powered Dynamic Orchestrator v6*
"""
        
        return final_display


def create_llm_powered_orchestrator_server():
    """LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì„œë²„ ìƒì„±"""
    
    agent_card = AgentCard(
        name="AI DS Team LLM Powered Dynamic Orchestrator v6",
        description="LLM ê¸°ë°˜ ë™ì  ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë©€í‹° ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì§€ì›",
        url="http://localhost:8100",
        version="6.0.0",
        capabilities=AgentCapabilities(
            streaming=True,
            pushNotifications=True,
            stateTransitionHistory=True
        ),
        defaultInputModes=["text/plain"],
        defaultOutputModes=["text/plain", "application/json"],
        skills=[
            AgentSkill(
                id="llm_powered_orchestration",
                name="LLM Powered Dynamic Context-Aware Orchestration",
                description="LLMì´ ì‚¬ìš©ì ìš”ì²­ì„ ê¹Šì´ ì´í•´í•˜ê³  ë™ì ìœ¼ë¡œ ìµœì ì˜ ì‹¤í–‰ ê³„íšì„ ìƒì„±í•˜ì—¬ AI DS Team ì—ì´ì „íŠ¸ë“¤ì„ ì¡°ì •í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.",
                tags=["llm-powered", "dynamic-orchestration", "context-aware", "streaming", "multi-agent", "data-science"]
            )
        ]
    )
    
    executor = LLMPoweredOrchestratorExecutor()
    task_store = InMemoryTaskStore()
    request_handler = DefaultRequestHandler(
        agent_executor=executor,
        task_store=task_store
    )
    
    return A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler
    )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("ğŸš€ Starting LLM Powered Dynamic Context-Aware Orchestrator v6.0")
    
    app = create_llm_powered_orchestrator_server()
    
    uvicorn.run(
        app.build(),
        host="0.0.0.0",
        port=8100,
        log_level="info"
    )


if __name__ == "__main__":
    main() 