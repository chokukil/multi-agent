#!/usr/bin/env python3
"""
AI_DS_Team DataCleaningAgent A2A Server (New Implementation)
Port: 8310

ì›ë³¸ ai-data-science-teamì˜ DataCleaningAgentë¥¼ ì°¸ì¡°í•˜ì—¬ A2A í”„ë¡œí† ì½œì— ë§ê²Œ êµ¬í˜„
ë°ì´í„° ë¶€ë¶„ì— pandas-ai íŒ¨í„´ ì ìš©
"""

import asyncio
import sys
import os
from pathlib import Path
import json
import logging
import pandas as pd
import numpy as np
import io

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€ (ë‹¨ìˆœí™”)
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(Path(__file__).parent))  # a2a_ds_servers ë””ë ‰í† ë¦¬ ì¶”ê°€

# A2A imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers.default_request_handler import DefaultRequestHandler
from a2a.server.tasks.inmemory_task_store import InMemoryTaskStore
from a2a.server.tasks.task_updater import TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events.event_queue import EventQueue
from a2a.types import TextPart, TaskState, AgentCard, AgentSkill, AgentCapabilities
from a2a.utils import new_agent_text_message
import uvicorn

# AI_DS_Team imports - ì§ì ‘ ëª¨ë“ˆ import ë°©ì‹
try:
    # __init__.pyë¥¼ ê±°ì¹˜ì§€ ì•Šê³  ì§ì ‘ ëª¨ë“ˆ import
    from ai_data_science_team.tools.dataframe import get_dataframe_summary
    from ai_data_science_team.agents.data_cleaning_agent import DataCleaningAgent, make_data_cleaning_agent
    AI_DS_TEAM_AVAILABLE = True
    logger.info("âœ… AI DS Team ì›ë³¸ ëª¨ë“ˆ ì§ì ‘ import ì„±ê³µ")
except ImportError as e:
    AI_DS_TEAM_AVAILABLE = False
    logger.warning(f"âš ï¸ AI DS Team ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    
    # í´ë°± í•¨ìˆ˜
    def get_dataframe_summary(df: pd.DataFrame) -> str:
        """DataFrame ìš”ì•½ ì •ë³´ ìƒì„± (í´ë°± ë²„ì „)"""
        return f"""
ë°ì´í„° í˜•íƒœ: {df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´
ì»¬ëŸ¼: {list(df.columns)}
ë°ì´í„° íƒ€ì…: {df.dtypes.to_dict()}
ê²°ì¸¡ê°’: {df.isnull().sum().to_dict()}
ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB
"""
    
    # ë”ë¯¸ í´ë˜ìŠ¤
    class DataCleaningAgent:
        pass
    
    def make_data_cleaning_agent(*args, **kwargs):
        return DataCleaningAgent()

# pandas-ai imports (for enhanced data handling)
try:
    from pandasai import Agent as PandasAIAgent
    from pandasai import DataFrame as PandasAIDataFrame
    PANDASAI_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("âœ… pandas-ai ì‚¬ìš© ê°€ëŠ¥")
except ImportError:
    PANDASAI_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("âš ï¸ pandas-ai ë¯¸ì„¤ì¹˜ - ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰")

# Core imports
from core.data_manager import DataManager
from dotenv import load_dotenv

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
data_manager = DataManager()

class PandasAIDataProcessor:
    """pandas-ai íŒ¨í„´ì„ í™œìš©í•œ ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.current_dataframe = None
        self.pandasai_df = None
        
    def parse_data_from_message(self, user_message: str) -> pd.DataFrame:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë°ì´í„°ë¥¼ íŒŒì‹±"""
        logger.info("ğŸ“Š pandas-ai íŒ¨í„´ìœ¼ë¡œ ë©”ì‹œì§€ì—ì„œ ë°ì´í„° íŒŒì‹±...")
        
        # CSV ë°ì´í„° íŒŒì‹±
        lines = user_message.split('\n')
        csv_lines = [line.strip() for line in lines if ',' in line and len(line.split(',')) >= 2]
        
        if len(csv_lines) >= 2:  # í—¤ë” + ë°ì´í„°
            try:
                csv_content = '\n'.join(csv_lines)
                df = pd.read_csv(io.StringIO(csv_content))
                logger.info(f"âœ… CSV ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                return df
            except Exception as e:
                logger.warning(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # JSON ë°ì´í„° íŒŒì‹±
        try:
            json_start = user_message.find('{')
            json_end = user_message.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_content = user_message[json_start:json_end]
                data = json.loads(json_content)
                
                if isinstance(data, list):
                    df = pd.DataFrame(data)
                elif isinstance(data, dict):
                    df = pd.DataFrame([data])
                else:
                    raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” JSON í˜•íƒœ")
                    
                logger.info(f"âœ… JSON ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                return df
        except Exception as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        if any(keyword in user_message.lower() for keyword in ["ìƒ˜í”Œ", "í…ŒìŠ¤íŠ¸", "example", "demo"]):
            return self._create_sample_data()
        
        return None
    
    def _create_sample_data(self) -> pd.DataFrame:
        """ì‚¬ìš©ì ìš”ì²­ì— ì˜í•œ ìƒ˜í”Œ ë°ì´í„° ìƒì„± (LLM First ì›ì¹™)"""
        logger.info("ğŸ”§ ì‚¬ìš©ì ìš”ì²­ìœ¼ë¡œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±...")
        
        # LLM First ì›ì¹™: í•˜ë“œì½”ë”© ëŒ€ì‹  ë™ì  ìƒì„±
        try:
            # ê°„ë‹¨í•œ ì˜ˆì‹œ ë°ì´í„° (ìµœì†Œí•œì˜ êµ¬ì¡°ë§Œ)
            df = pd.DataFrame({
                'id': range(1, 11),
                'name': [f'User_{i}' for i in range(1, 11)],
                'value': np.random.randint(1, 100, 10)
            })
            return df
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()
    
    def create_pandasai_dataframe(self, df: pd.DataFrame, name: str = "dataset", description: str = "User dataset") -> pd.DataFrame:
        """pandas DataFrameì„ PandasAI íŒ¨í„´ìœ¼ë¡œ ì²˜ë¦¬"""
        if not PANDASAI_AVAILABLE:
            logger.info("pandas-ai ì—†ìŒ - ê¸°ë³¸ DataFrame ì‚¬ìš©")
            return df
        
        try:
            self.pandasai_df = PandasAIDataFrame(
                df,
                name=name,
                description=description
            )
            logger.info(f"âœ… PandasAI DataFrame ìƒì„±: {name}")
            return df
        except Exception as e:
            logger.warning(f"PandasAI DataFrame ìƒì„± ì‹¤íŒ¨: {e}")
            return df

class EnhancedDataCleaner:
    """ì›ë³¸ DataCleaningAgent íŒ¨í„´ì„ ë”°ë¥¸ í–¥ìƒëœ ë°ì´í„° í´ë¦¬ë„ˆ"""
    
    def __init__(self):
        self.original_data = None
        self.cleaned_data = None
        self.cleaning_report = []
        self.recommended_steps = []
    
    def get_default_cleaning_steps(self) -> list:
        """ì›ë³¸ DataCleaningAgentì˜ ê¸°ë³¸ í´ë¦¬ë‹ ë‹¨ê³„ë“¤"""
        return [
            "40% ì´ìƒ ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ ì œê±°",
            "ìˆ«ìí˜• ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì„ í‰ê· ìœ¼ë¡œ ëŒ€ì²´", 
            "ë²”ì£¼í˜• ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì„ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´",
            "ì ì ˆí•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜",
            "ì¤‘ë³µ í–‰ ì œê±°",
            "ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±° (ì„ íƒì )",
            "ê·¹ë‹¨ì  ì´ìƒê°’ ì œê±° (IQR 3ë°° ê¸°ì¤€)"
        ]
    
    def clean_data(self, df: pd.DataFrame, user_instructions: str = None) -> dict:
        """
        ì›ë³¸ DataCleaningAgent ìŠ¤íƒ€ì¼ì˜ ë°ì´í„° í´ë¦¬ë‹
        pandas-ai íŒ¨í„´ìœ¼ë¡œ ê°•í™”
        """
        self.original_data = df.copy()
        self.cleaned_data = df.copy()
        self.cleaning_report = []
        
        logger.info(f"ğŸ§¹ Enhanced ë°ì´í„° í´ë¦¬ë‹ ì‹œì‘: {df.shape}")
        
        # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        original_shape = df.shape
        original_memory = df.memory_usage(deep=True).sum() / 1024**2  # MB
        
        # 1. ê²°ì¸¡ê°’ ë¹„ìœ¨ì´ ë†’ì€ ì»¬ëŸ¼ ì œê±° (40% ê¸°ì¤€)
        missing_ratios = df.isnull().mean()
        high_missing_cols = missing_ratios[missing_ratios > 0.4].index.tolist()
        
        if high_missing_cols and "outlier" not in user_instructions.lower():
            self.cleaned_data = self.cleaned_data.drop(columns=high_missing_cols)
            self.cleaning_report.append(f"âœ… 40% ì´ìƒ ê²°ì¸¡ê°’ ì»¬ëŸ¼ ì œê±°: {high_missing_cols}")
        
        # 2. ê²°ì¸¡ê°’ ì²˜ë¦¬
        for col in self.cleaned_data.columns:
            if self.cleaned_data[col].isnull().sum() > 0:
                if self.cleaned_data[col].dtype in ['int64', 'float64']:
                    # ìˆ«ìí˜•: í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
                    mean_val = self.cleaned_data[col].mean()
                    self.cleaned_data[col].fillna(mean_val, inplace=True)
                    self.cleaning_report.append(f"âœ… '{col}' ê²°ì¸¡ê°’ì„ í‰ê· ê°’({mean_val:.2f})ìœ¼ë¡œ ëŒ€ì²´")
                else:
                    # ë²”ì£¼í˜•: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
                    mode_val = self.cleaned_data[col].mode()
                    if not mode_val.empty:
                        self.cleaned_data[col].fillna(mode_val.iloc[0], inplace=True)
                        self.cleaning_report.append(f"âœ… '{col}' ê²°ì¸¡ê°’ì„ ìµœë¹ˆê°’('{mode_val.iloc[0]}')ìœ¼ë¡œ ëŒ€ì²´")
        
        # 3. ë°ì´í„° íƒ€ì… ìµœì í™”
        self._optimize_data_types()
        
        # 4. ì¤‘ë³µ í–‰ ì œê±°
        duplicates_count = self.cleaned_data.duplicated().sum()
        if duplicates_count > 0:
            self.cleaned_data = self.cleaned_data.drop_duplicates()
            self.cleaning_report.append(f"âœ… ì¤‘ë³µ í–‰ {duplicates_count}ê°œ ì œê±°")
        
        # 5. ì´ìƒê°’ ì²˜ë¦¬ (ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ê¸ˆì§€í•˜ì§€ ì•Šì€ ê²½ìš°)
        if user_instructions and "outlier" not in user_instructions.lower():
            self._handle_outliers()
        
        # 6. ìµœì¢… ê²°ê³¼ ê³„ì‚°
        final_shape = self.cleaned_data.shape
        final_memory = self.cleaned_data.memory_usage(deep=True).sum() / 1024**2  # MB
        data_quality_score = self._calculate_quality_score()
        
        return {
            'original_data': self.original_data,
            'cleaned_data': self.cleaned_data,
            'original_shape': original_shape,
            'final_shape': final_shape,
            'memory_saved': original_memory - final_memory,
            'cleaning_report': self.cleaning_report,
            'data_quality_score': data_quality_score,
            'recommended_steps': self.get_default_cleaning_steps()
        }
    
    def _optimize_data_types(self):
        """ë°ì´í„° íƒ€ì… ìµœì í™”"""
        for col in self.cleaned_data.columns:
            if self.cleaned_data[col].dtype == 'object':
                # ë¬¸ìì—´ ì •ê·œí™”
                self.cleaned_data[col] = self.cleaned_data[col].astype(str).str.strip()
            elif self.cleaned_data[col].dtype == 'int64':
                # ì •ìˆ˜í˜• ìµœì í™”
                col_min, col_max = self.cleaned_data[col].min(), self.cleaned_data[col].max()
                if col_min >= 0 and col_max < 255:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('uint8')
                elif col_min >= -128 and col_max < 127:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('int8')
                elif col_min >= -32768 and col_max < 32767:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('int16')
                    
        self.cleaning_report.append("âœ… ë°ì´í„° íƒ€ì… ìµœì í™” ì™„ë£Œ")
    
    def _handle_outliers(self):
        """IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒê°’ ì²˜ë¦¬"""
        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            Q1 = self.cleaned_data[col].quantile(0.25)
            Q3 = self.cleaned_data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 3 * IQR  # ì›ë³¸ì²˜ëŸ¼ 3ë°° ì‚¬ìš©
            upper_bound = Q3 + 3 * IQR
            
            outliers_mask = (self.cleaned_data[col] < lower_bound) | (self.cleaned_data[col] > upper_bound)
            outliers_count = outliers_mask.sum()
            
            if outliers_count > 0:
                # ì´ìƒê°’ì„ ê²½ê³„ê°’ìœ¼ë¡œ í´ë¦¬í•‘
                self.cleaned_data[col] = self.cleaned_data[col].clip(lower_bound, upper_bound)
                self.cleaning_report.append(f"âœ… '{col}' ì´ìƒê°’ {outliers_count}ê°œ ì²˜ë¦¬ (3Ã—IQR ê¸°ì¤€)")
    
    def _calculate_quality_score(self) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        score = 100.0
        
        # ê²°ì¸¡ê°’ ë¹„ìœ¨
        missing_ratio = self.cleaned_data.isnull().sum().sum() / (self.cleaned_data.shape[0] * self.cleaned_data.shape[1])
        score -= missing_ratio * 40
        
        # ì¤‘ë³µ ë¹„ìœ¨
        duplicate_ratio = self.cleaned_data.duplicated().sum() / self.cleaned_data.shape[0]
        score -= duplicate_ratio * 30
        
        # ë°ì´í„° íƒ€ì… ì¼ê´€ì„±
        numeric_ratio = len(self.cleaned_data.select_dtypes(include=[np.number]).columns) / len(self.cleaned_data.columns)
        score += numeric_ratio * 10
        
        return max(0, min(100, score))

class DataCleaningAgentExecutor(AgentExecutor):
    """A2A DataCleaningAgent Executor with pandas-ai pattern"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.data_processor = PandasAIDataProcessor()
        self.data_cleaner = EnhancedDataCleaner()
        logger.info("ğŸ§¹ DataCleaningAgent Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """pandas-ai íŒ¨í„´ì´ ì ìš©ëœ ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰"""
        logger.info(f"ğŸš€ DataCleaningAgent ì‹¤í–‰ ì‹œì‘ - Task: {context.task_id}")
        
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§¹ pandas-ai íŒ¨í„´ DataCleaningAgent ì‹œì‘...")
            )
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
            user_instructions = ""
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if part.root.kind == "text":
                        user_instructions += part.root.text + " "
                
                user_instructions = user_instructions.strip()
                logger.info(f"ğŸ“ ì‚¬ìš©ì ìš”ì²­: {user_instructions}")
                
                # pandas-ai íŒ¨í„´ìœ¼ë¡œ ë°ì´í„° íŒŒì‹±
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message("ğŸ“Š pandas-ai íŒ¨í„´ìœ¼ë¡œ ë°ì´í„° ë¶„ì„ ì¤‘...")
                )
                
                # 1ë‹¨ê³„: ë©”ì‹œì§€ì—ì„œ ë°ì´í„° íŒŒì‹±
                df = self.data_processor.parse_data_from_message(user_instructions)
                
                # 2ë‹¨ê³„: ë°ì´í„°ê°€ ì—†ìœ¼ë©´ DataManager í´ë°±
                if df is None:
                    available_data = data_manager.list_dataframes()
                    if available_data:
                        selected_id = available_data[0]
                        df = data_manager.get_dataframe(selected_id)
                        logger.info(f"âœ… DataManager í´ë°±: {selected_id}")
                    else:
                        # ìƒ˜í”Œ ë°ì´í„° ìƒì„±
                        df = self.data_processor._create_sample_data()
                
                if df is not None and not df.empty:
                    # 3ë‹¨ê³„: pandas-ai DataFrame ìƒì„±
                    self.data_processor.create_pandasai_dataframe(
                        df, name="user_dataset", description=user_instructions[:100]
                    )
                    
                    # 4ë‹¨ê³„: ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰
                    await task_updater.update_status(
                        TaskState.working,
                        message=new_agent_text_message("ğŸ§¹ Enhanced ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰ ì¤‘...")
                    )
                    
                    cleaning_results = self.data_cleaner.clean_data(df, user_instructions)
                    
                    # 5ë‹¨ê³„: ê²°ê³¼ ì €ì¥
                    output_path = f"a2a_ds_servers/artifacts/data/shared_dataframes/cleaned_data_{context.task_id}.csv"
                    os.makedirs(os.path.dirname(output_path), exist_ok=True)
                    cleaning_results['cleaned_data'].to_csv(output_path, index=False)
                    
                    # 6ë‹¨ê³„: ì‘ë‹µ ìƒì„±
                    result = self._generate_response(cleaning_results, user_instructions, output_path)
                    
                else:
                    result = self._generate_no_data_response(user_instructions)
                
                # ìµœì¢… ì‘ë‹µ
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(result)
                )
            else:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message("âŒ ë°ì´í„° í´ë¦¬ë‹ ìš”ì²­ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                )
                
        except Exception as e:
            logger.error(f"âŒ DataCleaningAgent ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"ë°ì´í„° í´ë¦¬ë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            )
    
    def _generate_response(self, results: dict, user_instructions: str, output_path: str) -> str:
        """í´ë¦¬ë‹ ê²°ê³¼ ì‘ë‹µ ìƒì„±"""
        return f"""# ğŸ§¹ **AI DataCleaningAgent ì™„ë£Œ** (pandas-ai íŒ¨í„´)

## ğŸ“Š **í´ë¦¬ë‹ ê²°ê³¼**
- **ì›ë³¸ ë°ì´í„°**: {results['original_shape'][0]:,}í–‰ Ã— {results['original_shape'][1]}ì—´
- **ì •ë¦¬ í›„**: {results['final_shape'][0]:,}í–‰ Ã— {results['final_shape'][1]}ì—´
- **ë©”ëª¨ë¦¬ ì ˆì•½**: {results['memory_saved']:.2f} MB
- **í’ˆì§ˆ ì ìˆ˜**: {results['data_quality_score']:.1f}/100

## ğŸ”§ **ìˆ˜í–‰ëœ ì‘ì—…**
{chr(10).join(f"- {report}" for report in results['cleaning_report'])}

## ğŸ“‹ **ê¸°ë³¸ í´ë¦¬ë‹ ë‹¨ê³„**
{chr(10).join(f"- {step}" for step in results['recommended_steps'])}

## ğŸ” **ì •ë¦¬ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{results['cleaned_data'].head().to_string()}
```

## ğŸ“ˆ **ë°ì´í„° í†µê³„ ìš”ì•½**
```
{results['cleaned_data'].describe().to_string()}
```

## ğŸ“ **ì €ì¥ ê²½ë¡œ**
`{output_path}`

---
**ğŸ’¬ ì‚¬ìš©ì ìš”ì²­**: {user_instructions}
**ğŸ¯ ì²˜ë¦¬ ë°©ì‹**: pandas-ai Enhanced Pattern + AI DataCleaningAgent
**ğŸ•’ ì²˜ë¦¬ ì‹œê°„**: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    def _generate_no_data_response(self, user_instructions: str) -> str:
        """ë°ì´í„° ì—†ìŒ ì‘ë‹µ ìƒì„±"""
        return f"""# âŒ **ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤**

**í•´ê²° ë°©ë²•**:
1. **CSV í˜•íƒœë¡œ ë°ì´í„° í¬í•¨**:
   ```
   name,age,income
   John,25,50000
   Jane,30,60000
   ```

2. **JSON í˜•íƒœë¡œ ë°ì´í„° í¬í•¨**:
   ```json
   {{"name": "John", "age": 25, "income": 50000}}
   ```

3. **ìƒ˜í”Œ ë°ì´í„° ìš”ì²­**: "ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•´ì£¼ì„¸ìš”"

4. **íŒŒì¼ ì—…ë¡œë“œ**: ë°ì´í„° íŒŒì¼ì„ ë¨¼ì € ì—…ë¡œë“œ

**ìš”ì²­**: {user_instructions}
"""
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.reject()
        logger.info(f"DataCleaningAgent ì‘ì—… ì·¨ì†Œ: {context.task_id}")

def main():
    """A2A ì„œë²„ ìƒì„± ë° ì‹¤í–‰"""
    
    # AgentSkill ì •ì˜
    skill = AgentSkill(
        id="enhanced_data_cleaning",
        name="Enhanced Data Cleaning with pandas-ai",
        description="pandas-ai íŒ¨í„´ì´ ì ìš©ëœ ì „ë¬¸ ë°ì´í„° í´ë¦¬ë‹ ì„œë¹„ìŠ¤. ì›ë³¸ ai-data-science-team DataCleaningAgent ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„.",
        tags=["data-cleaning", "pandas-ai", "preprocessing", "quality-improvement"],
        examples=[
            "ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•´ì£¼ì„¸ìš”",
            "ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
            "ì´ìƒê°’ ì œê±° ì—†ì´ ë°ì´í„°ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”",
            "ì¤‘ë³µ ë°ì´í„°ë¥¼ ì œê±°í•˜ê³  í’ˆì§ˆì„ ê°œì„ í•´ì£¼ì„¸ìš”"
        ]
    )
    
    # Agent Card ì •ì˜
    agent_card = AgentCard(
        name="AI DataCleaningAgent (Enhanced)",
        description="pandas-ai íŒ¨í„´ì´ ì ìš©ëœ í–¥ìƒëœ ë°ì´í„° í´ë¦¬ë‹ ì „ë¬¸ê°€. ì›ë³¸ ai-data-science-team ê¸°ë°˜.",
        url="http://localhost:8316/",
        version="2.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=False),
        skills=[skill],
        supportsAuthenticatedExtendedCard=False
    )
    
    # Request Handler ìƒì„±
    request_handler = DefaultRequestHandler(
        agent_executor=DataCleaningAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )
    
    # A2A Server ìƒì„±
    server = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler,
    )
    
    print("ğŸ§¹ Starting Enhanced AI DataCleaningAgent Server (pandas-ai pattern)")
    print("ğŸŒ Server starting on http://localhost:8316")
    print("ğŸ“‹ Agent card: http://localhost:8316/.well-known/agent.json")
    print("âœ¨ Features: pandas-ai pattern + ai-data-science-team compatibility")
    
    uvicorn.run(server.build(), host="0.0.0.0", port=8316, log_level="info")

if __name__ == "__main__":
    main() 