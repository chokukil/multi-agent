import sys
import os
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

#!/usr/bin/env python
"""

Report Generator Agent for A2A Data Analysis Platform
A2A SDK v0.2.9 compliant implementation

This agent synthesizes analysis results from multiple data analysis agents
and generates comprehensive, accurate reports without hallucinations.
"""

import os
import sys
import json
import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import httpx
from openai import AsyncOpenAI

# Add parent directory to Python path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from dotenv import load_dotenv
load_dotenv()

# A2A SDK imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.server.events import EventQueue
from a2a.types import AgentCard, AgentSkill, AgentCapabilities, TaskState
from a2a.utils import new_agent_text_message
from a2a.server.tasks.task_updater import TaskUpdater

# Import core utilities
from core.data_manager import DataManager
from core.session_data_manager import SessionDataManager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Agent port mapping for data collection
AGENT_PORTS = {
    "data_loader": 8307,
    "data_visualization": 8308,
    "eda_tools": 8312,
    "pandas_agent": 8210,
    "data_cleaning": 8306,
    "data_wrangling": 8309,
    "feature_engineering": 8310,
}

@dataclass
class AnalysisResult:
    """Structured analysis result from an agent"""
    agent_name: str
    timestamp: str
    status: str
    data: Dict[str, Any]
    artifacts: List[Dict[str, Any]]
    confidence_score: float = 1.0
    source_references: List[str] = None

    def __post_init__(self):
        if self.source_references is None:
            self.source_references = []


@dataclass
class ReportSection:
    """A section of the final report"""
    title: str
    content: str
    visualizations: List[Dict[str, Any]]
    code_artifacts: List[Dict[str, Any]]
    data_references: List[str]
    confidence_level: float


class LLMEvidenceValidator:
    """LLM-based evidence validation to prevent hallucinations"""
    
    def __init__(self, llm_client: AsyncOpenAI):
        self.llm_client = llm_client
    
    async def validate_with_llm(self, report_content: str, source_data: List[AnalysisResult]) -> Dict[str, Any]:
        """LLM validates report content against source data"""
        
        source_data_formatted = self._format_source_data_for_validation(source_data)
        
        validation_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ ê²€ì¦ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ë‹¤ìŒ ë³´ê³ ì„œì˜ ëª¨ë“  ìˆ˜ì¹˜, í†µê³„, ì£¼ìž¥ì„ ì›ë³¸ ë°ì´í„°ì™€ ëŒ€ì¡°í•˜ì—¬ ê²€ì¦í•˜ì„¸ìš”.

ë³´ê³ ì„œ ë‚´ìš©:
{report_content}

ì›ë³¸ ë°ì´í„°:
{source_data_formatted}

ê²€ì¦ ìž‘ì—…:
1. ë³´ê³ ì„œì˜ ëª¨ë“  ìˆ˜ì¹˜ê°€ ì›ë³¸ ë°ì´í„°ì—ì„œ í™•ì¸ ê°€ëŠ¥í•œì§€ ì²´í¬
2. í†µê³„ì  ì£¼ìž¥ì´ ë°ì´í„°ë¡œ ë’·ë°›ì¹¨ë˜ëŠ”ì§€ í™•ì¸
3. ìž˜ëª»ëœ í•´ì„ì´ë‚˜ ê³¼ìž¥ëœ í‘œí˜„ì´ ìžˆëŠ”ì§€ ì ê²€
4. ì›ë³¸ ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì´ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "validation_status": "valid|needs_correction|invalid",
    "confidence_score": 0.0-1.0,
    "issues_found": [
        {{
            "type": "numerical|statistical|interpretation|unsupported",
            "description": "êµ¬ì²´ì ì¸ ë¬¸ì œì ",
            "severity": "high|medium|low"
        }}
    ],
    "validated_report": "ìˆ˜ì •ëœ ë³´ê³ ì„œ ë‚´ìš© (í•„ìš”ì‹œ)",
    "validation_notes": "ê²€ì¦ ê³¼ì •ì—ì„œ ë°œê²¬ëœ ì£¼ìš” ì‚¬í•­ë“¤"
}}
"""
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": validation_prompt}],
                temperature=0.1,
                max_tokens=3000
            )
            
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            logger.error(f"LLM validation error: {e}")
            return {
                "validation_status": "error",
                "confidence_score": 0.0,
                "issues_found": [{"type": "system", "description": f"Validation error: {str(e)}", "severity": "high"}],
                "validated_report": report_content,
                "validation_notes": "Validation failed due to system error"
            }
    
    def _format_source_data_for_validation(self, source_data: List[AnalysisResult]) -> str:
        """Format source data for LLM validation"""
        formatted_data = []
        
        for result in source_data:
            agent_data = {
                "agent": result.agent_name,
                "timestamp": result.timestamp,
                "data": result.data,
                "confidence": result.confidence_score
            }
            formatted_data.append(f"=== {result.agent_name} ===\n{json.dumps(agent_data, indent=2, ensure_ascii=False)}")
        
        return "\n\n".join(formatted_data)


class VisualizationAggregator:
    """Aggregates and validates visualizations from multiple agents"""
    
    def __init__(self):
        self.supported_types = ["line", "bar", "scatter", "heatmap", "pie", "box", "histogram"]
    
    def aggregate(self, agent_results: List[AnalysisResult]) -> List[Dict[str, Any]]:
        """Aggregate visualizations from multiple agents"""
        all_visualizations = []
        
        for result in agent_results:
            if "visualizations" in result.data:
                for viz in result.data["visualizations"]:
                    # Add source agent information
                    viz["source_agent"] = result.agent_name
                    viz["confidence"] = result.confidence_score
                    all_visualizations.append(viz)
        
        # Remove duplicates and prioritize by confidence
        return self._deduplicate_visualizations(all_visualizations)
    
    def _deduplicate_visualizations(self, visualizations: List[Dict]) -> List[Dict]:
        """Remove duplicate visualizations, keeping highest confidence ones"""
        unique_viz = {}
        
        for viz in visualizations:
            # Create a key based on viz type and data characteristics
            key = f"{viz.get('type', 'unknown')}_{viz.get('title', 'untitled')}"
            
            if key not in unique_viz or viz.get("confidence", 0) > unique_viz[key].get("confidence", 0):
                unique_viz[key] = viz
        
        return list(unique_viz.values())
    
    def create_dashboard_layout(self, visualizations: List[Dict]) -> Dict[str, Any]:
        """Create a dashboard layout for visualizations"""
        layout = {
            "title": "Data Analysis Dashboard",
            "grid": [],
            "summary_charts": [],
            "detailed_charts": []
        }
        
        # Categorize visualizations
        for viz in visualizations:
            if viz.get("type") in ["pie", "bar"] and viz.get("priority") == "high":
                layout["summary_charts"].append(viz)
            else:
                layout["detailed_charts"].append(viz)
        
        return layout


class LLMReportGenerator:
    """LLM-driven report generator that creates user-intent-specific reports"""
    
    def __init__(self, llm_client: AsyncOpenAI):
        self.llm_client = llm_client
    
    async def analyze_report_intent(self, user_query: str) -> Dict[str, Any]:
        """Analyze user's intent for report generation"""
        
        intent_prompt = f"""
ì‚¬ìš©ìž ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ì–´ë–¤ ì¢…ë¥˜ì˜ ë³´ê³ ì„œë¥¼ ì›í•˜ëŠ”ì§€ íŒŒì•…í•˜ì„¸ìš”.

ì‚¬ìš©ìž ìš”ì²­: "{user_query}"

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "main_focus": "ì‚¬ìš©ìžì˜ ì£¼ìš” ê´€ì‹¬ì‚¬ (ì˜ˆ: íŠ¸ë Œë“œ ë¶„ì„, ì´ìƒì¹˜ íƒì§€, ìƒê´€ê´€ê³„ ë¶„ì„, ì˜ˆì¸¡, ìš”ì•½í†µê³„ ë“±)",
    "detail_level": "summary|detailed|technical",
    "target_audience": "executive|analyst|developer|general",
    "preferred_format": "narrative|bullet_points|structured|mixed",
    "visualization_preference": "high|medium|low",
    "code_inclusion": "high|medium|low|none",
    "key_questions": ["ì‚¬ìš©ìžê°€ ë‹µì„ ì›í•˜ëŠ” êµ¬ì²´ì ì¸ ì§ˆë¬¸ë“¤"],
    "analysis_scope": "broad|focused|comparative",
    "report_tone": "formal|casual|technical"
}}
"""
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": intent_prompt}],
                temperature=0.2,
                max_tokens=800
            )
            
            result = json.loads(response.choices[0].message.content)
            return result
            
        except Exception as e:
            logger.error(f"Intent analysis error: {e}")
            return {
                "main_focus": "general analysis",
                "detail_level": "detailed",
                "target_audience": "general",
                "preferred_format": "mixed",
                "visualization_preference": "medium",
                "code_inclusion": "medium",
                "key_questions": [user_query],
                "analysis_scope": "broad",
                "report_tone": "formal"
            }
    
    async def generate_report(self, user_query: str, intent: Dict[str, Any], agent_results: List[AnalysisResult]) -> str:
        """Generate a customized report based on user intent and agent results"""
        
        # Format agent results for LLM consumption
        formatted_results = self._format_agent_results_for_llm(agent_results)
        
        report_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ ìž‘ì„± ì „ë¬¸ê°€ìž…ë‹ˆë‹¤. ì‚¬ìš©ìžì˜ ìš”ì²­ê³¼ ì˜ë„ì— ë§žëŠ” ë§žì¶¤í˜• ë³´ê³ ì„œë¥¼ ìž‘ì„±í•˜ì„¸ìš”.

ì‚¬ìš©ìž ì›ë³¸ ìš”ì²­: "{user_query}"

ì‚¬ìš©ìž ì˜ë„ ë¶„ì„:
{json.dumps(intent, indent=2, ensure_ascii=False)}

ë¶„ì„ ê²°ê³¼ ë°ì´í„°:
{formatted_results}

ë³´ê³ ì„œ ìž‘ì„± ì§€ì¹¨:
1. ì‚¬ìš©ìžì˜ ì˜ë„({intent.get('main_focus', 'general analysis')})ì— ì •í™•ížˆ ë¶€í•©í•˜ëŠ” ë‚´ìš©ìœ¼ë¡œ êµ¬ì„±
2. ìƒì„¸ ìˆ˜ì¤€: {intent.get('detail_level', 'detailed')}
3. ëŒ€ìƒ ë…ìž: {intent.get('target_audience', 'general')}
4. í˜•ì‹: {intent.get('preferred_format', 'mixed')}
5. ì‹œê°í™” ì„ í˜¸ë„: {intent.get('visualization_preference', 'medium')}
6. ì½”ë“œ í¬í•¨ ìˆ˜ì¤€: {intent.get('code_inclusion', 'medium')}
7. í†¤: {intent.get('report_tone', 'formal')}

í•µì‹¬ ë‹µë³€ ì§ˆë¬¸ë“¤:
{chr(10).join(f"- {q}" for q in intent.get('key_questions', [user_query]))}

ì¤‘ìš” ì›ì¹™:
- ëª¨ë“  ìˆ˜ì¹˜ì™€ í†µê³„ëŠ” ì œê³µëœ ë¶„ì„ ê²°ê³¼ì—ì„œë§Œ ì¸ìš©
- ê° ì£¼ìž¥ì—ëŠ” ë°˜ë“œì‹œ ì¶œì²˜ ëª…ì‹œ (ì˜ˆ: [pandas_agent ë¶„ì„], [EDA ê²°ê³¼])
- ì¶”ì¸¡ì´ë‚˜ ê°€ì •ì€ ëª…ì‹œì ìœ¼ë¡œ í‘œí˜„
- ë°ì´í„°ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
- ì‚¬ìš©ìžê°€ ì›í•˜ëŠ” ë‹µë³€ì— ì§‘ì¤‘

ë³´ê³ ì„œë¥¼ ìž‘ì„±í•˜ì„¸ìš”:
"""
        
        try:
            response = await self.llm_client.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": report_prompt}],
                temperature=0.3,
                max_tokens=4000
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Report generation error: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _format_agent_results_for_llm(self, agent_results: List[AnalysisResult]) -> str:
        """Format agent results for LLM consumption"""
        formatted_sections = []
        
        for result in agent_results:
            section = f"""
=== {result.agent_name} ë¶„ì„ ê²°ê³¼ ===
ì‹¤í–‰ ì‹œê°„: {result.timestamp}
ì‹ ë¢°ë„: {result.confidence_score:.2f}

ë¶„ì„ ë°ì´í„°:
{json.dumps(result.data, indent=2, ensure_ascii=False)}

ì•„í‹°íŒ©íŠ¸:
{json.dumps(result.artifacts, indent=2, ensure_ascii=False)}

ë°ì´í„° ì¶œì²˜: {', '.join(result.source_references)}
"""
            formatted_sections.append(section)
        
        return "\n".join(formatted_sections)


class ReportGeneratorAgent:
    """Main Report Generator Agent that orchestrates LLM-driven report creation"""
    
    def __init__(self):
        # Initialize LLM client
        self.llm_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        # Initialize LLM-driven components
        self.validator = LLMEvidenceValidator(self.llm_client)
        self.visualizer = VisualizationAggregator()
        self.report_generator = LLMReportGenerator(self.llm_client)
        
        # Keep existing data managers
        self.data_manager = DataManager()
        self.session_manager = SessionDataManager()
        self.http_client = httpx.AsyncClient(timeout=30.0)
    
    async def generate_report(self, session_id: str, user_query: str, agent_list: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        Generate a comprehensive LLM-driven report based on agent analysis results
        
        Args:
            session_id: Session identifier for data retrieval
            user_query: Original user query for context
            agent_list: List of agents to collect results from (None = all)
        
        Returns:
            Generated report with all artifacts
        """
        logger.info(f"ðŸš€ Generating LLM-driven report for session: {session_id}")
        
        # Step 1: Analyze user intent
        intent_analysis = await self.report_generator.analyze_report_intent(user_query)
        logger.info(f"âœ“ User intent analyzed: {intent_analysis.get('main_focus', 'general')}")
        
        # Step 2: Collect results from all relevant agents
        agent_results = await self._collect_agent_results(session_id, agent_list)
        
        # Step 3: Basic confidence scoring (lightweight validation)
        validated_results = []
        for result in agent_results:
            # Simple confidence scoring based on data completeness
            confidence = self._calculate_basic_confidence(result)
            result.confidence_score = confidence
            validated_results.append(result)
            logger.info(f"âœ“ Processed results from {result.agent_name} (confidence: {confidence:.2f})")
        
        # Step 4: Generate LLM-driven report
        report_content = await self.report_generator.generate_report(
            user_query, 
            intent_analysis, 
            validated_results
        )
        
        # Step 5: Optional LLM validation (fact-checking)
        if os.getenv("ENABLE_FACT_CHECK", "true").lower() == "true":
            logger.info("ðŸ” Performing LLM-based fact checking...")
            validation_result = await self.validator.validate_with_llm(report_content, validated_results)
            
            if validation_result["validation_status"] == "needs_correction":
                logger.warning("âš ï¸ Report needed corrections after fact-checking")
                report_content = validation_result.get("validated_report", report_content)
            elif validation_result["validation_status"] == "valid":
                logger.info("âœ… Report passed fact-checking validation")
            
            # Store validation info
            validation_info = {
                "validation_status": validation_result["validation_status"],
                "confidence_score": validation_result["confidence_score"],
                "issues_found": validation_result["issues_found"],
                "validation_notes": validation_result["validation_notes"]
            }
        else:
            validation_info = {"validation_status": "skipped", "confidence_score": 0.8}
        
        # Step 6: Aggregate visualizations and artifacts
        aggregated_visualizations = self.visualizer.aggregate(validated_results)
        dashboard_layout = self.visualizer.create_dashboard_layout(aggregated_visualizations)
        
        # Step 7: Compile final report package
        report_package = {
            "report_content": report_content,
            "metadata": {
                "session_id": session_id,
                "user_query": user_query,
                "timestamp": datetime.now().isoformat(),
                "generator": "CherryAI LLM-First Report Generator v1.0",
                "intent_analysis": intent_analysis,
                "validation_info": validation_info,
                "agent_count": len(validated_results)
            },
            "visualizations": aggregated_visualizations,
            "dashboard_layout": dashboard_layout,
            "artifacts": self._collect_all_artifacts(validated_results),
            "agent_performance": self._calculate_agent_metrics(validated_results)
        }
        
        logger.info(f"âœ“ LLM-driven report generated successfully with {len(validated_results)} agent results")
        
        return report_package
    
    async def _collect_agent_results(self, session_id: str, agent_list: Optional[List[str]] = None) -> List[AnalysisResult]:
        """Collect analysis results from multiple agents"""
        results = []
        agents_to_query = agent_list or list(AGENT_PORTS.keys())
        
        # Try to get results from session artifacts first
        session_data = await self.session_manager.get_session_data(session_id)
        
        for agent_name in agents_to_query:
            try:
                # Check if agent has artifacts in session
                agent_artifacts = await self._get_agent_artifacts(session_id, agent_name)
                
                if agent_artifacts:
                    result = AnalysisResult(
                        agent_name=agent_name,
                        timestamp=datetime.now().isoformat(),
                        status="completed",
                        data=agent_artifacts.get("data", {}),
                        artifacts=agent_artifacts.get("artifacts", []),
                        source_references=[f"session:{session_id}", f"agent:{agent_name}"]
                    )
                    results.append(result)
                    logger.info(f"âœ“ Collected results from {agent_name}")
                else:
                    logger.warning(f"âœ— No artifacts found for {agent_name}")
                    
            except Exception as e:
                logger.error(f"Error collecting from {agent_name}: {e}")
        
        return results
    
    async def _get_agent_artifacts(self, session_id: str, agent_name: str) -> Optional[Dict]:
        """Retrieve artifacts from a specific agent"""
        # First try session-based artifacts
        artifacts_path = f"ai_ds_team/data/session_{session_id}/{agent_name}_artifacts.json"
        
        if os.path.exists(artifacts_path):
            with open(artifacts_path, "r") as f:
                return json.load(f)
        
        # Try shared dataframes directory
        shared_path = f"a2a_ds_servers/artifacts/data/shared_dataframes/session_{session_id}_{agent_name}.json"
        
        if os.path.exists(shared_path):
            with open(shared_path, "r") as f:
                return json.load(f)
        
        return None
    
    def _calculate_basic_confidence(self, result: AnalysisResult) -> float:
        """Calculate basic confidence score based on data completeness"""
        confidence = 0.5  # Base confidence
        
        # Boost confidence based on data completeness
        if result.data:
            confidence += 0.2
        
        if result.artifacts:
            confidence += 0.2
        
        if result.source_references:
            confidence += 0.1
        
        # Cap at 1.0
        return min(confidence, 1.0)
    
    def _calculate_agent_metrics(self, results: List[AnalysisResult]) -> List[Dict]:
        """Calculate performance metrics for each agent"""
        metrics = []
        
        for result in results:
            execution_time = result.data.get("execution_time", 0)
            if not execution_time and "timestamp" in result.data:
                # Estimate execution time
                execution_time = 1000  # Default 1 second
            
            metrics.append({
                "agent": result.agent_name,
                "execution_time": execution_time,
                "confidence": result.confidence_score
            })
        
        return metrics
    
    async def _assess_data_quality(self, session_id: str) -> Dict[str, Any]:
        """Assess overall data quality for the session"""
        quality_report = {
            "total_records": 0,
            "missing_values": 0,
            "outliers": 0
        }
        
        # Try to get data quality metrics from session data
        session_data = await self.session_manager.get_session_data(session_id)
        
        if session_data and "data_quality" in session_data:
            quality_report.update(session_data["data_quality"])
        else:
            # Default values if no quality data available
            quality_report = {
                "total_records": "N/A",
                "missing_values": "N/A",
                "outliers": "N/A"
            }
        
        return quality_report
    
    def _collect_all_artifacts(self, results: List[AnalysisResult]) -> List[Dict]:
        """Collect all artifacts from results"""
        all_artifacts = []
        
        for result in results:
            for artifact in result.artifacts:
                artifact["source_agent"] = result.agent_name
                all_artifacts.append(artifact)
        
        return all_artifacts
    
    async def invoke(self, query: str, session_id: Optional[str] = None) -> str:
        """Main entry point for LLM-driven report generation"""
        if not session_id:
            # Extract session ID from query if possible
            import re
            match = re.search(r'session[_-]?([a-f0-9\-]{8,})', query, re.IGNORECASE)
            if match:
                session_id = match.group(1)
            else:
                return "âš ï¸ ì„¸ì…˜ IDê°€ í•„ìš”í•©ë‹ˆë‹¤. ì¿¼ë¦¬ì— ì„¸ì…˜ IDë¥¼ í¬í•¨í•˜ê±°ë‚˜ ë³„ë„ë¡œ ì œê³µí•´ì£¼ì„¸ìš”."
        
        try:
            # Generate the LLM-driven report
            report_package = await self.generate_report(session_id, query)
            
            # Return the main report content
            return report_package["report_content"]
            
        except Exception as e:
            logger.error(f"Report generation failed: {e}")
            return f"ë³´ê³ ì„œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


class ReportGeneratorExecutor(AgentExecutor):
    """A2A Executor for Report Generator Agent"""
    
    def __init__(self):
        self.agent = ReportGeneratorAgent()
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Execute report generation request"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            # Extract user query
            user_query = ""
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if part.root.kind == "text":
                        user_query += part.root.text + " "
            
            user_query = user_query.strip()
            if not user_query:
                await task_updater.reject(message="Query is empty")
                return
            
            logger.info(f"ðŸš€ Report Generator processing: {user_query}")
            
            # Extract session ID from context or query
            session_id = context.metadata.get("session_id")
            if not session_id:
                # Try to extract from query
                import re
                match = re.search(r'session[_-]?([a-f0-9\-]+)', user_query, re.IGNORECASE)
                if match:
                    session_id = match.group(1)
            
            if not session_id:
                await task_updater.reject(message="Session ID not found. Please provide a session ID.")
                return
            
            # Generate LLM-driven report
            report_package = await self.agent.generate_report(session_id, user_query)
            
            # Send report content as response
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(report_package["report_content"])
            )
            
            # Log additional information
            metadata = report_package.get("metadata", {})
            logger.info(f"Report generated with intent: {metadata.get('intent_analysis', {}).get('main_focus', 'unknown')}")
            logger.info(f"Validation status: {metadata.get('validation_info', {}).get('validation_status', 'unknown')}")
            
            # Optionally store artifacts
            if "artifacts" in report_package:
                logger.info(f"Generated {len(report_package['artifacts'])} artifacts")
            
        except Exception as e:
            logger.error(f"Execution error: {e}")
            await task_updater.reject(message=f"Report generation failed: {str(e)}")
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Handle task cancellation"""
        logger.info(f"Cancelling task {context.task_id}")


# Initialize the A2A application
def create_app():
    """Create and configure the A2A application"""
    # Agent metadata
    agent_card = AgentCard(
        name="report_generator",
        description="Generates comprehensive, accurate data analysis reports by synthesizing results from multiple analysis agents",
        url="http://localhost:8315/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        skills=[
            AgentSkill(
                id="generate_report",
                name="Generate Comprehensive Report",
                description="Generate comprehensive, evidence-based reports from multiple analysis results",
                tags=["report", "synthesis", "analysis", "documentation"],
                examples=["generate report", "create summary", "synthesize results"]
            ),
            AgentSkill(
                id="validate_results",
                name="Validate Analysis Results",
                description="LLM-based validation of analysis results to prevent hallucinations",
                tags=["validation", "verification", "accuracy", "quality"],
                examples=["validate results", "check accuracy", "verify findings"]
            ),
            AgentSkill(
                id="aggregate_visualizations",
                name="Aggregate Visualizations",
                description="Combine and organize visualizations from multiple agents",
                tags=["visualization", "aggregation", "dashboard", "charts"],
                examples=["combine charts", "create dashboard", "aggregate plots"]
            )
        ],
        capabilities=AgentCapabilities(
            streaming=True,
            cancellation=True
        ),
        supportsAuthenticatedExtendedCard=False
    )
    
    # Create task store
    task_store = InMemoryTaskStore()
    
    # Create executor
    executor = ReportGeneratorExecutor()
    
    # Create request handler
    request_handler = DefaultRequestHandler(
        agent_executor=executor,
        task_store=task_store
    )
    
    # Create A2A app
    app = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler
    )
    
    return app


# Create the application instance
app = create_app()

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("REPORT_GENERATOR_PORT", 8315))
    logger.info(f"ðŸš€ Starting Report Generator Agent on port {port}")
    uvicorn.run(app, host="0.0.0.0", port=port)