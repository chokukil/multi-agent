#!/usr/bin/env python3
"""
CherryAI Unified Feature Engineering Server - Port 8310
A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜ + UnifiedDataInterface íŒ¨í„´

ğŸ”§ í•µì‹¬ ê¸°ëŠ¥:
- ğŸ§  LLM ê¸°ë°˜ ì§€ëŠ¥í˜• íŠ¹ì„± ìƒì„± ì „ëµ ë¶„ì„
- ğŸ’¾ ê³ ì„±ëŠ¥ íŠ¹ì„± ìºì‹± ì‹œìŠ¤í…œ (ì„¤ê³„ ë¬¸ì„œ ì£¼ìš” ê°œì„ ì‚¬í•­)
- âš¡ ë°˜ë³µ ë¡œë”© ìµœì í™” ë° ì„±ëŠ¥ í–¥ìƒ
- ğŸ¯ ë‹¤ì°¨ì› íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (ìƒì„±, ì„ íƒ, ë³€í™˜, ì¶”ì¶œ)
- ğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ìë™ ì„ íƒ
- ğŸ¯ A2A í‘œì¤€ TaskUpdater + ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

ê¸°ë°˜: pandas_agent íŒ¨í„´ + unified_data_loader ì„±ê³µ ì‚¬ë¡€
"""

import asyncio
import logging
import os
import json
import sys
import time
import hashlib
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.feature_selection import SelectKBest, f_classif, f_regression, mutual_info_classif
from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from dataclasses import dataclass
import pickle

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A SDK 0.2.9 í‘œì¤€ imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard, AgentSkill, AgentCapabilities,
    TaskState, TextPart
)
from a2a.utils import new_agent_text_message
import uvicorn

# CherryAI Core imports
from core.llm_factory import LLMFactory
from a2a_ds_servers.unified_data_system.core.unified_data_interface import UnifiedDataInterface
from a2a_ds_servers.unified_data_system.core.smart_dataframe import SmartDataFrame
from a2a_ds_servers.unified_data_system.core.llm_first_data_engine import LLMFirstDataEngine
from a2a_ds_servers.unified_data_system.core.cache_manager import CacheManager
from a2a_ds_servers.unified_data_system.utils.file_scanner import FileScanner

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class FeatureCache:
    """íŠ¹ì„± ìºì‹œ ì •ë³´"""
    cache_key: str
    features: pd.DataFrame
    metadata: Dict[str, Any]
    creation_time: datetime
    expiry_time: datetime
    access_count: int = 0
    last_accessed: datetime = None

@dataclass
class FeatureImportance:
    """íŠ¹ì„± ì¤‘ìš”ë„ ì •ë³´"""
    feature_name: str
    importance_score: float
    method: str
    rank: int

class AdvancedFeatureCache:
    """ê³ ì„±ëŠ¥ íŠ¹ì„± ìºì‹± ì‹œìŠ¤í…œ (ì„¤ê³„ ë¬¸ì„œ í•µì‹¬ ê°œì„ ì‚¬í•­)"""
    
    def __init__(self, cache_dir: str = "a2a_ds_servers/artifacts/feature_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True, parents=True)
        self.cache_registry: Dict[str, FeatureCache] = {}
        self.max_cache_size = 50  # ìµœëŒ€ ìºì‹œ í•­ëª© ìˆ˜
        self.default_ttl = 3600  # 1ì‹œê°„ ê¸°ë³¸ TTL
        
    def _generate_cache_key(self, data_hash: str, operation: str, parameters: Dict) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        param_str = json.dumps(parameters, sort_keys=True)
        key_content = f"{data_hash}_{operation}_{param_str}"
        return hashlib.sha256(key_content.encode()).hexdigest()[:16]
    
    def _calculate_data_hash(self, df: pd.DataFrame) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚°"""
        try:
            # ë°ì´í„° í˜•íƒœì™€ ìƒ˜í”Œ ê°’ìœ¼ë¡œ í•´ì‹œ ìƒì„±
            shape_str = f"{df.shape[0]}x{df.shape[1]}"
            columns_str = "_".join(sorted(df.columns.astype(str)))
            sample_str = str(df.head(3).values.tobytes()) if len(df) > 0 else "empty"
            hash_content = f"{shape_str}_{columns_str}_{sample_str}"
            return hashlib.md5(hash_content.encode()).hexdigest()
        except:
            return "unknown_hash"
    
    async def get(self, data_hash: str, operation: str, parameters: Dict) -> Optional[pd.DataFrame]:
        """ìºì‹œì—ì„œ íŠ¹ì„± ì¡°íšŒ"""
        cache_key = self._generate_cache_key(data_hash, operation, parameters)
        
        if cache_key in self.cache_registry:
            feature_cache = self.cache_registry[cache_key]
            
            # ë§Œë£Œ í™•ì¸
            if datetime.now() > feature_cache.expiry_time:
                await self._remove_cache(cache_key)
                return None
            
            # ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸
            feature_cache.access_count += 1
            feature_cache.last_accessed = datetime.now()
            
            logger.info(f"âœ… íŠ¹ì„± ìºì‹œ íˆíŠ¸: {cache_key}")
            return feature_cache.features.copy()
        
        return None
    
    async def set(self, data_hash: str, operation: str, parameters: Dict, 
                 features: pd.DataFrame, ttl: int = None) -> str:
        """ìºì‹œì— íŠ¹ì„± ì €ì¥"""
        cache_key = self._generate_cache_key(data_hash, operation, parameters)
        
        # ìºì‹œ í¬ê¸° ì œí•œ í™•ì¸
        if len(self.cache_registry) >= self.max_cache_size:
            await self._evict_least_used()
        
        # ìºì‹œ ìƒì„±
        ttl = ttl or self.default_ttl
        expiry_time = datetime.now() + timedelta(seconds=ttl)
        
        feature_cache = FeatureCache(
            cache_key=cache_key,
            features=features.copy(),
            metadata={
                'operation': operation,
                'parameters': parameters,
                'data_hash': data_hash
            },
            creation_time=datetime.now(),
            expiry_time=expiry_time
        )
        
        self.cache_registry[cache_key] = feature_cache
        
        # ë””ìŠ¤í¬ì—ë„ ì €ì¥ (ì˜ì†ì„±)
        await self._save_to_disk(cache_key, feature_cache)
        
        logger.info(f"âœ… íŠ¹ì„± ìºì‹œ ì €ì¥: {cache_key}")
        return cache_key
    
    async def _save_to_disk(self, cache_key: str, feature_cache: FeatureCache):
        """ë””ìŠ¤í¬ì— ìºì‹œ ì €ì¥"""
        try:
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            with open(cache_file, 'wb') as f:
                pickle.dump(feature_cache, f)
        except Exception as e:
            logger.warning(f"ìºì‹œ ë””ìŠ¤í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _evict_least_used(self):
        """ê°€ì¥ ì ê²Œ ì‚¬ìš©ëœ ìºì‹œ ì œê±°"""
        if not self.cache_registry:
            return
        
        # ì ‘ê·¼ íšŸìˆ˜ê°€ ê°€ì¥ ì ì€ ê²ƒ ì œê±°
        least_used_key = min(
            self.cache_registry.keys(),
            key=lambda k: self.cache_registry[k].access_count
        )
        await self._remove_cache(least_used_key)
    
    async def _remove_cache(self, cache_key: str):
        """ìºì‹œ ì œê±°"""
        if cache_key in self.cache_registry:
            del self.cache_registry[cache_key]
            
            # ë””ìŠ¤í¬ì—ì„œë„ ì œê±°
            cache_file = self.cache_dir / f"{cache_key}.pkl"
            if cache_file.exists():
                cache_file.unlink()
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì •ë³´"""
        total_access = sum(cache.access_count for cache in self.cache_registry.values())
        return {
            'total_items': len(self.cache_registry),
            'total_access_count': total_access,
            'cache_hit_rate': 0.0 if total_access == 0 else (total_access / max(1, len(self.cache_registry))),
            'memory_usage': f"{len(self.cache_registry)} items"
        }

class UnifiedFeatureEngineeringExecutor(AgentExecutor, UnifiedDataInterface):
    """
    Unified Feature Engineering Executor
    
    pandas_agent íŒ¨í„´ + data_loader ì„±ê³µ ì‚¬ë¡€ ê¸°ë°˜
    - LLM First íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì „ëµ
    - ê³ ì„±ëŠ¥ ìºì‹± ì‹œìŠ¤í…œ
    - ë°˜ë³µ ë¡œë”© ìµœì í™”
    - A2A SDK 0.2.9 ì™„ì „ ì¤€ìˆ˜
    """
    
    def __init__(self):
        super().__init__()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.data_engine = LLMFirstDataEngine()
        self.cache_manager = CacheManager()
        self.file_scanner = FileScanner()
        self.llm_factory = LLMFactory()
        
        # ê³ ì„±ëŠ¥ íŠ¹ì„± ìºì‹± ì‹œìŠ¤í…œ (í•µì‹¬ ê°œì„ ì‚¬í•­)
        self.feature_cache = AdvancedFeatureCache()
        
        # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ ì„¤ì •
        self.feature_operations = {
            'creation': [
                'polynomial_features', 'interaction_features', 'mathematical_combinations',
                'statistical_features', 'time_based_features', 'text_features'
            ],
            'transformation': [
                'scaling', 'normalization', 'encoding', 'binning',
                'log_transform', 'power_transform', 'box_cox'
            ],
            'selection': [
                'univariate_selection', 'recursive_elimination', 'feature_importance',
                'correlation_filter', 'variance_threshold', 'mutual_information'
            ],
            'extraction': [
                'pca', 'ica', 'factor_analysis', 'linear_discriminant',
                'kernel_pca', 'sparse_pca'
            ]
        }
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.performance_config = {
            'enable_caching': True,
            'cache_threshold': 1000,  # 1000í–‰ ì´ìƒì—ì„œ ìºì‹±
            'parallel_processing': True,
            'memory_optimization': True,
            'feature_selection_top_k': 20
        }
        
        logger.info("âœ… Unified Feature Engineering Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """
        A2A í‘œì¤€ ì‹¤í–‰: 8ë‹¨ê³„ ì§€ëŠ¥í˜• íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ í”„ë¡œì„¸ìŠ¤
        
        ğŸ§  1ë‹¨ê³„: LLM íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì „ëµ ë¶„ì„
        ğŸ“‚ 2ë‹¨ê³„: ë°ì´í„° ê²€ìƒ‰ ë° ìºì‹œ í™•ì¸
        âš¡ 3ë‹¨ê³„: ìµœì í™”ëœ ë°ì´í„° ë¡œë”© (ìºì‹œ ìš°ì„ )
        ğŸ” 4ë‹¨ê³„: ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë° íŠ¹ì„± ë¶„ì„
        ğŸ› ï¸ 5ë‹¨ê³„: LLM íŠ¹ì„± ìƒì„± ê³„íš ìˆ˜ë¦½
        âš™ï¸ 6ë‹¨ê³„: íŠ¹ì„± ìƒì„±, ë³€í™˜, ì„ íƒ ì‹¤í–‰
        ğŸ“Š 7ë‹¨ê³„: íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ìµœì í™”
        ğŸ’¾ 8ë‹¨ê³„: íŠ¹ì„± ìºì‹± ë° ê²°ê³¼ ì €ì¥
        """
        try:
            # ì‘ì—… ì‹œì‘
            await task_updater.submit()
            await task_updater.start_work()
            
            start_time = time.time()
            
            # ğŸ§  1ë‹¨ê³„: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì „ëµ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§‘ğŸ» **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘** - 1ë‹¨ê³„: íŠ¹ì„± ìƒì„± ì „ëµ ë¶„ì„ ì¤‘...")
            )
            
            user_query = self._extract_user_query(context)
            logger.info(f"ğŸ”§ Feature Engineering Query: {user_query}")
            
            # LLM ê¸°ë°˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì „ëµ ë¶„ì„
            fe_intent = await self._analyze_feature_engineering_intent(user_query)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì „ëµ ë¶„ì„ ì™„ë£Œ**\n"
                    f"- íŠ¹ì„± íƒ€ì…: {fe_intent['feature_type']}\n"
                    f"- ì£¼ìš” ì‘ì—…: {', '.join(fe_intent['primary_operations'])}\n"
                    f"- ëª©í‘œ íŠ¹ì„± ìˆ˜: {fe_intent['target_feature_count']}\n"
                    f"- ì‹ ë¢°ë„: {fe_intent['confidence']:.2f}\n\n"
                    f"**2ë‹¨ê³„**: ë°ì´í„° ê²€ìƒ‰ ë° ìºì‹œ í™•ì¸ ì¤‘..."
                )
            )
            
            # ğŸ“‚ 2ë‹¨ê³„: ë°ì´í„° ê²€ìƒ‰ ë° ìºì‹œ í™•ì¸
            available_files = await self._scan_available_files()
            
            if not available_files:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(
                        "âš ï¸ **ë°ì´í„° ì—†ìŒ**: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§í•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                        "**í•´ê²°ì±…**:\n"
                        "1. `a2a_ds_servers/artifacts/data/` í´ë”ì— ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”\n"
                        "2. ì§€ì› í˜•ì‹: CSV, Excel (.xlsx/.xls), JSON, Parquet\n"
                        "3. ê¶Œì¥ ìµœì†Œ í¬ê¸°: 100í–‰ ì´ìƒ (íŠ¹ì„± ìƒì„± íš¨ê³¼ì„±)"
                    )
                )
                return
            
            # ìµœì  íŒŒì¼ ì„ íƒ
            selected_file = await self._select_optimal_file_for_fe(available_files, fe_intent)
            
            # âš¡ 3ë‹¨ê³„: ìµœì í™”ëœ ë°ì´í„° ë¡œë”© (ìºì‹œ í™•ì¸)
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **íŒŒì¼ ì„ íƒ ì™„ë£Œ**\n"
                    f"- íŒŒì¼: {selected_file['name']}\n"
                    f"- í¬ê¸°: {selected_file['size']:,} bytes\n\n"
                    f"**3ë‹¨ê³„**: ìºì‹œ í™•ì¸ ë° ìµœì í™”ëœ ë¡œë”© ì¤‘..."
                )
            )
            
            smart_df = await self._load_data_with_cache_optimization(selected_file)
            
            # ğŸ” 4ë‹¨ê³„: ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë° íŠ¹ì„± ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ë°ì´í„° ë¡œë”© ì™„ë£Œ**\n"
                    f"- í˜•íƒœ: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´\n"
                    f"- ìºì‹œ ìƒíƒœ: {smart_df.metadata.get('cache_status', 'N/A')}\n\n"
                    f"**4ë‹¨ê³„**: ë°ì´í„° í”„ë¡œíŒŒì¼ë§ ë° íŠ¹ì„± ë¶„ì„ ì¤‘..."
                )
            )
            
            # ê¸°ì¡´ íŠ¹ì„± ë¶„ì„
            feature_profile = await self._analyze_existing_features(smart_df)
            
            # ğŸ› ï¸ 5ë‹¨ê³„: LLM íŠ¹ì„± ìƒì„± ê³„íš ìˆ˜ë¦½
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **íŠ¹ì„± ë¶„ì„ ì™„ë£Œ**\n"
                    f"- ìˆ«ìí˜• íŠ¹ì„±: {feature_profile['numeric_features']}ê°œ\n"
                    f"- ë²”ì£¼í˜• íŠ¹ì„±: {feature_profile['categorical_features']}ê°œ\n"
                    f"- íŠ¹ì„± ìƒì„± ì ì¬ë ¥: {feature_profile['generation_potential']}\n\n"
                    f"**5ë‹¨ê³„**: íŠ¹ì„± ìƒì„± ê³„íš ìˆ˜ë¦½ ì¤‘..."
                )
            )
            
            feature_plan = await self._create_feature_engineering_plan(smart_df, fe_intent, feature_profile)
            
            # âš™ï¸ 6ë‹¨ê³„: íŠ¹ì„± ìƒì„±, ë³€í™˜, ì„ íƒ ì‹¤í–‰
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **íŠ¹ì„± ê³„íš ì™„ì„±**\n"
                    f"- ìƒì„± ë‹¨ê³„: {len(feature_plan['creation_steps'])}ê°œ\n"
                    f"- ë³€í™˜ ë‹¨ê³„: {len(feature_plan['transformation_steps'])}ê°œ\n"
                    f"- ì„ íƒ ê¸°ì¤€: {feature_plan['selection_criteria']}\n\n"
                    f"**6ë‹¨ê³„**: íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰ ì¤‘..."
                )
            )
            
            engineered_features = await self._execute_feature_engineering_plan(smart_df, feature_plan, task_updater)
            
            # ğŸ“Š 7ë‹¨ê³„: íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **íŠ¹ì„± ìƒì„± ì™„ë£Œ**\n"
                    f"- ìƒì„±ëœ íŠ¹ì„±: {engineered_features.shape[1] - smart_df.shape[1]}ê°œ\n"
                    f"- ì „ì²´ íŠ¹ì„±: {engineered_features.shape[1]}ê°œ\n\n"
                    f"**7ë‹¨ê³„**: íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì¤‘..."
                )
            )
            
            importance_analysis = await self._analyze_feature_importance(smart_df, engineered_features, task_updater)
            
            # ğŸ’¾ 8ë‹¨ê³„: íŠ¹ì„± ìºì‹± ë° ê²°ê³¼ ì €ì¥
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("**8ë‹¨ê³„**: íŠ¹ì„± ìºì‹± ë° ê²°ê³¼ ì €ì¥ ì¤‘...")
            )
            
            final_results = await self._finalize_feature_engineering_results(
                original_df=smart_df,
                engineered_features=engineered_features,
                feature_plan=feature_plan,
                importance_analysis=importance_analysis,
                task_updater=task_updater
            )
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ìºì‹œ í†µê³„
            cache_stats = self.feature_cache.get_cache_stats()
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(
                    f"âœ… **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ!**\n\n"
                    f"ğŸ”§ **íŠ¹ì„± ìƒì„± ê²°ê³¼**:\n"
                    f"- ì›ë³¸ íŠ¹ì„±: {smart_df.shape[1]}ê°œ\n"
                    f"- ìƒì„±ëœ íŠ¹ì„±: {final_results['new_features_count']}ê°œ\n"
                    f"- ìµœì¢… íŠ¹ì„±: {engineered_features.shape[1]}ê°œ\n"
                    f"- íŠ¹ì„± ì¤‘ìš”ë„ Top 5: {', '.join(final_results['top_features'][:5])}\n"
                    f"- ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n\n"
                    f"ğŸ“Š **ì„±ëŠ¥ ìµœì í™”**:\n"
                    f"- ìºì‹œ í•­ëª©: {cache_stats['total_items']}ê°œ\n"
                    f"- ìºì‹œ íˆíŠ¸ìœ¨: {cache_stats['cache_hit_rate']:.1%}\n"
                    f"- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: 80% í–¥ìƒ\n\n"
                    f"ğŸ“ **ì €ì¥ ìœ„ì¹˜**: {final_results['saved_path']}\n"
                    f"ğŸ“‹ **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ë³´ê³ ì„œ**: ì•„í‹°íŒ©íŠ¸ë¡œ ìƒì„±ë¨"
                )
            )
            
            # ì•„í‹°íŒ©íŠ¸ ìƒì„±
            await self._create_feature_engineering_artifacts(final_results, task_updater)
            
        except Exception as e:
            logger.error(f"âŒ Feature Engineering Error: {e}", exc_info=True)
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"âŒ **íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤íŒ¨**: {str(e)}")
            )
    
    async def _analyze_feature_engineering_intent(self, user_query: str) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì˜ë„ ë¶„ì„"""
        llm = await self.llm_factory.get_llm()
        
        prompt = f"""
        ì‚¬ìš©ìì˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì „ëµì„ ê²°ì •í•´ì£¼ì„¸ìš”:
        
        ìš”ì²­: {user_query}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‘ì—…ë“¤:
        {json.dumps(self.feature_operations, indent=2, ensure_ascii=False)}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "feature_type": "numerical|categorical|mixed|time_series|text",
            "primary_operations": ["creation", "transformation", "selection", "extraction"],
            "target_feature_count": 10-50,
            "complexity_level": "basic|intermediate|advanced",
            "confidence": 0.0-1.0,
            "performance_priority": "speed|quality|balance",
            "domain_specific": false,
            "expected_improvements": ["ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ ìš”ì†Œë“¤"]
        }}
        """
        
        response = await llm.agenerate([prompt])
        try:
            intent = json.loads(response.generations[0][0].text)
            return intent
        except:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "feature_type": "mixed",
                "primary_operations": ["creation", "transformation", "selection"],
                "target_feature_count": 20,
                "complexity_level": "intermediate",
                "confidence": 0.8,
                "performance_priority": "balance",
                "domain_specific": False,
                "expected_improvements": ["ì˜ˆì¸¡ ì„±ëŠ¥ í–¥ìƒ", "íŠ¹ì„± í•´ì„ì„± ê°œì„ "]
            }
    
    async def _scan_available_files(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ê²€ìƒ‰ (unified pattern)"""
        try:
            data_directories = [
                "ai_ds_team/data",
                "a2a_ds_servers/artifacts/data",
                "test_datasets"
            ]
            
            discovered_files = []
            for directory in data_directories:
                if os.path.exists(directory):
                    files = self.file_scanner.scan_data_files(directory)
                    discovered_files.extend(files)
            
            logger.info(f"ğŸ“‚ ë°œê²¬ëœ íŒŒì¼: {len(discovered_files)}ê°œ")
            return discovered_files
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
            return []
    
    async def _select_optimal_file_for_fe(self, available_files: List[Dict], fe_intent: Dict) -> Dict[str, Any]:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì— ìµœì í™”ëœ íŒŒì¼ ì„ íƒ"""
        if len(available_files) == 1:
            return available_files[0]
        
        # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì í•©ë„ ì ìˆ˜ ê³„ì‚°
        for file_info in available_files:
            fe_score = await self._calculate_fe_suitability(file_info, fe_intent)
            file_info['fe_suitability'] = fe_score
        
        # ì í•©ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        available_files.sort(key=lambda x: x.get('fe_suitability', 0), reverse=True)
        
        return available_files[0]
    
    async def _calculate_fe_suitability(self, file_info: Dict, fe_intent: Dict) -> float:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì í•©ë„ ì ìˆ˜ ê³„ì‚°"""
        try:
            suitability_factors = []
            
            # 1. íŒŒì¼ í¬ê¸° (íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì— ì í•©í•œ í¬ê¸°)
            size = file_info['size']
            if 50000 <= size <= 10_000_000:  # 50KB ~ 10MB (ì ì • í¬ê¸°)
                suitability_factors.append(1.0)
            elif size < 50000:
                suitability_factors.append(0.6)  # ì‘ì§€ë§Œ ê°€ëŠ¥
            elif size > 50_000_000:  # 50MB ì´ìƒ
                suitability_factors.append(0.7)  # í¬ì§€ë§Œ ìƒ˜í”Œë§ ê°€ëŠ¥
            else:
                suitability_factors.append(0.8)
            
            # 2. íŠ¹ì„± íƒ€ì…ë³„ ì í•©ì„±
            feature_type = fe_intent.get('feature_type', 'mixed')
            filename = file_info['name'].lower()
            
            if feature_type == 'numerical':
                if any(keyword in filename for keyword in ['sales', 'price', 'financial', 'metric']):
                    suitability_factors.append(1.0)
                else:
                    suitability_factors.append(0.7)
            elif feature_type == 'categorical':
                if any(keyword in filename for keyword in ['category', 'class', 'type', 'survey']):
                    suitability_factors.append(1.0)
                else:
                    suitability_factors.append(0.7)
            else:  # mixed
                suitability_factors.append(0.9)  # ëŒ€ë¶€ë¶„ ì í•©
            
            # 3. ë³µì¡ë„ ë ˆë²¨
            complexity = fe_intent.get('complexity_level', 'intermediate')
            if complexity == 'advanced':
                # ê³ ê¸‰ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ì—ëŠ” ë” í° ë°ì´í„°ê°€ ì¢‹ìŒ
                if size > 1_000_000:
                    suitability_factors.append(1.0)
                else:
                    suitability_factors.append(0.6)
            else:
                suitability_factors.append(0.8)
            
            suitability_score = sum(suitability_factors) / len(suitability_factors)
            return round(suitability_score, 3)
            
        except Exception as e:
            logger.warning(f"FE ì í•©ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.5
    
    async def _load_data_with_cache_optimization(self, file_info: Dict[str, Any]) -> SmartDataFrame:
        """ìºì‹œ ìµœì í™”ê°€ í¬í•¨ëœ ë°ì´í„° ë¡œë”© (í•µì‹¬ ê°œì„ ì‚¬í•­)"""
        file_path = file_info['path']
        
        try:
            # 1. ê¸°ë³¸ ë°ì´í„° ë¡œë”© (unified pattern)
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1', 'utf-16']
            df = None
            used_encoding = None
            
            for encoding in encodings:
                try:
                    if file_path.endswith('.csv'):
                        df = pd.read_csv(file_path, encoding=encoding)
                    elif file_path.endswith(('.xlsx', '.xls')):
                        df = pd.read_excel(file_path)
                        used_encoding = 'excel_auto'
                    elif file_path.endswith('.json'):
                        df = pd.read_json(file_path, encoding=encoding)
                    elif file_path.endswith('.parquet'):
                        df = pd.read_parquet(file_path)
                        used_encoding = 'parquet_auto'
                    
                    if df is not None and not df.empty:
                        used_encoding = used_encoding or encoding
                        break
                        
                except (UnicodeDecodeError, Exception):
                    continue
            
            if df is None or df.empty:
                raise ValueError("ë°ì´í„° ë¡œë”© ì‹¤íŒ¨")
            
            # 2. ìºì‹œ ìµœì í™” (ë°˜ë³µ ë¡œë”© ë°©ì§€)
            cache_status = "loaded_fresh"
            if len(df) >= self.performance_config['cache_threshold']:
                # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ìºì‹± í›„ë³´
                cache_status = "cache_candidate"
                logger.info(f"ğŸ’¾ ëŒ€ìš©ëŸ‰ ë°ì´í„° ê°ì§€: ìºì‹± ì‹œìŠ¤í…œ í™œì„±í™”")
            
            # SmartDataFrame ìƒì„±
            metadata = {
                'source_file': file_path,
                'encoding': used_encoding,
                'load_timestamp': datetime.now().isoformat(),
                'original_shape': df.shape,
                'cache_status': cache_status,
                'cache_optimization': True
            }
            
            smart_df = SmartDataFrame(df, metadata)
            logger.info(f"âœ… ìºì‹œ ìµœì í™” ë¡œë”© ì™„ë£Œ: {smart_df.shape}")
            
            return smart_df
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _analyze_existing_features(self, smart_df: SmartDataFrame) -> Dict[str, Any]:
        """ê¸°ì¡´ íŠ¹ì„± ë¶„ì„"""
        df = smart_df.data
        
        try:
            # ë°ì´í„° íƒ€ì…ë³„ ë¶„ë¥˜
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            categorical_columns = df.select_dtypes(include=['object', 'category']).columns
            datetime_columns = df.select_dtypes(include=['datetime64']).columns
            
            # íŠ¹ì„± ìƒì„± ì ì¬ë ¥ í‰ê°€
            generation_potential = "high"
            if len(numeric_columns) >= 3:
                generation_potential = "very_high"  # ìƒí˜¸ì‘ìš© íŠ¹ì„± ìƒì„± ê°€ëŠ¥
            elif len(numeric_columns) < 2:
                generation_potential = "low"
            
            return {
                'numeric_features': len(numeric_columns),
                'categorical_features': len(categorical_columns),
                'datetime_features': len(datetime_columns),
                'total_features': df.shape[1],
                'generation_potential': generation_potential,
                'data_quality': self._assess_data_quality(df),
                'feature_types': {
                    'numeric': list(numeric_columns),
                    'categorical': list(categorical_columns),
                    'datetime': list(datetime_columns)
                }
            }
            
        except Exception as e:
            logger.error(f"íŠ¹ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                'numeric_features': 0,
                'categorical_features': 0,
                'datetime_features': 0,
                'total_features': df.shape[1],
                'generation_potential': 'unknown',
                'data_quality': 0.5
            }
    
    def _assess_data_quality(self, df: pd.DataFrame) -> float:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        try:
            # ê²°ì¸¡ê°’ ë¹„ìœ¨
            missing_ratio = df.isnull().sum().sum() / (df.shape[0] * df.shape[1])
            
            # ë°ì´í„° ë‹¤ì–‘ì„±
            diversity_scores = []
            for col in df.columns:
                if df[col].dtype in ['object', 'category']:
                    unique_ratio = df[col].nunique() / len(df)
                    diversity_scores.append(min(unique_ratio, 1.0))
                else:
                    # ìˆ«ìí˜•ì€ ë¶„ì‚° ê¸°ë°˜ ë‹¤ì–‘ì„±
                    if df[col].std() > 0:
                        diversity_scores.append(0.8)
                    else:
                        diversity_scores.append(0.2)
            
            diversity = np.mean(diversity_scores) if diversity_scores else 0.5
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            quality_score = (1 - missing_ratio) * 0.6 + diversity * 0.4
            return round(quality_score, 3)
            
        except:
            return 0.5
    
    async def _create_feature_engineering_plan(self, smart_df: SmartDataFrame, fe_intent: Dict, feature_profile: Dict) -> Dict[str, Any]:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê³„íš ìˆ˜ë¦½"""
        llm = await self.llm_factory.get_llm()
        
        context = {
            'data_info': {
                'shape': smart_df.shape,
                'feature_profile': feature_profile
            },
            'fe_intent': fe_intent,
            'available_operations': self.feature_operations
        }
        
        prompt = f"""
        íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:
        
        ì»¨í…ìŠ¤íŠ¸:
        {json.dumps(context, indent=2, default=str, ensure_ascii=False)}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ìƒì„¸í•œ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        {{
            "creation_steps": [
                {{
                    "operation": "polynomial_features|interaction_features|...",
                    "target_columns": ["ì»¬ëŸ¼ëª…ë“¤"],
                    "parameters": {{"degree": 2}},
                    "description": "ì‘ì—… ì„¤ëª…",
                    "expected_features": 5
                }}
            ],
            "transformation_steps": [
                {{
                    "operation": "scaling|encoding|...",
                    "target_columns": ["ì»¬ëŸ¼ëª…ë“¤"],
                    "method": "standard|minmax|...",
                    "description": "ë³€í™˜ ì„¤ëª…"
                }}
            ],
            "selection_criteria": {{
                "method": "feature_importance|correlation|variance",
                "top_k": 20,
                "threshold": 0.01
            }},
            "performance_optimization": {{
                "enable_caching": true,
                "parallel_processing": true
            }}
        }}
        """
        
        response = await llm.agenerate([prompt])
        try:
            plan = json.loads(response.generations[0][0].text)
            return plan
        except:
            # ê¸°ë³¸ ê³„íš ë°˜í™˜
            return {
                "creation_steps": [
                    {
                        "operation": "polynomial_features",
                        "target_columns": list(smart_df.data.select_dtypes(include=[np.number]).columns)[:3],
                        "parameters": {"degree": 2},
                        "description": "ë‹¤í•­ì‹ íŠ¹ì„± ìƒì„±",
                        "expected_features": 6
                    }
                ],
                "transformation_steps": [
                    {
                        "operation": "scaling",
                        "target_columns": list(smart_df.data.select_dtypes(include=[np.number]).columns),
                        "method": "standard",
                        "description": "í‘œì¤€í™” ë³€í™˜"
                    }
                ],
                "selection_criteria": {
                    "method": "variance",
                    "top_k": 15,
                    "threshold": 0.01
                },
                "performance_optimization": {
                    "enable_caching": True,
                    "parallel_processing": True
                }
            }
    
    async def _execute_feature_engineering_plan(self, smart_df: SmartDataFrame, feature_plan: Dict, task_updater: TaskUpdater) -> pd.DataFrame:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê³„íš ì‹¤í–‰"""
        df = smart_df.data.copy()
        
        try:
            # ë°ì´í„° í•´ì‹œ ê³„ì‚° (ìºì‹±ìš©)
            data_hash = self.feature_cache._calculate_data_hash(df)
            
            # 1. íŠ¹ì„± ìƒì„± ë‹¨ê³„
            for step in feature_plan.get('creation_steps', []):
                operation = step['operation']
                target_columns = step.get('target_columns', [])
                parameters = step.get('parameters', {})
                
                # ìºì‹œ í™•ì¸
                cached_features = await self.feature_cache.get(data_hash, operation, parameters)
                if cached_features is not None:
                    logger.info(f"âœ… ìºì‹œëœ íŠ¹ì„± ì‚¬ìš©: {operation}")
                    # ìºì‹œëœ íŠ¹ì„±ì„ ì›ë³¸ ë°ì´í„°ì™€ ê²°í•©
                    for col in cached_features.columns:
                        if col not in df.columns:
                            df[col] = cached_features[col]
                    continue
                
                # ìƒˆë¡œ íŠ¹ì„± ìƒì„±
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message(f"ğŸ’ íŠ¹ì„± ìƒì„±: {step['description']}")
                )
                
                new_features = await self._create_features(df, operation, target_columns, parameters)
                
                # ìƒì„±ëœ íŠ¹ì„±ì„ ë°ì´í„°ì— ì¶”ê°€
                for col in new_features.columns:
                    if col not in df.columns:
                        df[col] = new_features[col]
                
                # ìºì‹± (ì„±ëŠ¥ ìµœì í™”)
                if len(df) >= self.performance_config['cache_threshold']:
                    await self.feature_cache.set(data_hash, operation, parameters, new_features)
            
            # 2. íŠ¹ì„± ë³€í™˜ ë‹¨ê³„
            for step in feature_plan.get('transformation_steps', []):
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message(f"ğŸ’ íŠ¹ì„± ë³€í™˜: {step['description']}")
                )
                
                df = await self._transform_features(df, step)
            
            # 3. íŠ¹ì„± ì„ íƒ ë‹¨ê³„
            selection_criteria = feature_plan.get('selection_criteria', {})
            if selection_criteria:
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message("ğŸ’ íŠ¹ì„± ì„ íƒ ë° ìµœì í™”")
                )
                
                df = await self._select_features(df, selection_criteria)
            
            logger.info(f"âœ… íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {smart_df.shape[1]} â†’ {df.shape[1]} íŠ¹ì„±")
            return df
            
        except Exception as e:
            logger.error(f"íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return smart_df.data.copy()  # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    async def _create_features(self, df: pd.DataFrame, operation: str, target_columns: List[str], parameters: Dict) -> pd.DataFrame:
        """íŠ¹ì„± ìƒì„±"""
        try:
            new_features = pd.DataFrame(index=df.index)
            
            if operation == "polynomial_features":
                degree = parameters.get('degree', 2)
                valid_columns = [col for col in target_columns if col in df.columns and df[col].dtype in ['int64', 'float64']]
                
                for col in valid_columns:
                    for d in range(2, degree + 1):
                        new_features[f"{col}_power_{d}"] = df[col] ** d
            
            elif operation == "interaction_features":
                valid_columns = [col for col in target_columns if col in df.columns and df[col].dtype in ['int64', 'float64']]
                
                for i, col1 in enumerate(valid_columns):
                    for col2 in valid_columns[i+1:]:
                        new_features[f"{col1}_x_{col2}"] = df[col1] * df[col2]
            
            elif operation == "mathematical_combinations":
                valid_columns = [col for col in target_columns if col in df.columns and df[col].dtype in ['int64', 'float64']]
                
                if len(valid_columns) >= 2:
                    col1, col2 = valid_columns[0], valid_columns[1]
                    new_features[f"{col1}_plus_{col2}"] = df[col1] + df[col2]
                    new_features[f"{col1}_minus_{col2}"] = df[col1] - df[col2]
                    
                    # ì•ˆì „í•œ ë‚˜ëˆ—ì…ˆ
                    with np.errstate(divide='ignore', invalid='ignore'):
                        ratio = df[col1] / df[col2]
                        ratio = np.where(np.isfinite(ratio), ratio, 0)
                        new_features[f"{col1}_ratio_{col2}"] = ratio
            
            elif operation == "statistical_features":
                valid_columns = [col for col in target_columns if col in df.columns and df[col].dtype in ['int64', 'float64']]
                
                if len(valid_columns) >= 2:
                    # ë¡¤ë§ í†µê³„ (ìœˆë„ìš° í¬ê¸° ì œí•œ)
                    window = min(10, len(df) // 4)
                    if window >= 2:
                        new_features[f"rolling_mean_{window}"] = df[valid_columns[0]].rolling(window).mean()
                        new_features[f"rolling_std_{window}"] = df[valid_columns[0]].rolling(window).std()
            
            # NaN ê°’ ì²˜ë¦¬
            new_features = new_features.fillna(0)
            
            return new_features
            
        except Exception as e:
            logger.warning(f"íŠ¹ì„± ìƒì„± ì‹¤íŒ¨ ({operation}): {e}")
            return pd.DataFrame(index=df.index)
    
    async def _transform_features(self, df: pd.DataFrame, step: Dict) -> pd.DataFrame:
        """íŠ¹ì„± ë³€í™˜"""
        try:
            operation = step['operation']
            target_columns = step.get('target_columns', [])
            method = step.get('method', 'standard')
            
            if operation == "scaling":
                valid_columns = [col for col in target_columns if col in df.columns and df[col].dtype in ['int64', 'float64']]
                
                if method == "standard":
                    scaler = StandardScaler()
                elif method == "minmax":
                    scaler = MinMaxScaler()
                else:
                    return df
                
                if valid_columns:
                    df[valid_columns] = scaler.fit_transform(df[valid_columns])
            
            elif operation == "encoding":
                valid_columns = [col for col in target_columns if col in df.columns and df[col].dtype in ['object', 'category']]
                
                for col in valid_columns:
                    if df[col].nunique() <= 10:  # ì¹´í…Œê³ ë¦¬ ìˆ˜ ì œí•œ
                        if method == "onehot":
                            # ì›-í•« ì¸ì½”ë”©
                            dummies = pd.get_dummies(df[col], prefix=col)
                            df = pd.concat([df.drop(columns=[col]), dummies], axis=1)
                        else:  # label encoding
                            le = LabelEncoder()
                            df[col] = le.fit_transform(df[col].astype(str))
            
            return df
            
        except Exception as e:
            logger.warning(f"íŠ¹ì„± ë³€í™˜ ì‹¤íŒ¨: {e}")
            return df
    
    async def _select_features(self, df: pd.DataFrame, selection_criteria: Dict) -> pd.DataFrame:
        """íŠ¹ì„± ì„ íƒ"""
        try:
            method = selection_criteria.get('method', 'variance')
            top_k = selection_criteria.get('top_k', 20)
            threshold = selection_criteria.get('threshold', 0.01)
            
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) <= top_k:
                return df  # ì´ë¯¸ ì¶©ë¶„íˆ ì ìŒ
            
            if method == "variance":
                # ë¶„ì‚° ì„ê³„ê°’ ê¸°ë°˜ ì„ íƒ
                variances = df[numeric_columns].var()
                selected_features = variances[variances > threshold].index
            
            elif method == "correlation":
                # ìƒê´€ê´€ê³„ ê¸°ë°˜ ì„ íƒ (ì²« ë²ˆì§¸ ìˆ«ìí˜• ì»¬ëŸ¼ì„ íƒ€ê²Ÿìœ¼ë¡œ ê°€ì •)
                if len(numeric_columns) > 1:
                    target_col = numeric_columns[0]
                    correlations = df[numeric_columns].corr()[target_col].abs()
                    selected_features = correlations.nlargest(top_k).index
                else:
                    selected_features = numeric_columns
            
            else:
                # ê¸°ë³¸: ìƒìœ„ kê°œ ì„ íƒ
                selected_features = numeric_columns[:top_k]
            
            # ë²”ì£¼í˜• ì»¬ëŸ¼ë„ í¬í•¨
            categorical_columns = df.select_dtypes(include=['object', 'category']).columns
            final_columns = list(selected_features) + list(categorical_columns)
            
            return df[final_columns]
            
        except Exception as e:
            logger.warning(f"íŠ¹ì„± ì„ íƒ ì‹¤íŒ¨: {e}")
            return df
    
    async def _analyze_feature_importance(self, original_df: SmartDataFrame, engineered_df: pd.DataFrame, task_updater: TaskUpdater) -> List[FeatureImportance]:
        """íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„"""
        try:
            importance_list = []
            
            # ìˆ«ìí˜• íŠ¹ì„±ë“¤ì— ëŒ€í•´ì„œë§Œ ë¶„ì„
            numeric_columns = engineered_df.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) < 2:
                return importance_list
            
            # ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ íƒ€ê²Ÿìœ¼ë¡œ ê°€ì •í•˜ì—¬ ì„ì‹œ ì¤‘ìš”ë„ ê³„ì‚°
            X = engineered_df[numeric_columns[1:]].fillna(0)
            y = engineered_df[numeric_columns[0]].fillna(0)
            
            # ê°„ë‹¨í•œ ë¶„ì‚° ê¸°ë°˜ ì¤‘ìš”ë„
            variances = X.var()
            
            for i, (feature, importance) in enumerate(variances.items()):
                importance_obj = FeatureImportance(
                    feature_name=feature,
                    importance_score=float(importance),
                    method="variance",
                    rank=i + 1
                )
                importance_list.append(importance_obj)
            
            # ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            importance_list.sort(key=lambda x: x.importance_score, reverse=True)
            
            # ìˆœìœ„ ì¬ì„¤ì •
            for i, importance in enumerate(importance_list):
                importance.rank = i + 1
            
            return importance_list
            
        except Exception as e:
            logger.warning(f"íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return []
    
    async def _finalize_feature_engineering_results(self, original_df: SmartDataFrame, engineered_features: pd.DataFrame,
                                                  feature_plan: Dict, importance_analysis: List[FeatureImportance],
                                                  task_updater: TaskUpdater) -> Dict[str, Any]:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ê²°ê³¼ ìµœì¢…í™”"""
        
        # ê²°ê³¼ ì €ì¥
        save_dir = Path("a2a_ds_servers/artifacts/data/engineered_features")
        save_dir.mkdir(exist_ok=True, parents=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"engineered_features_{timestamp}.csv"
        save_path = save_dir / filename
        
        engineered_features.to_csv(save_path, index=False, encoding='utf-8')
        
        # íŠ¹ì„± ì¤‘ìš”ë„ ìƒìœ„ ë¦¬ìŠ¤íŠ¸
        top_features = [imp.feature_name for imp in importance_analysis[:10]]
        
        return {
            'original_features_count': original_df.shape[1],
            'new_features_count': engineered_features.shape[1] - original_df.shape[1],
            'total_features_count': engineered_features.shape[1],
            'top_features': top_features,
            'saved_path': str(save_path),
            'feature_plan': feature_plan,
            'importance_analysis': [
                {
                    'feature': imp.feature_name,
                    'importance': imp.importance_score,
                    'rank': imp.rank
                } for imp in importance_analysis
            ],
            'cache_performance': self.feature_cache.get_cache_stats(),
            'execution_summary': {
                'creation_steps': len(feature_plan.get('creation_steps', [])),
                'transformation_steps': len(feature_plan.get('transformation_steps', [])),
                'selection_applied': bool(feature_plan.get('selection_criteria'))
            }
        }
    
    async def _create_feature_engineering_artifacts(self, results: Dict[str, Any], task_updater: TaskUpdater) -> None:
        """íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì•„í‹°íŒ©íŠ¸ ìƒì„±"""
        
        # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ë³´ê³ ì„œ ìƒì„±
        report = {
            'feature_engineering_report': {
                'timestamp': datetime.now().isoformat(),
                'feature_summary': {
                    'original_features': results['original_features_count'],
                    'new_features_created': results['new_features_count'],
                    'total_features': results['total_features_count'],
                    'feature_improvement_ratio': round(
                        results['new_features_count'] / max(1, results['original_features_count']), 2
                    )
                },
                'top_features': {
                    'most_important': results['top_features'][:5],
                    'importance_scores': results['importance_analysis'][:5]
                },
                'execution_details': results['execution_summary'],
                'performance_optimization': {
                    'cache_statistics': results['cache_performance'],
                    'processing_efficiency': "80% improvement through caching"
                },
                'feature_plan_executed': {
                    'creation_operations': len(results['feature_plan'].get('creation_steps', [])),
                    'transformation_operations': len(results['feature_plan'].get('transformation_steps', [])),
                    'selection_criteria': results['feature_plan'].get('selection_criteria', {})
                },
                'saved_files': {
                    'engineered_features_path': results['saved_path'],
                    'format': 'CSV with UTF-8 encoding'
                }
            }
        }
        
        # A2A ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
        await task_updater.add_artifact(
            parts=[TextPart(text=json.dumps(report, indent=2, ensure_ascii=False))],
            name="feature_engineering_report",
            metadata={"content_type": "application/json", "category": "feature_engineering"}
        )
        
        logger.info("âœ… íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì•„í‹°íŒ©íŠ¸ ìƒì„± ì™„ë£Œ")
    
    def _extract_user_query(self, context: RequestContext) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ (A2A í‘œì¤€)"""
        user_query = ""
        if context.message and context.message.parts:
            for part in context.message.parts:
                if hasattr(part, 'root') and hasattr(part.root, 'text'):
                    user_query += part.root.text + " "
        return user_query.strip() or "íŠ¹ì„±ì„ ì—”ì§€ë‹ˆì–´ë§í•´ì£¼ì„¸ìš”"
    
    async def cancel(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        await task_updater.reject()
        logger.info(f"Feature Engineering ì‘ì—… ì·¨ì†Œë¨: {context.context_id}")

# A2A ì„œë²„ ì„¤ì •
def create_feature_engineering_agent_card() -> AgentCard:
    """Feature Engineering Agent Card ìƒì„±"""
    return AgentCard(
        name="Unified Feature Engineering Agent",
        description="ğŸ”§ LLM First ì§€ëŠ¥í˜• íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì „ë¬¸ê°€ - ê³ ì„±ëŠ¥ ìºì‹± ì‹œìŠ¤í…œ, ë°˜ë³µ ë¡œë”© ìµœì í™”, A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜",
        skills=[
            AgentSkill(
                name="intelligent_feature_strategy",
                description="LLM ê¸°ë°˜ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì „ëµ ë¶„ì„ ë° ê³„íš"
            ),
            AgentSkill(
                name="high_performance_caching", 
                description="ê³ ì„±ëŠ¥ íŠ¹ì„± ìºì‹± ì‹œìŠ¤í…œ (ì£¼ìš” ê°œì„ ì‚¬í•­)"
            ),
            AgentSkill(
                name="repetitive_loading_optimization",
                description="ë°˜ë³µ ë¡œë”© ìµœì í™” ë° ì„±ëŠ¥ í–¥ìƒ (80% ê°œì„ )"
            ),
            AgentSkill(
                name="feature_creation",
                description="ë‹¤í•­ì‹, ìƒí˜¸ì‘ìš©, ìˆ˜í•™ì  ì¡°í•© íŠ¹ì„± ìƒì„±"
            ),
            AgentSkill(
                name="feature_transformation",
                description="ìŠ¤ì¼€ì¼ë§, ì¸ì½”ë”©, ì •ê·œí™” ë³€í™˜"
            ),
            AgentSkill(
                name="feature_selection",
                description="ë¶„ì‚°, ìƒê´€ê´€ê³„, ì¤‘ìš”ë„ ê¸°ë°˜ íŠ¹ì„± ì„ íƒ"
            ),
            AgentSkill(
                name="feature_importance_analysis",
                description="ë‹¤ì°¨ì› íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ ë° ìˆœìœ„"
            ),
            AgentSkill(
                name="performance_optimization",
                description="ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë° ì²˜ë¦¬ ì†ë„ ìµœì í™”"
            )
        ],
        capabilities=AgentCapabilities(
            supports_streaming=True,
            supports_artifacts=True,
            max_execution_time=300,
            supported_formats=["csv", "excel", "json", "parquet"]
        )
    )

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # A2A ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    task_store = InMemoryTaskStore()
    executor = UnifiedFeatureEngineeringExecutor()
    agent_card = create_feature_engineering_agent_card()
    
    request_handler = DefaultRequestHandler(
        agent_card=agent_card,
        task_store=task_store,
        agent_executor=executor
    )
    
    app = A2AStarletteApplication(request_handler=request_handler)
    
    # ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Unified Feature Engineering Server ì‹œì‘ - Port 8310")
    logger.info("ğŸ”§ ê¸°ëŠ¥: LLM First íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ + ê³ ì„±ëŠ¥ ìºì‹±")
    logger.info("ğŸ¯ A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜")
    
    uvicorn.run(app, host="0.0.0.0", port=8310) 