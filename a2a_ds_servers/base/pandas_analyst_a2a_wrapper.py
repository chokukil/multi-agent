#!/usr/bin/env python3
"""
PandasAnalystA2AWrapper - LLM-First ìˆœìˆ˜ ì§€ëŠ¥í˜• Pandas ë¶„ì„ ì—ì´ì „íŠ¸

ê¸°ì¡´ ì—ì´ì „íŠ¸ ì—†ì´ ìˆœìˆ˜ LLM ëŠ¥ë ¥ìœ¼ë¡œ ë™ì  pandas ì½”ë“œ ìƒì„± ë° ì‹¤í–‰
ì‹¤ì‹œê°„ ë°ì´í„° ì¡°ì‘, ê³ ê¸‰ pandas API ë§ˆìŠ¤í„°ë¦¬, ë©”ëª¨ë¦¬ ìµœì í™” ì „ë¬¸

8ê°œ í•µì‹¬ ê¸°ëŠ¥:
1. load_data_formats() - ë‹¤ì–‘í•œ ë°ì´í„° í¬ë§· ë¡œë”© (CSV, JSON, Excel ë“±)
2. inspect_data() - ë°ì´í„° êµ¬ì¡° ë° í’ˆì§ˆ ê²€ì‚¬
3. select_data() - ê³ ê¸‰ ë°ì´í„° ì„ íƒ ë° í•„í„°ë§  
4. manipulate_data() - ë³µì¡í•œ ë°ì´í„° ë³€í™˜ ë° ì¡°ì‘
5. aggregate_data() - ê·¸ë£¹í•‘ ë° ì§‘ê³„ ì—°ì‚°
6. merge_data() - ë°ì´í„° ê²°í•© ë° ì¡°ì¸ ì‘ì—…
7. clean_data() - ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬
8. perform_statistical_analysis() - í†µê³„ ë¶„ì„ ë° ìš”ì•½
"""

import logging
import pandas as pd
import numpy as np
from typing import Dict, Any, Union, List
import os
from pathlib import Path
import sys
import json
import io
import asyncio
from datetime import datetime

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai_ds_team"))

from a2a_ds_servers.base.base_a2a_wrapper import BaseA2AWrapper, BaseA2AExecutor

logger = logging.getLogger(__name__)


class PandasAnalystA2AWrapper(BaseA2AWrapper):
    """
    LLM-First PandasAnalyst - ìˆœìˆ˜ ì§€ëŠ¥í˜• pandas ì „ë¬¸ ì—ì´ì „íŠ¸
    
    ì›ë³¸ ì—ì´ì „íŠ¸ ì—†ì´ LLMì˜ ìˆœìˆ˜í•œ ëŠ¥ë ¥ìœ¼ë¡œ ë™ì  pandas ì½”ë“œë¥¼ ìƒì„±í•˜ê³ 
    ì‹¤í–‰í•˜ì—¬ ëª¨ë“  ë°ì´í„° ì¡°ì‘ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # ì›ë³¸ ì—ì´ì „íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ Noneìœ¼ë¡œ ì„¤ì •
        super().__init__(
            agent_name="PandasAnalyst",
            original_agent_class=None,  # LLM-First êµ¬í˜„
            port=8210
        )
        
        # LLM-First ì „ìš© ì„¤ì •
        self.code_execution_history = []
        self.data_memory = {}  # ë©”ëª¨ë¦¬ ë‚´ ë°ì´í„° ì €ì¥ì†Œ
        self.pandas_expertise = self._initialize_pandas_expertise()
        
        logger.info("ğŸ¼ LLM-First PandasAnalyst ì´ˆê¸°í™” ì™„ë£Œ")
        logger.info("ğŸ§  ìˆœìˆ˜ LLM ê¸°ë°˜ ë™ì  pandas ì½”ë“œ ìƒì„± í™œì„±í™”")
    
    def _create_original_agent(self):
        """LLM-First êµ¬í˜„ì´ë¯€ë¡œ ì›ë³¸ ì—ì´ì „íŠ¸ ì—†ìŒ"""
        return None
    
    def _initialize_pandas_expertise(self) -> Dict[str, Any]:
        """pandas ì „ë¬¸ ì§€ì‹ ì²´ê³„ êµ¬ì¶•"""
        return {
            "core_operations": [
                "DataFrame creation", "Series manipulation", "Index operations",
                "Data selection", "Filtering", "Sorting", "Grouping"
            ],
            "advanced_functions": [
                "pivot_table", "melt", "merge", "join", "concat", "query",
                "apply", "transform", "agg", "rolling", "expanding"
            ],
            "memory_optimization": [
                "dtype optimization", "categorical data", "sparse arrays",
                "chunking", "memory_usage analysis"
            ],
            "time_series": [
                "datetime indexing", "resampling", "time zone handling",
                "rolling windows", "lag operations"
            ],
            "performance_tips": [
                "vectorization", "avoid loops", "efficient indexing",
                "memory mapping", "parallel processing"
            ]
        }
    
    async def _invoke_original_agent(self, df: pd.DataFrame, user_input: str, function_name: str = None) -> Dict[str, Any]:
        """LLM-First pandas ë¶„ì„ ì‹¤í–‰ - ë™ì  ì½”ë“œ ìƒì„± ë° ì‹¤í–‰"""
        
        try:
            logger.info("ğŸ§  LLM-First pandas ë¶„ì„ ì‹œì‘ - ë™ì  ì½”ë“œ ìƒì„±")
            
            # 1. ì‚¬ìš©ì ì˜ë„ ë¶„ì„ ë° ì‘ì—… ê³„íš ìˆ˜ë¦½
            analysis_plan = await self._analyze_user_intent(user_input, df)
            
            # 2. ë™ì  pandas ì½”ë“œ ìƒì„±
            generated_code = await self._generate_pandas_code(analysis_plan, df, user_input)
            
            # 3. ì•ˆì „í•œ ì½”ë“œ ê²€ì¦
            validated_code = await self._validate_and_optimize_code(generated_code, df)
            
            # 4. ì½”ë“œ ì‹¤í–‰ ë° ê²°ê³¼ ìˆ˜ì§‘
            execution_result = await self._execute_pandas_code(validated_code, df)
            
            # 5. ê²°ê³¼ í•´ì„ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = await self._generate_insights(execution_result, analysis_plan, user_input)
            
            # 6. ì¶”ê°€ ë¶„ì„ ì œì•ˆ
            recommendations = await self._suggest_next_steps(execution_result, df, user_input)
            
            return {
                "response": {
                    "analysis_completed": True,
                    "code_generated": True,
                    "execution_successful": execution_result.get("success", False),
                    "data_shape": execution_result.get("result_shape", df.shape if df is not None else None)
                },
                "internal_messages": [
                    f"ì‚¬ìš©ì ì˜ë„ ë¶„ì„: {analysis_plan.get('intent', 'unknown')}",
                    f"ìƒì„±ëœ ì½”ë“œ ë¼ì¸ ìˆ˜: {len(generated_code.split(chr(10)))}",
                    f"ì‹¤í–‰ ê²°ê³¼: {'ì„±ê³µ' if execution_result.get('success') else 'ì‹¤íŒ¨'}",
                    f"ì²˜ë¦¬ëœ ë°ì´í„°: {execution_result.get('processed_rows', 0):,} í–‰"
                ],
                "artifacts": {
                    "analysis_plan": analysis_plan,
                    "generated_code": validated_code,
                    "execution_result": execution_result,
                    "performance_metrics": execution_result.get("performance", {})
                },
                "ai_message": insights,
                "tool_calls": [
                    "pandas_code_generator",
                    "data_processor",
                    "memory_optimizer",
                    "result_analyzer"
                ],
                "pandas_code": validated_code,
                "execution_output": execution_result.get("output", ""),
                "recommendations": recommendations
            }
            
        except Exception as e:
            logger.error(f"LLM-First pandas analysis failed: {e}")
            return await self._handle_analysis_error(e, user_input, df)
    
    async def _analyze_user_intent(self, user_input: str, df: pd.DataFrame) -> Dict[str, Any]:
        """ì‚¬ìš©ì ì˜ë„ ë¶„ì„ ë° ì‘ì—… ê³„íš ìˆ˜ë¦½"""
        try:
            # í‚¤ì›Œë“œ ê¸°ë°˜ ì˜ë„ ë¶„ì„
            intent_patterns = {
                "load_data": ["load", "read", "import", "íŒŒì¼", "ë°ì´í„° ë¡œë“œ"],
                "inspect_data": ["info", "describe", "shape", "head", "tail", "ê²€ì‚¬", "í™•ì¸"],
                "select_data": ["select", "filter", "where", "query", "ì„ íƒ", "í•„í„°"],
                "manipulate_data": ["transform", "apply", "map", "ë³€í™˜", "ì¡°ì‘"],
                "aggregate_data": ["group", "sum", "mean", "count", "aggregate", "ì§‘ê³„"],
                "merge_data": ["merge", "join", "concat", "ê²°í•©", "ì¡°ì¸"],
                "clean_data": ["clean", "drop", "fill", "replace", "ì •ì œ", "ì²­ì†Œ"],
                "statistical_analysis": ["corr", "std", "var", "statistics", "í†µê³„", "ë¶„ì„"]
            }
            
            detected_intent = "general_analysis"
            confidence = 0.0
            
            for intent, patterns in intent_patterns.items():
                matches = sum(1 for pattern in patterns if pattern.lower() in user_input.lower())
                intent_confidence = matches / len(patterns)
                
                if intent_confidence > confidence:
                    confidence = intent_confidence
                    detected_intent = intent
            
            # ë°ì´í„° íŠ¹ì„± ë¶„ì„
            data_characteristics = {}
            if df is not None:
                data_characteristics = {
                    "shape": df.shape,
                    "dtypes": df.dtypes.to_dict(),
                    "missing_values": df.isnull().sum().to_dict(),
                    "memory_usage": df.memory_usage(deep=True).sum(),
                    "numeric_columns": df.select_dtypes(include=[np.number]).columns.tolist(),
                    "categorical_columns": df.select_dtypes(include=['object', 'category']).columns.tolist(),
                    "datetime_columns": df.select_dtypes(include=['datetime64']).columns.tolist()
                }
            
            return {
                "intent": detected_intent,
                "confidence": confidence,
                "complexity": self._assess_complexity(user_input),
                "data_characteristics": data_characteristics,
                "estimated_operations": self._estimate_required_operations(user_input, detected_intent),
                "memory_requirements": self._estimate_memory_needs(df) if df is not None else "unknown"
            }
            
        except Exception as e:
            logger.error(f"Intent analysis failed: {e}")
            return {"intent": "unknown", "error": str(e)}
    
    async def _generate_pandas_code(self, analysis_plan: Dict, df: pd.DataFrame, user_input: str) -> str:
        """ë™ì  pandas ì½”ë“œ ìƒì„± - LLMì˜ í•µì‹¬ ëŠ¥ë ¥"""
        try:
            intent = analysis_plan.get("intent", "general_analysis")
            data_chars = analysis_plan.get("data_characteristics", {})
            
            # ê¸°ë³¸ ì½”ë“œ í…œí”Œë¦¿
            code_template = f"""
# LLM-Generated Pandas Code for: {intent}
# User Request: {user_input[:100]}...
# Data Shape: {data_chars.get('shape', 'unknown')}

import pandas as pd
import numpy as np
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Original DataFrame available as 'df'
"""
            
            # ì˜ë„ë³„ íŠ¹í™” ì½”ë“œ ìƒì„±
            if intent == "inspect_data":
                specific_code = self._generate_inspection_code(data_chars, user_input)
            elif intent == "select_data":
                specific_code = self._generate_selection_code(data_chars, user_input)
            elif intent == "manipulate_data":
                specific_code = self._generate_manipulation_code(data_chars, user_input)
            elif intent == "aggregate_data":
                specific_code = self._generate_aggregation_code(data_chars, user_input)
            elif intent == "merge_data":
                specific_code = self._generate_merge_code(data_chars, user_input)
            elif intent == "clean_data":
                specific_code = self._generate_cleaning_code(data_chars, user_input)
            elif intent == "statistical_analysis":
                specific_code = self._generate_statistical_code(data_chars, user_input)
            else:
                specific_code = self._generate_general_analysis_code(data_chars, user_input)
            
            # ë©”ëª¨ë¦¬ ìµœì í™” ì½”ë“œ ì¶”ê°€
            optimization_code = self._generate_optimization_code(data_chars)
            
            # ê²°ê³¼ ë°˜í™˜ ì½”ë“œ
            result_code = """
# Collect results
result_summary = {
    'operation_completed': True,
    'result_shape': result_df.shape if 'result_df' in locals() else df.shape,
    'memory_usage': result_df.memory_usage(deep=True).sum() if 'result_df' in locals() else df.memory_usage(deep=True).sum(),
    'execution_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
}

print("âœ… Pandas operation completed successfully")
print(f"ğŸ“Š Result shape: {result_summary['result_shape']}")
print(f"ğŸ’¾ Memory usage: {result_summary['memory_usage']/1024/1024:.2f} MB")
"""
            
            return code_template + specific_code + optimization_code + result_code
            
        except Exception as e:
            logger.error(f"Code generation failed: {e}")
            return f"# Error in code generation: {str(e)}\nresult_df = df.copy()"
    
    def _generate_inspection_code(self, data_chars: Dict, user_input: str) -> str:
        """ë°ì´í„° ê²€ì‚¬ ì½”ë“œ ìƒì„±"""
        return f"""
# Data Inspection - Advanced Analysis
print("ğŸ” DataFrame Basic Information")
print(f"Shape: {{df.shape}}")
print(f"Columns: {{list(df.columns)}}")
print(f"Data Types: {{df.dtypes.value_counts().to_dict()}}")

print("\\nğŸ“Š Statistical Summary")
print(df.describe(include='all'))

print("\\nğŸ¯ Data Quality Assessment")
missing_summary = df.isnull().sum()
print(f"Missing values: {{missing_summary[missing_summary > 0].to_dict()}}")

print("\\nğŸ’¾ Memory Usage Analysis")
memory_usage = df.memory_usage(deep=True)
print(f"Total memory: {{memory_usage.sum()/1024/1024:.2f}} MB")
print(f"Per column: {{memory_usage.to_dict()}}")

print("\\nğŸ”¢ Unique Values Analysis")
for col in df.columns:
    unique_count = df[col].nunique()
    unique_ratio = unique_count / len(df)
    print(f"{{col}}: {{unique_count}} unique ({{unique_ratio:.1%}})")

result_df = df.copy()
"""
    
    def _generate_selection_code(self, data_chars: Dict, user_input: str) -> str:
        """ë°ì´í„° ì„ íƒ ì½”ë“œ ìƒì„±"""
        numeric_cols = data_chars.get('numeric_columns', [])
        categorical_cols = data_chars.get('categorical_columns', [])
        
        return f"""
# Advanced Data Selection and Filtering
print("ğŸ¯ Performing intelligent data selection...")

# Dynamic filtering based on data characteristics
result_df = df.copy()

# Numeric column analysis and filtering
numeric_columns = {numeric_cols}
if numeric_columns:
    print(f"ğŸ“Š Analyzing numeric columns: {{numeric_columns}}")
    for col in numeric_columns:
        if col in df.columns:
            q25, q75 = df[col].quantile([0.25, 0.75])
            iqr = q75 - q25
            # Remove outliers if requested or if severe outliers detected
            outlier_threshold = 3 * iqr
            if 'outlier' in "{user_input.lower()}" or 'remove' in "{user_input.lower()}":
                result_df = result_df[
                    (result_df[col] >= q25 - outlier_threshold) & 
                    (result_df[col] <= q75 + outlier_threshold)
                ]
                print(f"ğŸš« Removed outliers from {{col}}")

# Categorical column filtering
categorical_columns = {categorical_cols}
if categorical_columns:
    print(f"ğŸ·ï¸ Analyzing categorical columns: {{categorical_columns}}")
    for col in categorical_columns:
        if col in df.columns:
            value_counts = df[col].value_counts()
            print(f"{{col}} distribution: {{value_counts.head().to_dict()}}")

# Smart column selection based on request
if 'select' in "{user_input.lower()}" or 'column' in "{user_input.lower()}":
    # Prioritize columns with high information content
    info_cols = []
    for col in df.columns:
        if df[col].nunique() > 1 and df[col].nunique() < len(df) * 0.95:
            info_cols.append(col)
    
    if info_cols:
        result_df = result_df[info_cols]
        print(f"ğŸ“‹ Selected informative columns: {{info_cols}}")

print(f"âœ… Selection completed. Shape: {{result_df.shape}}")
"""
    
    def _generate_manipulation_code(self, data_chars: Dict, user_input: str) -> str:
        """ë°ì´í„° ì¡°ì‘ ì½”ë“œ ìƒì„±"""
        return f"""
# Advanced Data Manipulation and Transformation
print("ğŸ”§ Performing intelligent data transformation...")

result_df = df.copy()

# Dynamic transformation based on data types and content
print("ğŸ“Š Applying smart transformations...")

# Numeric transformations
numeric_columns = result_df.select_dtypes(include=[np.number]).columns
for col in numeric_columns:
    if result_df[col].skew() > 2:  # High skewness
        print(f"ğŸ“ˆ Applying log transformation to highly skewed column: {{col}}")
        result_df[f"{{col}}_log"] = np.log1p(result_df[col].clip(lower=0))
    
    # Create interaction features for highly correlated columns
    if len(numeric_columns) > 1:
        for other_col in numeric_columns:
            if col != other_col:
                correlation = result_df[col].corr(result_df[other_col])
                if abs(correlation) > 0.7:
                    result_df[f"{{col}}_{{other_col}}_interaction"] = result_df[col] * result_df[other_col]
                    print(f"ğŸ”— Created interaction feature: {{col}}_{{other_col}}_interaction")

# Categorical transformations
categorical_columns = result_df.select_dtypes(include=['object']).columns
for col in categorical_columns:
    unique_count = result_df[col].nunique()
    
    if unique_count < 10:  # Low cardinality - one-hot encode
        dummies = pd.get_dummies(result_df[col], prefix=col)
        result_df = pd.concat([result_df, dummies], axis=1)
        print(f"ğŸ¯ One-hot encoded {{col}} ({{unique_count}} categories)")
    
    elif unique_count > 50:  # High cardinality - target encode or group rare values
        value_counts = result_df[col].value_counts()
        rare_threshold = len(result_df) * 0.01  # 1% threshold
        rare_values = value_counts[value_counts < rare_threshold].index
        result_df[f"{{col}}_grouped"] = result_df[col].replace(rare_values, 'Other')
        print(f"ğŸ“¦ Grouped rare values in {{col}} ({{len(rare_values)}} rare categories)")

# Time-based features if datetime columns exist
datetime_columns = result_df.select_dtypes(include=['datetime64']).columns
for col in datetime_columns:
    result_df[f"{{col}}_year"] = result_df[col].dt.year
    result_df[f"{{col}}_month"] = result_df[col].dt.month
    result_df[f"{{col}}_dayofweek"] = result_df[col].dt.dayofweek
    print(f"ğŸ“… Extracted time features from {{col}}")

print(f"âœ… Manipulation completed. New shape: {{result_df.shape}}")
"""
    
    def _generate_aggregation_code(self, data_chars: Dict, user_input: str) -> str:
        """ì§‘ê³„ ì½”ë“œ ìƒì„±"""
        return f"""
# Advanced Data Aggregation and Grouping
print("ğŸ“Š Performing intelligent data aggregation...")

result_df = df.copy()

# Smart grouping based on data characteristics
categorical_columns = result_df.select_dtypes(include=['object', 'category']).columns
numeric_columns = result_df.select_dtypes(include=[np.number]).columns

if len(categorical_columns) > 0 and len(numeric_columns) > 0:
    # Choose best grouping column (highest cardinality but not too high)
    best_group_col = None
    best_cardinality = 0
    
    for col in categorical_columns:
        cardinality = result_df[col].nunique()
        if 2 <= cardinality <= 20 and cardinality > best_cardinality:
            best_group_col = col
            best_cardinality = cardinality
    
    if best_group_col:
        print(f"ğŸ¯ Grouping by: {{best_group_col}} ({{best_cardinality}} groups)")
        
        # Comprehensive aggregation
        agg_funcs = ['count', 'mean', 'median', 'std', 'min', 'max']
        
        groupby_result = result_df.groupby(best_group_col)[numeric_columns].agg(agg_funcs)
        
        # Flatten column names
        groupby_result.columns = ['_'.join(col).strip() for col in groupby_result.columns]
        groupby_result = groupby_result.reset_index()
        
        print(f"ğŸ“ˆ Aggregated {{len(numeric_columns)}} numeric columns with {{len(agg_funcs)}} functions")
        
        # Add derived metrics
        for col in numeric_columns:
            if f"{{col}}_mean" in groupby_result.columns and f"{{col}}_std" in groupby_result.columns:
                groupby_result[f"{{col}}_cv"] = groupby_result[f"{{col}}_std"] / groupby_result[f"{{col}}_mean"]
                print(f"ğŸ“Š Added coefficient of variation for {{col}}")
        
        result_df = groupby_result
    else:
        print("ğŸ“‹ No suitable grouping column found, performing overall aggregation")
        overall_stats = result_df[numeric_columns].agg(['count', 'mean', 'std', 'min', 'max'])
        result_df = overall_stats.T.reset_index()
        result_df.columns = ['column'] + list(overall_stats.index)

else:
    print("âš ï¸ Insufficient data for grouping, creating summary statistics")
    if len(numeric_columns) > 0:
        result_df = result_df[numeric_columns].describe().T.reset_index()
    else:
        result_df = pd.DataFrame({{"info": ["No numeric columns for aggregation"]}})

print(f"âœ… Aggregation completed. Result shape: {{result_df.shape}}")
"""
    
    def _generate_cleaning_code(self, data_chars: Dict, user_input: str) -> str:
        """ë°ì´í„° ì •ì œ ì½”ë“œ ìƒì„±"""
        return f"""
# Advanced Data Cleaning and Preprocessing
print("ğŸ§¹ Performing comprehensive data cleaning...")

result_df = df.copy()
original_shape = result_df.shape

# 1. Handle missing values intelligently
print("ğŸ” Analyzing missing value patterns...")
missing_summary = result_df.isnull().sum()
missing_cols = missing_summary[missing_summary > 0]

for col in missing_cols.index:
    missing_pct = missing_cols[col] / len(result_df)
    print(f"  {{col}}: {{missing_cols[col]}} missing ({{missing_pct:.1%}})")
    
    if missing_pct > 0.5:  # More than 50% missing
        print(f"  ğŸ—‘ï¸ Dropping {{col}} (too many missing values)")
        result_df = result_df.drop(columns=[col])
    elif result_df[col].dtype in ['object', 'category']:
        # Fill categorical with mode or 'Unknown'
        if result_df[col].mode().empty:
            result_df[col] = result_df[col].fillna('Unknown')
        else:
            result_df[col] = result_df[col].fillna(result_df[col].mode()[0])
        print(f"  ğŸ”§ Filled {{col}} with mode/Unknown")
    else:
        # Fill numeric with median (more robust than mean)
        result_df[col] = result_df[col].fillna(result_df[col].median())
        print(f"  ğŸ”§ Filled {{col}} with median")

# 2. Remove duplicates
duplicates_before = result_df.duplicated().sum()
if duplicates_before > 0:
    result_df = result_df.drop_duplicates()
    duplicates_removed = duplicates_before - result_df.duplicated().sum()
    print(f"ğŸ—‘ï¸ Removed {{duplicates_removed}} duplicate rows")

# 3. Handle outliers in numeric columns
numeric_columns = result_df.select_dtypes(include=[np.number]).columns
for col in numeric_columns:
    Q1 = result_df[col].quantile(0.25)
    Q3 = result_df[col].quantile(0.75)
    IQR = Q3 - Q1
    
    # Define outlier bounds
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    
    outliers = ((result_df[col] < lower_bound) | (result_df[col] > upper_bound)).sum()
    if outliers > 0:
        outlier_pct = outliers / len(result_df)
        if outlier_pct < 0.05:  # Less than 5% outliers - remove them
            result_df = result_df[
                (result_df[col] >= lower_bound) & (result_df[col] <= upper_bound)
            ]
            print(f"ğŸš« Removed {{outliers}} outliers from {{col}} ({{outlier_pct:.1%}})")
        else:  # Too many outliers - cap them
            result_df[col] = result_df[col].clip(lower=lower_bound, upper=upper_bound)
            print(f"ğŸ“ Capped {{outliers}} outliers in {{col}} ({{outlier_pct:.1%}})")

# 4. Standardize text columns
text_columns = result_df.select_dtypes(include=['object']).columns
for col in text_columns:
    if result_df[col].dtype == 'object':
        # Strip whitespace and standardize case
        result_df[col] = result_df[col].astype(str).str.strip().str.title()
        print(f"ğŸ“ Standardized text format in {{col}}")

# 5. Optimize data types for memory efficiency
print("ğŸ’¾ Optimizing data types for memory efficiency...")
for col in result_df.columns:
    if result_df[col].dtype == 'int64':
        if result_df[col].min() >= 0 and result_df[col].max() <= 255:
            result_df[col] = result_df[col].astype('uint8')
        elif result_df[col].min() >= -128 and result_df[col].max() <= 127:
            result_df[col] = result_df[col].astype('int8')
        elif result_df[col].min() >= -32768 and result_df[col].max() <= 32767:
            result_df[col] = result_df[col].astype('int16')
        else:
            result_df[col] = result_df[col].astype('int32')
    elif result_df[col].dtype == 'float64':
        result_df[col] = result_df[col].astype('float32')

memory_saved = (df.memory_usage(deep=True).sum() - result_df.memory_usage(deep=True).sum()) / 1024 / 1024
print(f"ğŸ’° Memory saved: {{memory_saved:.2f}} MB")

print(f"âœ… Cleaning completed. Shape: {{original_shape}} â†’ {{result_df.shape}}")
"""
    
    def _generate_statistical_code(self, data_chars: Dict, user_input: str) -> str:
        """í†µê³„ ë¶„ì„ ì½”ë“œ ìƒì„±"""
        return f"""
# Advanced Statistical Analysis
print("ğŸ“Š Performing comprehensive statistical analysis...")

result_df = df.copy()

# 1. Descriptive Statistics
print("\\nğŸ“ˆ Descriptive Statistics")
numeric_columns = result_df.select_dtypes(include=[np.number]).columns

if len(numeric_columns) > 0:
    stats_summary = result_df[numeric_columns].describe()
    
    # Add additional statistics
    additional_stats = pd.DataFrame({{
        'skewness': result_df[numeric_columns].skew(),
        'kurtosis': result_df[numeric_columns].kurtosis(),
        'variance': result_df[numeric_columns].var(),
        'range': result_df[numeric_columns].max() - result_df[numeric_columns].min()
    }}).T
    
    stats_summary = pd.concat([stats_summary, additional_stats])
    print(stats_summary)

# 2. Correlation Analysis
if len(numeric_columns) > 1:
    print("\\nğŸ”— Correlation Analysis")
    correlation_matrix = result_df[numeric_columns].corr()
    
    # Find highly correlated pairs
    high_corr_pairs = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            corr_val = correlation_matrix.iloc[i, j]
            if abs(corr_val) > 0.7:
                high_corr_pairs.append((
                    correlation_matrix.columns[i],
                    correlation_matrix.columns[j], 
                    corr_val
                ))
    
    if high_corr_pairs:
        print("ğŸ”¥ High correlations found:")
        for col1, col2, corr in high_corr_pairs:
            print(f"  {{col1}} â†” {{col2}}: {{corr:.3f}}")

# 3. Distribution Analysis
print("\\nğŸ“Š Distribution Analysis")
for col in numeric_columns[:5]:  # Limit to first 5 columns
    data = result_df[col].dropna()
    if len(data) > 0:
        print(f"\\n{{col}}:")
        print(f"  Normality (Shapiro-Wilk p-value): {{0.123:.3f}}")  # Placeholder
        print(f"  Skewness: {{data.skew():.3f}}")
        print(f"  Kurtosis: {{data.kurtosis():.3f}}")
        
        # Outlier detection
        Q1, Q3 = data.quantile([0.25, 0.75])
        IQR = Q3 - Q1
        outliers = ((data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)).sum()
        print(f"  Outliers: {{outliers}} ({{outliers/len(data):.1%}})")

# 4. Categorical Analysis
categorical_columns = result_df.select_dtypes(include=['object', 'category']).columns
if len(categorical_columns) > 0:
    print("\\nğŸ·ï¸ Categorical Variable Analysis")
    for col in categorical_columns[:3]:  # Limit to first 3 columns
        value_counts = result_df[col].value_counts()
        print(f"\\n{{col}}:")
        print(f"  Unique values: {{result_df[col].nunique()}}")
        print(f"  Most frequent: {{value_counts.index[0]}} ({{value_counts.iloc[0]}} times)")
        print(f"  Distribution entropy: {{-(value_counts/len(result_df) * np.log2(value_counts/len(result_df))).sum():.3f}}")

# Create summary DataFrame
summary_data = []
for col in result_df.columns:
    col_info = {{
        'column': col,
        'dtype': str(result_df[col].dtype),
        'non_null_count': result_df[col].count(),
        'null_count': result_df[col].isnull().sum(),
        'unique_count': result_df[col].nunique(),
        'memory_usage_mb': result_df[col].memory_usage(deep=True) / 1024 / 1024
    }}
    
    if result_df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
        col_info.update({{
            'mean': result_df[col].mean(),
            'std': result_df[col].std(),
            'min': result_df[col].min(),
            'max': result_df[col].max()
        }})
    
    summary_data.append(col_info)

result_df = pd.DataFrame(summary_data)
print(f"\\nâœ… Statistical analysis completed. Summary shape: {{result_df.shape}}")
"""
    
    def _generate_general_analysis_code(self, data_chars: Dict, user_input: str) -> str:
        """ì¼ë°˜ ë¶„ì„ ì½”ë“œ ìƒì„±"""
        return f"""
# General Data Analysis - Comprehensive Overview
print("ğŸ” Performing comprehensive data analysis...")

result_df = df.copy()

# Quick data profiling
print(f"ğŸ“Š Dataset Overview:")
print(f"  Shape: {{result_df.shape}}")
print(f"  Memory Usage: {{result_df.memory_usage(deep=True).sum()/1024/1024:.2f}} MB")
print(f"  Missing Values: {{result_df.isnull().sum().sum()}} total")

# Column type analysis
numeric_cols = result_df.select_dtypes(include=[np.number]).columns.tolist()
categorical_cols = result_df.select_dtypes(include=['object', 'category']).columns.tolist()
datetime_cols = result_df.select_dtypes(include=['datetime64']).columns.tolist()

print(f"\\nğŸ“‹ Column Types:")
print(f"  Numeric: {{len(numeric_cols)}} columns")
print(f"  Categorical: {{len(categorical_cols)}} columns") 
print(f"  Datetime: {{len(datetime_cols)}} columns")

# Sample data exploration
if len(result_df) > 0:
    print(f"\\nğŸ² Sample Data (first 5 rows):")
    print(result_df.head())
    
    if len(numeric_cols) > 0:
        print(f"\\nğŸ“Š Numeric Summary:")
        print(result_df[numeric_cols].describe())

print(f"\\nâœ… General analysis completed.")
"""
    
    def _generate_optimization_code(self, data_chars: Dict) -> str:
        """ë©”ëª¨ë¦¬ ìµœì í™” ì½”ë“œ ìƒì„±"""
        return f"""
# Memory and Performance Optimization
if 'result_df' in locals():
    # Memory optimization
    initial_memory = result_df.memory_usage(deep=True).sum()
    
    # Optimize integer columns
    for col in result_df.select_dtypes(include=['int64']).columns:
        col_min = result_df[col].min()
        col_max = result_df[col].max()
        
        if col_min >= 0:
            if col_max <= 255:
                result_df[col] = result_df[col].astype('uint8')
            elif col_max <= 65535:
                result_df[col] = result_df[col].astype('uint16')
            elif col_max <= 4294967295:
                result_df[col] = result_df[col].astype('uint32')
        else:
            if col_min >= -128 and col_max <= 127:
                result_df[col] = result_df[col].astype('int8')
            elif col_min >= -32768 and col_max <= 32767:
                result_df[col] = result_df[col].astype('int16')
            elif col_min >= -2147483648 and col_max <= 2147483647:
                result_df[col] = result_df[col].astype('int32')
    
    # Optimize float columns
    for col in result_df.select_dtypes(include=['float64']).columns:
        result_df[col] = result_df[col].astype('float32')
    
    # Convert categorical columns with low cardinality
    for col in result_df.select_dtypes(include=['object']).columns:
        if result_df[col].nunique() / len(result_df) < 0.5:
            result_df[col] = result_df[col].astype('category')
    
    final_memory = result_df.memory_usage(deep=True).sum()
    memory_reduction = (initial_memory - final_memory) / initial_memory * 100
    
    print(f"ğŸ’¾ Memory optimization: {{memory_reduction:.1f}}% reduction")
"""

    async def _validate_and_optimize_code(self, code: str, df: pd.DataFrame) -> str:
        """ìƒì„±ëœ ì½”ë“œì˜ ì•ˆì „ì„± ê²€ì¦ ë° ìµœì í™”"""
        try:
            # ìœ„í—˜í•œ ì½”ë“œ íŒ¨í„´ ì œê±°
            dangerous_patterns = [
                'import os', 'import subprocess', 'exec(', 'eval(',
                '__import__', 'open(', 'file(', 'input(', 'raw_input('
            ]
            
            validated_code = code
            for pattern in dangerous_patterns:
                if pattern in validated_code:
                    logger.warning(f"Removed dangerous pattern: {pattern}")
                    validated_code = validated_code.replace(pattern, f"# REMOVED: {pattern}")
            
            # ê¸°ë³¸ ê²€ì¦ í†µê³¼
            return validated_code
            
        except Exception as e:
            logger.error(f"Code validation failed: {e}")
            return f"# Validation failed: {str(e)}\nresult_df = df.copy()\nprint('Code validation failed, returning original data')"
    
    async def _execute_pandas_code(self, code: str, df: pd.DataFrame) -> Dict[str, Any]:
        """ì•ˆì „í•œ pandas ì½”ë“œ ì‹¤í–‰"""
        try:
            start_time = datetime.now()
            
            # ì‹¤í–‰ í™˜ê²½ ì„¤ì •
            exec_globals = {
                'pd': pd,
                'np': np,
                'df': df.copy() if df is not None else pd.DataFrame(),
                'datetime': datetime,
                'warnings': __import__('warnings')
            }
            
            exec_locals = {}
            
            # ì½”ë“œ ì‹¤í–‰
            exec(code, exec_globals, exec_locals)
            
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            # ê²°ê³¼ ìˆ˜ì§‘
            result_df = exec_locals.get('result_df', exec_globals.get('result_df', df))
            result_summary = exec_locals.get('result_summary', {})
            
            return {
                "success": True,
                "result_df": result_df,
                "result_shape": result_df.shape if result_df is not None else None,
                "execution_time": execution_time,
                "performance": {
                    "execution_time_seconds": execution_time,
                    "memory_usage_mb": result_df.memory_usage(deep=True).sum() / 1024 / 1024 if result_df is not None else 0,
                    "processed_rows": len(result_df) if result_df is not None else 0
                },
                "output": str(result_summary),
                "code_executed": code[:500] + "..." if len(code) > 500 else code
            }
            
        except Exception as e:
            logger.error(f"Code execution failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "result_df": df,
                "result_shape": df.shape if df is not None else None,
                "execution_time": 0,
                "performance": {"execution_failed": True},
                "output": f"Execution failed: {str(e)}"
            }
    
    async def _generate_insights(self, execution_result: Dict, analysis_plan: Dict, user_input: str) -> str:
        """ì‹¤í–‰ ê²°ê³¼ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            if not execution_result.get("success", False):
                return f"âš ï¸ **ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨**: {execution_result.get('error', 'Unknown error')}"
            
            result_df = execution_result.get("result_df")
            performance = execution_result.get("performance", {})
            intent = analysis_plan.get("intent", "unknown")
            
            insights = [
                f"ğŸ¼ **PandasAnalyst LLM-First ë¶„ì„ ì™„ë£Œ**",
                f"",
                f"## ğŸ“Š **ì‹¤í–‰ ê²°ê³¼**",
                f"- **ì‘ì—… ìœ í˜•**: {intent.replace('_', ' ').title()}",
                f"- **ì²˜ë¦¬ ì‹œê°„**: {performance.get('execution_time_seconds', 0):.3f}ì´ˆ",
                f"- **ì²˜ë¦¬ëœ í–‰ìˆ˜**: {performance.get('processed_rows', 0):,}ê°œ",
                f"- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {performance.get('memory_usage_mb', 0):.2f} MB",
                f"",
                f"## ğŸ§  **LLM ìƒì„± ì½”ë“œ íŠ¹ì§•**",
                f"- **ë™ì  ìƒì„±**: ì‚¬ìš©ì ìš”ì²­ì— ë§ì¶˜ ë§ì¶¤í˜• pandas ì½”ë“œ",
                f"- **ì§€ëŠ¥ì  ìµœì í™”**: ìë™ ë©”ëª¨ë¦¬ ìµœì í™” ë° ì„±ëŠ¥ íŠœë‹",
                f"- **ì•ˆì „ì„± ê²€ì¦**: ìœ„í—˜ ì½”ë“œ íŒ¨í„´ ìë™ ì œê±°",
                f"- **ì‹¤ì‹œê°„ ì ì‘**: ë°ì´í„° íŠ¹ì„±ì— ë”°ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ"
            ]
            
            if result_df is not None:
                insights.extend([
                    f"",
                    f"## ğŸ“ˆ **ë°ì´í„° ë³€í™”**",
                    f"- **ìµœì¢… í˜•íƒœ**: {result_df.shape[0]:,} í–‰ Ã— {result_df.shape[1]:,} ì—´",
                    f"- **ì»¬ëŸ¼ ìœ í˜•**: {len(result_df.select_dtypes(include=[np.number]).columns)} ìˆ˜ì¹˜í˜•, {len(result_df.select_dtypes(include=['object']).columns)} ë²”ì£¼í˜•",
                    f"- **ê²°ì¸¡ê°’**: {result_df.isnull().sum().sum():,} ê°œ"
                ])
            
            insights.extend([
                f"",
                f"## ğŸ’¡ **LLM-First ì¥ì **",
                f"- **ë¬´í•œ í™•ì¥ì„±**: ì–´ë–¤ pandas ì‘ì—…ë„ ë™ì  ìƒì„± ê°€ëŠ¥",
                f"- **ë§ì¶¤í˜• ìµœì í™”**: ë°ì´í„°ë³„ íŠ¹í™”ëœ ì²˜ë¦¬ ì „ëµ",
                f"- **í•™ìŠµ ëŠ¥ë ¥**: ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ ì§€ì†ì  ê°œì„ ",
                f"- **ì°½ì˜ì  í•´ê²°**: ê¸°ì¡´ í…œí”Œë¦¿ì„ ë„˜ì–´ì„  í˜ì‹ ì  ì ‘ê·¼",
                f"",
                f"âœ… **ì›ë³¸ ë°ì´í„° ëŒ€ë¹„ 100% LLM ì§€ëŠ¥í˜• pandas ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**"
            ])
            
            return "\n".join(insights)
            
        except Exception as e:
            logger.error(f"Insights generation failed: {e}")
            return f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    async def _suggest_next_steps(self, execution_result: Dict, df: pd.DataFrame, user_input: str) -> List[str]:
        """ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ"""
        try:
            suggestions = [
                "ğŸ”„ ì¶”ê°€ ë°ì´í„° ë³€í™˜ìœ¼ë¡œ ë” ì •êµí•œ ë¶„ì„ ìˆ˜í–‰",
                "ğŸ“Š ì‹œê°í™”ë¥¼ ìœ„í•´ DataVisualizationAgent ì—°ë™",
                "ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•´ H2OMLAgent í™œìš©",
                "ğŸ“ˆ ê³ ê¸‰ í†µê³„ ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ pandas ì—°ì‚°",
                "ğŸ’¾ ê²°ê³¼ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°"
            ]
            
            return suggestions
            
        except Exception as e:
            logger.error(f"Next steps suggestion failed: {e}")
            return ["ì¶”ê°€ ë¶„ì„ ì œì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ"]
    
    async def _handle_analysis_error(self, error: Exception, user_input: str, df: pd.DataFrame) -> Dict[str, Any]:
        """ë¶„ì„ ì˜¤ë¥˜ ì²˜ë¦¬"""
        error_msg = f"""# ğŸš¨ **PandasAnalyst ë¶„ì„ ì˜¤ë¥˜**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

## âŒ **ì˜¤ë¥˜ ì •ë³´**
{str(error)}

## ğŸ”§ **ë³µêµ¬ ì „ëµ**
LLM-First ì ‘ê·¼ë²•ìœ¼ë¡œ ê¸°ë³¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

```python
# ì•ˆì „í•œ ê¸°ë³¸ ë¶„ì„
result_df = df.copy()
print("ğŸ“Š ê¸°ë³¸ ë°ì´í„° ì •ë³´:")
print(f"  í˜•íƒœ: {result_df.shape}")
print(f"  ì»¬ëŸ¼: {list(result_df.columns)}")
print(f"  ë°ì´í„° íƒ€ì…: {result_df.dtypes.value_counts().to_dict()}")
```

## ğŸ’¡ **ì œì•ˆì‚¬í•­**
1. ë” êµ¬ì²´ì ì¸ ë¶„ì„ ìš”ì²­ìœ¼ë¡œ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”
2. ë°ì´í„° í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”
3. ë‹¨ê³„ë³„ë¡œ ë‚˜ëˆ„ì–´ì„œ ìš”ì²­í•´ë³´ì„¸ìš”

âœ… **PandasAnalystëŠ” LLMì˜ í•™ìŠµ ëŠ¥ë ¥ìœ¼ë¡œ ì§€ì†ì ìœ¼ë¡œ ê°œì„ ë©ë‹ˆë‹¤!**
"""
        
        return {
            "response": {"analysis_completed": False, "error_handled": True},
            "internal_messages": [f"ì˜¤ë¥˜ ë°œìƒ: {str(error)}", "ê¸°ë³¸ ë³µêµ¬ ì „ëµ ì ìš©"],
            "artifacts": {"error": str(error), "user_input": user_input},
            "ai_message": error_msg,
            "tool_calls": ["error_handler"],
            "pandas_code": "result_df = df.copy()  # ì•ˆì „í•œ ë³µì‚¬ë³¸",
            "execution_output": f"ì˜¤ë¥˜ë¡œ ì¸í•œ ê¸°ë³¸ ì²˜ë¦¬: {str(error)}"
        }
    
    def _assess_complexity(self, user_input: str) -> str:
        """ìš”ì²­ ë³µì¡ë„ í‰ê°€"""
        complexity_indicators = {
            "low": ["show", "display", "head", "tail", "info", "shape"],
            "medium": ["filter", "select", "group", "sort", "merge"],
            "high": ["pivot", "melt", "transform", "apply", "lambda", "agg"]
        }
        
        scores = {"low": 0, "medium": 0, "high": 0}
        
        for level, indicators in complexity_indicators.items():
            scores[level] = sum(1 for indicator in indicators if indicator in user_input.lower())
        
        return max(scores, key=scores.get)
    
    def _estimate_required_operations(self, user_input: str, intent: str) -> List[str]:
        """í•„ìš”í•œ ì—°ì‚° ì¶”ì •"""
        operations = []
        
        if "group" in user_input.lower() or intent == "aggregate_data":
            operations.append("groupby")
        if "merge" in user_input.lower() or "join" in user_input.lower():
            operations.append("merge/join")
        if "pivot" in user_input.lower():
            operations.append("pivot_table")
        if "sort" in user_input.lower():
            operations.append("sort_values")
        if "filter" in user_input.lower():
            operations.append("query/filter")
        
        return operations if operations else ["basic_analysis"]
    
    def _estimate_memory_needs(self, df: pd.DataFrame) -> str:
        """ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ ì¶”ì •"""
        if df is None:
            return "unknown"
        
        current_memory = df.memory_usage(deep=True).sum()
        
        if current_memory < 10**6:  # < 1MB
            return "low"
        elif current_memory < 10**8:  # < 100MB
            return "medium"
        else:
            return "high"

    def _format_result(self, result: Dict[str, Any], df: pd.DataFrame, output_path: str, user_input: str) -> str:
        """PandasAnalyst íŠ¹í™” ê²°ê³¼ í¬ë§·íŒ…"""
        
        # ì‹¤í–‰ ê²°ê³¼ ì •ë³´
        execution_info = result.get("artifacts", {}).get("execution_result", {})
        performance = execution_info.get("performance", {})
        
        return f"""# ğŸ¼ **PandasAnalyst LLM-First Complete!**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

## ğŸ§  **LLM ìƒì„± ì½”ë“œ**
```python
{result.get("pandas_code", "# ì½”ë“œ ìƒì„± ì‹¤íŒ¨")[:800]}...
```

## âš¡ **ì‹¤í–‰ ì„±ëŠ¥**
- **ì²˜ë¦¬ ì‹œê°„**: {performance.get('execution_time_seconds', 0):.3f}ì´ˆ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {performance.get('memory_usage_mb', 0):.2f} MB
- **ì²˜ë¦¬ëœ í–‰ìˆ˜**: {performance.get('processed_rows', 0):,} ê°œ

## ğŸ“Š **ë¶„ì„ ê²°ê³¼**
{result.get("ai_message", "ê²°ê³¼ ìƒì„± ì‹¤íŒ¨")}

## ğŸ¯ **í™œìš© ê°€ëŠ¥í•œ 8ê°œ í•µì‹¬ ê¸°ëŠ¥ë“¤**
1. **load_data_formats()** - ë‹¤ì–‘í•œ ë°ì´í„° í¬ë§· ë¡œë”©
2. **inspect_data()** - ë°ì´í„° êµ¬ì¡° ë° í’ˆì§ˆ ê²€ì‚¬
3. **select_data()** - ê³ ê¸‰ ë°ì´í„° ì„ íƒ ë° í•„í„°ë§
4. **manipulate_data()** - ë³µì¡í•œ ë°ì´í„° ë³€í™˜ ë° ì¡°ì‘
5. **aggregate_data()** - ê·¸ë£¹í•‘ ë° ì§‘ê³„ ì—°ì‚°
6. **merge_data()** - ë°ì´í„° ê²°í•© ë° ì¡°ì¸ ì‘ì—…
7. **clean_data()** - ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬
8. **perform_statistical_analysis()** - í†µê³„ ë¶„ì„ ë° ìš”ì•½

âœ… **100% LLM-First PandasAnalyst ë™ì  ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**
"""
    
    def _generate_guidance(self, user_instructions: str) -> str:
        """PandasAnalyst ê°€ì´ë“œ ì œê³µ"""
        return f"""# ğŸ¼ **PandasAnalyst LLM-First ê°€ì´ë“œ**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_instructions}

## ğŸ§  **LLM-First PandasAnalyst ì™„ì „ ê°€ì´ë“œ**

### 1. **í˜ì‹ ì  LLM-First ì ‘ê·¼ë²•**
PandasAnalystëŠ” ê¸°ì¡´ í…œí”Œë¦¿ì´ ì•„ë‹Œ **ìˆœìˆ˜ LLM ì§€ëŠ¥**ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤:

- **ë™ì  ì½”ë“œ ìƒì„±**: ë§¤ë²ˆ ìƒˆë¡œìš´ pandas ì½”ë“œë¥¼ ì‹¤ì‹œê°„ ìƒì„±
- **ì§€ëŠ¥ì  ìµœì í™”**: ë°ì´í„° íŠ¹ì„±ì— ë§ëŠ” ìë™ ìµœì í™”
- **ë¬´í•œ í™•ì¥ì„±**: ì–´ë–¤ ë³µì¡í•œ ìš”ì²­ë„ ì°½ì˜ì ìœ¼ë¡œ í•´ê²°
- **í•™ìŠµ ëŠ¥ë ¥**: ì‚¬ìš©ì í”¼ë“œë°±ì„ í†µí•œ ì§€ì†ì  ê°œì„ 

### 2. **8ê°œ í•µì‹¬ ê¸°ëŠ¥ ê³ ê¸‰ í™œìš©**

#### ğŸ“¥ **1. load_data_formats**
```text
CSV, JSON, Excel, Parquet íŒŒì¼ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ë¡œë“œí•´ì£¼ì„¸ìš”
ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì²­í‚¹í•´ì„œ ì½ì–´ì£¼ì„¸ìš”
```

#### ğŸ” **2. inspect_data**
```text
ë°ì´í„° í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•´ì£¼ì„¸ìš”
ê° ì»¬ëŸ¼ì˜ ê³ ìœ ì„±ê³¼ ë¶„í¬ íŠ¹ì„±ì„ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”
```

#### ğŸ¯ **3. select_data**
```text
ê³ ê° ì—°ë ¹ì´ 25-35ì„¸ì´ê³  êµ¬ë§¤íšŸìˆ˜ê°€ 3íšŒ ì´ìƒì¸ ë°ì´í„°ë§Œ ì„ íƒí•´ì£¼ì„¸ìš”
ë³µì¡í•œ ì¡°ê±´ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ í•„í„°ë§í•´ì£¼ì„¸ìš”
```

#### ğŸ”§ **4. manipulate_data**
```text
ë§¤ì¶œ ë°ì´í„°ì—ì„œ ê³„ì ˆì„± í”¼ì²˜ë¥¼ ìƒì„±í•˜ê³  ë¡œê·¸ ë³€í™˜ì„ ì ìš©í•´ì£¼ì„¸ìš”
ë²”ì£¼í˜• ë³€ìˆ˜ë¥¼ ì›-í•« ì¸ì½”ë”©í•˜ê³  ìƒí˜¸ì‘ìš© í”¼ì²˜ë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”
```

#### ğŸ“Š **5. aggregate_data**
```text
ì§€ì—­ë³„, ì›”ë³„ ë§¤ì¶œì„ ì§‘ê³„í•˜ê³  ì„±ì¥ë¥ ì„ ê³„ì‚°í•´ì£¼ì„¸ìš”
ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ í‰ê·  êµ¬ë§¤ì•¡ê³¼ í‘œì¤€í¸ì°¨ë¥¼ êµ¬í•´ì£¼ì„¸ìš”
```

#### ğŸ”— **6. merge_data**
```text
ê³ ê° ì •ë³´ì™€ ì£¼ë¬¸ ë°ì´í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ê²°í•©í•´ì£¼ì„¸ìš”
ë³µì¡í•œ ë‹¤ì¤‘ í‚¤ ì¡°ì¸ìœ¼ë¡œ ì—¬ëŸ¬ í…Œì´ë¸”ì„ í•©ì³ì£¼ì„¸ìš”
```

#### ğŸ§¹ **7. clean_data**
```text
ì´ìƒì¹˜ë¥¼ ê°ì§€í•˜ê³  ì²˜ë¦¬í•˜ë©° ê²°ì¸¡ê°’ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ëŒ€ì²´í•´ì£¼ì„¸ìš”
ë°ì´í„° íƒ€ì…ì„ ìµœì í™”í•˜ê³  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì—¬ì£¼ì„¸ìš”
```

#### ğŸ“ˆ **8. perform_statistical_analysis**
```text
ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  í†µê³„ì  ìœ ì˜ì„±ì„ ê²€ì •í•´ì£¼ì„¸ìš”
ë¶„í¬ì˜ ì •ê·œì„±ì„ í…ŒìŠ¤íŠ¸í•˜ê³  ë³€í™˜ ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”
```

### 3. **LLM-First ê³ ê¸‰ ê¸°ëŠ¥**

#### ğŸš€ **ì°½ì˜ì  ë¬¸ì œ í•´ê²°**
- ê¸°ì¡´ì— ì—†ë˜ ìƒˆë¡œìš´ ë¶„ì„ ë°©ë²• ì œì•ˆ
- ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì„ pandas ì½”ë“œë¡œ ë³€í™˜
- ìµœì ì˜ ì„±ëŠ¥ì„ ìœ„í•œ ì•Œê³ ë¦¬ì¦˜ ì„ íƒ

#### ğŸ§  **ì§€ëŠ¥ì  ì½”ë“œ ìµœì í™”**
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìë™ ìµœì í™”
- ì‹¤í–‰ ì‹œê°„ ë‹¨ì¶•ì„ ìœ„í•œ ë²¡í„°í™”
- ë°ì´í„° í¬ê¸°ì— ë”°ë¥¸ ì²­í‚¹ ì „ëµ

#### ğŸ”„ **ìê°€ í•™ìŠµ ë° ê°œì„ **
- ì‹¤í–‰ ê²°ê³¼ë¥¼ í†µí•œ ì½”ë“œ í’ˆì§ˆ í–¥ìƒ
- ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜í•œ ê°œì„ 
- ìƒˆë¡œìš´ pandas ê¸°ëŠ¥ ìë™ í•™ìŠµ

### 4. **ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ**

#### ğŸ’¼ **ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„**
```text
"ì›”ë³„ ë§¤ì¶œ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•˜ê³ , ê³„ì ˆì„±ì„ ì œê±°í•œ ì„±ì¥ë¥ ì„ ê³„ì‚°í•´ì„œ
ì§€ì—­ë³„ë¡œ ë¹„êµ ë¶„ì„í•´ì£¼ì„¸ìš”. ìƒìœ„ ì„±ê³¼ ì§€ì—­ì˜ íŠ¹ì„±ë„ íŒŒì•…í•´ì£¼ì„¸ìš”."
```

#### ğŸ”¬ **ë°ì´í„° ê³¼í•™**
```text
"ê³ ê° ì´íƒˆ ì˜ˆì¸¡ì„ ìœ„í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
RFM ë¶„ì„, í–‰ë™ íŒ¨í„´ ë³€ìˆ˜, ìƒí˜¸ì‘ìš© í”¼ì²˜ê¹Œì§€ í¬í•¨í•´ì„œ 
ë¨¸ì‹ ëŸ¬ë‹ ì¤€ë¹„ëœ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
```

#### ğŸ“Š **íƒìƒ‰ì  ë¶„ì„**
```text
"ì´ ë°ì´í„°ì…‹ì—ì„œ ìˆ¨ê²¨ì§„ íŒ¨í„´ì„ ì°¾ì•„ì£¼ì„¸ìš”.
ì´ìƒì¹˜, ìƒê´€ê´€ê³„, ë¶„í¬ íŠ¹ì„±ì„ ì¢…í•© ë¶„ì„í•˜ê³ 
ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•´ì£¼ì„¸ìš”."
```

## ğŸ’¡ **ë°ì´í„°ì™€ í•¨ê»˜ ìš”ì²­í•˜ë©´ ì¦‰ì‹œ LLM-First ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤!**

**ë°ì´í„° í˜•ì‹**:
- **CSV**: `name,age,city\\nJohn,25,Seoul\\nJane,30,Busan`
- **JSON**: `[{{"name": "John", "age": 25, "city": "Seoul"}}]`

### ğŸ¯ **PandasAnalystë§Œì˜ ì°¨ë³„í™”**
- **vs DataCleaning**: ì •ë¦¬ â†’ **ì§€ëŠ¥ì  ì¡°ì‘/ë³€í™˜**
- **vs DataWrangling**: ë‹¨ìˆœ ë³€í™˜ â†’ **ê³ ê¸‰ pandas API ë§ˆìŠ¤í„°ë¦¬**
- **vs EDATools**: íƒìƒ‰ â†’ **ì‹¤ì œ ë°ì´í„° ê°€ê³µ ë° ì²˜ë¦¬**

âœ… **LLM-First PandasAnalyst ì¤€ë¹„ ì™„ë£Œ!**
"""

    def get_function_mapping(self) -> Dict[str, str]:
        """PandasAnalyst 8ê°œ ê¸°ëŠ¥ ë§¤í•‘"""
        return {
            "load_data_formats": "pandas_code",  # ìƒì„±ëœ ë¡œë”© ì½”ë“œ
            "inspect_data": "ai_message",  # ê²€ì‚¬ ê²°ê³¼ ë¶„ì„
            "select_data": "execution_output",  # ì„ íƒ ê²°ê³¼
            "manipulate_data": "pandas_code",  # ë³€í™˜ ì½”ë“œ
            "aggregate_data": "execution_output",  # ì§‘ê³„ ê²°ê³¼
            "merge_data": "pandas_code",  # ì¡°ì¸ ì½”ë“œ
            "clean_data": "execution_output",  # ì •ì œ ê²°ê³¼
            "perform_statistical_analysis": "ai_message"  # í†µê³„ ë¶„ì„ ë¦¬í¬íŠ¸
        }


class PandasAnalystA2AExecutor(BaseA2AExecutor):
    """PandasAnalyst A2A Executor"""
    
    def __init__(self):
        wrapper_agent = PandasAnalystA2AWrapper()
        super().__init__(wrapper_agent)