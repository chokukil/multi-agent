#!/usr/bin/env python3
"""
DataCleaningA2AWrapper - A2A SDK 0.2.9 ë˜í•‘ DataCleaningAgent

ì›ë³¸ ai-data-science-team DataCleaningAgentë¥¼ A2A SDK 0.2.9 í”„ë¡œí† ì½œë¡œ 
ë˜í•‘í•˜ì—¬ 8ê°œ ê¸°ëŠ¥ì„ 100% ë³´ì¡´í•©ë‹ˆë‹¤.

8ê°œ í•µì‹¬ ê¸°ëŠ¥:
1. detect_missing_values() - ê²°ì¸¡ê°’ ê°ì§€
2. handle_missing_values() - ê²°ì¸¡ê°’ ì²˜ë¦¬  
3. detect_outliers() - ì´ìƒì¹˜ ê°ì§€
4. treat_outliers() - ì´ìƒì¹˜ ì²˜ë¦¬
5. validate_data_types() - ë°ì´í„° íƒ€ì… ê²€ì¦
6. detect_duplicates() - ì¤‘ë³µ ë°ì´í„° ê°ì§€
7. standardize_data() - ë°ì´í„° í‘œì¤€í™”
8. apply_validation_rules() - ê²€ì¦ ê·œì¹™ ì ìš©
"""

import logging
import pandas as pd
import numpy as np
from typing import Dict, Any
import os
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai_ds_team"))

from a2a_ds_servers.base.base_a2a_wrapper import BaseA2AWrapper, BaseA2AExecutor
from ai_data_science_team.agents import DataCleaningAgent

logger = logging.getLogger(__name__)


class DataCleaningA2AWrapper(BaseA2AWrapper):
    """
    DataCleaningAgentì˜ A2A SDK 0.2.9 ë˜í¼
    
    ì›ë³¸ ai-data-science-team DataCleaningAgentì˜ ëª¨ë“  ê¸°ëŠ¥ì„ 
    A2A í”„ë¡œí† ì½œë¡œ ë˜í•‘í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        super().__init__(
            agent_name="DataCleaningAgent",
            original_agent_class=DataCleaningAgent,
            port=8306
        )
    
    def _create_original_agent(self):
        """ì›ë³¸ DataCleaningAgent ìƒì„±"""
        return DataCleaningAgent(
            model=self.llm,
            n_samples=30,
            log=True,
            log_path="logs/data_cleaning/",
            file_name="data_cleaner.py",
            function_name="data_cleaner",
            overwrite=True,
            human_in_the_loop=False,
            bypass_recommended_steps=False,
            bypass_explain_code=False,
            checkpointer=None
        )
    
    async def _invoke_original_agent(self, df: pd.DataFrame, user_input: str, function_name: str = None) -> Dict[str, Any]:
        """ì›ë³¸ DataCleaningAgent invoke_agent í˜¸ì¶œ"""
        
        # íŠ¹ì • ê¸°ëŠ¥ ìš”ì²­ì´ ìˆëŠ” ê²½ìš° í•´ë‹¹ ê¸°ëŠ¥ì— ë§ëŠ” ì§€ì‹œì‚¬í•­ ìƒì„±
        if function_name:
            user_input = self._get_function_specific_instructions(function_name, user_input)
        
        # ì›ë³¸ ì—ì´ì „íŠ¸ í˜¸ì¶œ
        self.agent.invoke_agent(
            data_raw=df,
            user_instructions=user_input
        )
        
        # 8ê°œ ê¸°ëŠ¥ ê²°ê³¼ ìˆ˜ì§‘
        results = {
            "response": self.agent.response,
            "data_cleaned": self.agent.get_data_cleaned(),
            "data_raw": self.agent.get_data_raw(),
            "data_cleaner_function": self.agent.get_data_cleaner_function(),
            "recommended_cleaning_steps": self.agent.get_recommended_cleaning_steps(),
            "workflow_summary": self.agent.get_workflow_summary(),
            "log_summary": self.agent.get_log_summary(),
            "ai_message": None
        }
        
        # AI ë©”ì‹œì§€ ì¶”ì¶œ
        if results["response"] and results["response"].get("messages"):
            last_message = results["response"]["messages"][-1]
            if hasattr(last_message, 'content'):
                results["ai_message"] = last_message.content
        
        return results
    
    def _get_function_specific_instructions(self, function_name: str, user_input: str) -> str:
        """8ê°œ ê¸°ëŠ¥ë³„ íŠ¹í™”ëœ ì§€ì‹œì‚¬í•­ ìƒì„±"""
        
        function_instructions = {
            "detect_missing_values": """
Focus on detecting and reporting missing values in the dataset:
- Identify columns with missing values
- Calculate missing value percentages
- Report patterns in missing data
- Recommend actions for columns with high missing rates (>40%)

Original user request: {}
""",
            "handle_missing_values": """
Focus on handling missing values using appropriate imputation strategies:
- Impute numeric columns with mean values
- Impute categorical columns with mode values
- Remove columns with >40% missing values
- Document imputation strategies used

Original user request: {}
""",
            "detect_outliers": """
Focus on detecting outliers in the dataset:
- Use IQR method to identify outliers (3x interquartile range)
- Report outliers in numeric columns
- Provide statistics on outlier counts per column
- Suggest outlier treatment strategies

Original user request: {}
""",
            "treat_outliers": """
Focus on treating outliers in the dataset:
- Remove extreme outliers (3x interquartile range)
- Apply winsorization if appropriate
- Document outlier treatment methods
- Preserve data integrity during treatment

Original user request: {}
""",
            "validate_data_types": """
Focus on validating and correcting data types:
- Check current data types for each column
- Convert columns to appropriate data types
- Handle type conversion errors gracefully
- Report data type changes made

Original user request: {}
""",
            "detect_duplicates": """
Focus on detecting duplicate records:
- Identify exact duplicate rows
- Report duplicate counts and percentages
- Show examples of duplicate records
- Recommend deduplication strategy

Original user request: {}
""",
            "standardize_data": """
Focus on standardizing data formats and values:
- Standardize text case and formats
- Normalize numeric ranges if needed
- Ensure consistent data representations
- Apply data standardization rules

Original user request: {}
""",
            "apply_validation_rules": """
Focus on applying comprehensive data validation rules:
- Apply business logic validation
- Check data constraints and ranges
- Validate relationships between columns
- Report validation failures and corrections

Original user request: {}
"""
        }
        
        return function_instructions.get(function_name, user_input).format(user_input)
    
    def _format_result(self, result: Dict[str, Any], df: pd.DataFrame, output_path: str, user_input: str) -> str:
        """DataCleaningAgent íŠ¹í™” ê²°ê³¼ í¬ë§·íŒ…"""
        
        # ê¸°ë³¸ ì •ë³´
        data_preview = df.head().to_string()
        
        # ì •ë¦¬ëœ ë°ì´í„° ì •ë³´
        cleaned_data_info = ""
        if result.get("data_cleaned") is not None:
            cleaned_df = result["data_cleaned"]
            cleaned_data_info = f"""

## ğŸ§¹ **ì •ë¦¬ëœ ë°ì´í„° ì •ë³´**  
- **ì •ë¦¬ í›„ í¬ê¸°**: {cleaned_df.shape[0]:,} í–‰ Ã— {cleaned_df.shape[1]:,} ì—´
- **ì œê±°ëœ í–‰**: {df.shape[0] - cleaned_df.shape[0]:,} ê°œ
- **ë³€ê²½ëœ ì»¬ëŸ¼**: {abs(df.shape[1] - cleaned_df.shape[1]):,} ê°œ
"""
        
        # ìƒì„±ëœ í•¨ìˆ˜ ì •ë³´
        function_info = ""
        if result.get("data_cleaner_function"):
            function_info = f"""

## ğŸ’» **ìƒì„±ëœ ë°ì´í„° ì •ë¦¬ í•¨ìˆ˜**
```python
{result["data_cleaner_function"]}
```
"""
        
        # ì¶”ì²œ ë‹¨ê³„ ì •ë³´
        recommended_steps_info = ""
        if result.get("recommended_cleaning_steps"):
            recommended_steps_info = f"""

## ğŸ“‹ **ì¶”ì²œ ì •ë¦¬ ë‹¨ê³„**
{result["recommended_cleaning_steps"]}
"""
        
        # ì›Œí¬í”Œë¡œìš° ìš”ì•½
        workflow_info = ""
        if result.get("workflow_summary"):
            workflow_info = f"""

## ğŸ”„ **ì›Œí¬í”Œë¡œìš° ìš”ì•½**
{result["workflow_summary"]}
"""
        
        # ë¡œê·¸ ìš”ì•½
        log_info = ""
        if result.get("log_summary"):
            log_info = f"""

## ğŸ“„ **ë¡œê·¸ ìš”ì•½**
{result["log_summary"]}
"""
        
        return f"""# ğŸ§¹ **DataCleaningAgent Complete!**

## ğŸ“Š **ì›ë³¸ ë°ì´í„° ì •ë³´**
- **íŒŒì¼ ìœ„ì¹˜**: `{output_path}`
- **ë°ì´í„° í¬ê¸°**: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´
- **ì»¬ëŸ¼**: {', '.join(df.columns.tolist())}
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB

{cleaned_data_info}

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

{recommended_steps_info}

{workflow_info}

{function_info}

{log_info}

## ğŸ“ˆ **ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{data_preview}
```

## ğŸ”— **í™œìš© ê°€ëŠ¥í•œ 8ê°œ í•µì‹¬ ê¸°ëŠ¥ë“¤**
1. **detect_missing_values()** - ê²°ì¸¡ê°’ ê°ì§€ ë° ë¶„ì„
2. **handle_missing_values()** - ê²°ì¸¡ê°’ ì²˜ë¦¬ ë° ëŒ€ì²´
3. **detect_outliers()** - ì´ìƒì¹˜ ê°ì§€ ë° ì‹ë³„
4. **treat_outliers()** - ì´ìƒì¹˜ ì²˜ë¦¬ ë° ì œê±°
5. **validate_data_types()** - ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
6. **detect_duplicates()** - ì¤‘ë³µ ë°ì´í„° ê°ì§€
7. **standardize_data()** - ë°ì´í„° í‘œì¤€í™” ë° ì •ê·œí™”
8. **apply_validation_rules()** - ê²€ì¦ ê·œì¹™ ì ìš©

âœ… **ì›ë³¸ ai-data-science-team DataCleaningAgent 100% ê¸°ëŠ¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**
"""
    
    def _generate_guidance(self, user_instructions: str) -> str:
        """DataCleaningAgent ê°€ì´ë“œ ì œê³µ"""
        return f"""# ğŸ§¹ **DataCleaningAgent ê°€ì´ë“œ**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_instructions}

## ğŸ¯ **DataCleaningAgent ì™„ì „ ê°€ì´ë“œ**

### 1. **ê¸°ë³¸ ë°ì´í„° ì •ë¦¬ ê³¼ì •**
DataCleaningAgentëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ë³¸ ì •ë¦¬ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:

1. **ê²°ì¸¡ê°’ ì²˜ë¦¬**: 40% ì´ìƒ ê²°ì¸¡ì¸ ì»¬ëŸ¼ ì œê±°
2. **ê²°ì¸¡ê°’ ëŒ€ì²´**: ìˆ«ìí˜•(í‰ê· ), ë²”ì£¼í˜•(ìµœë¹ˆê°’)
3. **ë°ì´í„° íƒ€ì… ë³€í™˜**: ì ì ˆí•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜
4. **ì¤‘ë³µ ì œê±°**: ì¤‘ë³µëœ í–‰ ì œê±°
5. **ì´ìƒì¹˜ ì²˜ë¦¬**: 3Ã—IQR ë²”ìœ„ ë°– ê·¹ë‹¨ê°’ ì œê±°

### 2. **8ê°œ í•µì‹¬ ê¸°ëŠ¥ ê°œë³„ í™œìš©**

#### ğŸ” **1. detect_missing_values**
```text
ê²°ì¸¡ê°’ íŒ¨í„´ì„ ìì„¸íˆ ë¶„ì„í•´ì£¼ì„¸ìš”
```

#### ğŸ”§ **2. handle_missing_values**  
```text
ê²°ì¸¡ê°’ì„ ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”
```

#### ğŸ“Š **3. detect_outliers**
```text
ë°ì´í„°ì˜ ì´ìƒì¹˜ë¥¼ ê°ì§€í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”  
```

#### âš¡ **4. treat_outliers**
```text
ì´ìƒì¹˜ë¥¼ ì ì ˆíˆ ì²˜ë¦¬í•´ì£¼ì„¸ìš”
```

#### âœ… **5. validate_data_types**
```text
ë°ì´í„° íƒ€ì…ì„ ê²€ì¦í•˜ê³  ìˆ˜ì •í•´ì£¼ì„¸ìš”
```

#### ğŸ”„ **6. detect_duplicates**
```text
ì¤‘ë³µëœ ë°ì´í„°ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”
```

#### ğŸ“ **7. standardize_data**
```text
ë°ì´í„°ë¥¼ í‘œì¤€í™”í•´ì£¼ì„¸ìš”
```

#### ğŸ›¡ï¸ **8. apply_validation_rules**
```text
ë°ì´í„° ê²€ì¦ ê·œì¹™ì„ ì ìš©í•´ì£¼ì„¸ìš”
```

### 3. **ì›ë³¸ DataCleaningAgent íŠ¹ì§•**
- **LangGraph ê¸°ë°˜**: ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°
- **ìë™ ì½”ë“œ ìƒì„±**: Python í•¨ìˆ˜ ìë™ ìƒì„±
- **ì˜¤ë¥˜ ë³µêµ¬**: ìë™ ì¬ì‹œë„ ë° ìˆ˜ì •
- **ë¡œê¹… ì§€ì›**: ìƒì„¸í•œ ì²˜ë¦¬ ê³¼ì • ê¸°ë¡
- **ì‚¬ìš©ì ê²€í† **: Human-in-the-loop ì§€ì›

## ğŸ’¡ **ë°ì´í„°ë¥¼ í¬í•¨í•´ì„œ ë‹¤ì‹œ ìš”ì²­í•˜ë©´ ì‹¤ì œ DataCleaningAgent ì‘ì—…ì„ ìˆ˜í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!**

**ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ**:
- **CSV**: `name,age,salary\\nJohn,25,50000\\nJane,,60000\\nBob,30,`
- **JSON**: `[{{"name": "John", "age": 25, "salary": 50000}}]`

### ğŸ”— **ì¶”ê°€ ë¦¬ì†ŒìŠ¤**
- DataCleaningAgent ë¬¸ì„œ: ai-data-science-team íŒ¨í‚¤ì§€
- pandas ë°ì´í„° ì •ë¦¬: https://pandas.pydata.org/docs/
- scikit-learn ì „ì²˜ë¦¬: https://scikit-learn.org/stable/modules/preprocessing.html

âœ… **DataCleaningAgent ì¤€ë¹„ ì™„ë£Œ!**
"""

    def get_function_mapping(self) -> Dict[str, str]:
        """DataCleaningAgent 8ê°œ ê¸°ëŠ¥ ë§¤í•‘"""
        return {
            "detect_missing_values": "get_data_raw",  # ì›ë³¸ ë°ì´í„°ì—ì„œ ê²°ì¸¡ê°’ ë¶„ì„
            "handle_missing_values": "get_data_cleaned",  # ì •ë¦¬ëœ ë°ì´í„° (ê²°ì¸¡ê°’ ì²˜ë¦¬ë¨) 
            "detect_outliers": "get_data_raw",  # ì›ë³¸ ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ ë¶„ì„
            "treat_outliers": "get_data_cleaned",  # ì •ë¦¬ëœ ë°ì´í„° (ì´ìƒì¹˜ ì²˜ë¦¬ë¨)
            "validate_data_types": "get_data_cleaned",  # ì •ë¦¬ëœ ë°ì´í„° (íƒ€ì… ë³€í™˜ë¨)
            "detect_duplicates": "get_data_raw",  # ì›ë³¸ ë°ì´í„°ì—ì„œ ì¤‘ë³µ ë¶„ì„
            "standardize_data": "get_data_cleaned",  # ì •ë¦¬ëœ ë°ì´í„° (í‘œì¤€í™”ë¨)
            "apply_validation_rules": "get_data_cleaner_function"  # ìƒì„±ëœ ê²€ì¦ í•¨ìˆ˜
        }

    # ğŸ”¥ ì›ë³¸ DataCleaningAgent 8ê°œ ë©”ì„œë“œë“¤ êµ¬í˜„
    def get_data_cleaned(self):
        """ì›ë³¸ DataCleaningAgent.get_data_cleaned() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_data_cleaned()
        return None
    
    def get_data_raw(self):
        """ì›ë³¸ DataCleaningAgent.get_data_raw() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_data_raw()
        return None
    
    def get_data_cleaner_function(self, markdown=False):
        """ì›ë³¸ DataCleaningAgent.get_data_cleaner_function() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_data_cleaner_function(markdown=markdown)
        return None
    
    def get_recommended_cleaning_steps(self, markdown=False):
        """ì›ë³¸ DataCleaningAgent.get_recommended_cleaning_steps() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_recommended_cleaning_steps(markdown=markdown)
        return None
    
    def get_workflow_summary(self, markdown=False):
        """ì›ë³¸ DataCleaningAgent.get_workflow_summary() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_workflow_summary(markdown=markdown)
        return None
    
    def get_log_summary(self, markdown=False):
        """ì›ë³¸ DataCleaningAgent.get_log_summary() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_log_summary(markdown=markdown)
        return None
    
    def get_state_keys(self):
        """ì›ë³¸ DataCleaningAgent.get_state_keys() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_state_keys()
        return None
    
    def get_state_properties(self):
        """ì›ë³¸ DataCleaningAgent.get_state_properties() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_state_properties()
        return None


class DataCleaningA2AExecutor(BaseA2AExecutor):
    """DataCleaningAgent A2A Executor"""
    
    def __init__(self):
        wrapper_agent = DataCleaningA2AWrapper()
        super().__init__(wrapper_agent)