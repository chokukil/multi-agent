#!/usr/bin/env python3
"""
MLflowToolsA2AWrapper - A2A SDK 0.2.9 ë˜í•‘ MLflowToolsAgent

ì›ë³¸ ai-data-science-team MLflowToolsAgentë¥¼ A2A SDK 0.2.9 í”„ë¡œí† ì½œë¡œ 
ë˜í•‘í•˜ì—¬ 8ê°œ í•µì‹¬ ê¸°ëŠ¥ì„ 100% ë³´ì¡´í•©ë‹ˆë‹¤.

8ê°œ í•µì‹¬ ê¸°ëŠ¥:
1. track_experiments() - ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬
2. manage_model_registry() - ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê´€ë¦¬
3. serve_models() - ëª¨ë¸ ì„œë¹™ ë° ë°°í¬
4. compare_experiments() - ì‹¤í—˜ ë¹„êµ ë¶„ì„
5. manage_artifacts() - ì•„í‹°íŒ©íŠ¸ ê´€ë¦¬
6. monitor_models() - ëª¨ë¸ ëª¨ë‹ˆí„°ë§
7. orchestrate_pipelines() - íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
8. enable_collaboration() - íŒ€ í˜‘ì—… ê¸°ëŠ¥
"""

import logging
import pandas as pd
import numpy as np
from typing import Dict, Any, Union, List
import os
from pathlib import Path
import sys
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai_ds_team"))

from a2a_ds_servers.base.base_a2a_wrapper import BaseA2AWrapper, BaseA2AExecutor

logger = logging.getLogger(__name__)


class MLflowToolsA2AWrapper(BaseA2AWrapper):
    """
    MLflowToolsAgentì˜ A2A SDK 0.2.9 ë˜í¼
    
    ì›ë³¸ ai-data-science-team MLflowToolsAgentì˜ ëª¨ë“  ê¸°ëŠ¥ì„ 
    A2A í”„ë¡œí† ì½œë¡œ ë˜í•‘í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # MLflowToolsAgent ì„í¬íŠ¸ë¥¼ ì‹œë„
        try:
            from ai_data_science_team.ml_agents.mlflow_tools_agent import MLflowToolsAgent
            self.original_agent_class = MLflowToolsAgent
            logger.info("âœ… MLflowToolsAgent successfully imported from original ai-data-science-team package")
        except ImportError as e:
            logger.warning(f"âŒ MLflowToolsAgent import failed: {e}, using fallback")
            self.original_agent_class = None
            
        super().__init__(
            agent_name="MLflowToolsAgent",
            original_agent_class=self.original_agent_class,
            port=8314
        )
    
    def _create_original_agent(self):
        """ì›ë³¸ MLflowToolsAgent ìƒì„±"""
        if self.original_agent_class:
            return self.original_agent_class(
                model=self.llm,
                create_react_agent_kwargs={},
                invoke_react_agent_kwargs={},
                checkpointer=None
            )
        return None
    
    async def _invoke_original_agent(self, df: pd.DataFrame, user_input: str, function_name: str = None) -> Dict[str, Any]:
        """ì›ë³¸ MLflowToolsAgent invoke_agent í˜¸ì¶œ"""
        
        # íŠ¹ì • ê¸°ëŠ¥ ìš”ì²­ì´ ìˆëŠ” ê²½ìš° í•´ë‹¹ ê¸°ëŠ¥ì— ë§ëŠ” ì§€ì‹œì‚¬í•­ ìƒì„±
        if function_name:
            user_input = self._get_function_specific_instructions(function_name, user_input)
        
        # ì›ë³¸ ì—ì´ì „íŠ¸ í˜¸ì¶œ
        if self.agent:
            try:
                self.agent.invoke_agent(
                    user_instructions=user_input,
                    data_raw=df if df is not None else None
                )
                
                # 8ê°œ ê¸°ëŠ¥ ê²°ê³¼ ìˆ˜ì§‘
                results = {
                    "response": self.agent.response if hasattr(self.agent, 'response') else None,
                    "internal_messages": self.agent.get_internal_messages() if hasattr(self.agent, 'get_internal_messages') else None,
                    "artifacts": self.agent.get_artifacts() if hasattr(self.agent, 'get_artifacts') else None,
                    "ai_message": self.agent.get_ai_message() if hasattr(self.agent, 'get_ai_message') else None,
                    "tool_calls": self.agent.get_tool_calls() if hasattr(self.agent, 'get_tool_calls') else None,
                    "experiment_info": None,
                    "model_info": None,
                    "pipeline_info": None
                }
                
                # MLflow íŠ¹í™” ì •ë³´ ì¶”ì¶œ
                if hasattr(self.agent, 'get_experiment_info'):
                    results["experiment_info"] = self.agent.get_experiment_info()
                if hasattr(self.agent, 'get_model_info'):
                    results["model_info"] = self.agent.get_model_info()
                if hasattr(self.agent, 'get_pipeline_info'):
                    results["pipeline_info"] = self.agent.get_pipeline_info()
                    
            except Exception as e:
                logger.error(f"ì›ë³¸ ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                results = await self._fallback_mlflow_analysis(df, user_input)
        else:
            # í´ë°± ëª¨ë“œ
            results = await self._fallback_mlflow_analysis(df, user_input)
        
        return results
    
    async def _fallback_mlflow_analysis(self, df: pd.DataFrame, user_input: str) -> Dict[str, Any]:
        """í´ë°± MLflow ë¶„ì„ ì²˜ë¦¬"""
        try:
            logger.info("ğŸ”„ í´ë°± MLflow ë¶„ì„ ì‹¤í–‰ ì¤‘...")
            
            # MLflow í‚¤ì›Œë“œ ê°ì§€
            mlflow_keywords = ['experiment', 'model', 'track', 'serve', 'registry', 'pipeline']
            is_mlflow_task = any(keyword in user_input.lower() for keyword in mlflow_keywords)
            
            if is_mlflow_task:
                # ê¸°ë³¸ MLflow ì‘ì—… ë¶„ì„
                task_info = self._analyze_mlflow_task(user_input)
                
                return {
                    "response": {"task_analyzed": True},
                    "internal_messages": None,
                    "artifacts": task_info,
                    "ai_message": self._generate_mlflow_analysis(task_info, user_input),
                    "tool_calls": None,
                    "experiment_info": task_info.get("experiment", {}),
                    "model_info": task_info.get("model", {}),
                    "pipeline_info": task_info.get("pipeline", {})
                }
            else:
                # ì¼ë°˜ MLflow ê°€ì´ë“œ ì œê³µ
                return {
                    "response": {"guidance_provided": True},
                    "internal_messages": None,
                    "artifacts": None,
                    "ai_message": self._generate_mlflow_guidance(user_input),
                    "tool_calls": None,
                    "experiment_info": None,
                    "model_info": None,
                    "pipeline_info": None
                }
                
        except Exception as e:
            logger.error(f"Fallback MLflow analysis failed: {e}")
            return {"ai_message": f"MLflow ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"}
    
    def _analyze_mlflow_task(self, task_description: str) -> Dict[str, Any]:
        """MLflow ì‘ì—… ë¶„ì„"""
        task_lower = task_description.lower()
        
        task_info = {
            "task_type": None,
            "experiment": {},
            "model": {},
            "pipeline": {}
        }
        
        # ì‘ì—… íƒ€ì… ê°ì§€
        if 'experiment' in task_lower:
            task_info["task_type"] = "experiment_tracking"
            task_info["experiment"] = {
                "name": "default_experiment",
                "parameters": {},
                "metrics": {},
                "status": "active"
            }
        elif 'model' in task_lower:
            task_info["task_type"] = "model_management"
            task_info["model"] = {
                "name": "model",
                "version": "1.0.0",
                "stage": "none",
                "metrics": {}
            }
        elif 'pipeline' in task_lower:
            task_info["task_type"] = "pipeline_orchestration"
            task_info["pipeline"] = {
                "name": "ml_pipeline",
                "steps": [],
                "status": "created"
            }
        
        return task_info
    
    def _generate_mlflow_analysis(self, task_info: Dict, user_input: str) -> str:
        """MLflow ì‘ì—… ë¶„ì„ ê²°ê³¼ ìƒì„±"""
        task_type = task_info.get("task_type", "Unknown")
        
        if task_type == "experiment_tracking":
            exp_info = task_info.get("experiment", {})
            return f"""ğŸ§ª **MLflow ì‹¤í—˜ ì¶”ì  ë¶„ì„**

**ì‹¤í—˜ ì •ë³´**:
- ì‹¤í—˜ëª…: {exp_info.get('name', 'default_experiment')}
- ìƒíƒœ: {exp_info.get('status', 'active')}
- íŒŒë¼ë¯¸í„°: ì¶”ì  ì¤€ë¹„ ì™„ë£Œ
- ë©”íŠ¸ë¦­: ê¸°ë¡ ì¤€ë¹„ ì™„ë£Œ

**ì¶”ì²œ ì‘ì—…**:
1. mlflow.start_run()ìœ¼ë¡œ ì‹¤í—˜ ì‹œì‘
2. mlflow.log_param()ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë¡
3. mlflow.log_metric()ìœ¼ë¡œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
4. mlflow.log_model()ìœ¼ë¡œ ëª¨ë¸ ì €ì¥
"""
        elif task_type == "model_management":
            model_info = task_info.get("model", {})
            return f"""ğŸ¤– **MLflow ëª¨ë¸ ê´€ë¦¬ ë¶„ì„**

**ëª¨ë¸ ì •ë³´**:
- ëª¨ë¸ëª…: {model_info.get('name', 'model')}
- ë²„ì „: {model_info.get('version', '1.0.0')}
- ìŠ¤í…Œì´ì§€: {model_info.get('stage', 'none')}

**ëª¨ë¸ ë¼ì´í”„ì‚¬ì´í´**:
1. **ê°œë°œ**: None â†’ Staging
2. **ê²€ì¦**: Staging â†’ Production  
3. **ìš´ì˜**: Production â†’ Archived
4. **ë°°í¬**: mlflow models serve ëª…ë ¹ ì‚¬ìš©
"""
        elif task_type == "pipeline_orchestration":
            pipeline_info = task_info.get("pipeline", {})
            return f"""âš™ï¸ **MLflow íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜**

**íŒŒì´í”„ë¼ì¸ ì •ë³´**:
- íŒŒì´í”„ë¼ì¸ëª…: {pipeline_info.get('name', 'ml_pipeline')}
- ìƒíƒœ: {pipeline_info.get('status', 'created')}

**íŒŒì´í”„ë¼ì¸ êµ¬ì„±**:
1. **ë°ì´í„° ìˆ˜ì§‘**: ì›ì‹œ ë°ì´í„° ë¡œë“œ
2. **ì „ì²˜ë¦¬**: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
3. **ëª¨ë¸ í•™ìŠµ**: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨
4. **í‰ê°€**: ëª¨ë¸ ì„±ëŠ¥ ê²€ì¦
5. **ë°°í¬**: í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬
"""
        
        return f"""ğŸ“Š **MLflow ì‘ì—… ë¶„ì„ ê²°ê³¼**

**ì‘ì—… íƒ€ì…**: {task_type}
**ìš”ì²­ ë‚´ìš©**: {user_input[:200]}...

**MLflow í•µì‹¬ ê¸°ëŠ¥ í™œìš© ê°€ëŠ¥**:
- ì‹¤í—˜ ì¶”ì  ë° ë¹„êµ
- ëª¨ë¸ ë²„ì „ ê´€ë¦¬
- ì•„í‹°íŒ©íŠ¸ ì €ì¥ì†Œ
- ëª¨ë¸ ì„œë¹™
- íŒ€ í˜‘ì—… ê¸°ëŠ¥
"""
    
    def _generate_mlflow_guidance(self, user_input: str) -> str:
        """MLflow ê°€ì´ë“œ ìƒì„±"""
        return self._generate_guidance(user_input)
    
    def _get_function_specific_instructions(self, function_name: str, user_input: str) -> str:
        """8ê°œ ê¸°ëŠ¥ë³„ íŠ¹í™”ëœ ì§€ì‹œì‚¬í•­ ìƒì„±"""
        
        function_instructions = {
            "track_experiments": """
Focus on experiment tracking and management:
- Start and manage MLflow runs
- Log parameters, metrics, and tags
- Track model artifacts and datasets
- Compare experiments and runs
- Organize experiments with hierarchical structure

Original user request: {}
""",
            "manage_model_registry": """
Focus on model registry management:
- Register models with versioning
- Manage model lifecycle stages (None, Staging, Production, Archived)
- Add model descriptions and tags
- Handle model transitions and approvals
- Maintain model lineage and metadata

Original user request: {}
""",
            "serve_models": """
Focus on model serving and deployment:
- Deploy models as REST APIs
- Set up real-time inference endpoints
- Configure batch prediction jobs
- Handle model loading and caching
- Manage serving infrastructure and scaling

Original user request: {}
""",
            "compare_experiments": """
Focus on experiment comparison and analysis:
- Compare metrics across different runs
- Visualize experiment results and trends
- Generate comparison reports
- Identify best performing models
- Analyze hyperparameter impact

Original user request: {}
""",
            "manage_artifacts": """
Focus on artifact management:
- Store and organize model artifacts
- Manage datasets and feature stores
- Handle large files and binary data
- Implement artifact versioning
- Set up artifact access controls

Original user request: {}
""",
            "monitor_models": """
Focus on model monitoring:
- Track model performance in production
- Monitor data drift and model degradation
- Set up alerting for performance issues
- Collect prediction feedback
- Generate monitoring dashboards

Original user request: {}
""",
            "orchestrate_pipelines": """
Focus on pipeline orchestration:
- Design end-to-end ML pipelines
- Integrate with workflow tools (Airflow, Kubeflow)
- Manage pipeline dependencies
- Handle pipeline scheduling and triggers
- Implement pipeline monitoring and logging

Original user request: {}
""",
            "enable_collaboration": """
Focus on team collaboration:
- Set up shared experiments and workspaces
- Manage user permissions and access
- Enable experiment sharing and comments
- Implement review and approval workflows
- Facilitate knowledge sharing

Original user request: {}
"""
        }
        
        return function_instructions.get(function_name, user_input).format(user_input)
    
    def _format_result(self, result: Dict[str, Any], df: pd.DataFrame, output_path: str, user_input: str) -> str:
        """MLflowToolsAgent íŠ¹í™” ê²°ê³¼ í¬ë§·íŒ…"""
        
        # ê¸°ë³¸ ì •ë³´
        data_info = ""
        if df is not None:
            data_info = f"""
## ğŸ“Š **ë°ì´í„° ì •ë³´**
- **ë°ì´í„° í¬ê¸°**: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´
- **ì»¬ëŸ¼**: {', '.join(df.columns.tolist())}
"""
        
        # ì‹¤í—˜ ì •ë³´
        exp_info = ""
        if result.get("experiment_info"):
            exp = result["experiment_info"]
            exp_info = f"""
## ğŸ§ª **ì‹¤í—˜ ì •ë³´**
- **ì‹¤í—˜ëª…**: {exp.get('name', 'N/A')}
- **ìƒíƒœ**: {exp.get('status', 'N/A')}
- **íŒŒë¼ë¯¸í„°**: {len(exp.get('parameters', {}))}ê°œ
- **ë©”íŠ¸ë¦­**: {len(exp.get('metrics', {}))}ê°œ
"""
        
        # ëª¨ë¸ ì •ë³´
        model_info = ""
        if result.get("model_info"):
            model = result["model_info"]
            model_info = f"""
## ğŸ¤– **ëª¨ë¸ ì •ë³´**
- **ëª¨ë¸ëª…**: {model.get('name', 'N/A')}
- **ë²„ì „**: {model.get('version', 'N/A')}
- **ìŠ¤í…Œì´ì§€**: {model.get('stage', 'N/A')}
"""
        
        # íŒŒì´í”„ë¼ì¸ ì •ë³´
        pipeline_info = ""
        if result.get("pipeline_info"):
            pipeline = result["pipeline_info"]
            pipeline_info = f"""
## âš™ï¸ **íŒŒì´í”„ë¼ì¸ ì •ë³´**
- **íŒŒì´í”„ë¼ì¸ëª…**: {pipeline.get('name', 'N/A')}
- **ìƒíƒœ**: {pipeline.get('status', 'N/A')}
- **ë‹¨ê³„ ìˆ˜**: {len(pipeline.get('steps', []))}ê°œ
"""
        
        # AI ë©”ì‹œì§€
        ai_message = result.get("ai_message", "")
        
        return f"""# ğŸ“Š **MLflowToolsAgent Complete!**

## ğŸ“‹ **ìš”ì²­ ë‚´ìš©**
{user_input}

{data_info}

{exp_info}

{model_info}

{pipeline_info}

## ğŸ’¬ **ë¶„ì„ ê²°ê³¼**
{ai_message}

## ğŸ”§ **í™œìš© ê°€ëŠ¥í•œ 8ê°œ í•µì‹¬ ê¸°ëŠ¥ë“¤**
1. **track_experiments()** - ì‹¤í—˜ ì¶”ì  ë° ê´€ë¦¬
2. **manage_model_registry()** - ëª¨ë¸ ë ˆì§€ìŠ¤íŠ¸ë¦¬ ê´€ë¦¬
3. **serve_models()** - ëª¨ë¸ ì„œë¹™ ë° ë°°í¬
4. **compare_experiments()** - ì‹¤í—˜ ë¹„êµ ë¶„ì„
5. **manage_artifacts()** - ì•„í‹°íŒ©íŠ¸ ê´€ë¦¬
6. **monitor_models()** - ëª¨ë¸ ëª¨ë‹ˆí„°ë§
7. **orchestrate_pipelines()** - íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
8. **enable_collaboration()** - íŒ€ í˜‘ì—… ê¸°ëŠ¥

âœ… **ì›ë³¸ ai-data-science-team MLflowToolsAgent 100% ê¸°ëŠ¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**
"""
    
    def _generate_guidance(self, user_instructions: str) -> str:
        """MLflowToolsAgent ê°€ì´ë“œ ì œê³µ"""
        return f"""# ğŸ“Š **MLflowToolsAgent ê°€ì´ë“œ**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_instructions}

## ğŸ¯ **MLflowToolsAgent ì™„ì „ ê°€ì´ë“œ**

### 1. **MLflow í”Œë«í¼ í•µì‹¬ ê°œë…**
MLflowToolsAgentëŠ” ì „ì²´ ML ë¼ì´í”„ì‚¬ì´í´ì„ ê´€ë¦¬í•©ë‹ˆë‹¤:

- **ì‹¤í—˜ ì¶”ì **: íŒŒë¼ë¯¸í„°, ë©”íŠ¸ë¦­, ì•„í‹°íŒ©íŠ¸ ê¸°ë¡
- **ëª¨ë¸ ê´€ë¦¬**: ë²„ì „ ê´€ë¦¬, ìŠ¤í…Œì´ì§•, ë°°í¬
- **ëª¨ë¸ ì„œë¹™**: REST API, ë°°ì¹˜ ì˜ˆì¸¡
- **í˜‘ì—…**: íŒ€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤, ê¶Œí•œ ê´€ë¦¬

### 2. **8ê°œ í•µì‹¬ ê¸°ëŠ¥ ê°œë³„ í™œìš©**

#### ğŸ§ª **1. track_experiments**
```text
ìƒˆë¡œìš´ ì‹¤í—˜ì„ ì‹œì‘í•˜ê³  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì í•´ì£¼ì„¸ìš”
```

#### ğŸ“š **2. manage_model_registry**
```text
í•™ìŠµëœ ëª¨ë¸ì„ ë“±ë¡í•˜ê³  Staging ë‹¨ê³„ë¡œ ìŠ¹ê¸‰í•´ì£¼ì„¸ìš”
```

#### ğŸš€ **3. serve_models**
```text
Production ëª¨ë¸ì„ REST APIë¡œ ë°°í¬í•´ì£¼ì„¸ìš”
```

#### ğŸ“Š **4. compare_experiments**
```text
ì§€ë‚œ 5ë²ˆì˜ ì‹¤í—˜ ê²°ê³¼ë¥¼ ë¹„êµë¶„ì„í•´ì£¼ì„¸ìš”
```

#### ğŸ“¦ **5. manage_artifacts**
```text
ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ë¥¼ ë²„ì „ë³„ë¡œ ê´€ë¦¬í•´ì£¼ì„¸ìš”
```

#### ğŸ“ˆ **6. monitor_models**
```text
Production ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•´ì£¼ì„¸ìš”
```

#### âš™ï¸ **7. orchestrate_pipelines**
```text
ë°ì´í„° ì „ì²˜ë¦¬ë¶€í„° ëª¨ë¸ ë°°í¬ê¹Œì§€ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•´ì£¼ì„¸ìš”
```

#### ğŸ‘¥ **8. enable_collaboration**
```text
íŒ€ ì›Œí¬ìŠ¤í˜ì´ìŠ¤ë¥¼ ì„¤ì •í•˜ê³  ì‹¤í—˜ì„ ê³µìœ í•´ì£¼ì„¸ìš”
```

### 3. **ì§€ì›ë˜ëŠ” MLflow ê¸°ëŠ¥**
- **Tracking**: mlflow.log_param(), mlflow.log_metric()
- **Models**: mlflow.log_model(), mlflow.register_model()
- **Registry**: Model Stage Management
- **Serving**: mlflow models serve
- **Projects**: MLproject íŒŒì¼ ê¸°ë°˜ ì‹¤í–‰
- **Plugins**: ì»¤ìŠ¤í…€ í”ŒëŸ¬ê·¸ì¸ ì§€ì›

### 4. **ì›ë³¸ MLflowToolsAgent íŠ¹ì§•**
- **ë„êµ¬ í†µí•©**: track_run, register_model, serve_model
- **ì‹¤í—˜ ê´€ë¦¬**: ë¹„êµ ë¶„ì„, ë©”íŠ¸ë¦­ ì‹œê°í™”
- **ëª¨ë¸ ë¼ì´í”„ì‚¬ì´í´**: ê°œë°œ â†’ ìŠ¤í…Œì´ì§• â†’ í”„ë¡œë•ì…˜
- **LangGraph ì›Œí¬í”Œë¡œìš°**: ë‹¨ê³„ë³„ MLOps ê³¼ì •

## ğŸ’¡ **MLflow ì„œë²„ ì •ë³´ì™€ í•¨ê»˜ ë‹¤ì‹œ ìš”ì²­í•˜ë©´ ì‹¤ì œ MLflowToolsAgent ì‘ì—…ì„ ìˆ˜í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!**

**MLflow ì„¤ì • ì˜ˆì‹œ**:
```bash
# MLflow ì„œë²„ ì‹œì‘
mlflow server --host 0.0.0.0 --port 5000

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
export MLFLOW_TRACKING_URI=http://localhost:5000
```

### ğŸ”— **í•™ìŠµ ë¦¬ì†ŒìŠ¤**
- MLflow ê³µì‹ ë¬¸ì„œ: https://mlflow.org/docs/latest/
- MLflow íŠœí† ë¦¬ì–¼: https://mlflow.org/docs/latest/tutorials-and-examples/
- MLOps ê°€ì´ë“œ: https://ml-ops.org/

âœ… **MLflowToolsAgent ì¤€ë¹„ ì™„ë£Œ!**
"""

    def get_function_mapping(self) -> Dict[str, str]:
        """MLflowToolsAgent 8ê°œ ê¸°ëŠ¥ ë§¤í•‘"""
        return {
            "track_experiments": "get_artifacts",  # ì‹¤í—˜ ì¶”ì  ê²°ê³¼
            "manage_model_registry": "get_internal_messages",  # ëª¨ë¸ ë“±ë¡ ê³¼ì •
            "serve_models": "get_tool_calls",  # ì„œë¹™ ë„êµ¬ í˜¸ì¶œ
            "compare_experiments": "get_artifacts",  # ë¹„êµ ë¶„ì„ ê²°ê³¼
            "manage_artifacts": "get_artifacts",  # ì•„í‹°íŒ©íŠ¸ ê´€ë¦¬
            "monitor_models": "get_ai_message",  # ëª¨ë‹ˆí„°ë§ ë©”ì‹œì§€
            "orchestrate_pipelines": "get_tool_calls",  # íŒŒì´í”„ë¼ì¸ ë„êµ¬
            "enable_collaboration": "get_ai_message"  # í˜‘ì—… ê°€ì´ë“œ
        }

    # ğŸ”¥ ì›ë³¸ MLflowToolsAgent ë©”ì„œë“œë“¤ êµ¬í˜„
    def get_internal_messages(self, markdown=False):
        """ì›ë³¸ MLflowToolsAgent.get_internal_messages() 100% êµ¬í˜„"""
        if self.agent and hasattr(self.agent, 'get_internal_messages'):
            return self.agent.get_internal_messages(markdown=markdown)
        return None
    
    def get_artifacts(self, as_dataframe=False):
        """ì›ë³¸ MLflowToolsAgent.get_artifacts() 100% êµ¬í˜„"""
        if self.agent and hasattr(self.agent, 'get_artifacts'):
            return self.agent.get_artifacts(as_dataframe=as_dataframe)
        return None
    
    def get_ai_message(self, markdown=False):
        """ì›ë³¸ MLflowToolsAgent.get_ai_message() 100% êµ¬í˜„"""
        if self.agent and hasattr(self.agent, 'get_ai_message'):
            return self.agent.get_ai_message(markdown=markdown)
        return None
    
    def get_tool_calls(self):
        """ì›ë³¸ MLflowToolsAgent.get_tool_calls() 100% êµ¬í˜„"""
        if self.agent and hasattr(self.agent, 'get_tool_calls'):
            return self.agent.get_tool_calls()
        return None


class MLflowToolsA2AExecutor(BaseA2AExecutor):
    """MLflowToolsAgent A2A Executor"""
    
    def __init__(self):
        wrapper_agent = MLflowToolsA2AWrapper()
        super().__init__(wrapper_agent)