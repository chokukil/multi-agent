import json
import logging
from typing import Dict, Tuple, Optional, Any
from dataclasses import dataclass
from enum import Enum

logger = logging.getLogger(__name__)

class ContextType(Enum):
    """ì»¨í…ìŠ¤íŠ¸ íƒ€ì… ë¶„ë¥˜"""
    DOMAIN_EXPERT = "domain_expert"  # ë„ë©”ì¸ ì „ë¬¸ê°€ ì—­í• 
    TECHNICAL_SPECIALIST = "technical_specialist"  # ê¸°ìˆ  ì „ë¬¸ê°€
    BUSINESS_ANALYST = "business_analyst"  # ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€
    GENERAL_ASSISTANT = "general_assistant"  # ì¼ë°˜ ì–´ì‹œìŠ¤í„´íŠ¸
    CUSTOM_ROLE = "custom_role"  # ì‚¬ìš©ì ì •ì˜ ì—­í• 

@dataclass
class PromptContext:
    """í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ êµ¬ì¡°"""
    context_type: ContextType
    role_description: str
    domain_knowledge: str
    task_requirements: str
    data_context: str
    original_prompt: str
    confidence_score: float

class IntelligentPromptHandler:
    """LLM ê¸°ë°˜ ì§€ëŠ¥í˜• í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´ ì‹œìŠ¤í…œ"""
    
    def __init__(self, llm_instance=None):
        """
        Args:
            llm_instance: LLM ì¸ìŠ¤í„´ìŠ¤ (ì—†ìœ¼ë©´ ê¸°ë³¸ LLM ì‚¬ìš©)
        """
        if llm_instance:
            self.llm = llm_instance
        else:
            from core.llm_factory import create_llm_instance
            self.llm = create_llm_instance()
        
        logger.info("IntelligentPromptHandler initialized with LLM-based context analysis")
    
    async def analyze_prompt_context(self, user_request: str, data_info: str = "") -> PromptContext:
        """
        LLMì„ ì‚¬ìš©í•˜ì—¬ í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„
        
        Args:
            user_request: ì‚¬ìš©ìì˜ ì›ë³¸ ìš”ì²­
            data_info: ë°ì´í„° ì •ë³´ (íŒŒì¼ëª…, í˜•íƒœ ë“±)
            
        Returns:
            PromptContext: ë¶„ì„ëœ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        """
        
        analysis_prompt = f"""
ë‹¹ì‹ ì€ AI í”„ë¡¬í”„íŠ¸ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì‚¬ìš©ì ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•íƒœë¡œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­:
```
{user_request}
```

ë°ì´í„° ì •ë³´: {data_info}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë¶„ì„ ê²°ê³¼ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:

{{
    "context_type": "domain_expert|technical_specialist|business_analyst|general_assistant|custom_role",
    "role_description": "ì‚¬ìš©ìê°€ ìš”êµ¬í•˜ëŠ” ì—­í• ì´ë‚˜ ê´€ì  (ì˜ˆ: '20ë…„ ê²½ë ¥ì˜ ë°˜ë„ì²´ ì´ì˜¨ì£¼ì… ê³µì • ì—”ì§€ë‹ˆì–´', 'ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸', 'ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€' ë“±)",
    "domain_knowledge": "í•´ë‹¹ ì—­í• ì— í•„ìš”í•œ ë„ë©”ì¸ ì§€ì‹ì´ë‚˜ ì „ë¬¸ì„± ìš”êµ¬ì‚¬í•­",
    "task_requirements": "êµ¬ì²´ì ìœ¼ë¡œ ìˆ˜í–‰í•´ì•¼ í•  ì‘ì—…ì´ë‚˜ ë¶„ì„ ìš”êµ¬ì‚¬í•­",
    "data_context": "ë°ì´í„°ì™€ ê´€ë ¨ëœ íŠ¹ë³„í•œ ìš”êµ¬ì‚¬í•­ì´ë‚˜ ì»¨í…ìŠ¤íŠ¸",
    "key_constraints": "ì¤‘ìš”í•œ ì œì•½ì‚¬í•­ì´ë‚˜ ê³ ë ¤ì‚¬í•­",
    "output_format": "ì›í•˜ëŠ” ê²°ê³¼ í˜•íƒœë‚˜ í¬ë§·",
    "confidence_score": 0.0-1.0 (ë¶„ì„ ì‹ ë¢°ë„)
}}

ë¶„ì„ ê¸°ì¤€:
1. ì—­í•  ì§€ì •ì´ ëª…í™•í•œê°€? (ì˜ˆ: "ë‹¹ì‹ ì€ ~ì…ë‹ˆë‹¤", "~ë¡œì„œ", "~ì˜ ê´€ì ì—ì„œ")
2. ë„ë©”ì¸ ì „ë¬¸ì„±ì´ ìš”êµ¬ë˜ëŠ”ê°€?
3. íŠ¹ì • ì—…ë¬´ë‚˜ ê¸°ìˆ  ì˜ì—­ì˜ ì§€ì‹ì´ í•„ìš”í•œê°€?
4. ë°ì´í„° ë¶„ì„ì˜ ëª©ì ì´ë‚˜ ë°©í–¥ì„±ì´ ëª…í™•í•œê°€?
5. ê²°ê³¼ë¬¼ì— ëŒ€í•œ íŠ¹ë³„í•œ ìš”êµ¬ì‚¬í•­ì´ ìˆëŠ”ê°€?

JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”:
"""

        try:
            # LLMì„ í†µí•œ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            response = await self._call_llm_async(analysis_prompt)
            
            # JSON íŒŒì‹±
            analysis_result = self._extract_json_from_response(response)
            
            # PromptContext ê°ì²´ ìƒì„±
            context = PromptContext(
                context_type=ContextType(analysis_result.get("context_type", "general_assistant")),
                role_description=analysis_result.get("role_description", ""),
                domain_knowledge=analysis_result.get("domain_knowledge", ""),
                task_requirements=analysis_result.get("task_requirements", user_request),
                data_context=analysis_result.get("data_context", data_info),
                original_prompt=user_request,
                confidence_score=float(analysis_result.get("confidence_score", 0.7))
            )
            
            logger.info(f"âœ… Prompt context analyzed - Type: {context.context_type.value}, Confidence: {context.confidence_score}")
            return context
            
        except Exception as e:
            logger.warning(f"âš ï¸ LLM context analysis failed: {e}, using fallback")
            return self._create_fallback_context(user_request, data_info)
    
    async def create_enhanced_prompt(self, context: PromptContext, agent_type: str = "data_analysis") -> str:
        """
        ë¶„ì„ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
        
        Args:
            context: ë¶„ì„ëœ í”„ë¡¬í”„íŠ¸ ì»¨í…ìŠ¤íŠ¸
            agent_type: ì—ì´ì „íŠ¸ íƒ€ì… (data_analysis, visualization, etc.)
            
        Returns:
            str: í–¥ìƒëœ í”„ë¡¬í”„íŠ¸
        """
        
        enhancement_prompt = f"""
ë‹¹ì‹ ì€ AI í”„ë¡¬í”„íŠ¸ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë¶„ì„ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ {agent_type} ì—ì´ì „íŠ¸ê°€ ìµœì ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ìš”ì²­: {context.original_prompt}

ë¶„ì„ëœ ì»¨í…ìŠ¤íŠ¸:
- ì—­í•  íƒ€ì…: {context.context_type.value}
- ì—­í•  ì„¤ëª…: {context.role_description}
- ë„ë©”ì¸ ì§€ì‹: {context.domain_knowledge}
- ì‘ì—… ìš”êµ¬ì‚¬í•­: {context.task_requirements}
- ë°ì´í„° ì»¨í…ìŠ¤íŠ¸: {context.data_context}

ì—ì´ì „íŠ¸ íƒ€ì…: {agent_type}

ë‹¤ìŒ ì›ì¹™ì— ë”°ë¼ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

1. **ì—­í•  ë³´ì¡´**: ì‚¬ìš©ìê°€ ì§€ì •í•œ ì—­í• ê³¼ ê´€ì ì„ ì •í™•íˆ ìœ ì§€
2. **ë„ë©”ì¸ ì „ë¬¸ì„±**: í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ ì§€ì‹ê³¼ ìš©ì–´ í™œìš©
3. **ì‘ì—… ëª…í™•ì„±**: êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì‘ì—… ì§€ì‹œ
4. **ë°ì´í„° ì»¨í…ìŠ¤íŠ¸**: ì£¼ì–´ì§„ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ëª©ì  ë°˜ì˜
5. **ê²°ê³¼ í’ˆì§ˆ**: ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ê²°ê³¼ë¬¼ ìƒì„±

ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸:
"""

        try:
            enhanced_prompt = await self._call_llm_async(enhancement_prompt)
            
            # í”„ë¡¬í”„íŠ¸ ê²€ì¦
            if self._validate_enhanced_prompt(enhanced_prompt, context):
                logger.info("âœ… Enhanced prompt generated successfully")
                return enhanced_prompt.strip()
            else:
                logger.warning("âš ï¸ Enhanced prompt validation failed, using template")
                return self._create_template_prompt(context, agent_type)
                
        except Exception as e:
            logger.warning(f"âš ï¸ Enhanced prompt generation failed: {e}, using template")
            return self._create_template_prompt(context, agent_type)
    
    async def _call_llm_async(self, prompt: str) -> str:
        """LLM ë¹„ë™ê¸° í˜¸ì¶œ"""
        try:
            # LLM íƒ€ì…ì— ë”°ë¥¸ í˜¸ì¶œ ë°©ì‹ ì²˜ë¦¬
            if hasattr(self.llm, 'ainvoke'):
                # LangChain ìŠ¤íƒ€ì¼
                response = await self.llm.ainvoke(prompt)
                return response.content if hasattr(response, 'content') else str(response)
            elif hasattr(self.llm, 'acall'):
                # ì¼ë°˜ì ì¸ ë¹„ë™ê¸° í˜¸ì¶œ
                return await self.llm.acall(prompt)
            else:
                # ë™ê¸° í˜¸ì¶œ (fallback)
                import asyncio
                return await asyncio.get_event_loop().run_in_executor(
                    None, self._call_llm_sync, prompt
                )
        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            raise
    
    def _call_llm_sync(self, prompt: str) -> str:
        """LLM ë™ê¸° í˜¸ì¶œ (fallback)"""
        if hasattr(self.llm, 'invoke'):
            response = self.llm.invoke(prompt)
            return response.content if hasattr(response, 'content') else str(response)
        elif hasattr(self.llm, 'call'):
            return self.llm.call(prompt)
        else:
            return self.llm(prompt)
    
    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """ì‘ë‹µì—ì„œ JSON ì¶”ì¶œ"""
        try:
            # JSON ë¸”ë¡ ì°¾ê¸°
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            
            if start_idx != -1 and end_idx != 0:
                json_str = response[start_idx:end_idx]
                return json.loads(json_str)
            else:
                raise ValueError("No JSON found in response")
                
        except (json.JSONDecodeError, ValueError) as e:
            logger.warning(f"JSON parsing failed: {e}")
            # ê°„ë‹¨í•œ íŒŒì‹± ì‹œë„
            return self._simple_parse_response(response)
    
    def _simple_parse_response(self, response: str) -> Dict[str, Any]:
        """ê°„ë‹¨í•œ ì‘ë‹µ íŒŒì‹± (JSON ì‹¤íŒ¨ ì‹œ fallback)"""
        result = {
            "context_type": "general_assistant",
            "role_description": "",
            "domain_knowledge": "",
            "task_requirements": "",
            "data_context": "",
            "key_constraints": "",
            "output_format": "",
            "confidence_score": 0.5
        }
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê°„ë‹¨ ë¶„ì„
        response_lower = response.lower()
        
        if any(word in response_lower for word in ['ì—”ì§€ë‹ˆì–´', 'ì „ë¬¸ê°€', 'ë¶„ì„ê°€', 'engineer', 'expert', 'analyst']):
            result["context_type"] = "domain_expert"
            result["confidence_score"] = 0.6
        
        if any(word in response_lower for word in ['ê¸°ìˆ ', 'ê³µì •', 'ì‹œìŠ¤í…œ', 'technical', 'process', 'system']):
            result["context_type"] = "technical_specialist"
            result["confidence_score"] = 0.7
        
        return result
    
    def _validate_enhanced_prompt(self, prompt: str, context: PromptContext) -> bool:
        """í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ ê²€ì¦"""
        if not prompt or len(prompt.strip()) < 50:
            return False
        
        # ì—­í•  ë³´ì¡´ í™•ì¸
        if context.role_description and context.role_description not in prompt:
            # ìœ ì‚¬í•œ ì—­í•  í‘œí˜„ì´ ìˆëŠ”ì§€ í™•ì¸
            role_keywords = context.role_description.split()[:3]  # ì²« 3ë‹¨ì–´
            if not any(keyword in prompt for keyword in role_keywords):
                return False
        
        return True
    
    def _create_template_prompt(self, context: PromptContext, agent_type: str) -> str:
        """í…œí”Œë¦¿ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„± (fallback)"""
        
        base_templates = {
            "data_analysis": """
{role_context}

í˜„ì¬ ì‘ì—…: {task_requirements}

ë°ì´í„° ì •ë³´: {data_context}

ìœ„ì˜ ì—­í• ê³¼ ì „ë¬¸ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ì— ëŒ€í•´ ì² ì €í•˜ê³  ì „ë¬¸ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
ë¶„ì„ ê²°ê³¼ëŠ” í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ ìˆ˜ì¤€ìœ¼ë¡œ ì œê³µí•˜ë©°, ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•´ì£¼ì„¸ìš”.
""",
            "visualization": """
{role_context}

ì‹œê°í™” ìš”ì²­: {task_requirements}

ë°ì´í„° ì •ë³´: {data_context}

í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ ê´€ì ì—ì„œ ë°ì´í„°ì˜ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” ì‹œê°í™”ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
ì°¨íŠ¸ì™€ ê·¸ë˜í”„ëŠ” ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ êµ¬ì„±í•˜ê³ , í•„ìš”í•œ ì„¤ëª…ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
""",
            "feature_engineering": """
{role_context}

í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ìš”ì²­: {task_requirements}

ë°ì´í„° ì •ë³´: {data_context}

ë„ë©”ì¸ ì „ë¬¸ ì§€ì‹ì„ í™œìš©í•˜ì—¬ ì˜ë¯¸ ìˆëŠ” í”¼ì²˜ë¥¼ ìƒì„±í•˜ê³ , ê° í”¼ì²˜ì˜ ë¹„ì¦ˆë‹ˆìŠ¤ì  ì˜ë¯¸ì™€ ì˜ˆì¸¡ë ¥ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.
""",
            "default": """
{role_context}

ìš”ì²­ì‚¬í•­: {task_requirements}

ë°ì´í„° ì •ë³´: {data_context}

ìœ„ì˜ ì „ë¬¸ì„±ê³¼ ì—­í• ì„ ë°”íƒ•ìœ¼ë¡œ ìš”ì²­ëœ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
"""
        }
        
        template = base_templates.get(agent_type, base_templates["default"])
        
        # ì—­í•  ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        role_context = ""
        if context.role_description:
            role_context = f"ë‹¹ì‹ ì€ {context.role_description}ì…ë‹ˆë‹¤."
            if context.domain_knowledge:
                role_context += f"\n\nì „ë¬¸ ì§€ì‹: {context.domain_knowledge}"
        
        return template.format(
            role_context=role_context,
            task_requirements=context.task_requirements,
            data_context=context.data_context
        ).strip()
    
    def _create_fallback_context(self, user_request: str, data_info: str) -> PromptContext:
        """fallback ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        return PromptContext(
            context_type=ContextType.GENERAL_ASSISTANT,
            role_description="",
            domain_knowledge="",
            task_requirements=user_request,
            data_context=data_info,
            original_prompt=user_request,
            confidence_score=0.3
        )

    async def process_request_with_context(self, user_request: str, data_info: str, agent_type: str) -> Tuple[str, PromptContext]:
        """
        ì „ì²´ í”„ë¡œì„¸ìŠ¤: ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ â†’ í”„ë¡¬í”„íŠ¸ í–¥ìƒ
        
        Returns:
            Tuple[enhanced_prompt, context]: í–¥ìƒëœ í”„ë¡¬í”„íŠ¸ì™€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        """
        # 1. ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
        context = await self.analyze_prompt_context(user_request, data_info)
        
        # 2. í”„ë¡¬í”„íŠ¸ í–¥ìƒ
        enhanced_prompt = await self.create_enhanced_prompt(context, agent_type)
        
        logger.info(f"ğŸ§  Intelligent prompt processing completed - Confidence: {context.confidence_score}")
        
        return enhanced_prompt, context 