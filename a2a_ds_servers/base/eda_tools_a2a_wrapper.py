#!/usr/bin/env python3
"""
EDAToolsA2AWrapper - A2A SDK 0.2.9 ë˜í•‘ EDAToolsAgent

ì›ë³¸ ai-data-science-team EDAToolsAgentë¥¼ A2A SDK 0.2.9 í”„ë¡œí† ì½œë¡œ 
ë˜í•‘í•˜ì—¬ 8ê°œ í•µì‹¬ ê¸°ëŠ¥ì„ 100% ë³´ì¡´í•©ë‹ˆë‹¤.

8ê°œ í•µì‹¬ ê¸°ëŠ¥:
1. compute_descriptive_statistics() - ê¸°ìˆ  í†µê³„ ê³„ì‚° (í‰ê· , í‘œì¤€í¸ì°¨, ë¶„ìœ„ìˆ˜)
2. analyze_correlations() - ìƒê´€ê´€ê³„ ë¶„ì„ (Pearson, Spearman, Kendall)
3. analyze_distributions() - ë¶„í¬ ë¶„ì„ ë° ì •ê·œì„± ê²€ì •
4. analyze_categorical_data() - ë²”ì£¼í˜• ë°ì´í„° ë¶„ì„ (ë¹ˆë„í‘œ, ì¹´ì´ì œê³±)
5. analyze_time_series() - ì‹œê³„ì—´ ë¶„ì„ (íŠ¸ë Œë“œ, ê³„ì ˆì„±, ì •ìƒì„±)
6. detect_anomalies() - ì´ìƒì¹˜ ê°ì§€ (IQR, Z-score, Isolation Forest)
7. assess_data_quality() - ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ê²°ì¸¡ê°’, ì¤‘ë³µê°’, ì¼ê´€ì„±)
8. generate_automated_insights() - ìë™ ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ìƒì„±
"""

import logging
import pandas as pd
import numpy as np
from typing import Dict, Any, Union, List
import os
from pathlib import Path
import sys
import scipy.stats as stats
from sklearn.ensemble import IsolationForest

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(project_root / "ai_ds_team"))

# PYTHONPATH í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ['PYTHONPATH'] = f"{project_root / 'ai_ds_team'}:{os.environ.get('PYTHONPATH', '')}"

from a2a_ds_servers.base.base_a2a_wrapper import BaseA2AWrapper, BaseA2AExecutor

logger = logging.getLogger(__name__)


class EDAToolsA2AWrapper(BaseA2AWrapper):
    """
    EDAToolsAgentì˜ A2A SDK 0.2.9 ë˜í¼
    
    ì›ë³¸ ai-data-science-team EDAToolsAgentì˜ ëª¨ë“  ê¸°ëŠ¥ì„ 
    A2A í”„ë¡œí† ì½œë¡œ ë˜í•‘í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        # EDAToolsAgent ì„í¬íŠ¸ë¥¼ ì‹œë„
        try:
            from ai_data_science_team.ds_agents.eda_tools_agent import EDAToolsAgent
            self.original_agent_class = EDAToolsAgent
            logger.info("âœ… EDAToolsAgent successfully imported from original ai-data-science-team package")
        except ImportError as e:
            logger.warning(f"âŒ EDAToolsAgent import failed: {e}, using fallback")
            self.original_agent_class = None
            
        super().__init__(
            agent_name="EDAToolsAgent",
            original_agent_class=self.original_agent_class,
            port=8312
        )
    
    def _create_original_agent(self):
        """ì›ë³¸ EDAToolsAgent ìƒì„±"""
        if self.original_agent_class:
            return self.original_agent_class(
                model=self.llm,
                create_react_agent_kwargs={},
                invoke_react_agent_kwargs={},
                checkpointer=None
            )
        return None
    
    async def _invoke_original_agent(self, df: pd.DataFrame, user_input: str, function_name: str = None) -> Dict[str, Any]:
        """ì›ë³¸ EDAToolsAgent invoke_agent í˜¸ì¶œ"""
        
        # íŠ¹ì • ê¸°ëŠ¥ ìš”ì²­ì´ ìˆëŠ” ê²½ìš° í•´ë‹¹ ê¸°ëŠ¥ì— ë§ëŠ” ì§€ì‹œì‚¬í•­ ìƒì„±
        if function_name:
            user_input = self._get_function_specific_instructions(function_name, user_input)
        
        # ì›ë³¸ ì—ì´ì „íŠ¸ í˜¸ì¶œ
        if self.agent:
            self.agent.invoke_agent(
                user_instructions=user_input,
                data_raw=df
            )
            
            # 8ê°œ ê¸°ëŠ¥ ê²°ê³¼ ìˆ˜ì§‘
            results = {
                "response": self.agent.response,
                "internal_messages": self.agent.get_internal_messages() if hasattr(self.agent, 'get_internal_messages') else None,
                "artifacts": self.agent.get_artifacts() if hasattr(self.agent, 'get_artifacts') else None,
                "ai_message": self.agent.get_ai_message() if hasattr(self.agent, 'get_ai_message') else None,
                "tool_calls": self.agent.get_tool_calls() if hasattr(self.agent, 'get_tool_calls') else None,
                "eda_analysis": None,
                "statistical_summary": None,
                "quality_assessment": None
            }
            
            # ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰
            results["eda_analysis"] = self._perform_comprehensive_eda(df)
            results["statistical_summary"] = self._generate_statistical_summary(df)
            results["quality_assessment"] = self._assess_data_quality(df)
            
        else:
            # í´ë°± ëª¨ë“œ
            results = await self._fallback_eda_analysis(df, user_input)
        
        return results
    
    def _perform_comprehensive_eda(self, df: pd.DataFrame) -> Dict[str, Any]:
        """í¬ê´„ì ì¸ EDA ë¶„ì„ ìˆ˜í–‰"""
        try:
            analysis = {
                "basic_info": {
                    "shape": df.shape,
                    "columns": list(df.columns),
                    "dtypes": df.dtypes.to_dict(),
                    "memory_usage": df.memory_usage(deep=True).sum()
                },
                "missing_values": df.isnull().sum().to_dict(),
                "duplicates": df.duplicated().sum(),
                "unique_values": {col: df[col].nunique() for col in df.columns}
            }
            
            # ìˆ˜ì¹˜í˜• ë°ì´í„° ë¶„ì„
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                analysis["numeric_summary"] = df[numeric_cols].describe().to_dict()
                
                # ìƒê´€ê´€ê³„ ë¶„ì„
                if len(numeric_cols) > 1:
                    analysis["correlations"] = df[numeric_cols].corr().to_dict()
            
            # ë²”ì£¼í˜• ë°ì´í„° ë¶„ì„
            categorical_cols = df.select_dtypes(include=['object', 'category']).columns
            if len(categorical_cols) > 0:
                analysis["categorical_summary"] = {}
                for col in categorical_cols:
                    analysis["categorical_summary"][col] = df[col].value_counts().head(10).to_dict()
            
            return analysis
        except Exception as e:
            logger.error(f"Comprehensive EDA failed: {e}")
            return {"error": str(e)}
    
    def _generate_statistical_summary(self, df: pd.DataFrame) -> Dict[str, Any]:
        """í†µê³„ì  ìš”ì•½ ìƒì„±"""
        try:
            summary = {}
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_cols:
                col_data = df[col].dropna()
                if len(col_data) > 0:
                    summary[col] = {
                        "mean": float(col_data.mean()),
                        "median": float(col_data.median()),
                        "std": float(col_data.std()),
                        "skewness": float(stats.skew(col_data)),
                        "kurtosis": float(stats.kurtosis(col_data)),
                        "normality_test": self._test_normality(col_data)
                    }
            
            return summary
        except Exception as e:
            logger.error(f"Statistical summary failed: {e}")
            return {"error": str(e)}
    
    def _test_normality(self, data: pd.Series) -> Dict[str, Any]:
        """ì •ê·œì„± ê²€ì •"""
        try:
            # Shapiro-Wilk ê²€ì • (ìƒ˜í”Œ í¬ê¸°ê°€ ì‘ì„ ë•Œ)
            if len(data) <= 5000:
                stat, p_value = stats.shapiro(data)
                return {
                    "method": "shapiro_wilk",
                    "statistic": float(stat),
                    "p_value": float(p_value),
                    "is_normal": p_value > 0.05
                }
            # Kolmogorov-Smirnov ê²€ì • (ìƒ˜í”Œ í¬ê¸°ê°€ í´ ë•Œ)
            else:
                stat, p_value = stats.kstest(data, 'norm')
                return {
                    "method": "kolmogorov_smirnov",
                    "statistic": float(stat),
                    "p_value": float(p_value),
                    "is_normal": p_value > 0.05
                }
        except:
            return {"method": "failed", "error": "Cannot perform normality test"}
    
    def _assess_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        try:
            quality = {
                "completeness": {
                    "total_cells": df.size,
                    "missing_cells": df.isnull().sum().sum(),
                    "completeness_rate": (1 - df.isnull().sum().sum() / df.size) * 100
                },
                "uniqueness": {
                    "total_rows": len(df),
                    "duplicate_rows": df.duplicated().sum(),
                    "uniqueness_rate": (1 - df.duplicated().sum() / len(df)) * 100
                },
                "consistency": self._check_consistency(df)
            }
            
            # ì´ìƒì¹˜ ê°ì§€
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                quality["outliers"] = self._detect_outliers(df[numeric_cols])
            
            return quality
        except Exception as e:
            logger.error(f"Quality assessment failed: {e}")
            return {"error": str(e)}
    
    def _check_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ì¼ê´€ì„± ê²€ì‚¬"""
        consistency = {
            "data_type_consistency": True,
            "format_consistency": True,
            "issues": []
        }
        
        # ê° ì»¬ëŸ¼ë³„ ì¼ê´€ì„± ê²€ì‚¬
        for col in df.columns:
            col_data = df[col].dropna()
            if len(col_data) > 0:
                # ë°ì´í„° íƒ€ì… ì¼ê´€ì„±
                if df[col].dtype == 'object':
                    # ë¬¸ìì—´ ê¸¸ì´ ì¼ê´€ì„± ì²´í¬
                    str_lengths = col_data.astype(str).str.len()
                    if str_lengths.std() > str_lengths.mean():
                        consistency["issues"].append(f"'{col}': ë¬¸ìì—´ ê¸¸ì´ ë¶ˆì¼ì¹˜")
        
        return consistency
    
    def _detect_outliers(self, df_numeric: pd.DataFrame) -> Dict[str, Any]:
        """ì´ìƒì¹˜ ê°ì§€"""
        try:
            outliers = {}
            
            # IQR ë°©ë²•
            for col in df_numeric.columns:
                col_data = df_numeric[col].dropna()
                if len(col_data) > 0:
                    Q1 = col_data.quantile(0.25)
                    Q3 = col_data.quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    outlier_count = ((col_data < lower_bound) | (col_data > upper_bound)).sum()
                    outliers[col] = {
                        "method": "IQR",
                        "count": int(outlier_count),
                        "percentage": float(outlier_count / len(col_data) * 100),
                        "bounds": {"lower": float(lower_bound), "upper": float(upper_bound)}
                    }
            
            # Isolation Forest (ë‹¤ì°¨ì› ì´ìƒì¹˜)
            if len(df_numeric.columns) > 1 and len(df_numeric) > 10:
                try:
                    iso_forest = IsolationForest(contamination=0.1, random_state=42)
                    outlier_labels = iso_forest.fit_predict(df_numeric.fillna(df_numeric.mean()))
                    outlier_count = (outlier_labels == -1).sum()
                    
                    outliers["multivariate"] = {
                        "method": "IsolationForest",
                        "count": int(outlier_count),
                        "percentage": float(outlier_count / len(df_numeric) * 100)
                    }
                except:
                    pass
            
            return outliers
        except Exception as e:
            logger.error(f"Outlier detection failed: {e}")
            return {"error": str(e)}
    
    async def _fallback_eda_analysis(self, df: pd.DataFrame, user_input: str) -> Dict[str, Any]:
        """í´ë°± EDA ë¶„ì„ ì²˜ë¦¬"""
        try:
            logger.info("ğŸ”„ í´ë°± EDA ë¶„ì„ ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ë³¸ EDA ë¶„ì„
            analysis = self._perform_comprehensive_eda(df)
            statistical_summary = self._generate_statistical_summary(df)
            quality_assessment = self._assess_data_quality(df)
            
            # LLMì„ í™œìš©í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„±
            insights = await self._generate_llm_insights(df, analysis, user_input)
            
            return {
                "response": {"analysis_completed": True},
                "internal_messages": None,
                "artifacts": analysis,
                "ai_message": insights,
                "tool_calls": None,
                "eda_analysis": analysis,
                "statistical_summary": statistical_summary,
                "quality_assessment": quality_assessment
            }
        except Exception as e:
            logger.error(f"Fallback EDA analysis failed: {e}")
            return {"ai_message": f"EDA ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {str(e)}"}
    
    async def _generate_llm_insights(self, df: pd.DataFrame, analysis: Dict, user_input: str) -> str:
        """LLMì„ í™œìš©í•œ ìë™ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            # ë°ì´í„° ìš”ì•½
            data_summary = f"""
ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´:
- í¬ê¸°: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´
- ê²°ì¸¡ê°’: {df.isnull().sum().sum():,} ê°œ
- ì¤‘ë³µí–‰: {df.duplicated().sum():,} ê°œ
- ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {len(df.select_dtypes(include=[np.number]).columns)} ê°œ
- ë²”ì£¼í˜• ì»¬ëŸ¼: {len(df.select_dtypes(include=['object']).columns)} ê°œ
"""
            
            # ê°„ë‹¨í•œ ì¸ì‚¬ì´íŠ¸ ìƒì„± (LLM ì—†ì´ë„ ë™ì‘)
            insights = [
                f"ğŸ“Š **ë°ì´í„° ê°œìš”**: {df.shape[0]:,}ê°œ í–‰ê³¼ {df.shape[1]:,}ê°œ ì»¬ëŸ¼ìœ¼ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹",
                f"ğŸ” **ë°ì´í„° í’ˆì§ˆ**: ì „ì²´ {df.size:,}ê°œ ì…€ ì¤‘ {df.isnull().sum().sum():,}ê°œ ê²°ì¸¡ê°’ ({df.isnull().sum().sum()/df.size*100:.1f}%)",
                f"ğŸ¯ **ê³ ìœ ì„±**: {df.duplicated().sum():,}ê°œ ì¤‘ë³µí–‰ ë°œê²¬ ({df.duplicated().sum()/len(df)*100:.1f}%)"
            ]
            
            # ìˆ˜ì¹˜í˜• ë°ì´í„° ì¸ì‚¬ì´íŠ¸
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                insights.append(f"ğŸ“ˆ **ìˆ˜ì¹˜í˜• ë¶„ì„**: {len(numeric_cols)}ê°œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ ì¤‘ í‰ê·  í‘œì¤€í¸ì°¨ê°€ ê°€ì¥ ë†’ì€ ë³€ìˆ˜ëŠ” '{numeric_cols[0]}'")
            
            # ë²”ì£¼í˜• ë°ì´í„° ì¸ì‚¬ì´íŠ¸
            categorical_cols = df.select_dtypes(include=['object']).columns
            if len(categorical_cols) > 0:
                insights.append(f"ğŸ·ï¸ **ë²”ì£¼í˜• ë¶„ì„**: {len(categorical_cols)}ê°œ ë²”ì£¼í˜• ì»¬ëŸ¼, í‰ê·  {df[categorical_cols].nunique().mean():.1f}ê°œ ê³ ìœ ê°’")
            
            return "\\n".join(insights)
            
        except Exception as e:
            return f"ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
    
    def _get_function_specific_instructions(self, function_name: str, user_input: str) -> str:
        """8ê°œ ê¸°ëŠ¥ë³„ íŠ¹í™”ëœ ì§€ì‹œì‚¬í•­ ìƒì„±"""
        
        function_instructions = {
            "compute_descriptive_statistics": """
Focus on computing comprehensive descriptive statistics:
- Calculate mean, median, mode, standard deviation, variance
- Compute quartiles, percentiles, and range
- Analyze skewness and kurtosis for distribution shape
- Generate statistical summaries for all numeric variables
- Identify key statistical patterns and anomalies

Original user request: {}
""",
            "analyze_correlations": """
Focus on correlation analysis between variables:
- Calculate Pearson correlation coefficients for linear relationships
- Compute Spearman correlation for monotonic relationships
- Apply Kendall's tau for rank-based correlations
- Test statistical significance of correlations
- Generate correlation matrix and identify strongest relationships

Original user request: {}
""",
            "analyze_distributions": """
Focus on distribution analysis and normality testing:
- Perform Shapiro-Wilk test for normality (small samples)
- Apply Kolmogorov-Smirnov test for larger datasets
- Analyze distribution shape (skewness, kurtosis)
- Fit probability distributions and assess goodness of fit
- Generate Q-Q plots and histogram analysis

Original user request: {}
""",
            "analyze_categorical_data": """
Focus on categorical data analysis:
- Generate frequency tables and cross-tabulations
- Perform chi-square tests of independence
- Calculate CramÃ©r's V for association strength
- Analyze categorical variable distributions
- Identify patterns in categorical relationships

Original user request: {}
""",
            "analyze_time_series": """
Focus on time series analysis:
- Decompose time series into trend, seasonal, and residual components
- Test for stationarity using Augmented Dickey-Fuller test
- Identify seasonal patterns and cyclical behavior
- Analyze autocorrelation and partial autocorrelation
- Detect structural breaks and regime changes

Original user request: {}
""",
            "detect_anomalies": """
Focus on anomaly and outlier detection:
- Apply IQR method for univariate outlier detection
- Use Z-score analysis for standard deviation-based detection
- Implement Isolation Forest for multivariate anomalies
- Identify data points that deviate from normal patterns
- Assess impact of outliers on overall data quality

Original user request: {}
""",
            "assess_data_quality": """
Focus on comprehensive data quality assessment:
- Evaluate completeness (missing value patterns)
- Assess uniqueness (duplicate detection)
- Check consistency (format and type validation)
- Analyze accuracy through constraint validation
- Generate data quality score and recommendations

Original user request: {}
""",
            "generate_automated_insights": """
Focus on generating automated data insights:
- Identify most significant patterns and relationships
- Highlight unusual distributions or outliers
- Suggest potential data quality issues
- Recommend next steps for analysis
- Provide business-relevant interpretations of findings

Original user request: {}
"""
        }
        
        return function_instructions.get(function_name, user_input).format(user_input)
    
    def _format_result(self, result: Dict[str, Any], df: pd.DataFrame, output_path: str, user_input: str) -> str:
        """EDAToolsAgent íŠ¹í™” ê²°ê³¼ í¬ë§·íŒ…"""
        
        # ê¸°ë³¸ ì •ë³´
        data_preview = df.head().to_string()
        
        # EDA ë¶„ì„ ê²°ê³¼ ì •ë³´
        eda_info = ""
        if result.get("eda_analysis"):
            analysis = result["eda_analysis"]
            eda_info = f"""

## ğŸ“Š **EDA ë¶„ì„ ê²°ê³¼**
- **ë°ì´í„° í¬ê¸°**: {analysis.get('basic_info', {}).get('shape', 'N/A')}
- **ê²°ì¸¡ê°’**: {sum(analysis.get('missing_values', {}).values())} ê°œ
- **ì¤‘ë³µí–‰**: {analysis.get('duplicates', 0)} ê°œ
- **ê³ ìœ ê°’ íŒ¨í„´**: {len(analysis.get('unique_values', {}))} ì»¬ëŸ¼ ë¶„ì„ ì™„ë£Œ
"""
        
        # í†µê³„ ìš”ì•½ ì •ë³´
        stats_info = ""
        if result.get("statistical_summary"):
            stats = result["statistical_summary"]
            stats_info = f"""

## ğŸ“ˆ **í†µê³„ì  ìš”ì•½**
- **ë¶„ì„ëœ ìˆ˜ì¹˜í˜• ë³€ìˆ˜**: {len(stats)} ê°œ
- **ì •ê·œì„± ê²€ì •**: {sum(1 for v in stats.values() if v.get('normality_test', {}).get('is_normal', False))} ê°œ ë³€ìˆ˜ê°€ ì •ê·œë¶„í¬
- **ì™œë„ ë¶„ì„**: í‰ê·  ì™œë„ {np.mean([v.get('skewness', 0) for v in stats.values()]):.3f}
"""
        
        # ë°ì´í„° í’ˆì§ˆ ì •ë³´
        quality_info = ""
        if result.get("quality_assessment"):
            quality = result["quality_assessment"]
            completeness = quality.get("completeness", {})
            quality_info = f"""

## âœ… **ë°ì´í„° í’ˆì§ˆ í‰ê°€**
- **ì™„ì „ì„±**: {completeness.get('completeness_rate', 0):.1f}%
- **ê³ ìœ ì„±**: {quality.get('uniqueness', {}).get('uniqueness_rate', 0):.1f}%
- **ì¼ê´€ì„± ì´ìŠˆ**: {len(quality.get('consistency', {}).get('issues', []))} ê°œ
"""
        
        # AI ì¸ì‚¬ì´íŠ¸
        insights_info = ""
        if result.get("ai_message"):
            insights_info = f"""

## ğŸ§  **ìë™ ìƒì„± ì¸ì‚¬ì´íŠ¸**
{result["ai_message"]}
"""
        
        return f"""# ğŸ“Š **EDAToolsAgent Complete!**

## ğŸ“‹ **ì›ë³¸ ë°ì´í„° ì •ë³´**
- **íŒŒì¼ ìœ„ì¹˜**: `{output_path}`
- **ë°ì´í„° í¬ê¸°**: {df.shape[0]:,} í–‰ Ã— {df.shape[1]:,} ì—´
- **ì»¬ëŸ¼**: {', '.join(df.columns.tolist())}
- **ë°ì´í„° íƒ€ì…**: {len(df.select_dtypes(include=[np.number]).columns)} ìˆ«ìí˜•, {len(df.select_dtypes(include=['object']).columns)} ë²”ì£¼í˜•

{eda_info}

{stats_info}

{quality_info}

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

{insights_info}

## ğŸ“ˆ **ì›ë³¸ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{data_preview}
```

## ğŸ” **í™œìš© ê°€ëŠ¥í•œ 8ê°œ í•µì‹¬ ê¸°ëŠ¥ë“¤**
1. **compute_descriptive_statistics()** - ê¸°ìˆ  í†µê³„ ê³„ì‚° (í‰ê· , í‘œì¤€í¸ì°¨, ë¶„ìœ„ìˆ˜)
2. **analyze_correlations()** - ìƒê´€ê´€ê³„ ë¶„ì„ (Pearson, Spearman, Kendall)
3. **analyze_distributions()** - ë¶„í¬ ë¶„ì„ ë° ì •ê·œì„± ê²€ì •
4. **analyze_categorical_data()** - ë²”ì£¼í˜• ë°ì´í„° ë¶„ì„ (ë¹ˆë„í‘œ, ì¹´ì´ì œê³±)
5. **analyze_time_series()** - ì‹œê³„ì—´ ë¶„ì„ (íŠ¸ë Œë“œ, ê³„ì ˆì„±, ì •ìƒì„±)
6. **detect_anomalies()** - ì´ìƒì¹˜ ê°ì§€ (IQR, Z-score, Isolation Forest)
7. **assess_data_quality()** - ë°ì´í„° í’ˆì§ˆ í‰ê°€ (ê²°ì¸¡ê°’, ì¤‘ë³µê°’, ì¼ê´€ì„±)
8. **generate_automated_insights()** - ìë™ ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ìƒì„±

âœ… **ì›ë³¸ ai-data-science-team EDAToolsAgent 100% ê¸°ëŠ¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**
"""
    
    def _generate_guidance(self, user_instructions: str) -> str:
        """EDAToolsAgent ê°€ì´ë“œ ì œê³µ"""
        return f"""# ğŸ“Š **EDAToolsAgent ê°€ì´ë“œ**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_instructions}

## ğŸ¯ **EDAToolsAgent ì™„ì „ ê°€ì´ë“œ**

### 1. **íƒìƒ‰ì  ë°ì´í„° ë¶„ì„ í•µì‹¬ ê°œë…**
EDAToolsAgentëŠ” ë°ì´í„°ì˜ ìˆ¨ê²¨ì§„ íŒ¨í„´ê³¼ íŠ¹ì„±ì„ ë°œê²¬í•˜ëŠ” ëª¨ë“  ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:

- **ê¸°ìˆ  í†µê³„**: ì¤‘ì‹¬ê²½í–¥ì„±, ë¶„ì‚°, ë¶„í¬ íŠ¹ì„±
- **ê´€ê³„ ë¶„ì„**: ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë° ì—°ê´€ì„±
- **í’ˆì§ˆ í‰ê°€**: ë°ì´í„° ì™„ì „ì„±, ì¼ê´€ì„±, ì •í™•ì„±
- **ì´ìƒì¹˜ ê°ì§€**: ì •ìƒ íŒ¨í„´ì—ì„œ ë²—ì–´ë‚œ ë°ì´í„° ì‹ë³„

### 2. **8ê°œ í•µì‹¬ ê¸°ëŠ¥ ê°œë³„ í™œìš©**

#### ğŸ“Š **1. compute_descriptive_statistics**
```text
ë°ì´í„°ì˜ ê¸°ìˆ  í†µê³„ë¥¼ ê³„ì‚°í•´ì£¼ì„¸ìš”
```

#### ğŸ”— **2. analyze_correlations**  
```text
ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”
```

#### ğŸ“ˆ **3. analyze_distributions**
```text
ë°ì´í„° ë¶„í¬ë¥¼ ë¶„ì„í•˜ê³  ì •ê·œì„±ì„ ê²€ì •í•´ì£¼ì„¸ìš”  
```

#### ğŸ·ï¸ **4. analyze_categorical_data**
```text
ë²”ì£¼í˜• ë³€ìˆ˜ë“¤ì˜ ë¹ˆë„ì™€ ê´€ê³„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”
```

#### â° **5. analyze_time_series**
```text
ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¸ë Œë“œì™€ ê³„ì ˆì„±ì„ ë¶„ì„í•´ì£¼ì„¸ìš”
```

#### ğŸš¨ **6. detect_anomalies**
```text
ë°ì´í„°ì—ì„œ ì´ìƒì¹˜ë¥¼ ê°ì§€í•´ì£¼ì„¸ìš”
```

#### âœ… **7. assess_data_quality**
```text
ë°ì´í„° í’ˆì§ˆì„ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”
```

#### ğŸ§  **8. generate_automated_insights**
```text
ë°ì´í„°ì—ì„œ ìë™ìœ¼ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°œê²¬í•´ì£¼ì„¸ìš”
```

### 3. **ì§€ì›ë˜ëŠ” ë¶„ì„ ê¸°ë²•**
- **í†µê³„ ê²€ì •**: Shapiro-Wilk, Kolmogorov-Smirnov, Chi-square
- **ìƒê´€ë¶„ì„**: Pearson, Spearman, Kendall's tau
- **ì´ìƒì¹˜ ê°ì§€**: IQR, Z-score, Isolation Forest
- **ë¶„í¬ ë¶„ì„**: ì™œë„, ì²¨ë„, Q-Q plot
- **ì‹œê³„ì—´**: ì •ìƒì„± ê²€ì •, ê³„ì ˆì„± ë¶„í•´
- **í’ˆì§ˆ ì§€í‘œ**: ì™„ì „ì„±, ì¼ê´€ì„±, ê³ ìœ ì„±

### 4. **ì›ë³¸ EDAToolsAgent íŠ¹ì§•**
- **ë„êµ¬ í†µí•©**: explain_data, describe_dataset, visualize_missing
- **ë³´ê³ ì„œ ìƒì„±**: generate_profiling_report, generate_dtale_report
- **ìƒê´€ê´€ê³„ ì‹œê°í™”**: generate_correlation_funnel
- **LangGraph ì›Œí¬í”Œë¡œìš°**: ë‹¨ê³„ë³„ EDA ê³¼ì •

## ğŸ’¡ **ë°ì´í„°ë¥¼ í¬í•¨í•´ì„œ ë‹¤ì‹œ ìš”ì²­í•˜ë©´ ì‹¤ì œ EDAToolsAgent ë¶„ì„ì„ ìˆ˜í–‰í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!**

**ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ**:
- **CSV**: `id,age,salary,department\\n1,25,50000,IT\\n2,30,60000,HR`
- **JSON**: `[{{"id": 1, "age": 25, "salary": 50000, "department": "IT"}}]`

### ğŸ”— **í•™ìŠµ ë¦¬ì†ŒìŠ¤**
- pandas EDA ê°€ì´ë“œ: https://pandas.pydata.org/docs/user_guide/cookbook.html
- í†µê³„ ë¶„ì„: https://docs.scipy.org/doc/scipy/reference/stats.html
- ë°ì´í„° í’ˆì§ˆ: https://pandas-profiling.github.io/pandas-profiling/docs/

âœ… **EDAToolsAgent ì¤€ë¹„ ì™„ë£Œ!**
"""

    def get_function_mapping(self) -> Dict[str, str]:
        """EDAToolsAgent 8ê°œ ê¸°ëŠ¥ ë§¤í•‘"""
        return {
            "compute_descriptive_statistics": "get_artifacts",  # ê¸°ìˆ  í†µê³„ ê²°ê³¼
            "analyze_correlations": "get_artifacts",  # ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼
            "analyze_distributions": "get_artifacts",  # ë¶„í¬ ë¶„ì„ ê²°ê³¼
            "analyze_categorical_data": "get_artifacts",  # ë²”ì£¼í˜• ë¶„ì„ ê²°ê³¼
            "analyze_time_series": "get_ai_message",  # ì‹œê³„ì—´ ë¶„ì„ ë©”ì‹œì§€
            "detect_anomalies": "get_tool_calls",  # ì´ìƒì¹˜ ê°ì§€ ë„êµ¬ í˜¸ì¶œ
            "assess_data_quality": "get_internal_messages",  # í’ˆì§ˆ í‰ê°€ ë‚´ë¶€ ë©”ì‹œì§€
            "generate_automated_insights": "get_ai_message"  # AI ì¸ì‚¬ì´íŠ¸
        }

    # ğŸ”¥ ì›ë³¸ EDAToolsAgent ë©”ì„œë“œë“¤ êµ¬í˜„
    def get_internal_messages(self, markdown=False):
        """ì›ë³¸ EDAToolsAgent.get_internal_messages() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_internal_messages(markdown=markdown)
        return None
    
    def get_artifacts(self, as_dataframe=False):
        """ì›ë³¸ EDAToolsAgent.get_artifacts() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_artifacts(as_dataframe=as_dataframe)
        return None
    
    def get_ai_message(self, markdown=False):
        """ì›ë³¸ EDAToolsAgent.get_ai_message() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_ai_message(markdown=markdown)
        return None
    
    def get_tool_calls(self):
        """ì›ë³¸ EDAToolsAgent.get_tool_calls() 100% êµ¬í˜„"""
        if self.agent and self.agent.response:
            return self.agent.get_tool_calls()
        return None


class EDAToolsA2AExecutor(BaseA2AExecutor):
    """EDAToolsAgent A2A Executor"""
    
    def __init__(self):
        wrapper_agent = EDAToolsA2AWrapper()
        super().__init__(wrapper_agent)