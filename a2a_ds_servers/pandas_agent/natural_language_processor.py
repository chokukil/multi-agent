"""
ğŸ§  Natural Language Processor

ìì—°ì–´ ì¿¼ë¦¬ ë¶„ì„ ë° ì²˜ë¦¬ ì‹œìŠ¤í…œ
ë°ì´í„° ë¶„ì„ ì˜ë„ íŒŒì•… ë° ì ì ˆí•œ ë¶„ì„ ë°©ë²• ë§¤í•‘

Author: CherryAI Team
License: MIT License
"""

import re
import logging
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
from dataclasses import dataclass
import pandas as pd

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QueryType(Enum):
    """ì¿¼ë¦¬ ìœ í˜• ë¶„ë¥˜"""
    SUMMARY = "summary"                    # ë°ì´í„° ìš”ì•½
    STATISTICS = "statistics"              # ê¸°ìˆ í†µê³„
    VISUALIZATION = "visualization"        # ì‹œê°í™”
    FILTERING = "filtering"                # ë°ì´í„° í•„í„°ë§
    AGGREGATION = "aggregation"            # ì§‘ê³„ ì—°ì‚°
    CORRELATION = "correlation"            # ìƒê´€ê´€ê³„ ë¶„ì„
    MISSING_DATA = "missing_data"          # ê²°ì¸¡ ë°ì´í„° ë¶„ì„
    COMPARISON = "comparison"              # ë¹„êµ ë¶„ì„
    TREND = "trend"                        # íŠ¸ë Œë“œ ë¶„ì„
    DISTRIBUTION = "distribution"          # ë¶„í¬ ë¶„ì„
    GROUPBY = "groupby"                    # ê·¸ë£¹ë³„ ë¶„ì„
    MERGE_JOIN = "merge_join"              # ë°ì´í„° ë³‘í•©
    TRANSFORMATION = "transformation"       # ë°ì´í„° ë³€í™˜
    GENERAL = "general"                    # ì¼ë°˜ ì§ˆë¬¸


@dataclass
class QueryIntent:
    """ì¿¼ë¦¬ ì˜ë„ ë¶„ì„ ê²°ê³¼"""
    query_type: QueryType
    confidence: float
    keywords: List[str]
    target_columns: List[str]
    operations: List[str]
    filters: Dict[str, Any]
    aggregations: List[str]
    visualization_type: Optional[str] = None


class NaturalLanguageProcessor:
    """ìì—°ì–´ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.query_patterns = self._initialize_patterns()
        self.column_aliases = {
            # ì¼ë°˜ì ì¸ ì»¬ëŸ¼ ë³„ì¹­
            "ë‚˜ì´": ["age", "ë‚˜ì´", "ì—°ë ¹"],
            "ì„±ë³„": ["gender", "sex", "ì„±ë³„"],
            "ê°€ê²©": ["price", "cost", "ê°€ê²©", "ë¹„ìš©", "ê¸ˆì•¡"],
            "ë‚ ì§œ": ["date", "time", "ë‚ ì§œ", "ì‹œê°„", "ì¼ì"],
            "ì´ë¦„": ["name", "ì´ë¦„", "ëª…ì¹­"],
            "ìˆ˜ëŸ‰": ["quantity", "count", "ìˆ˜ëŸ‰", "ê°œìˆ˜"],
            "ìƒíƒœ": ["status", "state", "ìƒíƒœ"],
            "ì¹´í…Œê³ ë¦¬": ["category", "type", "ì¹´í…Œê³ ë¦¬", "ìœ í˜•", "ë¶„ë¥˜"]
        }
    
    def _initialize_patterns(self) -> Dict[QueryType, List[Dict]]:
        """ì¿¼ë¦¬ íŒ¨í„´ ì´ˆê¸°í™”"""
        return {
            QueryType.SUMMARY: [
                {"pattern": r"ìš”ì•½|ê°œìš”|overview|summary|ì „ì²´|ì‚´í´", "weight": 1.0},
                {"pattern": r"ì–´ë–¤.*ë°ì´í„°|ë¬´ì—‡.*í¬í•¨|ë­.*ë“¤ì–´", "weight": 0.8},
                {"pattern": r"ë³´ì—¬ì¤˜|ì•Œë ¤ì¤˜|ì„¤ëª…", "weight": 0.6}
            ],
            QueryType.STATISTICS: [
                {"pattern": r"í†µê³„|ê¸°ìˆ í†µê³„|describe|í‰ê· |mean|ì¤‘ì•™ê°’|median", "weight": 1.0},
                {"pattern": r"ìµœëŒ€|ìµœì†Œ|max|min|í‘œì¤€í¸ì°¨|std|ë¶„ì‚°|variance", "weight": 0.9},
                {"pattern": r"ë¶„í¬|distribution|íˆìŠ¤í† ê·¸ë¨|histogram", "weight": 0.8}
            ],
            QueryType.VISUALIZATION: [
                {"pattern": r"ê·¸ë˜í”„|ì°¨íŠ¸|plot|ê·¸ë¦¼|ì‹œê°í™”|visualization", "weight": 1.0},
                {"pattern": r"ê·¸ë ¤|ê·¸ë¦°|í”Œë¡¯|ë§‰ëŒ€|ì„ |ì›|scatter|bar|line|pie", "weight": 0.9},
                {"pattern": r"ë³´ì—¬ì¤˜.*ê·¸ë˜í”„|ì°¨íŠ¸.*ìƒì„±", "weight": 0.8}
            ],
            QueryType.FILTERING: [
                {"pattern": r"í•„í„°|filter|ì¡°ê±´|condition|where", "weight": 1.0},
                {"pattern": r"~ì¸|~ê°€ ìˆëŠ”|~ë³´ë‹¤ í°|~ë³´ë‹¤ ì‘ì€|~ì™€ ê°™ì€", "weight": 0.9},
                {"pattern": r"í¬í•¨.*ë°ì´í„°|í•´ë‹¹.*í–‰|íŠ¹ì •.*ê°’", "weight": 0.7}
            ],
            QueryType.AGGREGATION: [
                {"pattern": r"í•©ê³„|sum|ì´í•©|ì „ì²´.*ë”í•œ|ì´.*ê°œìˆ˜", "weight": 1.0},
                {"pattern": r"í‰ê· |mean|average|ì¤‘ì•™ê°’|median", "weight": 0.9},
                {"pattern": r"ìµœëŒ€|ìµœì†Œ|max|min|ê°œìˆ˜|count", "weight": 0.8}
            ],
            QueryType.CORRELATION: [
                {"pattern": r"ìƒê´€ê´€ê³„|correlation|ê´€ê³„|ì—°ê´€|ì˜í–¥", "weight": 1.0},
                {"pattern": r"~ì™€.*ê´€ë ¨|~ì—.*ë”°ë¥¸|~ì™€.*ë¹„ë¡€", "weight": 0.9},
                {"pattern": r"ê´€ë ¨ì„±|ìƒê´€ì„±|ì—°ê´€ì„±", "weight": 0.8}
            ],
            QueryType.MISSING_DATA: [
                {"pattern": r"ê²°ì¸¡|missing|null|nan|ë¹ˆ.*ê°’|ì—†ëŠ”.*ë°ì´í„°", "weight": 1.0},
                {"pattern": r"ëˆ„ë½|ë¹„ì–´ìˆëŠ”|ê³µë°±|empty", "weight": 0.9}
            ],
            QueryType.COMPARISON: [
                {"pattern": r"ë¹„êµ|compare|ì°¨ì´|difference|ëŒ€ë¹„", "weight": 1.0},
                {"pattern": r"~ë³´ë‹¤|~ì™€.*ë‹¤ë¥¸|~ì—.*ë¹„í•´", "weight": 0.9},
                {"pattern": r"ë†’ì€|ë‚®ì€|ë§ì€|ì ì€", "weight": 0.7}
            ],
            QueryType.TREND: [
                {"pattern": r"íŠ¸ë Œë“œ|trend|ì¶”ì„¸|ë³€í™”|ì‹œê°„.*ë”°ë¥¸", "weight": 1.0},
                {"pattern": r"ì¦ê°€|ê°ì†Œ|ìƒìŠ¹|í•˜ë½|ë³€ë™", "weight": 0.9},
                {"pattern": r"ì‹œê³„ì—´|time.*series|ì›”ë³„|ë…„ë„ë³„", "weight": 0.8}
            ],
            QueryType.GROUPBY: [
                {"pattern": r"ê·¸ë£¹|group|ë³„ë¡œ|~ë§ˆë‹¤|~ë‹¹", "weight": 1.0},
                {"pattern": r"ì¹´í…Œê³ ë¦¬.*ë³„|ì§€ì—­.*ë³„|ì„±ë³„.*ë³„", "weight": 0.9},
                {"pattern": r"ë¶„ë¥˜.*í•´ì„œ|ë‚˜ëˆ„ì–´ì„œ", "weight": 0.8}
            ],
            QueryType.MERGE_JOIN: [
                {"pattern": r"í•©ì¹˜|merge|join|ë³‘í•©|ê²°í•©", "weight": 1.0},
                {"pattern": r"ì—°ê²°|í•©ì³ì„œ|í•¨ê»˜.*ë¶„ì„", "weight": 0.9}
            ]
        }
    
    async def analyze_query(self, query: str, available_columns: List[str] = None) -> QueryIntent:
        """ì¿¼ë¦¬ ì˜ë„ ë¶„ì„"""
        try:
            query_lower = query.lower().strip()
            
            # 1. ì¿¼ë¦¬ ìœ í˜• ë¶„ë¥˜
            query_type, confidence = self._classify_query_type(query_lower)
            
            # 2. í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(query_lower)
            
            # 3. ëŒ€ìƒ ì»¬ëŸ¼ ì¶”ì¶œ
            target_columns = self._extract_target_columns(query, available_columns or [])
            
            # 4. ì—°ì‚° ì¶”ì¶œ
            operations = self._extract_operations(query_lower)
            
            # 5. í•„í„° ì¡°ê±´ ì¶”ì¶œ
            filters = self._extract_filters(query_lower)
            
            # 6. ì§‘ê³„ ì—°ì‚° ì¶”ì¶œ
            aggregations = self._extract_aggregations(query_lower)
            
            # 7. ì‹œê°í™” ìœ í˜• ì¶”ì¶œ
            visualization_type = self._extract_visualization_type(query_lower) if query_type == QueryType.VISUALIZATION else None
            
            intent = QueryIntent(
                query_type=query_type,
                confidence=confidence,
                keywords=keywords,
                target_columns=target_columns,
                operations=operations,
                filters=filters,
                aggregations=aggregations,
                visualization_type=visualization_type
            )
            
            logger.info(f"ğŸ§  ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ: {query_type.value} (ì‹ ë¢°ë„: {confidence:.2f})")
            return intent
            
        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì˜ë„ ë°˜í™˜
            return QueryIntent(
                query_type=QueryType.GENERAL,
                confidence=0.5,
                keywords=[],
                target_columns=[],
                operations=[],
                filters={},
                aggregations=[]
            )
    
    def _classify_query_type(self, query: str) -> Tuple[QueryType, float]:
        """ì¿¼ë¦¬ ìœ í˜• ë¶„ë¥˜"""
        scores = {}
        
        for query_type, patterns in self.query_patterns.items():
            score = 0.0
            for pattern_info in patterns:
                pattern = pattern_info["pattern"]
                weight = pattern_info["weight"]
                
                matches = len(re.findall(pattern, query, re.IGNORECASE))
                score += matches * weight
            
            if score > 0:
                scores[query_type] = score
        
        if not scores:
            return QueryType.GENERAL, 0.5
        
        # ìµœê³  ì ìˆ˜ ì¿¼ë¦¬ ìœ í˜• ë°˜í™˜
        best_type = max(scores, key=scores.get)
        max_score = scores[best_type]
        
        # ì‹ ë¢°ë„ ê³„ì‚° (0.5 ~ 1.0 ë²”ìœ„ë¡œ ì •ê·œí™”)
        confidence = min(0.5 + (max_score * 0.1), 1.0)
        
        return best_type, confidence
    
    def _extract_keywords(self, query: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ - í•œêµ­ì–´ ì–´ë¯¸ ì²˜ë¦¬ ê°œì„ """
        # ë¶ˆìš©ì–´ ì œê±°
        stop_words = {
            "ì˜", "ë¥¼", "ì„", "ì´", "ê°€", "ì€", "ëŠ”", "ì—", "ë¡œ", "ìœ¼ë¡œ", "ì™€", "ê³¼", 
            "í•´ì¤˜", "ë³´ì—¬ì¤˜", "ì•Œë ¤ì¤˜", "ì£¼ì„¸ìš”", "í•´ì£¼ì„¸ìš”", "ì…ë‹ˆë‹¤", "ìŠµë‹ˆë‹¤", "ì—ì„œ", "ë¶€í„°", "ê¹Œì§€"
        }
        
        # 1. íŒ¨í„´ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        pattern_keywords = []
        for query_type, patterns in self.query_patterns.items():
            for pattern_info in patterns:
                pattern = pattern_info["pattern"]
                matches = re.findall(pattern, query, re.IGNORECASE)
                pattern_keywords.extend(matches)
        
        # 2. ë‹¨ì–´ ë¶„ë¦¬ ë° ì •ì œ
        words = re.findall(r'[ê°€-í£\w]+', query)
        
        # 3. í•œêµ­ì–´ ì–´ë¯¸ ì œê±°
        processed_words = []
        for word in words:
            if len(word) > 1 and word not in stop_words:
                # í•œêµ­ì–´ ì–´ë¯¸ ì œê±° (ìš”ì•½ì„ -> ìš”ì•½, ë¶„ì„í•´ -> ë¶„ì„)
                stem = re.sub(r'[ì„ë¥¼ì€ëŠ”ì´ê°€ì—ì„œì˜ì™€ê³¼ë„ë§Œê¹Œì§€ë¶€í„°ì•¼ë¼]$', '', word)
                stem = re.sub(r'[í•´í•˜ê²Œë ¤ê³ ë ¤ë©´í•˜ë©´í•œë‹¤ê³ ë„¤ìš”]$', '', stem)
                processed_words.append(stem if len(stem) > 1 else word)
        
        # 4. ëª¨ë“  í‚¤ì›Œë“œ í•©ì¹˜ê¸°
        all_keywords = pattern_keywords + processed_words
        
        # ì¤‘ë³µ ì œê±° ë° ë¹ˆ ë¬¸ìì—´ ì œê±°
        keywords = list(set(word for word in all_keywords if word and len(word) > 1))
        
        return keywords[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
    
    def _extract_target_columns(self, query: str, available_columns: List[str]) -> List[str]:
        """ëŒ€ìƒ ì»¬ëŸ¼ ì¶”ì¶œ"""
        target_columns = []
        query_lower = query.lower()
        
        # 1. ì§ì ‘ ì»¬ëŸ¼ëª… ë§¤ì¹­
        for col in available_columns:
            if col.lower() in query_lower:
                target_columns.append(col)
        
        # 2. ë³„ì¹­ì„ í†µí•œ ë§¤ì¹­
        for korean_name, aliases in self.column_aliases.items():
            for alias in aliases:
                if alias in query_lower:
                    # í•´ë‹¹ ë³„ì¹­ê³¼ ë§¤ì¹­ë˜ëŠ” ì‹¤ì œ ì»¬ëŸ¼ ì°¾ê¸°
                    matching_cols = [col for col in available_columns 
                                   if any(a.lower() in col.lower() for a in aliases)]
                    target_columns.extend(matching_cols)
        
        # ì¤‘ë³µ ì œê±°
        return list(set(target_columns))
    
    def _extract_operations(self, query: str) -> List[str]:
        """ì—°ì‚° ì¶”ì¶œ"""
        operations = []
        
        operation_patterns = {
            "count": r"ê°œìˆ˜|count|ìˆ˜|ê°¯ìˆ˜",
            "sum": r"í•©ê³„|sum|ì´í•©|ë”í•œ",
            "mean": r"í‰ê· |mean|average",
            "median": r"ì¤‘ì•™ê°’|median",
            "max": r"ìµœëŒ€|ìµœê³ |max|ê°€ì¥.*í°",
            "min": r"ìµœì†Œ|ìµœì €|min|ê°€ì¥.*ì‘ì€",
            "std": r"í‘œì¤€í¸ì°¨|std|standard",
            "var": r"ë¶„ì‚°|variance|var",
            "unique": r"ê³ ìœ |unique|ìœ ì¼|distinct",
            "sort": r"ì •ë ¬|sort|ìˆœì„œ"
        }
        
        for op, pattern in operation_patterns.items():
            if re.search(pattern, query, re.IGNORECASE):
                operations.append(op)
        
        return operations
    
    def _extract_filters(self, query: str) -> Dict[str, Any]:
        """í•„í„° ì¡°ê±´ ì¶”ì¶œ"""
        filters = {}
        
        # ìˆ«ì í•„í„° íŒ¨í„´
        number_patterns = [
            (r"(\w+).*ë³´ë‹¤.*í°.*?(\d+(?:\.\d+)?)", "gt"),
            (r"(\w+).*ë³´ë‹¤.*ì‘ì€.*?(\d+(?:\.\d+)?)", "lt"),
            (r"(\w+).*ì´ìƒ.*?(\d+(?:\.\d+)?)", "gte"),
            (r"(\w+).*ì´í•˜.*?(\d+(?:\.\d+)?)", "lte"),
            (r"(\w+).*ê°™ì€.*?(\d+(?:\.\d+)?)", "eq")
        ]
        
        for pattern, operator in number_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for column, value in matches:
                if column not in filters:
                    filters[column] = []
                filters[column].append({"operator": operator, "value": float(value)})
        
        # í…ìŠ¤íŠ¸ í•„í„° íŒ¨í„´
        text_patterns = [
            (r"(\w+).*í¬í•¨.*['\"](.+?)['\"]", "contains"),
            (r"(\w+).*ê°™ì€.*['\"](.+?)['\"]", "equals"),
            (r"(\w+).*ì‹œì‘.*['\"](.+?)['\"]", "startswith")
        ]
        
        for pattern, operator in text_patterns:
            matches = re.findall(pattern, query, re.IGNORECASE)
            for column, value in matches:
                if column not in filters:
                    filters[column] = []
                filters[column].append({"operator": operator, "value": value})
        
        return filters
    
    def _extract_aggregations(self, query: str) -> List[str]:
        """ì§‘ê³„ ì—°ì‚° ì¶”ì¶œ"""
        aggregations = []
        
        agg_patterns = {
            "sum": r"í•©ê³„|sum|ì´í•©",
            "mean": r"í‰ê· |mean|average",
            "count": r"ê°œìˆ˜|count|ê°¯ìˆ˜",
            "median": r"ì¤‘ì•™ê°’|median",
            "max": r"ìµœëŒ€|ìµœê³ |max",
            "min": r"ìµœì†Œ|ìµœì €|min",
            "std": r"í‘œì¤€í¸ì°¨|std",
            "var": r"ë¶„ì‚°|variance"
        }
        
        for agg, pattern in agg_patterns.items():
            if re.search(pattern, query, re.IGNORECASE):
                aggregations.append(agg)
        
        return aggregations
    
    def _extract_visualization_type(self, query: str) -> Optional[str]:
        """ì‹œê°í™” ìœ í˜• ì¶”ì¶œ"""
        viz_patterns = {
            "bar": r"ë§‰ëŒ€.*ì°¨íŠ¸|bar.*chart|ë§‰ëŒ€.*ê·¸ë˜í”„",
            "line": r"ì„ .*ì°¨íŠ¸|line.*chart|ì„ .*ê·¸ë˜í”„|ì‹œê³„ì—´",
            "scatter": r"ì‚°ì ë„|scatter|ì .*ê·¸ë˜í”„",
            "pie": r"ì›.*ì°¨íŠ¸|pie.*chart|íŒŒì´.*ì°¨íŠ¸",
            "histogram": r"íˆìŠ¤í† ê·¸ë¨|histogram|ë¶„í¬.*ì°¨íŠ¸",
            "box": r"ë°•ìŠ¤.*í”Œë¡¯|box.*plot|ìƒì.*ê·¸ë¦¼",
            "heatmap": r"íˆíŠ¸ë§µ|heatmap|ì—´.*ì§€ë„"
        }
        
        for viz_type, pattern in viz_patterns.items():
            if re.search(pattern, query, re.IGNORECASE):
                return viz_type
        
        return "auto"  # ìë™ ì„ íƒ
    
    def generate_analysis_plan(self, intent: QueryIntent, dataframe_info: Dict) -> List[Dict[str, Any]]:
        """ë¶„ì„ ê³„íš ìƒì„±"""
        plan = []
        
        # ì¿¼ë¦¬ ìœ í˜•ë³„ ë¶„ì„ ë‹¨ê³„ ì •ì˜
        if intent.query_type == QueryType.SUMMARY:
            plan.extend([
                {"step": "basic_info", "description": "ë°ì´í„° ê¸°ë³¸ ì •ë³´ í™•ì¸"},
                {"step": "data_types", "description": "ë°ì´í„° íƒ€ì… ë¶„ì„"},
                {"step": "sample_data", "description": "ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ"}
            ])
        
        elif intent.query_type == QueryType.STATISTICS:
            plan.extend([
                {"step": "descriptive_stats", "description": "ê¸°ìˆ í†µê³„ ê³„ì‚°"},
                {"step": "distribution_analysis", "description": "ë¶„í¬ ë¶„ì„"}
            ])
            if intent.target_columns:
                plan.append({
                    "step": "column_stats", 
                    "description": f"ì§€ì • ì»¬ëŸ¼ í†µê³„: {', '.join(intent.target_columns)}"
                })
        
        elif intent.query_type == QueryType.CORRELATION:
            plan.extend([
                {"step": "correlation_matrix", "description": "ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°"},
                {"step": "high_correlations", "description": "ë†’ì€ ìƒê´€ê´€ê³„ ì‹ë³„"}
            ])
        
        elif intent.query_type == QueryType.MISSING_DATA:
            plan.extend([
                {"step": "missing_data_count", "description": "ê²°ì¸¡ ë°ì´í„° ê°œìˆ˜ í™•ì¸"},
                {"step": "missing_data_pattern", "description": "ê²°ì¸¡ ë°ì´í„° íŒ¨í„´ ë¶„ì„"}
            ])
        
        elif intent.query_type == QueryType.VISUALIZATION:
            plan.extend([
                {"step": "data_preparation", "description": "ì‹œê°í™”ìš© ë°ì´í„° ì¤€ë¹„"},
                {"step": "chart_generation", "description": f"{intent.visualization_type} ì°¨íŠ¸ ìƒì„±"}
            ])
        
        elif intent.query_type == QueryType.GROUPBY:
            if intent.target_columns:
                plan.extend([
                    {"step": "group_analysis", "description": f"ê·¸ë£¹ë³„ ë¶„ì„: {', '.join(intent.target_columns)}"},
                    {"step": "group_statistics", "description": "ê·¸ë£¹ë³„ í†µê³„ ê³„ì‚°"}
                ])
        
        else:
            # ê¸°ë³¸ ë¶„ì„ ê³„íš
            plan.extend([
                {"step": "general_analysis", "description": "ì¼ë°˜ì ì¸ ë°ì´í„° ë¶„ì„"},
                {"step": "basic_statistics", "description": "ê¸°ë³¸ í†µê³„ ì •ë³´"}
            ])
        
        return plan
    
    def format_analysis_result(self, intent: QueryIntent, results: Dict[str, Any]) -> str:
        """ë¶„ì„ ê²°ê³¼ í¬ë§·íŒ…"""
        try:
            # ì¿¼ë¦¬ ìœ í˜•ë³„ ê²°ê³¼ í¬ë§·íŒ…
            if intent.query_type == QueryType.SUMMARY:
                return self._format_summary_result(results)
            elif intent.query_type == QueryType.STATISTICS:
                return self._format_statistics_result(results)
            elif intent.query_type == QueryType.CORRELATION:
                return self._format_correlation_result(results)
            elif intent.query_type == QueryType.MISSING_DATA:
                return self._format_missing_data_result(results)
            else:
                return self._format_general_result(results)
                
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
            return f"ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆì§€ë§Œ ê²°ê³¼ í¬ë§·íŒ… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    def _format_summary_result(self, results: Dict[str, Any]) -> str:
        """ìš”ì•½ ê²°ê³¼ í¬ë§·íŒ…"""
        return f"""# ğŸ“Š **ë°ì´í„° ìš”ì•½**

{results.get('summary', 'ìš”ì•½ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')}

## ğŸ” **ì£¼ìš” íŠ¹ì§•**
{results.get('key_features', 'íŠ¹ì§• ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}
"""
    
    def _format_statistics_result(self, results: Dict[str, Any]) -> str:
        """í†µê³„ ê²°ê³¼ í¬ë§·íŒ…"""
        return f"""# ğŸ“ˆ **í†µê³„ ë¶„ì„ ê²°ê³¼**

{results.get('statistics', 'í†µê³„ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')}

## ğŸ¯ **ì£¼ìš” ì¸ì‚¬ì´íŠ¸**
{results.get('insights', 'ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')}
"""
    
    def _format_correlation_result(self, results: Dict[str, Any]) -> str:
        """ìƒê´€ê´€ê³„ ê²°ê³¼ í¬ë§·íŒ…"""
        return f"""# ğŸ”— **ìƒê´€ê´€ê³„ ë¶„ì„**

{results.get('correlation', 'ìƒê´€ê´€ê³„ ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')}

## â­ **ë†’ì€ ìƒê´€ê´€ê³„**
{results.get('high_correlations', 'ë†’ì€ ìƒê´€ê´€ê³„ê°€ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')}
"""
    
    def _format_missing_data_result(self, results: Dict[str, Any]) -> str:
        """ê²°ì¸¡ ë°ì´í„° ê²°ê³¼ í¬ë§·íŒ…"""
        return f"""# ğŸ” **ê²°ì¸¡ ë°ì´í„° ë¶„ì„**

{results.get('missing_analysis', 'ê²°ì¸¡ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')}

## ğŸ’¡ **ê¶Œì¥ì‚¬í•­**
{results.get('recommendations', 'ê¶Œì¥ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.')}
"""
    
    def _format_general_result(self, results: Dict[str, Any]) -> str:
        """ì¼ë°˜ ê²°ê³¼ í¬ë§·íŒ…"""
        return f"""# ğŸ“Š **ë¶„ì„ ê²°ê³¼**

{results.get('analysis', 'ë¶„ì„ ê²°ê³¼ë¥¼ í‘œì‹œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')}

## ğŸ“‹ **ìƒì„¸ ì •ë³´**
{results.get('details', 'ìƒì„¸ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.')}
""" 