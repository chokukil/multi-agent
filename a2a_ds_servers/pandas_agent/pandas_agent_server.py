#!/usr/bin/env python3
"""
ğŸ¼ Pandas Agent A2A Server

A2A SDK 0.2.9 ê¸°ë°˜ ìì—°ì–´ ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸
PandasAI ì°¸ê³  êµ¬í˜„ (MIT License ì¤€ìˆ˜)

Key Features:
- ìì—°ì–´ ê¸°ë°˜ ë°ì´í„° ë¶„ì„
- A2A SDK 0.2.9 ì™„ì „ í˜¸í™˜  
- ë©€í‹° ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
- ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
- ì•ˆì „í•œ ì½”ë“œ ì‹¤í–‰ í™˜ê²½
- ë²”ìš© ë°ì´í„° í¬ë§· ì§€ì›

Author: CherryAI Team
License: MIT License
"""

import asyncio
import pandas as pd
import numpy as np
import json
import os
import sys
import logging
import traceback
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Union, AsyncIterator
from contextlib import asynccontextmanager

# A2A SDK 0.2.9 Import
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers.default_request_handler import DefaultRequestHandler
from a2a.server.tasks.inmemory_task_store import InMemoryTaskStore
from a2a.server.tasks.task_updater import TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.types import TextPart, TaskState, AgentCard, AgentSkill, AgentCapabilities
from a2a.utils import new_agent_text_message
import uvicorn

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

# Enhanced Tracking System (ì„ íƒì )
try:
    from core.enhanced_langfuse_tracer import get_enhanced_tracer
    ENHANCED_TRACKING_AVAILABLE = True
except ImportError:
    ENHANCED_TRACKING_AVAILABLE = False

# UserFileTracker í†µí•© (ì„ íƒì )
try:
    from core.user_file_tracker import get_user_file_tracker
    from core.session_data_manager import SessionDataManager
    USER_FILE_TRACKER_AVAILABLE = True
except ImportError:
    USER_FILE_TRACKER_AVAILABLE = False

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PandasAgentCore:
    """
    Pandas Agent í•µì‹¬ ì—”ì§„
    
    ìì—°ì–´ ê¸°ë°˜ ë°ì´í„° ë¶„ì„ì„ ìœ„í•œ í•µì‹¬ ë¡œì§
    PandasAI íŒ¨í„´ì„ ì°¸ê³ í•˜ì—¬ ì•ˆì „í•˜ê²Œ êµ¬í˜„
    """
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.dataframes: List[pd.DataFrame] = []
        self.dataframe_metadata: List[Dict] = []
        self.conversation_history: List[Dict] = []
        self.session_id: Optional[str] = None
        
        # Enhanced Tracking ì´ˆê¸°í™”
        self.enhanced_tracer = None
        if ENHANCED_TRACKING_AVAILABLE:
            try:
                self.enhanced_tracer = get_enhanced_tracer()
                logger.info("âœ… Enhanced Langfuse Tracking í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ Enhanced Tracking ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # UserFileTracker ì´ˆê¸°í™”
        self.user_file_tracker = None
        self.session_data_manager = None
        if USER_FILE_TRACKER_AVAILABLE:
            try:
                self.user_file_tracker = get_user_file_tracker()
                self.session_data_manager = SessionDataManager()
                logger.info("âœ… UserFileTracker í†µí•© í™œì„±í™”")
            except Exception as e:
                logger.warning(f"âš ï¸ UserFileTracker ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ ì„¤ì •
        self._setup_default_config()
    
    def _setup_default_config(self):
        """ê¸°ë³¸ ì„¤ì • ì´ˆê¸°í™”"""
        default_config = {
            "verbose": False,
            "save_logs": True,
            "max_retries": 3,
            "enable_cache": True,
            "custom_whitelisted_dependencies": ["pandas", "numpy", "matplotlib", "seaborn", "plotly"]
        }
        
        for key, value in default_config.items():
            if key not in self.config:
                self.config[key] = value
    
    async def add_dataframe(self, df: pd.DataFrame, name: str = None, description: str = None) -> str:
        """ë°ì´í„°í”„ë ˆì„ ì¶”ê°€"""
        if not isinstance(df, pd.DataFrame):
            raise ValueError("ì…ë ¥ì´ pandas DataFrameì´ ì•„ë‹™ë‹ˆë‹¤.")
        
        df_id = name or f"dataframe_{len(self.dataframes)}"
        
        # ë©”íƒ€ë°ì´í„° ìƒì„±
        metadata = {
            "id": df_id,
            "name": name or df_id,
            "description": description or f"ë°ì´í„°í”„ë ˆì„ {df_id}",
            "shape": df.shape,
            "columns": list(df.columns),
            "dtypes": df.dtypes.to_dict(),
            "memory_usage": df.memory_usage(deep=True).sum(),
            "null_counts": df.isnull().sum().to_dict(),
            "created_at": datetime.now().isoformat()
        }
        
        self.dataframes.append(df)
        self.dataframe_metadata.append(metadata)
        
        logger.info(f"âœ… ë°ì´í„°í”„ë ˆì„ ì¶”ê°€: {df_id} (shape: {df.shape})")
        return df_id
    
    async def process_natural_language_query(self, query: str) -> str:
        """ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬"""
        try:
            # Enhanced tracking ì‹œì‘
            if self.enhanced_tracer:
                self.enhanced_tracer.log_data_operation(
                    "natural_language_query",
                    {"query": query, "dataframe_count": len(self.dataframes)},
                    "ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘"
                )
            
            if not self.dataframes:
                return "âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            # í˜„ì¬ëŠ” ê¸°ë³¸ ë¶„ì„ì„ ì œê³µ (ì¶”í›„ LLM í†µí•© ì˜ˆì •)
            analysis_result = await self._perform_basic_analysis(query)
            
            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self.conversation_history.append({
                "timestamp": datetime.now().isoformat(),
                "query": query,
                "response": analysis_result,
                "dataframes_used": len(self.dataframes)
            })
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _perform_basic_analysis(self, query: str) -> str:
        """ê¸°ë³¸ ë°ì´í„° ë¶„ì„ ìˆ˜í–‰"""
        if not self.dataframes:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        # ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
        df = self.dataframes[0]
        metadata = self.dataframe_metadata[0]
        
        # ì¿¼ë¦¬ í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì„ ê²°ì •
        query_lower = query.lower()
        
        if any(keyword in query_lower for keyword in ['summary', 'ìš”ì•½', 'overview', 'ê°œìš”']):
            return self._generate_data_summary(df, metadata)
        elif any(keyword in query_lower for keyword in ['describe', 'ê¸°ìˆ í†µê³„', 'statistics', 'í†µê³„']):
            return self._generate_descriptive_statistics(df, metadata)
        elif any(keyword in query_lower for keyword in ['null', 'ê²°ì¸¡', 'missing', 'ëˆ„ë½']):
            return self._generate_missing_data_analysis(df, metadata)
        elif any(keyword in query_lower for keyword in ['correlation', 'ìƒê´€', 'ê´€ê³„']):
            return self._generate_correlation_analysis(df, metadata)
        else:
            # ê¸°ë³¸ ì¢…í•© ë¶„ì„
            return self._generate_comprehensive_analysis(df, metadata, query)
    
    def _generate_data_summary(self, df: pd.DataFrame, metadata: Dict) -> str:
        """ë°ì´í„° ìš”ì•½ ìƒì„±"""
        return f"""# ğŸ“Š **ë°ì´í„° ìš”ì•½**

## ğŸ” **ê¸°ë³¸ ì •ë³´**
- **ë°ì´í„°ì…‹**: {metadata['name']}
- **í¬ê¸°**: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {metadata['memory_usage'] / 1024**2:.1f} MB

## ğŸ“‹ **ì»¬ëŸ¼ ì •ë³´**
{chr(10).join([f"- **{col}**: {dtype} (ê²°ì¸¡: {metadata['null_counts'][col]}ê°œ)" 
               for col, dtype in zip(df.columns, df.dtypes)])}

## ğŸ“ˆ **ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
{df.head().to_string()}
"""
    
    def _generate_descriptive_statistics(self, df: pd.DataFrame, metadata: Dict) -> str:
        """ê¸°ìˆ í†µê³„ ìƒì„±"""
        numeric_df = df.select_dtypes(include=[np.number])
        
        if numeric_df.empty:
            return "âŒ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ì–´ ê¸°ìˆ í†µê³„ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        stats = numeric_df.describe()
        
        return f"""# ğŸ“Š **ê¸°ìˆ í†µê³„**

## ğŸ“ˆ **ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ í†µê³„**
{stats.to_string()}

## ğŸ¯ **ì£¼ìš” ì¸ì‚¬ì´íŠ¸**
- **ì´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼**: {len(numeric_df.columns)}ê°œ
- **ê°€ì¥ ë†’ì€ í‰ê· ê°’**: {stats.loc['mean'].max():.2f}
- **ê°€ì¥ í° í‘œì¤€í¸ì°¨**: {stats.loc['std'].max():.2f}
"""
    
    def _generate_missing_data_analysis(self, df: pd.DataFrame, metadata: Dict) -> str:
        """ê²°ì¸¡ ë°ì´í„° ë¶„ì„"""
        null_counts = df.isnull().sum()
        null_percentages = (null_counts / len(df) * 100).round(2)
        
        missing_summary = []
        for col in df.columns:
            if null_counts[col] > 0:
                missing_summary.append(f"- **{col}**: {null_counts[col]}ê°œ ({null_percentages[col]}%)")
        
        if not missing_summary:
            return "âœ… **ê²°ì¸¡ ë°ì´í„° ì—†ìŒ**: ëª¨ë“  ì»¬ëŸ¼ì´ ì™„ì „í•©ë‹ˆë‹¤."
        
        return f"""# ğŸ” **ê²°ì¸¡ ë°ì´í„° ë¶„ì„**

## âš ï¸ **ê²°ì¸¡ ë°ì´í„° ë°œê²¬**
{chr(10).join(missing_summary)}

## ğŸ“Š **ì „ì²´ ìš”ì•½**
- **ì´ ê²°ì¸¡ ì…€**: {null_counts.sum():,}ê°œ
- **ì˜í–¥ë°›ì€ ì»¬ëŸ¼**: {(null_counts > 0).sum()}ê°œ
- **ì™„ì „í•œ í–‰**: {len(df) - df.isnull().any(axis=1).sum():,}ê°œ
"""
    
    def _generate_correlation_analysis(self, df: pd.DataFrame, metadata: Dict) -> str:
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        numeric_df = df.select_dtypes(include=[np.number])
        
        if len(numeric_df.columns) < 2:
            return "âŒ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        correlation_matrix = numeric_df.corr()
        
        # ë†’ì€ ìƒê´€ê´€ê³„ ì°¾ê¸°
        high_corr_pairs = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > 0.7:  # ë†’ì€ ìƒê´€ê´€ê³„ ê¸°ì¤€
                    high_corr_pairs.append(
                        f"- **{correlation_matrix.columns[i]}** â†” **{correlation_matrix.columns[j]}**: {corr_value:.3f}"
                    )
        
        return f"""# ğŸ”— **ìƒê´€ê´€ê³„ ë¶„ì„**

## ğŸ“Š **ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤**
{correlation_matrix.round(3).to_string()}

## â­ **ë†’ì€ ìƒê´€ê´€ê³„ (|r| > 0.7)**
{chr(10).join(high_corr_pairs) if high_corr_pairs else "- ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì´ëŠ” ë³€ìˆ˜ ìŒì´ ì—†ìŠµë‹ˆë‹¤."}
"""
    
    def _generate_comprehensive_analysis(self, df: pd.DataFrame, metadata: Dict, query: str) -> str:
        """ì¢…í•© ë¶„ì„"""
        return f"""# ğŸ” **ì¢…í•© ë°ì´í„° ë¶„ì„**

## ğŸ“ **ì¿¼ë¦¬**: "{query}"

## ğŸ“Š **ë°ì´í„° ê°œìš”**
- **ë°ì´í„°ì…‹**: {metadata['name']}
- **í¬ê¸°**: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´
- **ìˆ˜ì¹˜í˜• ì»¬ëŸ¼**: {len(df.select_dtypes(include=[np.number]).columns)}ê°œ
- **ë²”ì£¼í˜• ì»¬ëŸ¼**: {len(df.select_dtypes(include=['object']).columns)}ê°œ

## ğŸ¯ **ë¹ ë¥¸ ì¸ì‚¬ì´íŠ¸**
- **ê²°ì¸¡ ë°ì´í„°**: {df.isnull().sum().sum()}ê°œ ì…€
- **ì™„ì „í•œ í–‰**: {len(df) - df.isnull().any(axis=1).sum():,}ê°œ
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {metadata['memory_usage'] / 1024**2:.1f} MB

## ğŸ’¡ **ì¶”ì²œ ë¶„ì„**
ìì—°ì–´ë¡œ ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ë³´ì„¸ìš”:
- "ì´ ë°ì´í„°ì˜ ê¸°ìˆ í†µê³„ë¥¼ ë³´ì—¬ì¤˜"
- "ê²°ì¸¡ ë°ì´í„° ìƒí™©ì„ ë¶„ì„í•´ì¤˜" 
- "ì»¬ëŸ¼ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì•Œë ¤ì¤˜"
"""
    
    def clear_conversation(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        self.conversation_history = []
        logger.info("ğŸ“ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    def get_conversation_history(self) -> List[Dict]:
        """ëŒ€í™” ê¸°ë¡ ë°˜í™˜"""
        return self.conversation_history


class PandasAgentExecutor(AgentExecutor):
    """A2A SDK 0.2.9 í˜¸í™˜ Pandas Agent Executor"""
    
    def __init__(self):
        self.agent = PandasAgentCore()
        logger.info("âœ… Pandas Agent Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def cancel(self) -> None:
        """A2A SDK 0.2.9 í‘œì¤€ cancel ë©”ì„œë“œ"""
        logger.info("ğŸ›‘ Pandas Agent Executor ì·¨ì†Œ ìš”ì²­")
        if hasattr(self.agent, 'clear_conversation'):
            self.agent.clear_conversation()
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> Any:
        """A2A SDK 0.2.9 í‘œì¤€ execute ë©”ì„œë“œ"""
        try:
            # ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ
            user_input = self._extract_user_input(context)
            
            # ì„¸ì…˜ ë°ì´í„° ë¡œë”© ì‹œë„
            session_id = context.request.get("session_id")
            if session_id and USER_FILE_TRACKER_AVAILABLE:
                await self._load_session_data(session_id, task_updater)
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸
            await task_updater.update_status(
                TaskState.working,
                message="ğŸ¼ Pandas Agentê°€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
            )
            
            # ìì—°ì–´ ì¿¼ë¦¬ ì²˜ë¦¬
            response = await self.agent.process_natural_language_query(user_input)
            
            # ìµœì¢… ì‘ë‹µ
            await task_updater.update_status(
                TaskState.completed,
                message=response,
                final=True
            )
            
        except Exception as e:
            logger.error(f"âŒ Pandas Agent ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            await task_updater.update_status(
                TaskState.completed,
                message=f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                final=True
            )
    
    def _extract_user_input(self, context: RequestContext) -> str:
        """ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ"""
        try:
            if hasattr(context, 'message') and context.message:
                if hasattr(context.message, 'parts') and context.message.parts:
                    for part in context.message.parts:
                        if hasattr(part, 'root') and hasattr(part.root, 'text'):
                            return part.root.text
                        elif hasattr(part, 'text'):
                            return part.text
            return "ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”"
        except Exception as e:
            logger.warning(f"âš ï¸ ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return "ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”"
    
    async def _load_session_data(self, session_id: str, task_updater: TaskUpdater):
        """ì„¸ì…˜ ë°ì´í„° ë¡œë”©"""
        try:
            if self.agent.user_file_tracker and self.agent.session_data_manager:
                # ì„¸ì…˜ ë°ì´í„° ë¡œë”©
                session_data = await self.agent.session_data_manager.get_session_data(session_id)
                
                if session_data and session_data.get('uploaded_files'):
                    await task_updater.update_status(
                        TaskState.working,
                        message="ğŸ“‚ ì„¸ì…˜ ë°ì´í„°ë¥¼ ë¡œë”©í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
                    )
                    
                    for file_info in session_data['uploaded_files']:
                        file_path = file_info.get('file_path')
                        if file_path and os.path.exists(file_path):
                            # íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ ë¡œë”©
                            if file_path.endswith('.csv'):
                                df = pd.read_csv(file_path)
                            elif file_path.endswith(('.xlsx', '.xls')):
                                df = pd.read_excel(file_path)
                            elif file_path.endswith('.json'):
                                df = pd.read_json(file_path)
                            else:
                                continue
                            
                            # ë°ì´í„°í”„ë ˆì„ ì¶”ê°€
                            await self.agent.add_dataframe(
                                df, 
                                name=file_info.get('name', 'uploaded_data'),
                                description=file_info.get('description', 'ì—…ë¡œë“œëœ ë°ì´í„°')
                            )
                            
                            logger.info(f"âœ… ì„¸ì…˜ ë°ì´í„° ë¡œë”© ì™„ë£Œ: {file_info.get('name')}")
                
        except Exception as e:
            logger.warning(f"âš ï¸ ì„¸ì…˜ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")


# A2A ì„œë²„ ì„¤ì •
async def create_pandas_agent_server():
    """Pandas Agent A2A ì„œë²„ ìƒì„±"""
    
    # Agent Card ì„¤ì •
    skills_list = [
        AgentSkill(
            id="natural_language_analysis",
            name="natural_language_analysis",
            description="ìì—°ì–´ë¡œ ë°ì´í„° ë¶„ì„ ìˆ˜í–‰",
            tags=["analysis", "nlp"]
        ),
        AgentSkill(
            id="multi_dataframe_processing",
            name="multi_dataframe_processing", 
            description="ì—¬ëŸ¬ ë°ì´í„°í”„ë ˆì„ ë™ì‹œ ì²˜ë¦¬",
            tags=["data", "multi-df"]
        ),
        AgentSkill(
            id="descriptive_statistics",
            name="descriptive_statistics",
            description="ê¸°ìˆ í†µê³„ ë° ë°ì´í„° ìš”ì•½",
            tags=["statistics", "summary"]
        ),
        AgentSkill(
            id="data_quality_analysis",
            name="data_quality_analysis",
            description="ë°ì´í„° í’ˆì§ˆ ë° ê²°ì¸¡ì¹˜ ë¶„ì„",
            tags=["quality", "analysis"]
        )
    ]
    
    agent_card = AgentCard(
        name="Pandas Agent",
        description="ìì—°ì–´ ê¸°ë°˜ ë°ì´í„° ë¶„ì„ ì „ë¬¸ ì—ì´ì „íŠ¸",
        version="1.0.0",
        url="http://localhost:8315",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        skills=skills_list,
        capabilities=AgentCapabilities(
            skills=skills_list
        )
    )
    
    # A2A ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„± (A2A SDK 0.2.9 API)
    executor = PandasAgentExecutor()
    task_store = InMemoryTaskStore()
    
    app = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=DefaultRequestHandler(executor, task_store)
    )
    
    return app


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    app = await create_pandas_agent_server()
    return app

# ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì•± ìƒì„± ì§€ì›
app = None

def get_app():
    """ì•± ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global app
    if app is None:
        app = asyncio.run(create_pandas_agent_server())
    return app

if __name__ == "__main__":
    import sys
    
    # ëª…ë ¹í–‰ ì¸ì í™•ì¸
    port = 8315
    if len(sys.argv) > 1:
        try:
            port = int(sys.argv[1])
        except ValueError:
            logger.warning(f"âš ï¸ ì˜ëª»ëœ í¬íŠ¸ ë²ˆí˜¸: {sys.argv[1]}, ê¸°ë³¸ê°’ 8315 ì‚¬ìš©")
    
    # ì„œë²„ ì •ë³´ ì¶œë ¥
    logger.info(f"ğŸš€ Pandas Agent A2A ì„œë²„ ì‹œì‘")
    logger.info(f"ğŸ“ ì£¼ì†Œ: http://0.0.0.0:{port}")
    logger.info(f"ğŸ”§ Agent Card: http://0.0.0.0:{port}/.well-known/agent.json")
    
    # ì•± ìƒì„±
    app = asyncio.run(create_pandas_agent_server())
    
    # ì„œë²„ ì‹¤í–‰
    try:
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=port,
            log_level="info"
        )
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ ì„œë²„ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"âŒ ì„œë²„ ì‹¤í–‰ ì˜¤ë¥˜: {e}") 