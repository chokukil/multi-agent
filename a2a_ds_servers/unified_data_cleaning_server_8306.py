#!/usr/bin/env python3
"""
CherryAI Unified Data Cleaning Server - Port 8306
A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜ + UnifiedDataInterface íŒ¨í„´

ğŸ“Š í•µì‹¬ ê¸°ëŠ¥:
- ğŸ§¹ LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ë°ì´í„° ì •ë¦¬ ì „ëµ ë¶„ì„
- ğŸ” ë¹ˆ ë°ì´í„° ì™„ë²½ ì²˜ë¦¬ (ì„¤ê³„ ë¬¸ì„œ ì£¼ìš” ë¬¸ì œì  í•´ê²°)
- ğŸ“‹ 7ë‹¨ê³„ í‘œì¤€ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤
- ğŸ’¾ SmartDataFrame í’ˆì§ˆ ê²€ì¦ ì‹œìŠ¤í…œ
- âš¡ ìºì‹± ë° ì„±ëŠ¥ ìµœì í™”
- ğŸ¯ A2A í‘œì¤€ TaskUpdater + ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°

ê¸°ë°˜: pandas_agent íŒ¨í„´ + unified_data_loader ì„±ê³µ ì‚¬ë¡€
"""

import asyncio
import logging
import os
import json
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import pandas as pd
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A SDK 0.2.9 í‘œì¤€ imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events import EventQueue
from a2a.types import (
    AgentCard, AgentSkill, AgentCapabilities,
    TaskState, TextPart
)
from a2a.utils import new_agent_text_message
import uvicorn

# CherryAI Core imports
from core.llm_factory import LLMFactory
from a2a_ds_servers.unified_data_system.core.unified_data_interface import UnifiedDataInterface
from a2a_ds_servers.unified_data_system.core.smart_dataframe import SmartDataFrame
from a2a_ds_servers.unified_data_system.core.llm_first_data_engine import LLMFirstDataEngine
from a2a_ds_servers.unified_data_system.core.cache_manager import CacheManager
from a2a_ds_servers.unified_data_system.utils.file_scanner import FileScanner

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class UnifiedDataCleaningExecutor(AgentExecutor, UnifiedDataInterface):
    """
    Unified Data Cleaning Executor
    
    pandas_agent íŒ¨í„´ + data_loader ì„±ê³µ ì‚¬ë¡€ ê¸°ë°˜
    - LLM First ë°ì´í„° ì •ë¦¬ ì „ëµ
    - ë¹ˆ ë°ì´í„° ì™„ë²½ ì²˜ë¦¬
    - SmartDataFrame í’ˆì§ˆ ì‹œìŠ¤í…œ
    - A2A SDK 0.2.9 ì™„ì „ ì¤€ìˆ˜
    """
    
    def __init__(self):
        super().__init__()
        
        # í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.data_engine = LLMFirstDataEngine()
        self.cache_manager = CacheManager()
        self.file_scanner = FileScanner()
        self.llm_factory = LLMFactory()
        
        # ì •ë¦¬ ì „ë¬¸ ì„¤ì •
        self.cleaning_strategies = {
            'missing_values': ['drop', 'impute_mean', 'impute_median', 'impute_mode', 'forward_fill', 'backward_fill'],
            'outliers': ['iqr_removal', 'zscore_removal', 'isolation_forest', 'clip_values'],
            'duplicates': ['drop_exact', 'drop_subset', 'keep_first', 'keep_last'],
            'data_types': ['auto_convert', 'optimize_memory', 'categorical_conversion']
        }
        
        # í’ˆì§ˆ ì§€í‘œ ì„ê³„ê°’
        self.quality_thresholds = {
            'completeness_min': 0.7,  # 70% ì´ìƒ ë°ì´í„° ì™„ì „ì„±
            'consistency_min': 0.8,   # 80% ì´ìƒ ì¼ê´€ì„±
            'validity_min': 0.9       # 90% ì´ìƒ ìœ íš¨ì„±
        }
        
        logger.info("âœ… Unified Data Cleaning Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """
        A2A í‘œì¤€ ì‹¤í–‰: 7ë‹¨ê³„ ì§€ëŠ¥í˜• ë°ì´í„° ì •ë¦¬ í”„ë¡œì„¸ìŠ¤
        
        ğŸ§¹ 1ë‹¨ê³„: LLM ì •ë¦¬ ì˜ë„ ë¶„ì„
        ğŸ“‚ 2ë‹¨ê³„: ë°ì´í„° ê²€ìƒ‰ ë° ì§€ëŠ¥í˜• ì„ íƒ  
        ğŸ“Š 3ë‹¨ê³„: ì•ˆì „í•œ ë°ì´í„° ë¡œë”©
        ğŸ” 4ë‹¨ê³„: ë¹ˆ ë°ì´í„° ê°ì§€ ë° í’ˆì§ˆ ì§„ë‹¨
        ğŸ› ï¸ 5ë‹¨ê³„: LLM ì •ë¦¬ ê³„íš ìˆ˜ë¦½
        âš¡ 6ë‹¨ê³„: ì •ë¦¬ ì‘ì—… ì‹¤í–‰
        âœ… 7ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ ë° ì €ì¥
        """
        try:
            # ì‘ì—… ì‹œì‘
            await task_updater.submit()
            await task_updater.start_work()
            
            start_time = time.time()
            
            # ğŸ§¹ 1ë‹¨ê³„: ì‚¬ìš©ì ì˜ë„ ë¶„ì„
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§‘ğŸ» **ë°ì´í„° ì •ë¦¬ ì‹œì‘** - 1ë‹¨ê³„: ì •ë¦¬ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì¤‘...")
            )
            
            user_query = self._extract_user_query(context)
            logger.info(f"ğŸ§¹ Data Cleaning Query: {user_query}")
            
            # LLM ê¸°ë°˜ ì •ë¦¬ ì˜ë„ ë¶„ì„
            cleaning_intent = await self._analyze_cleaning_intent(user_query)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì˜ë„ ë¶„ì„ ì™„ë£Œ**\n"
                    f"- ì •ë¦¬ íƒ€ì…: {cleaning_intent['cleaning_type']}\n"
                    f"- ìš°ì„ ìˆœìœ„: {', '.join(cleaning_intent['priority_areas'])}\n"
                    f"- ì‹ ë¢°ë„: {cleaning_intent['confidence']:.2f}\n\n"
                    f"**2ë‹¨ê³„**: ë°ì´í„° ê²€ìƒ‰ ì¤‘..."
                )
            )
            
            # ğŸ“‚ 2ë‹¨ê³„: íŒŒì¼ ê²€ìƒ‰ ë° ì§€ëŠ¥í˜• ì„ íƒ
            available_files = await self._scan_available_files()
            
            if not available_files:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(
                        "âš ï¸ **ë°ì´í„° ì—†ìŒ**: ì •ë¦¬í•  ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                        "**í•´ê²°ì±…**:\n"
                        "1. `a2a_ds_servers/artifacts/data/` í´ë”ì— ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”\n"
                        "2. ì§€ì› í˜•ì‹: CSV, Excel (.xlsx/.xls), JSON, Parquet\n"
                        "3. ê¶Œì¥ ì¸ì½”ë”©: UTF-8"
                    )
                )
                return
            
            # LLM ê¸°ë°˜ ìµœì  íŒŒì¼ ì„ íƒ
            selected_file = await self._select_optimal_file(available_files, cleaning_intent)
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **íŒŒì¼ ì„ íƒ ì™„ë£Œ**\n"
                    f"- íŒŒì¼: {selected_file['name']}\n"
                    f"- í¬ê¸°: {selected_file['size']:,} bytes\n"
                    f"- í˜•ì‹: {selected_file['extension']}\n\n"
                    f"**3ë‹¨ê³„**: ë°ì´í„° ë¡œë”© ì¤‘..."
                )
            )
            
            # ğŸ“Š 3ë‹¨ê³„: ì•ˆì „í•œ ë°ì´í„° ë¡œë”© (unified pattern)
            smart_df = await self._load_data_safely(selected_file)
            
            # ğŸ” 4ë‹¨ê³„: ë¹ˆ ë°ì´í„° ê°ì§€ ë° í’ˆì§ˆ ì§„ë‹¨
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ë°ì´í„° ë¡œë”© ì™„ë£Œ**\n"
                    f"- í˜•íƒœ: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´\n"
                    f"- ì»¬ëŸ¼: {list(smart_df.data.columns)}\n\n"
                    f"**4ë‹¨ê³„**: ë°ì´í„° í’ˆì§ˆ ì§„ë‹¨ ì¤‘..."
                )
            )
            
            # ë¹ˆ ë°ì´í„° ì²˜ë¦¬ (í•µì‹¬ ë¬¸ì œ í•´ê²°)
            if await self._is_empty_or_invalid(smart_df):
                await self._handle_empty_data(smart_df, task_updater)
                return
            
            # í’ˆì§ˆ ì§„ë‹¨
            quality_report = await self._comprehensive_quality_assessment(smart_df)
            
            # ğŸ› ï¸ 5ë‹¨ê³„: LLM ì •ë¦¬ ê³„íš ìˆ˜ë¦½
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **í’ˆì§ˆ ì§„ë‹¨ ì™„ë£Œ**\n"
                    f"- ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {quality_report['overall_score']:.2f}/1.00\n"
                    f"- ì™„ì „ì„±: {quality_report['completeness']:.2f}\n"
                    f"- ì¼ê´€ì„±: {quality_report['consistency']:.2f}\n"
                    f"- ìœ íš¨ì„±: {quality_report['validity']:.2f}\n\n"
                    f"**5ë‹¨ê³„**: ì •ë¦¬ ê³„íš ìˆ˜ë¦½ ì¤‘..."
                )
            )
            
            cleaning_plan = await self._create_cleaning_plan(smart_df, quality_report, cleaning_intent)
            
            # âš¡ 6ë‹¨ê³„: ì •ë¦¬ ì‘ì—… ì‹¤í–‰
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message(
                    f"ğŸ’ **ì •ë¦¬ ê³„íš ì™„ì„±**\n"
                    f"- ì‹¤í–‰ ë‹¨ê³„: {len(cleaning_plan['steps'])}ê°œ\n"
                    f"- ì˜ˆìƒ ê°œì„ ë„: +{cleaning_plan['expected_improvement']:.2f}\n\n"
                    f"**6ë‹¨ê³„**: ë°ì´í„° ì •ë¦¬ ì‹¤í–‰ ì¤‘..."
                )
            )
            
            cleaned_smart_df = await self._execute_cleaning_plan(smart_df, cleaning_plan, task_updater)
            
            # âœ… 7ë‹¨ê³„: ê²°ê³¼ ê²€ì¦ ë° ì €ì¥
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("**7ë‹¨ê³„**: ì •ë¦¬ ê²°ê³¼ ê²€ì¦ ë° ì €ì¥ ì¤‘...")
            )
            
            final_results = await self._finalize_cleaning_results(
                original_df=smart_df,
                cleaned_df=cleaned_smart_df,
                cleaning_plan=cleaning_plan,
                task_updater=task_updater
            )
            
            # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            processing_time = time.time() - start_time
            
            # ìµœì¢… ì™„ë£Œ ë©”ì‹œì§€
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(
                    f"âœ… **ë°ì´í„° ì •ë¦¬ ì™„ë£Œ!**\n\n"
                    f"ğŸ“Š **ì •ë¦¬ ê²°ê³¼**:\n"
                    f"- ì›ë³¸: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´\n"
                    f"- ì •ë¦¬ëœ ë°ì´í„°: {cleaned_smart_df.shape[0]}í–‰ Ã— {cleaned_smart_df.shape[1]}ì—´\n"
                    f"- í’ˆì§ˆ ê°œì„ : {final_results['quality_improvement']:.2f} â†’ {final_results['final_quality']:.2f}\n"
                    f"- ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ\n\n"
                    f"ğŸ“ **ì €ì¥ ìœ„ì¹˜**: {final_results['saved_path']}\n"
                    f"ğŸ“‹ **ì •ë¦¬ ë³´ê³ ì„œ**: ì•„í‹°íŒ©íŠ¸ë¡œ ìƒì„±ë¨"
                )
            )
            
            # ì•„í‹°íŒ©íŠ¸ ìƒì„±
            await self._create_cleaning_artifacts(final_results, task_updater)
            
        except Exception as e:
            logger.error(f"âŒ Data Cleaning Error: {e}", exc_info=True)
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"âŒ **ì •ë¦¬ ì‹¤íŒ¨**: {str(e)}")
            )
    
    async def _analyze_cleaning_intent(self, user_query: str) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ì •ë¦¬ ì˜ë„ ë¶„ì„"""
        llm = await self.llm_factory.get_llm()
        
        prompt = f"""
        ì‚¬ìš©ìì˜ ë°ì´í„° ì •ë¦¬ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ìµœì ì˜ ì •ë¦¬ ì „ëµì„ ê²°ì •í•´ì£¼ì„¸ìš”:
        
        ìš”ì²­: {user_query}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
        {{
            "cleaning_type": "comprehensive|missing_values|outliers|duplicates|data_types|custom",
            "priority_areas": ["ê²°ì¸¡ê°’", "ì´ìƒê°’", "ì¤‘ë³µ", "ë°ì´í„°íƒ€ì…", "ì¼ê´€ì„±"],
            "confidence": 0.0-1.0,
            "aggressive_level": "conservative|moderate|aggressive",
            "preserve_original": true/false,
            "expected_operations": ["êµ¬ì²´ì ì¸ ì •ë¦¬ ì‘ì—…ë“¤"]
        }}
        """
        
        response = await llm.agenerate([prompt])
        try:
            intent = json.loads(response.generations[0][0].text)
            return intent
        except:
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return {
                "cleaning_type": "comprehensive",
                "priority_areas": ["ê²°ì¸¡ê°’", "ì´ìƒê°’", "ì¤‘ë³µ"],
                "confidence": 0.8,
                "aggressive_level": "moderate",
                "preserve_original": True,
                "expected_operations": ["ê²°ì¸¡ê°’ ì²˜ë¦¬", "ì´ìƒê°’ íƒì§€", "ì¤‘ë³µ ì œê±°"]
            }
    
    async def _scan_available_files(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„° íŒŒì¼ ê²€ìƒ‰ (unified pattern)"""
        try:
            # FileScanner ì‚¬ìš© (unified pattern)
            data_directories = [
                "ai_ds_team/data",
                "a2a_ds_servers/artifacts/data",
                "test_datasets"
            ]
            
            discovered_files = []
            for directory in data_directories:
                if os.path.exists(directory):
                    files = self.file_scanner.scan_data_files(directory)
                    discovered_files.extend(files)
            
            logger.info(f"ğŸ“‚ ë°œê²¬ëœ íŒŒì¼: {len(discovered_files)}ê°œ")
            return discovered_files
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: {e}")
            return []
    
    async def _select_optimal_file(self, available_files: List[Dict], cleaning_intent: Dict) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ìµœì  íŒŒì¼ ì„ íƒ (unified pattern)"""
        if len(available_files) == 1:
            return available_files[0]
        
        llm = await self.llm_factory.get_llm()
        
        files_info = "\n".join([
            f"- {f['name']} ({f['size']} bytes, {f['extension']})"
            for f in available_files
        ])
        
        prompt = f"""
        ë°ì´í„° ì •ë¦¬ ëª©ì ì— ê°€ì¥ ì í•©í•œ íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”:
        
        ì •ë¦¬ ì˜ë„: {cleaning_intent['cleaning_type']}
        ìš°ì„ ìˆœìœ„: {cleaning_intent['priority_areas']}
        
        ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:
        {files_info}
        
        ê°€ì¥ ì í•©í•œ íŒŒì¼ëª…ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
        """
        
        response = await llm.agenerate([prompt])
        selected_name = response.generations[0][0].text.strip()
        
        # íŒŒì¼ëª… ë§¤ì¹­
        for file_info in available_files:
            if selected_name in file_info['name'] or file_info['name'] in selected_name:
                return file_info
        
        # ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ì²« ë²ˆì§¸ íŒŒì¼ ë°˜í™˜
        return available_files[0]
    
    async def _load_data_safely(self, file_info: Dict[str, Any]) -> SmartDataFrame:
        """ì•ˆì „í•œ ë°ì´í„° ë¡œë”© (unified patternê³¼ ë™ì¼)"""
        file_path = file_info['path']
        
        try:
            # ë‹¤ì¤‘ ì¸ì½”ë”© ì‹œë„
            encodings = ['utf-8', 'cp949', 'euc-kr', 'latin1', 'utf-16']
            df = None
            used_encoding = None
            
            for encoding in encodings:
                try:
                    if file_path.endswith('.csv'):
                        df = pd.read_csv(file_path, encoding=encoding)
                    elif file_path.endswith(('.xlsx', '.xls')):
                        df = pd.read_excel(file_path)
                    elif file_path.endswith('.json'):
                        df = pd.read_json(file_path, encoding=encoding)
                    elif file_path.endswith('.parquet'):
                        df = pd.read_parquet(file_path)
                    
                    used_encoding = encoding
                    break
                except UnicodeDecodeError:
                    continue
                except Exception:
                    continue
            
            if df is None:
                raise ValueError("ì§€ì›ë˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì´ê±°ë‚˜ ì½ê¸°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
            
            # SmartDataFrame ìƒì„±
            metadata = {
                'source_file': file_path,
                'encoding': used_encoding,
                'load_timestamp': datetime.now().isoformat(),
                'original_shape': df.shape
            }
            
            smart_df = SmartDataFrame(df, metadata)
            logger.info(f"âœ… ë°ì´í„° ë¡œë”© ì„±ê³µ: {smart_df.shape}, ì¸ì½”ë”©: {used_encoding}")
            
            return smart_df
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    async def _is_empty_or_invalid(self, smart_df: SmartDataFrame) -> bool:
        """ë¹ˆ ë°ì´í„° ë˜ëŠ” ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„° ê²€ì‚¬ (í•µì‹¬ ë¬¸ì œ í•´ê²°)"""
        # ë‹¤ì–‘í•œ ë¹ˆ ë°ì´í„° ìƒíƒœ ê²€ì‚¬
        checks = [
            smart_df.data.empty,                    # ì™„ì „íˆ ë¹ˆ DataFrame
            smart_df.shape[0] == 0,                 # í–‰ì´ ì—†ìŒ
            smart_df.shape[1] == 0,                 # ì—´ì´ ì—†ìŒ
            len(smart_df.data.columns) == 0,        # ì»¬ëŸ¼ì´ ì—†ìŒ
            smart_df.data.isna().all().all(),       # ëª¨ë“  ê°’ì´ NaN
            all(smart_df.data[col].astype(str).str.strip().eq('').all() for col in smart_df.data.columns if smart_df.data[col].dtype == 'object')  # ëª¨ë“  ê°’ì´ ë¹ˆ ë¬¸ìì—´
        ]
        
        return any(checks)
    
    async def _handle_empty_data(self, smart_df: SmartDataFrame, task_updater: TaskUpdater) -> None:
        """ë¹ˆ ë°ì´í„° ì „ìš© ì²˜ë¦¬ ë¡œì§ (ì„¤ê³„ ë¬¸ì„œ ì£¼ìš” ë¬¸ì œì  í•´ê²°)"""
        
        # ë¹ˆ ë°ì´í„° ìƒíƒœ ì§„ë‹¨
        diagnosis = []
        if smart_df.data.empty:
            diagnosis.append("DataFrameì´ ì™„ì „íˆ ë¹„ì–´ìˆìŒ")
        if smart_df.shape[0] == 0:
            diagnosis.append("ë°ì´í„° í–‰ì´ ì—†ìŒ") 
        if smart_df.shape[1] == 0:
            diagnosis.append("ë°ì´í„° ì»¬ëŸ¼ì´ ì—†ìŒ")
        if len(smart_df.data.columns) == 0:
            diagnosis.append("ì»¬ëŸ¼ ì •ì˜ê°€ ì—†ìŒ")
        
        await task_updater.update_status(
            TaskState.completed,
            message=new_agent_text_message(
                f"âš ï¸ **ë¹ˆ ë°ì´í„° ê°ì§€ë¨**\n\n"
                f"ğŸ“Š **ì§„ë‹¨ ê²°ê³¼**:\n" +
                "\n".join(f"- {d}" for d in diagnosis) +
                f"\n\nğŸ“ **ì›ë³¸ ì •ë³´**:\n"
                f"- íŒŒì¼: {smart_df.metadata.get('source_file', 'Unknown')}\n"
                f"- í˜•íƒœ: {smart_df.shape[0]}í–‰ Ã— {smart_df.shape[1]}ì—´\n\n"
                f"ğŸ”§ **í•´ê²° ë°©ë²•**:\n"
                f"1. ë‹¤ë¥¸ ë°ì´í„° íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”\n"
                f"2. ë°ì´í„° íŒŒì¼ì— ì‹¤ì œ ë°ì´í„°ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”\n"
                f"3. íŒŒì¼ í˜•ì‹ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš” (CSV, Excel, JSON, Parquet)\n"
                f"4. í—¤ë”ë‚˜ ì¸ë±ìŠ¤ ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”\n\n"
                f"ğŸ’¡ **ì¶”ì²œ**: EDA Tools ì—ì´ì „íŠ¸ë¥¼ ì‚¬ìš©í•´ ë°ì´í„° êµ¬ì¡°ë¥¼ ë¨¼ì € ë¶„ì„í•´ë³´ì„¸ìš”"
            )
        )
    
    async def _comprehensive_quality_assessment(self, smart_df: SmartDataFrame) -> Dict[str, float]:
        """í¬ê´„ì  ë°ì´í„° í’ˆì§ˆ í‰ê°€"""
        df = smart_df.data
        
        # ì™„ì „ì„± (Completeness) ê³„ì‚°
        total_cells = df.shape[0] * df.shape[1]
        missing_cells = df.isna().sum().sum()
        completeness = (total_cells - missing_cells) / total_cells if total_cells > 0 else 0
        
        # ì¼ê´€ì„± (Consistency) ê³„ì‚°
        consistency_scores = []
        for col in df.columns:
            if df[col].dtype == 'object':
                # ë¬¸ìì—´ ì»¬ëŸ¼ì˜ í˜•ì‹ ì¼ê´€ì„±
                non_null_values = df[col].dropna()
                if len(non_null_values) > 0:
                    unique_patterns = len(set(str(val).strip().lower() for val in non_null_values))
                    consistency_scores.append(1.0 - (unique_patterns / len(non_null_values)))
                else:
                    consistency_scores.append(1.0)
            else:
                # ìˆ«ì ì»¬ëŸ¼ì˜ ë²”ìœ„ ì¼ê´€ì„±
                consistency_scores.append(0.9)  # ê¸°ë³¸ ì ìˆ˜
        
        consistency = np.mean(consistency_scores) if consistency_scores else 1.0
        
        # ìœ íš¨ì„± (Validity) ê³„ì‚°
        validity_scores = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                # ìˆ«ì ìœ íš¨ì„± (ë¬´í•œê°’, NaN ì œì™¸)
                valid_numbers = df[col].replace([np.inf, -np.inf], np.nan).dropna()
                validity_scores.append(len(valid_numbers) / len(df) if len(df) > 0 else 0)
            else:
                # ë¬¸ìì—´ ìœ íš¨ì„± (ë¹ˆ ë¬¸ìì—´ ì œì™¸)
                valid_strings = df[col].dropna()
                if len(valid_strings) > 0:
                    non_empty = valid_strings.astype(str).str.strip().ne('')
                    validity_scores.append(non_empty.sum() / len(df) if len(df) > 0 else 0)
                else:
                    validity_scores.append(1.0)
        
        validity = np.mean(validity_scores) if validity_scores else 1.0
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        overall_score = (completeness + consistency + validity) / 3
        
        return {
            'overall_score': round(overall_score, 3),
            'completeness': round(completeness, 3),
            'consistency': round(consistency, 3), 
            'validity': round(validity, 3),
            'missing_cells': int(missing_cells),
            'total_cells': int(total_cells)
        }
    
    async def _create_cleaning_plan(self, smart_df: SmartDataFrame, quality_report: Dict, cleaning_intent: Dict) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ì •ë¦¬ ê³„íš ìˆ˜ë¦½"""
        llm = await self.llm_factory.get_llm()
        
        df_info = {
            'shape': smart_df.shape,
            'columns': list(smart_df.data.columns),
            'dtypes': smart_df.data.dtypes.to_dict(),
            'missing_info': smart_df.data.isna().sum().to_dict()
        }
        
        prompt = f"""
        ë°ì´í„° í’ˆì§ˆ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ì •ë¦¬ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”:
        
        ë°ì´í„° ì •ë³´:
        {json.dumps(df_info, indent=2, default=str)}
        
        í’ˆì§ˆ ë³´ê³ ì„œ:
        {json.dumps(quality_report, indent=2)}
        
        ì‚¬ìš©ì ì˜ë„:
        {json.dumps(cleaning_intent, indent=2)}
        
        ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì •ë¦¬ ê³„íšì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
        {{
            "steps": [
                {{
                    "step_number": 1,
                    "operation": "missing_values|outliers|duplicates|data_types",
                    "method": "êµ¬ì²´ì  ë°©ë²•",
                    "target_columns": ["ì»¬ëŸ¼ëª…ë“¤"],
                    "parameters": {{"ë§¤ê°œë³€ìˆ˜": "ê°’"}},
                    "expected_improvement": 0.1
                }}
            ],
            "expected_improvement": 0.3,
            "estimated_time": "2-3ë¶„",
            "backup_required": true
        }}
        """
        
        response = await llm.agenerate([prompt])
        try:
            plan = json.loads(response.generations[0][0].text)
            return plan
        except:
            # ê¸°ë³¸ ê³„íš ë°˜í™˜
            return {
                "steps": [
                    {
                        "step_number": 1,
                        "operation": "missing_values",
                        "method": "auto_imputation",
                        "target_columns": list(smart_df.data.columns),
                        "parameters": {"strategy": "appropriate"},
                        "expected_improvement": 0.2
                    }
                ],
                "expected_improvement": 0.2,
                "estimated_time": "1-2ë¶„",
                "backup_required": True
            }
    
    async def _execute_cleaning_plan(self, smart_df: SmartDataFrame, cleaning_plan: Dict, task_updater: TaskUpdater) -> SmartDataFrame:
        """ì •ë¦¬ ê³„íš ì‹¤í–‰"""
        df_cleaned = smart_df.data.copy()
        execution_log = []
        
        for step in cleaning_plan['steps']:
            step_num = step['step_number']
            operation = step['operation']
            method = step['method']
            target_columns = step.get('target_columns', [])
            
            try:
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message(f"ğŸ’ ì •ë¦¬ ë‹¨ê³„ {step_num}: {operation} ì‹¤í–‰ ì¤‘...")
                )
                
                if operation == "missing_values":
                    df_cleaned = await self._handle_missing_values(df_cleaned, method, target_columns)
                    execution_log.append(f"âœ… ë‹¨ê³„ {step_num}: ê²°ì¸¡ê°’ ì²˜ë¦¬ ì™„ë£Œ")
                
                elif operation == "outliers":
                    df_cleaned = await self._handle_outliers(df_cleaned, method, target_columns)
                    execution_log.append(f"âœ… ë‹¨ê³„ {step_num}: ì´ìƒê°’ ì²˜ë¦¬ ì™„ë£Œ")
                
                elif operation == "duplicates":
                    df_cleaned = await self._handle_duplicates(df_cleaned, method)
                    execution_log.append(f"âœ… ë‹¨ê³„ {step_num}: ì¤‘ë³µ ì œê±° ì™„ë£Œ")
                
                elif operation == "data_types":
                    df_cleaned = await self._optimize_data_types(df_cleaned, target_columns)
                    execution_log.append(f"âœ… ë‹¨ê³„ {step_num}: ë°ì´í„° íƒ€ì… ìµœì í™” ì™„ë£Œ")
                
            except Exception as e:
                execution_log.append(f"âš ï¸ ë‹¨ê³„ {step_num}: {operation} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ - {str(e)}")
                logger.warning(f"ì •ë¦¬ ë‹¨ê³„ {step_num} ì˜¤ë¥˜: {e}")
        
        # ì •ë¦¬ëœ SmartDataFrame ìƒì„±
        cleaned_metadata = smart_df.metadata.copy()
        cleaned_metadata.update({
            'cleaning_timestamp': datetime.now().isoformat(),
            'cleaning_steps': execution_log,
            'original_shape': smart_df.shape,
            'cleaned_shape': df_cleaned.shape
        })
        
        return SmartDataFrame(df_cleaned, cleaned_metadata)
    
    async def _handle_missing_values(self, df: pd.DataFrame, method: str, target_columns: List[str]) -> pd.DataFrame:
        """ê²°ì¸¡ê°’ ì²˜ë¦¬"""
        if not target_columns:
            target_columns = df.columns.tolist()
        
        for col in target_columns:
            if col not in df.columns:
                continue
                
            if df[col].isna().any():
                if method == "auto_imputation":
                    if df[col].dtype in ['int64', 'float64']:
                        # ìˆ«ìí˜•: ì¤‘ì•™ê°’ ì‚¬ìš©
                        df[col].fillna(df[col].median(), inplace=True)
                    else:
                        # ë¬¸ìí˜•: ìµœë¹ˆê°’ ì‚¬ìš©
                        mode_val = df[col].mode()
                        if not mode_val.empty:
                            df[col].fillna(mode_val[0], inplace=True)
                elif method == "drop":
                    df.dropna(subset=[col], inplace=True)
                elif method == "forward_fill":
                    df[col].fillna(method='ffill', inplace=True)
                elif method == "backward_fill":
                    df[col].fillna(method='bfill', inplace=True)
        
        return df
    
    async def _handle_outliers(self, df: pd.DataFrame, method: str, target_columns: List[str]) -> pd.DataFrame:
        """ì´ìƒê°’ ì²˜ë¦¬"""
        if not target_columns:
            target_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        
        for col in target_columns:
            if col not in df.columns or df[col].dtype not in ['int64', 'float64']:
                continue
            
            if method == "iqr_removal":
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
            
            elif method == "zscore_removal":
                z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
                df = df[z_scores < 3]
            
            elif method == "clip_values":
                Q1 = df[col].quantile(0.05)
                Q3 = df[col].quantile(0.95)
                df[col] = df[col].clip(lower=Q1, upper=Q3)
        
        return df
    
    async def _handle_duplicates(self, df: pd.DataFrame, method: str) -> pd.DataFrame:
        """ì¤‘ë³µ ì²˜ë¦¬"""
        if method == "drop_exact":
            df = df.drop_duplicates()
        elif method == "keep_first":
            df = df.drop_duplicates(keep='first')
        elif method == "keep_last":
            df = df.drop_duplicates(keep='last')
        
        return df
    
    async def _optimize_data_types(self, df: pd.DataFrame, target_columns: List[str]) -> pd.DataFrame:
        """ë°ì´í„° íƒ€ì… ìµœì í™”"""
        if not target_columns:
            target_columns = df.columns.tolist()
        
        for col in target_columns:
            if col not in df.columns:
                continue
            
            # ìˆ«ìí˜• ìµœì í™”
            if df[col].dtype in ['int64', 'float64']:
                # ì •ìˆ˜í˜• ë‹¤ìš´ìºìŠ¤íŒ…
                if df[col].dtype == 'int64':
                    df[col] = pd.to_numeric(df[col], downcast='integer')
                # ì‹¤ìˆ˜í˜• ë‹¤ìš´ìºìŠ¤íŒ…  
                elif df[col].dtype == 'float64':
                    df[col] = pd.to_numeric(df[col], downcast='float')
            
            # ì¹´í…Œê³ ë¦¬í˜• ë³€í™˜
            elif df[col].dtype == 'object':
                unique_ratio = df[col].nunique() / len(df)
                if unique_ratio < 0.5:  # 50% ë¯¸ë§Œì´ ê³ ìœ ê°’ì´ë©´ ì¹´í…Œê³ ë¦¬ë¡œ ë³€í™˜
                    df[col] = df[col].astype('category')
        
        return df
    
    async def _finalize_cleaning_results(self, original_df: SmartDataFrame, cleaned_df: SmartDataFrame, 
                                       cleaning_plan: Dict, task_updater: TaskUpdater) -> Dict[str, Any]:
        """ì •ë¦¬ ê²°ê³¼ ìµœì¢…í™”"""
        
        # í’ˆì§ˆ ê°œì„ ë„ ê³„ì‚°
        original_quality = await self._comprehensive_quality_assessment(original_df)
        final_quality = await self._comprehensive_quality_assessment(cleaned_df)
        
        quality_improvement = final_quality['overall_score'] - original_quality['overall_score']
        
        # ì •ë¦¬ëœ ë°ì´í„° ì €ì¥
        save_dir = Path("a2a_ds_servers/artifacts/data/cleaned_dataframes")
        save_dir.mkdir(exist_ok=True, parents=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"cleaned_data_{timestamp}.csv"
        save_path = save_dir / filename
        
        cleaned_df.data.to_csv(save_path, index=False, encoding='utf-8')
        
        return {
            'original_shape': original_df.shape,
            'cleaned_shape': cleaned_df.shape,
            'quality_improvement': round(quality_improvement, 3),
            'final_quality': final_quality['overall_score'],
            'saved_path': str(save_path),
            'cleaning_steps': cleaned_df.metadata.get('cleaning_steps', []),
            'metadata': cleaned_df.metadata
        }
    
    async def _create_cleaning_artifacts(self, results: Dict[str, Any], task_updater: TaskUpdater) -> None:
        """ì •ë¦¬ ì•„í‹°íŒ©íŠ¸ ìƒì„±"""
        
        # ì •ë¦¬ ë³´ê³ ì„œ ìƒì„±
        report = {
            'data_cleaning_report': {
                'timestamp': datetime.now().isoformat(),
                'original_data': {
                    'shape': results['original_shape'],
                },
                'cleaned_data': {
                    'shape': results['cleaned_shape'],
                    'quality_score': results['final_quality'],
                    'saved_path': results['saved_path']
                },
                'improvements': {
                    'quality_gain': results['quality_improvement'],
                    'steps_executed': len(results['cleaning_steps'])
                },
                'cleaning_steps': results['cleaning_steps']
            }
        }
        
        # A2A ì•„í‹°íŒ©íŠ¸ë¡œ ì „ì†¡
        await task_updater.add_artifact(
            parts=[TextPart(text=json.dumps(report, indent=2, ensure_ascii=False))],
            name="data_cleaning_report",
            metadata={"content_type": "application/json", "category": "data_cleaning"}
        )
        
        logger.info("âœ… ë°ì´í„° ì •ë¦¬ ì•„í‹°íŒ©íŠ¸ ìƒì„± ì™„ë£Œ")
    
    def _extract_user_query(self, context: RequestContext) -> str:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì¶”ì¶œ (A2A í‘œì¤€)"""
        user_query = ""
        if context.message and context.message.parts:
            for part in context.message.parts:
                if hasattr(part, 'root') and hasattr(part.root, 'text'):
                    user_query += part.root.text + " "
        return user_query.strip() or "ë°ì´í„°ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”"
    
    async def cancel(self, context: RequestContext, task_updater: TaskUpdater) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        await task_updater.reject()
        logger.info(f"Data Cleaning ì‘ì—… ì·¨ì†Œë¨: {context.context_id}")

# A2A ì„œë²„ ì„¤ì •
def create_data_cleaning_agent_card() -> AgentCard:
    """Data Cleaning Agent Card ìƒì„±"""
    return AgentCard(
        name="Unified Data Cleaning Agent",
        description="ğŸ§¹ LLM First ì§€ëŠ¥í˜• ë°ì´í„° ì •ë¦¬ ì „ë¬¸ê°€ - ë¹ˆ ë°ì´í„° ì™„ë²½ ì²˜ë¦¬, í’ˆì§ˆ ê°œì„ , A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜",
        skills=[
            AgentSkill(
                name="intelligent_data_cleaning",
                description="LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ë°ì´í„° ì •ë¦¬ ì „ëµ ë¶„ì„ ë° ì‹¤í–‰"
            ),
            AgentSkill(
                name="empty_data_handling", 
                description="ë¹ˆ ë°ì´í„° ê°ì§€ ë° ì™„ë²½ ì²˜ë¦¬ (ì£¼ìš” ë¬¸ì œì  í•´ê²°)"
            ),
            AgentSkill(
                name="quality_assessment",
                description="í¬ê´„ì  ë°ì´í„° í’ˆì§ˆ í‰ê°€ ë° ê°œì„ "
            ),
            AgentSkill(
                name="missing_value_processing",
                description="ê²°ì¸¡ê°’ ì§€ëŠ¥í˜• ì²˜ë¦¬ (ìë™ imputation, ì‚­ì œ, ì±„ìš°ê¸°)"
            ),
            AgentSkill(
                name="outlier_detection",
                description="ì´ìƒê°’ íƒì§€ ë° ì²˜ë¦¬ (IQR, Z-score, Isolation Forest)"
            ),
            AgentSkill(
                name="duplicate_removal",
                description="ì¤‘ë³µ ë°ì´í„° ì‹ë³„ ë° ì œê±°"
            ),
            AgentSkill(
                name="data_type_optimization",
                description="ë°ì´í„° íƒ€ì… ìµœì í™” ë° ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ "
            )
        ],
        capabilities=AgentCapabilities(
            supports_streaming=True,
            supports_artifacts=True,
            max_execution_time=300,
            supported_formats=["csv", "excel", "json", "parquet"]
        )
    )

# ë©”ì¸ ì‹¤í–‰ë¶€
if __name__ == "__main__":
    # A2A ì„œë²„ ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    task_store = InMemoryTaskStore()
    executor = UnifiedDataCleaningExecutor()
    agent_card = create_data_cleaning_agent_card()
    
    request_handler = DefaultRequestHandler(
        agent_card=agent_card,
        task_store=task_store,
        agent_executor=executor
    )
    
    app = A2AStarletteApplication(request_handler=request_handler)
    
    # ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Unified Data Cleaning Server ì‹œì‘ - Port 8306")
    logger.info("ğŸ“Š ê¸°ëŠ¥: LLM First ë°ì´í„° ì •ë¦¬ + ë¹ˆ ë°ì´í„° ì™„ë²½ ì²˜ë¦¬")
    logger.info("ğŸ¯ A2A SDK 0.2.9 ì™„ì „ í‘œì¤€ ì¤€ìˆ˜")
    
    uvicorn.run(app, host="0.0.0.0", port=8306) 