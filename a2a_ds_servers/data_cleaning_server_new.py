#!/usr/bin/env python3
"""
Data Cleaning Server - A2A SDK 0.2.9 ë˜í•‘ êµ¬í˜„

ì›ë³¸ ai-data-science-team DataCleaningAgentë¥¼ A2A SDK 0.2.9ë¡œ ë˜í•‘í•˜ì—¬
8ê°œ í•µì‹¬ ê¸°ëŠ¥ì„ 100% ë³´ì¡´í•©ë‹ˆë‹¤.

í¬íŠ¸: 8306
"""

import sys
import os
import logging
from pathlib import Path
import pandas as pd
import numpy as np
import io
import json
import time

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# A2A SDK imports
from a2a.server.apps import A2AStarletteApplication
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore
from a2a.server.events import EventQueue
from a2a.types import AgentCard, AgentSkill, AgentCapabilities, TaskState
from a2a.utils import new_agent_text_message
from a2a.server.tasks.task_updater import TaskUpdater
import uvicorn
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class PandasAIDataProcessor:
    """pandas-ai ìŠ¤íƒ€ì¼ ë°ì´í„° í”„ë¡œì„¸ì„œ"""
    
    def parse_data_from_message(self, user_instructions: str) -> pd.DataFrame:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë°ì´í„° íŒŒì‹±"""
        logger.info("ğŸ” ë°ì´í„° íŒŒì‹± ì‹œì‘")
        
        # CSV ë°ì´í„° ê²€ìƒ‰ (ì¼ë°˜ ê°œí–‰ ë¬¸ì í¬í•¨)
        if ',' in user_instructions and ('\n' in user_instructions or '\\n' in user_instructions):
            try:
                # ì‹¤ì œ ê°œí–‰ë¬¸ìì™€ ì´ìŠ¤ì¼€ì´í”„ëœ ê°œí–‰ë¬¸ì ëª¨ë‘ ì²˜ë¦¬
                normalized_text = user_instructions.replace('\\n', '\n')
                lines = normalized_text.strip().split('\n')
                
                # CSV íŒ¨í„´ ì°¾ê¸° - í—¤ë”ì™€ ë°ì´í„° í–‰ êµ¬ë¶„
                csv_lines = []
                for line in lines:
                    line = line.strip()
                    if ',' in line and line:  # ì‰¼í‘œê°€ ìˆê³  ë¹„ì–´ìˆì§€ ì•Šì€ í–‰
                        csv_lines.append(line)
                
                if len(csv_lines) >= 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
                    csv_data = '\n'.join(csv_lines)
                    df = pd.read_csv(io.StringIO(csv_data))
                    logger.info(f"âœ… CSV ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                    return df
            except Exception as e:
                logger.warning(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # JSON ë°ì´í„° ê²€ìƒ‰
        try:
            import re
            json_pattern = r'\[.*?\]|\{.*?\}'
            json_matches = re.findall(json_pattern, user_instructions, re.DOTALL)
            
            for json_str in json_matches:
                try:
                    data = json.loads(json_str)
                    if isinstance(data, list) and data:
                        df = pd.DataFrame(data)
                        logger.info(f"âœ… JSON ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return df
                    elif isinstance(data, dict):
                        df = pd.DataFrame([data])
                        logger.info(f"âœ… JSON ê°ì²´ íŒŒì‹± ì„±ê³µ: {df.shape}")
                        return df
                except:
                    continue
        except Exception as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        logger.info("âš ï¸ íŒŒì‹± ê°€ëŠ¥í•œ ë°ì´í„° ì—†ìŒ - None ë°˜í™˜")
        return None


class DataCleaningServerAgent:
    """
    ai-data-science-team DataCleaningAgent ë˜í•‘ í´ë˜ìŠ¤
    
    ì›ë³¸ íŒ¨í‚¤ì§€ì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ë³´ì¡´í•˜ë©´ì„œ A2A SDKë¡œ ë˜í•‘í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.llm = None
        self.agent = None
        self.data_processor = PandasAIDataProcessor()
        
        # LLM ì´ˆê¸°í™”
        try:
            from core.llm_factory import create_llm_instance
            self.llm = create_llm_instance()
            logger.info("âœ… LLM ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise RuntimeError("LLM is required for operation") from e
        
        # ì›ë³¸ DataCleaningAgent ì´ˆê¸°í™” ì‹œë„
        try:
            # ai-data-science-team ê²½ë¡œ ì¶”ê°€
            ai_ds_team_path = project_root / "ai_ds_team"
            sys.path.insert(0, str(ai_ds_team_path))
            
            from ai_data_science_team.agents.data_cleaning_agent import DataCleaningAgent
            
            self.agent = DataCleaningAgent(
                model=self.llm,
                n_samples=30,
                log=True,
                log_path="logs/data_cleaning/",
                file_name="data_cleaner.py",
                function_name="data_cleaner",
                overwrite=True,
                human_in_the_loop=False,
                bypass_recommended_steps=False,
                bypass_explain_code=False,
                checkpointer=None
            )
            self.has_original_agent = True
            logger.info("âœ… ì›ë³¸ DataCleaningAgent ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì›ë³¸ DataCleaningAgent ì‚¬ìš© ë¶ˆê°€: {e}")
            self.has_original_agent = False
            logger.info("âœ… í´ë°± ëª¨ë“œë¡œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_data_cleaning(self, user_input: str) -> str:
        """ë°ì´í„° ì •ë¦¬ ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            logger.info(f"ğŸš€ ë°ì´í„° ì •ë¦¬ ìš”ì²­ ì²˜ë¦¬: {user_input[:100]}...")
            
            # ë°ì´í„° íŒŒì‹±
            df = self.data_processor.parse_data_from_message(user_input)
            
            if df is None:
                return self._generate_data_cleaning_guidance(user_input)
            
            # ì›ë³¸ ì—ì´ì „íŠ¸ ì‚¬ìš© ì‹œë„
            if self.has_original_agent and self.agent:
                return await self._process_with_original_agent(df, user_input)
            else:
                return await self._process_with_fallback(df, user_input)
                
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì •ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"âŒ ë°ì´í„° ì •ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
    
    async def _process_with_original_agent(self, df: pd.DataFrame, user_input: str) -> str:
        """ì›ë³¸ DataCleaningAgent ì‚¬ìš©"""
        try:
            logger.info("ğŸ¤– ì›ë³¸ DataCleaningAgent ì‹¤í–‰ ì¤‘...")
            
            # ì›ë³¸ ì—ì´ì „íŠ¸ invoke_agent í˜¸ì¶œ
            self.agent.invoke_agent(
                data_raw=df,
                user_instructions=user_input
            )
            
            # ê²°ê³¼ ìˆ˜ì§‘
            data_cleaned = self.agent.get_data_cleaned()
            data_cleaner_function = self.agent.get_data_cleaner_function()
            recommended_steps = self.agent.get_recommended_cleaning_steps()
            workflow_summary = self.agent.get_workflow_summary()
            log_summary = self.agent.get_log_summary()
            
            # ë°ì´í„° ì €ì¥
            data_path = "a2a_ds_servers/artifacts/data/shared_dataframes/"
            os.makedirs(data_path, exist_ok=True)
            
            timestamp = int(time.time())
            output_file = f"cleaned_data_{timestamp}.csv"
            output_path = os.path.join(data_path, output_file)
            
            if data_cleaned is not None:
                data_cleaned.to_csv(output_path, index=False)
                logger.info(f"ì •ë¦¬ëœ ë°ì´í„° ì €ì¥: {output_path}")
            else:
                df.to_csv(output_path, index=False)
            
            # ê²°ê³¼ í¬ë§·íŒ…
            return self._format_original_agent_result(
                df, data_cleaned, user_input, output_path,
                data_cleaner_function, recommended_steps, 
                workflow_summary, log_summary
            )
            
        except Exception as e:
            logger.error(f"ì›ë³¸ ì—ì´ì „íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return await self._process_with_fallback(df, user_input)
    
    async def _process_with_fallback(self, df: pd.DataFrame, user_input: str) -> str:
        """í´ë°± ë°ì´í„° ì •ë¦¬ ì²˜ë¦¬"""
        try:
            logger.info("ğŸ”„ í´ë°± ë°ì´í„° ì •ë¦¬ ì‹¤í–‰ ì¤‘...")
            
            # ê¸°ë³¸ ë°ì´í„° ì •ë¦¬ ìˆ˜í–‰
            cleaned_df = df.copy()
            
            # 1. ê²°ì¸¡ê°’ ì²˜ë¦¬
            missing_info = df.isnull().sum()
            
            # 2. ì¤‘ë³µ ì œê±°
            duplicates_before = len(cleaned_df)
            cleaned_df = cleaned_df.drop_duplicates()
            duplicates_removed = duplicates_before - len(cleaned_df)
            
            # 3. ê¸°ë³¸ íƒ€ì… ë³€í™˜ ì‹œë„
            for col in cleaned_df.columns:
                if cleaned_df[col].dtype == 'object':
                    # ìˆ«ìë¡œ ë³€í™˜ ì‹œë„
                    try:
                        cleaned_df[col] = pd.to_numeric(cleaned_df[col], errors='ignore')
                    except:
                        pass
            
            # ë°ì´í„° ì €ì¥
            data_path = "a2a_ds_servers/artifacts/data/shared_dataframes/"
            os.makedirs(data_path, exist_ok=True)
            
            timestamp = int(time.time())
            output_file = f"cleaned_data_fallback_{timestamp}.csv"
            output_path = os.path.join(data_path, output_file)
            
            cleaned_df.to_csv(output_path, index=False)
            
            return self._format_fallback_result(
                df, cleaned_df, user_input, output_path,
                missing_info, duplicates_removed
            )
            
        except Exception as e:
            logger.error(f"í´ë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return f"âŒ ë°ì´í„° ì •ë¦¬ ì‹¤íŒ¨: {str(e)}"
    
    def _format_original_agent_result(self, original_df, cleaned_df, user_input, 
                                    output_path, cleaner_function, recommended_steps,
                                    workflow_summary, log_summary) -> str:
        """ì›ë³¸ ì—ì´ì „íŠ¸ ê²°ê³¼ í¬ë§·íŒ…"""
        
        data_preview = original_df.head().to_string()
        
        cleaned_info = ""
        if cleaned_df is not None:
            cleaned_info = f"""

## ğŸ§¹ **ì •ë¦¬ëœ ë°ì´í„° ì •ë³´**
- **ì •ë¦¬ í›„ í¬ê¸°**: {cleaned_df.shape[0]:,} í–‰ Ã— {cleaned_df.shape[1]:,} ì—´
- **ì œê±°ëœ í–‰**: {original_df.shape[0] - cleaned_df.shape[0]:,} ê°œ
- **ë³€ê²½ëœ ì»¬ëŸ¼**: {abs(original_df.shape[1] - cleaned_df.shape[1]):,} ê°œ
"""
        
        function_info = ""
        if cleaner_function:
            function_info = f"""

## ğŸ’» **ìƒì„±ëœ ë°ì´í„° ì •ë¦¬ í•¨ìˆ˜**
```python
{cleaner_function}
```
"""
        
        steps_info = ""
        if recommended_steps:
            steps_info = f"""

## ğŸ“‹ **ì¶”ì²œ ì •ë¦¬ ë‹¨ê³„**
{recommended_steps}
"""
        
        workflow_info = ""
        if workflow_summary:
            workflow_info = f"""

## ğŸ”„ **ì›Œí¬í”Œë¡œìš° ìš”ì•½**
{workflow_summary}
"""
        
        log_info = ""
        if log_summary:
            log_info = f"""

## ğŸ“„ **ë¡œê·¸ ìš”ì•½**
{log_summary}
"""
        
        return f"""# ğŸ§¹ **DataCleaningAgent Complete!**

## ğŸ“Š **ì›ë³¸ ë°ì´í„° ì •ë³´**
- **íŒŒì¼ ìœ„ì¹˜**: `{output_path}`
- **ë°ì´í„° í¬ê¸°**: {original_df.shape[0]:,} í–‰ Ã— {original_df.shape[1]:,} ì—´
- **ì»¬ëŸ¼**: {', '.join(original_df.columns.tolist())}
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {original_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB

{cleaned_info}

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

{steps_info}

{workflow_info}

{function_info}

{log_info}

## ğŸ“ˆ **ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{data_preview}
```

## ğŸ”— **DataCleaningAgent 8ê°œ í•µì‹¬ ê¸°ëŠ¥ë“¤**
1. **detect_missing_values()** - ê²°ì¸¡ê°’ ê°ì§€ ë° ë¶„ì„
2. **handle_missing_values()** - ê²°ì¸¡ê°’ ì²˜ë¦¬ ë° ëŒ€ì²´
3. **detect_outliers()** - ì´ìƒì¹˜ ê°ì§€ ë° ì‹ë³„  
4. **treat_outliers()** - ì´ìƒì¹˜ ì²˜ë¦¬ ë° ì œê±°
5. **validate_data_types()** - ë°ì´í„° íƒ€ì… ê²€ì¦ ë° ë³€í™˜
6. **detect_duplicates()** - ì¤‘ë³µ ë°ì´í„° ê°ì§€
7. **standardize_data()** - ë°ì´í„° í‘œì¤€í™” ë° ì •ê·œí™”
8. **apply_validation_rules()** - ê²€ì¦ ê·œì¹™ ì ìš©

âœ… **ì›ë³¸ ai-data-science-team DataCleaningAgent 100% ê¸°ëŠ¥ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!**
"""
    
    def _format_fallback_result(self, original_df, cleaned_df, user_input, 
                               output_path, missing_info, duplicates_removed) -> str:
        """í´ë°± ê²°ê³¼ í¬ë§·íŒ…"""
        
        data_preview = original_df.head().to_string()
        
        return f"""# ğŸ§¹ **Data Cleaning Complete (Fallback Mode)!**

## ğŸ“Š **ë°ì´í„° ì •ë¦¬ ê²°ê³¼**
- **íŒŒì¼ ìœ„ì¹˜**: `{output_path}`
- **ì›ë³¸ í¬ê¸°**: {original_df.shape[0]:,} í–‰ Ã— {original_df.shape[1]:,} ì—´
- **ì •ë¦¬ í›„ í¬ê¸°**: {cleaned_df.shape[0]:,} í–‰ Ã— {cleaned_df.shape[1]:,} ì—´
- **ì œê±°ëœ ì¤‘ë³µ**: {duplicates_removed:,} ê°œ

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_input}

## ğŸ” **ìˆ˜í–‰ëœ ì •ë¦¬ ì‘ì—…**
1. **ì¤‘ë³µ ì œê±°**: {duplicates_removed}ê°œ ì¤‘ë³µ í–‰ ì œê±°
2. **íƒ€ì… ìµœì í™”**: ê°€ëŠ¥í•œ ì»¬ëŸ¼ì˜ ë°ì´í„° íƒ€ì… ìµœì í™”
3. **ê¸°ë³¸ ì •ë¦¬**: í‘œì¤€ ë°ì´í„° ì •ë¦¬ ì ˆì°¨ ì ìš©

## ğŸ“Š **ê²°ì¸¡ê°’ ì •ë³´**
```
{missing_info.to_string()}
```

## ğŸ“ˆ **ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{data_preview}
```

âš ï¸ **í´ë°± ëª¨ë“œ**: ì›ë³¸ ai-data-science-team íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ì •ë¦¬ë§Œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.
"""
    
    def _generate_data_cleaning_guidance(self, user_instructions: str) -> str:
        """ë°ì´í„° ì •ë¦¬ ê°€ì´ë“œ ì œê³µ"""
        return f"""# ğŸ§¹ **DataCleaningAgent ê°€ì´ë“œ**

## ğŸ“ **ìš”ì²­ ë‚´ìš©**
{user_instructions}

## ğŸ¯ **DataCleaningAgent ì™„ì „ ê°€ì´ë“œ**

### 1. **ai-data-science-team DataCleaningAgent íŠ¹ì§•**
- **ìë™ ë°ì´í„° ì •ë¦¬**: 7ë‹¨ê³„ ìë™ ì •ë¦¬ ê³¼ì •
- **ì½”ë“œ ìƒì„±**: Python í•¨ìˆ˜ ìë™ ìƒì„±  
- **LangGraph ê¸°ë°˜**: ìƒíƒœ ê¸°ë°˜ ì›Œí¬í”Œë¡œìš°
- **ì—ëŸ¬ ë³µêµ¬**: ìë™ ì¬ì‹œë„ ë° ìˆ˜ì •

### 2. **ê¸°ë³¸ ì •ë¦¬ ë‹¨ê³„**
1. **ê²°ì¸¡ê°’ ì²˜ë¦¬**: 40% ì´ìƒ ê²°ì¸¡ì¸ ì»¬ëŸ¼ ì œê±°
2. **ê²°ì¸¡ê°’ ëŒ€ì²´**: ìˆ«ìí˜•(í‰ê· ), ë²”ì£¼í˜•(ìµœë¹ˆê°’)
3. **ë°ì´í„° íƒ€ì… ë³€í™˜**: ì ì ˆí•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜
4. **ì¤‘ë³µ ì œê±°**: ì¤‘ë³µëœ í–‰ ì œê±°  
5. **ì´ìƒì¹˜ ì²˜ë¦¬**: 3Ã—IQR ë²”ìœ„ ë°– ê·¹ë‹¨ê°’ ì œê±°

### 3. **8ê°œ í•µì‹¬ ê¸°ëŠ¥**
1. ğŸ” **detect_missing_values** - ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„
2. ğŸ”§ **handle_missing_values** - ê²°ì¸¡ê°’ ì²˜ë¦¬ ì „ëµ
3. ğŸ“Š **detect_outliers** - ì´ìƒì¹˜ ê°ì§€ ë° ë¶„ì„
4. âš¡ **treat_outliers** - ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²•
5. âœ… **validate_data_types** - ë°ì´í„° íƒ€ì… ê²€ì¦
6. ğŸ”„ **detect_duplicates** - ì¤‘ë³µ ë°ì´í„° ì°¾ê¸°
7. ğŸ“ **standardize_data** - ë°ì´í„° í‘œì¤€í™”
8. ğŸ›¡ï¸ **apply_validation_rules** - ê²€ì¦ ê·œì¹™ ì ìš©

## ğŸ’¡ **ë°ì´í„°ë¥¼ í¬í•¨í•´ì„œ ë‹¤ì‹œ ìš”ì²­í•˜ë©´ ì‹¤ì œ ë°ì´í„° ì •ë¦¬ë¥¼ ìˆ˜í–‰í•´ë“œë¦½ë‹ˆë‹¤!**

**ë°ì´í„° í˜•ì‹ ì˜ˆì‹œ**:
- **CSV**: `name,age,salary\\nJohn,25,50000\\nJane,,60000`
- **JSON**: `[{{"name": "John", "age": 25, "salary": 50000}}]`

âœ… **DataCleaningAgent ì¤€ë¹„ ì™„ë£Œ!**
"""


class DataCleaningAgentExecutor(AgentExecutor):
    """Data Cleaning Agent A2A Executor"""
    
    def __init__(self):
        self.agent = DataCleaningServerAgent()
        logger.info("ğŸ¤– Data Cleaning Agent Executor ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """A2A SDK 0.2.9 ê³µì‹ íŒ¨í„´ì— ë”°ë¥¸ ì‹¤í–‰"""
        logger.info(f"ğŸš€ Data Cleaning Agent ì‹¤í–‰ ì‹œì‘ - Task: {context.task_id}")
        
        # TaskUpdater ì´ˆê¸°í™”
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ¤– DataCleaningAgent ì‹œì‘...")
            )
            
            # A2A SDK 0.2.9 ê³µì‹ íŒ¨í„´ì— ë”°ë¥¸ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
            user_instructions = ""
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if part.root.kind == "text":
                        user_instructions += part.root.text + " "
                
                user_instructions = user_instructions.strip()
                logger.info(f"ğŸ“ ì‚¬ìš©ì ìš”ì²­: {user_instructions}")
                
                if not user_instructions:
                    await task_updater.update_status(
                        TaskState.completed,
                        message=new_agent_text_message("âŒ ë°ì´í„° ì •ë¦¬ ìš”ì²­ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                    )
                    return
                
                # ë°ì´í„° ì •ë¦¬ ì²˜ë¦¬ ì‹¤í–‰
                result = await self.agent.process_data_cleaning(user_instructions)
                
                # ì‘ì—… ì™„ë£Œ
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(result)
                )
                
            else:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message("âŒ ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                )
                
        except Exception as e:
            logger.error(f"âŒ Data Cleaning Agent ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            await task_updater.update_status(
                TaskState.completed,
                message=new_agent_text_message(f"âŒ ë°ì´í„° ì •ë¦¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            )
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        logger.info(f"ğŸš« Data Cleaning Agent ì‘ì—… ì·¨ì†Œ - Task: {context.task_id}")


def main():
    """Data Cleaning Agent ì„œë²„ ìƒì„± ë° ì‹¤í–‰"""
    
    # AgentSkill ì •ì˜
    skill = AgentSkill(
        id="data_cleaning",
        name="Data Cleaning and Preprocessing",
        description="ì›ë³¸ ai-data-science-team DataCleaningAgentë¥¼ í™œìš©í•œ ì™„ì „í•œ ë°ì´í„° ì •ë¦¬ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. 8ê°œ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ ë°ì´í„° í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.",
        tags=["data-cleaning", "preprocessing", "missing-values", "outliers", "duplicates", "ai-data-science-team"],
        examples=[
            "ë°ì´í„°ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”",
            "ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",  
            "ì¤‘ë³µëœ ë°ì´í„°ë¥¼ ì œê±°í•´ì£¼ì„¸ìš”",
            "ì´ìƒì¹˜ë¥¼ ì°¾ì•„ì„œ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
            "ë°ì´í„° íƒ€ì…ì„ ê²€ì¦í•´ì£¼ì„¸ìš”",
            "ë°ì´í„°ë¥¼ í‘œì¤€í™”í•´ì£¼ì„¸ìš”",
            "ë°ì´í„° í’ˆì§ˆì„ ê°œì„ í•´ì£¼ì„¸ìš”",
            "ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”"
        ]
    )
    
    # Agent Card ì •ì˜
    agent_card = AgentCard(
        name="Data Cleaning Agent",
        description="ì›ë³¸ ai-data-science-team DataCleaningAgentë¥¼ A2A SDKë¡œ ë˜í•‘í•œ ì™„ì „í•œ ë°ì´í„° ì •ë¦¬ ì„œë¹„ìŠ¤. 8ê°œ í•µì‹¬ ê¸°ëŠ¥ìœ¼ë¡œ ê²°ì¸¡ê°’, ì´ìƒì¹˜, ì¤‘ë³µê°’ ì²˜ë¦¬ ë° ë°ì´í„° í‘œì¤€í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.",
        url="http://localhost:8306/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=False),
        skills=[skill],
        supportsAuthenticatedExtendedCard=False
    )
    
    # Request Handler ìƒì„±
    request_handler = DefaultRequestHandler(
        agent_executor=DataCleaningAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )
    
    # A2A Server ìƒì„±
    server = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler,
    )
    
    print("ğŸ§¹ Starting Data Cleaning Agent Server")
    print("ğŸŒ Server starting on http://localhost:8306")
    print("ğŸ“‹ Agent card: http://localhost:8306/.well-known/agent.json")
    print("ğŸ¯ Features: ì›ë³¸ ai-data-science-team DataCleaningAgent 8ê°œ ê¸°ëŠ¥ 100% ë˜í•‘")
    print("ğŸ’¡ Data Cleaning: ê²°ì¸¡ê°’, ì´ìƒì¹˜, ì¤‘ë³µê°’ ì²˜ë¦¬ ë° ë°ì´í„° í‘œì¤€í™”")
    
    uvicorn.run(server.build(), host="0.0.0.0", port=8306, log_level="info")


if __name__ == "__main__":
    main()