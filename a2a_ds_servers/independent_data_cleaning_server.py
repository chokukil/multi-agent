#!/usr/bin/env python3
"""
Independent Data Cleaning Server
ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° í´ë¦¬ë‹ ì„œë²„ - ì™¸ë¶€ ëª¨ë“ˆ ì˜ì¡´ì„± ì—†ìŒ
Port: 8320
"""

import asyncio
import sys
import os
import json
import logging
import pandas as pd
import numpy as np
import io
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime

# A2A ê¸°ë³¸ importsë§Œ ì‚¬ìš©
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers.default_request_handler import DefaultRequestHandler
from a2a.server.tasks.inmemory_task_store import InMemoryTaskStore
from a2a.server.tasks.task_updater import TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events.event_queue import EventQueue
from a2a.types import TextPart, TaskState, AgentCard, AgentSkill, AgentCapabilities
from a2a.utils import new_agent_text_message
import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class IndependentDataProcessor:
    """ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        self.current_dataframe = None
        
    def parse_data_from_message(self, user_message: str) -> Optional[pd.DataFrame]:
        """ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ ë°ì´í„°ë¥¼ íŒŒì‹±"""
        logger.info("ğŸ“Š ë©”ì‹œì§€ì—ì„œ ë°ì´í„° íŒŒì‹± ì‹œì‘...")
        
        # CSV ë°ì´í„° íŒŒì‹±
        df = self._parse_csv_data(user_message)
        if df is not None:
            return df
            
        # JSON ë°ì´í„° íŒŒì‹±
        df = self._parse_json_data(user_message)
        if df is not None:
            return df
        
        # ìƒ˜í”Œ ë°ì´í„° ìš”ì²­ í™•ì¸
        if self._is_sample_request(user_message):
            return self._create_sample_data()
        
        return None
    
    def _parse_csv_data(self, message: str) -> Optional[pd.DataFrame]:
        """CSV í˜•íƒœ ë°ì´í„° íŒŒì‹±"""
        try:
            lines = message.split('\n')
            csv_lines = [line.strip() for line in lines if ',' in line and len(line.split(',')) >= 2]
            
            if len(csv_lines) >= 2:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
                csv_content = '\n'.join(csv_lines)
                df = pd.read_csv(io.StringIO(csv_content))
                logger.info(f"âœ… CSV ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                return df
        except Exception as e:
            logger.warning(f"CSV íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None
    
    def _parse_json_data(self, message: str) -> Optional[pd.DataFrame]:
        """JSON í˜•íƒœ ë°ì´í„° íŒŒì‹±"""
        try:
            json_start = message.find('{')
            json_end = message.rfind('}') + 1
            
            if json_start != -1 and json_end > json_start:
                json_content = message[json_start:json_end]
                data = json.loads(json_content)
                
                if isinstance(data, list):
                    df = pd.DataFrame(data)
                elif isinstance(data, dict):
                    df = pd.DataFrame([data])
                else:
                    return None
                    
                logger.info(f"âœ… JSON ë°ì´í„° íŒŒì‹± ì„±ê³µ: {df.shape}")
                return df
        except Exception as e:
            logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None
    
    def _is_sample_request(self, message: str) -> bool:
        """ìƒ˜í”Œ ë°ì´í„° ìš”ì²­ì¸ì§€ í™•ì¸"""
        keywords = ["ìƒ˜í”Œ", "í…ŒìŠ¤íŠ¸", "example", "demo", "sample", "test"]
        return any(keyword in message.lower() for keyword in keywords)
    
    def _create_sample_data(self) -> pd.DataFrame:
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        logger.info("ğŸ”§ ìƒ˜í”Œ ë°ì´í„° ìƒì„±...")
        
        np.random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼
        
        data = {
            'id': range(1, 101),
            'name': [f'User_{i}' for i in range(1, 101)],
            'age': np.random.randint(18, 80, 100),
            'income': np.random.randint(20000, 150000, 100),
            'category': np.random.choice(['A', 'B', 'C', 'D'], 100),
            'score': np.random.normal(75, 15, 100)
        }
        
        # ì˜ë„ì ìœ¼ë¡œ ê²°ì¸¡ê°’ê³¼ ì´ìƒê°’ ì¶”ê°€
        df = pd.DataFrame(data)
        
        # ê²°ì¸¡ê°’ ì¶”ê°€
        missing_indices = np.random.choice(df.index, 15, replace=False)
        df.loc[missing_indices[:5], 'age'] = np.nan
        df.loc[missing_indices[5:10], 'income'] = np.nan
        df.loc[missing_indices[10:], 'category'] = np.nan
        
        # ì´ìƒê°’ ì¶”ê°€
        df.loc[0, 'age'] = 200  # ì´ìƒê°’
        df.loc[1, 'income'] = 1000000  # ì´ìƒê°’
        df.loc[2, 'score'] = -50  # ì´ìƒê°’
        
        # ì¤‘ë³µ í–‰ ì¶”ê°€
        df = pd.concat([df, df.iloc[:3]], ignore_index=True)
        
        logger.info(f"âœ… ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {df.shape}")
        return df

class IndependentDataCleaner:
    """ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° í´ë¦¬ë„ˆ"""
    
    def __init__(self):
        self.original_data = None
        self.cleaned_data = None
        self.cleaning_report = []
    
    def clean_data(self, df: pd.DataFrame, user_instructions: str = "") -> Dict[str, Any]:
        """ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰"""
        self.original_data = df.copy()
        self.cleaned_data = df.copy()
        self.cleaning_report = []
        
        logger.info(f"ğŸ§¹ ë°ì´í„° í´ë¦¬ë‹ ì‹œì‘: {df.shape}")
        
        # ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        original_shape = df.shape
        original_memory = df.memory_usage(deep=True).sum() / 1024**2  # MB
        
        # í´ë¦¬ë‹ ë‹¨ê³„ ì‹¤í–‰
        self._remove_high_missing_columns()
        self._handle_missing_values()
        self._optimize_data_types()
        self._remove_duplicates()
        
        # ì´ìƒê°’ ì²˜ë¦¬ (ì‚¬ìš©ìê°€ ê¸ˆì§€í•˜ì§€ ì•Šì€ ê²½ìš°)
        if "outlier" not in user_instructions.lower() and "ì´ìƒê°’" not in user_instructions:
            self._handle_outliers()
        
        # ìµœì¢… ê²°ê³¼ ê³„ì‚°
        final_shape = self.cleaned_data.shape
        final_memory = self.cleaned_data.memory_usage(deep=True).sum() / 1024**2  # MB
        quality_score = self._calculate_quality_score()
        
        return {
            'original_data': self.original_data,
            'cleaned_data': self.cleaned_data,
            'original_shape': original_shape,
            'final_shape': final_shape,
            'memory_saved': original_memory - final_memory,
            'cleaning_report': self.cleaning_report,
            'quality_score': quality_score
        }
    
    def _remove_high_missing_columns(self):
        """40% ì´ìƒ ê²°ì¸¡ê°’ì´ ìˆëŠ” ì»¬ëŸ¼ ì œê±°"""
        missing_ratios = self.cleaned_data.isnull().mean()
        high_missing_cols = missing_ratios[missing_ratios > 0.4].index.tolist()
        
        if high_missing_cols:
            self.cleaned_data = self.cleaned_data.drop(columns=high_missing_cols)
            self.cleaning_report.append(f"âœ… 40% ì´ìƒ ê²°ì¸¡ê°’ ì»¬ëŸ¼ ì œê±°: {high_missing_cols}")
    
    def _handle_missing_values(self):
        """ê²°ì¸¡ê°’ ì²˜ë¦¬"""
        for col in self.cleaned_data.columns:
            missing_count = self.cleaned_data[col].isnull().sum()
            if missing_count > 0:
                if self.cleaned_data[col].dtype in ['int64', 'float64']:
                    # ìˆ«ìí˜•: í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
                    mean_val = self.cleaned_data[col].mean()
                    self.cleaned_data[col].fillna(mean_val, inplace=True)
                    self.cleaning_report.append(f"âœ… '{col}' ê²°ì¸¡ê°’ {missing_count}ê°œë¥¼ í‰ê· ê°’({mean_val:.2f})ìœ¼ë¡œ ëŒ€ì²´")
                else:
                    # ë²”ì£¼í˜•: ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
                    mode_val = self.cleaned_data[col].mode()
                    if not mode_val.empty:
                        self.cleaned_data[col].fillna(mode_val.iloc[0], inplace=True)
                        self.cleaning_report.append(f"âœ… '{col}' ê²°ì¸¡ê°’ {missing_count}ê°œë¥¼ ìµœë¹ˆê°’('{mode_val.iloc[0]}')ìœ¼ë¡œ ëŒ€ì²´")
    
    def _optimize_data_types(self):
        """ë°ì´í„° íƒ€ì… ìµœì í™”"""
        optimized_count = 0
        
        for col in self.cleaned_data.columns:
            original_dtype = self.cleaned_data[col].dtype
            
            if self.cleaned_data[col].dtype == 'object':
                # ë¬¸ìì—´ ì •ê·œí™”
                self.cleaned_data[col] = self.cleaned_data[col].astype(str).str.strip()
            elif self.cleaned_data[col].dtype == 'int64':
                # ì •ìˆ˜í˜• ìµœì í™”
                col_min, col_max = self.cleaned_data[col].min(), self.cleaned_data[col].max()
                if col_min >= 0 and col_max < 255:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('uint8')
                    optimized_count += 1
                elif col_min >= -128 and col_max < 127:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('int8')
                    optimized_count += 1
                elif col_min >= -32768 and col_max < 32767:
                    self.cleaned_data[col] = self.cleaned_data[col].astype('int16')
                    optimized_count += 1
        
        if optimized_count > 0:
            self.cleaning_report.append(f"âœ… ë°ì´í„° íƒ€ì… ìµœì í™”: {optimized_count}ê°œ ì»¬ëŸ¼")
    
    def _remove_duplicates(self):
        """ì¤‘ë³µ í–‰ ì œê±°"""
        duplicates_count = self.cleaned_data.duplicated().sum()
        if duplicates_count > 0:
            self.cleaned_data = self.cleaned_data.drop_duplicates()
            self.cleaning_report.append(f"âœ… ì¤‘ë³µ í–‰ {duplicates_count}ê°œ ì œê±°")
    
    def _handle_outliers(self):
        """IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒê°’ ì²˜ë¦¬"""
        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            Q1 = self.cleaned_data[col].quantile(0.25)
            Q3 = self.cleaned_data[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            
            outliers_mask = (self.cleaned_data[col] < lower_bound) | (self.cleaned_data[col] > upper_bound)
            outliers_count = outliers_mask.sum()
            
            if outliers_count > 0:
                # ì´ìƒê°’ì„ ê²½ê³„ê°’ìœ¼ë¡œ í´ë¦¬í•‘
                self.cleaned_data[col] = self.cleaned_data[col].clip(lower_bound, upper_bound)
                self.cleaning_report.append(f"âœ… '{col}' ì´ìƒê°’ {outliers_count}ê°œ ì²˜ë¦¬ (IQR 1.5ë°° ê¸°ì¤€)")
    
    def _calculate_quality_score(self) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-100)"""
        score = 100.0
        
        # ê²°ì¸¡ê°’ ë¹„ìœ¨
        total_cells = self.cleaned_data.shape[0] * self.cleaned_data.shape[1]
        missing_ratio = self.cleaned_data.isnull().sum().sum() / total_cells if total_cells > 0 else 0
        score -= missing_ratio * 40
        
        # ì¤‘ë³µ ë¹„ìœ¨
        duplicate_ratio = self.cleaned_data.duplicated().sum() / self.cleaned_data.shape[0] if self.cleaned_data.shape[0] > 0 else 0
        score -= duplicate_ratio * 30
        
        # ë°ì´í„° íƒ€ì… ì¼ê´€ì„±
        if len(self.cleaned_data.columns) > 0:
            numeric_ratio = len(self.cleaned_data.select_dtypes(include=[np.number]).columns) / len(self.cleaned_data.columns)
            score += numeric_ratio * 10
        
        return max(0, min(100, score))

class DataCleaningAgentExecutor(AgentExecutor):
    """ë…ë¦½ì ì¸ ë°ì´í„° í´ë¦¬ë‹ ì—ì´ì „íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.data_processor = IndependentDataProcessor()
        self.data_cleaner = IndependentDataCleaner()
        logger.info("ğŸ§¹ Independent DataCleaningAgent ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰"""
        logger.info(f"ğŸš€ DataCleaningAgent ì‹¤í–‰ ì‹œì‘ - Task: {context.task_id}")
        
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        
        try:
            await task_updater.submit()
            await task_updater.start_work()
            
            await task_updater.update_status(
                TaskState.working,
                message=new_agent_text_message("ğŸ§¹ ë…ë¦½ì ì¸ ë°ì´í„° í´ë¦¬ë‹ ì—ì´ì „íŠ¸ ì‹œì‘...")
            )
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
            user_instructions = ""
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if part.root.kind == "text":
                        user_instructions += part.root.text + " "
                
                user_instructions = user_instructions.strip()
                logger.info(f"ğŸ“ ì‚¬ìš©ì ìš”ì²­: {user_instructions}")
                
                # ë°ì´í„° íŒŒì‹±
                await task_updater.update_status(
                    TaskState.working,
                    message=new_agent_text_message("ğŸ“Š ë°ì´í„° ë¶„ì„ ì¤‘...")
                )
                
                df = self.data_processor.parse_data_from_message(user_instructions)
                
                if df is not None and not df.empty:
                    # ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰
                    await task_updater.update_status(
                        TaskState.working,
                        message=new_agent_text_message("ğŸ§¹ ë°ì´í„° í´ë¦¬ë‹ ì‹¤í–‰ ì¤‘...")
                    )
                    
                    cleaning_results = self.data_cleaner.clean_data(df, user_instructions)
                    
                    # ê²°ê³¼ ì €ì¥
                    output_dir = Path("a2a_ds_servers/artifacts/cleaned_data")
                    output_dir.mkdir(parents=True, exist_ok=True)
                    output_path = output_dir / f"cleaned_data_{context.task_id}.csv"
                    
                    cleaning_results['cleaned_data'].to_csv(output_path, index=False)
                    
                    # ì‘ë‹µ ìƒì„±
                    result = self._generate_response(cleaning_results, user_instructions, str(output_path))
                    
                else:
                    result = self._generate_no_data_response(user_instructions)
                
                # ìµœì¢… ì‘ë‹µ
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message(result)
                )
            else:
                await task_updater.update_status(
                    TaskState.completed,
                    message=new_agent_text_message("âŒ ë°ì´í„° í´ë¦¬ë‹ ìš”ì²­ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                )
                
        except Exception as e:
            logger.error(f"âŒ DataCleaningAgent ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            await task_updater.update_status(
                TaskState.failed,
                message=new_agent_text_message(f"ë°ì´í„° í´ë¦¬ë‹ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            )
    
    def _generate_response(self, results: Dict[str, Any], user_instructions: str, output_path: str) -> str:
        """í´ë¦¬ë‹ ê²°ê³¼ ì‘ë‹µ ìƒì„±"""
        return f"""# ğŸ§¹ **ë…ë¦½ì ì¸ ë°ì´í„° í´ë¦¬ë‹ ì™„ë£Œ**

## ğŸ“Š **í´ë¦¬ë‹ ê²°ê³¼**
- **ì›ë³¸ ë°ì´í„°**: {results['original_shape'][0]:,}í–‰ Ã— {results['original_shape'][1]}ì—´
- **ì •ë¦¬ í›„**: {results['final_shape'][0]:,}í–‰ Ã— {results['final_shape'][1]}ì—´
- **ë©”ëª¨ë¦¬ ì ˆì•½**: {results['memory_saved']:.2f} MB
- **í’ˆì§ˆ ì ìˆ˜**: {results['quality_score']:.1f}/100

## ğŸ”§ **ìˆ˜í–‰ëœ ì‘ì—…**
{chr(10).join(f"- {report}" for report in results['cleaning_report'])}

## ğŸ” **ì •ë¦¬ëœ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°**
```
{results['cleaned_data'].head().to_string()}
```

## ğŸ“ˆ **ë°ì´í„° í†µê³„ ìš”ì•½**
```
{results['cleaned_data'].describe().to_string()}
```

## ğŸ“ **ì €ì¥ ê²½ë¡œ**
`{output_path}`

---
**ğŸ’¬ ì‚¬ìš©ì ìš”ì²­**: {user_instructions}
**ğŸ¯ ì²˜ë¦¬ ë°©ì‹**: ë…ë¦½ì ì¸ ë°ì´í„° í´ë¦¬ë‹ (ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ)
**ğŸ•’ ì²˜ë¦¬ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
    
    def _generate_no_data_response(self, user_instructions: str) -> str:
        """ë°ì´í„° ì—†ìŒ ì‘ë‹µ ìƒì„±"""
        return f"""# âŒ **ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤**

**í•´ê²° ë°©ë²•**:
1. **CSV í˜•íƒœë¡œ ë°ì´í„° í¬í•¨**:
   ```
   name,age,income
   John,25,50000
   Jane,30,60000
   ```

2. **JSON í˜•íƒœë¡œ ë°ì´í„° í¬í•¨**:
   ```json
   {{"name": "John", "age": 25, "income": 50000}}
   ```

3. **ìƒ˜í”Œ ë°ì´í„° ìš”ì²­**: "ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•´ì£¼ì„¸ìš”"

**ìš”ì²­**: {user_instructions}
"""
    
    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """ì‘ì—… ì·¨ì†Œ"""
        task_updater = TaskUpdater(event_queue, context.task_id, context.context_id)
        await task_updater.reject()
        logger.info(f"DataCleaningAgent ì‘ì—… ì·¨ì†Œ: {context.task_id}")

def main():
    """A2A ì„œë²„ ìƒì„± ë° ì‹¤í–‰"""
    
    # AgentSkill ì •ì˜
    skill = AgentSkill(
        id="independent_data_cleaning",
        name="Independent Data Cleaning",
        description="ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° í´ë¦¬ë‹ ì„œë¹„ìŠ¤ - ì™¸ë¶€ ëª¨ë“ˆ ì˜ì¡´ì„± ì—†ìŒ",
        tags=["data-cleaning", "independent", "preprocessing", "pandas"],
        examples=[
            "ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•´ì£¼ì„¸ìš”",
            "ê²°ì¸¡ê°’ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”",
            "ì´ìƒê°’ ì œê±° ì—†ì´ ë°ì´í„°ë¥¼ ì •ë¦¬í•´ì£¼ì„¸ìš”",
            "ì¤‘ë³µ ë°ì´í„°ë¥¼ ì œê±°í•˜ê³  í’ˆì§ˆì„ ê°œì„ í•´ì£¼ì„¸ìš”"
        ]
    )
    
    # Agent Card ì •ì˜
    agent_card = AgentCard(
        name="Independent Data Cleaning Agent",
        description="ì™„ì „íˆ ë…ë¦½ì ì¸ ë°ì´í„° í´ë¦¬ë‹ ì „ë¬¸ê°€ - ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ",
        url="http://localhost:8320/",
        version="1.0.0",
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        capabilities=AgentCapabilities(streaming=False),
        skills=[skill],
        supportsAuthenticatedExtendedCard=False
    )
    
    # Request Handler ìƒì„±
    request_handler = DefaultRequestHandler(
        agent_executor=DataCleaningAgentExecutor(),
        task_store=InMemoryTaskStore(),
    )
    
    # A2A Server ìƒì„±
    server = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler,
    )
    
    print("ğŸ§¹ Starting Independent Data Cleaning Agent Server")
    print("ğŸŒ Server starting on http://localhost:8320")
    print("ğŸ“‹ Agent card: http://localhost:8320/.well-known/agent.json")
    print("âœ¨ Features: ì™„ì „íˆ ë…ë¦½ì ì¸ êµ¬í˜„ (ì™¸ë¶€ ì˜ì¡´ì„± ì—†ìŒ)")
    
    uvicorn.run(server.build(), host="0.0.0.0", port=8320, log_level="info")

if __name__ == "__main__":
    main()