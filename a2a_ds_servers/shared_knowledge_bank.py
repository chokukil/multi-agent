#!/usr/bin/env python3
"""
ğŸ§  Shared Knowledge Bank - A2A SDK 0.2.9 í‘œì¤€ ì„œë²„
Port: 8602

Context Engineering MEMORY ë ˆì´ì–´ì˜ í•µì‹¬ êµ¬í˜„
- ì—ì´ì „íŠ¸ ê°„ ê³µìœ  ì§€ì‹ ì €ì¥ ë° ê²€ìƒ‰
- í˜‘ì—… íŒ¨í„´ í•™ìŠµ ë° ìµœì í™”
- ì„ë² ë”© ê¸°ë°˜ ê³ ê¸‰ ê²€ìƒ‰ ì‹œìŠ¤í…œ
- ì§€ì‹ ê·¸ë˜í”„ êµ¬ì¶• ë° ê´€ë¦¬
- ì‚¬ìš©ìë³„ ì—ì´ì „íŠ¸ ì¡°í•© ì„ í˜¸ë„ í•™ìŠµ
"""

import asyncio
import json
import logging
import os
import pickle
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple
from dataclasses import dataclass, asdict
from enum import Enum

import numpy as np
import pandas as pd
import networkx as nx
import uvicorn

# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© êµ¬í˜„ (sentence-transformers ëŒ€ì‹ )
import hashlib
import re
from collections import Counter
import math

# A2A SDK 0.2.9 í‘œì¤€ ì„í¬íŠ¸
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.tasks import InMemoryTaskStore, TaskUpdater
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.types import (
    AgentCard,
    AgentSkill,
    AgentCapabilities,
    TaskState,
    TextPart,
    Part
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
KNOWLEDGE_BASE_DIR = Path("a2a_ds_servers/artifacts/knowledge_base")
KNOWLEDGE_BASE_DIR.mkdir(parents=True, exist_ok=True)

EMBEDDINGS_DIR = KNOWLEDGE_BASE_DIR / "embeddings"
EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)

GRAPH_DIR = KNOWLEDGE_BASE_DIR / "graphs"
GRAPH_DIR.mkdir(parents=True, exist_ok=True)

PATTERNS_DIR = KNOWLEDGE_BASE_DIR / "patterns"
PATTERNS_DIR.mkdir(parents=True, exist_ok=True)

class SimpleTextEmbedding:
    """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì„ë² ë”© êµ¬í˜„"""
    
    def __init__(self, vector_size=100):
        self.vector_size = vector_size
        self.vocab = {}
        self.vocab_size = 0
    
    def _preprocess_text(self, text):
        """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ì²˜ë¦¬
        text = text.lower()
        text = re.sub(r'[^a-zA-Z0-9ê°€-í£\s]', '', text)
        words = text.split()
        return words
    
    def _get_word_vector(self, word):
        """ë‹¨ì–´ ë²¡í„° ìƒì„±"""
        if word not in self.vocab:
            # ë‹¨ì–´ í•´ì‹œ ê¸°ë°˜ ë²¡í„° ìƒì„±
            hash_value = int(hashlib.md5(word.encode()).hexdigest(), 16)
            np.random.seed(hash_value % (2**32))
            vector = np.random.randn(self.vector_size)
            self.vocab[word] = vector
            self.vocab_size += 1
        return self.vocab[word]
    
    def encode(self, text):
        """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if isinstance(text, str):
            words = self._preprocess_text(text)
            if not words:
                return np.zeros(self.vector_size)
            
            # ë‹¨ì–´ ë²¡í„°ë“¤ì˜ í‰ê·  ê³„ì‚°
            word_vectors = [self._get_word_vector(word) for word in words]
            return np.mean(word_vectors, axis=0)
        elif isinstance(text, list):
            return [self.encode(t) for t in text]
        else:
            return np.zeros(self.vector_size)

def cosine_similarity_simple(vec1, vec2):
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
    vec1 = np.array(vec1)
    vec2 = np.array(vec2)
    
    dot_product = np.dot(vec1, vec2)
    norm_a = np.linalg.norm(vec1)
    norm_b = np.linalg.norm(vec2)
    
    if norm_a == 0 or norm_b == 0:
        return 0.0
    
    return dot_product / (norm_a * norm_b)

class KnowledgeType(Enum):
    """ì§€ì‹ íƒ€ì… ì •ì˜"""
    AGENT_EXPERTISE = "agent_expertise"
    COLLABORATION_PATTERN = "collaboration_pattern"
    USER_PREFERENCE = "user_preference"
    MESSAGE_OPTIMIZATION = "message_optimization"
    CROSS_AGENT_INSIGHT = "cross_agent_insight"
    DOMAIN_KNOWLEDGE = "domain_knowledge"
    WORKFLOW_TEMPLATE = "workflow_template"
    ERROR_SOLUTION = "error_solution"

@dataclass
class KnowledgeEntry:
    """ì§€ì‹ í•­ëª© ì •ì˜"""
    id: str
    type: KnowledgeType
    title: str
    content: str
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None
    related_entries: List[str] = None
    created_at: datetime = None
    updated_at: datetime = None
    usage_count: int = 0
    relevance_score: float = 0.0
    
    def __post_init__(self):
        if self.created_at is None:
            self.created_at = datetime.now()
        if self.updated_at is None:
            self.updated_at = datetime.now()
        if self.related_entries is None:
            self.related_entries = []

@dataclass
class CollaborationPattern:
    """í˜‘ì—… íŒ¨í„´ ì •ì˜"""
    id: str
    agents: List[str]
    user_query_type: str
    success_rate: float
    average_execution_time: float
    typical_workflow: List[str]
    common_errors: List[str]
    optimization_tips: List[str]
    usage_frequency: int
    created_at: datetime
    updated_at: datetime

@dataclass
class UserPreference:
    """ì‚¬ìš©ì ì„ í˜¸ë„ ì •ì˜"""
    user_id: str
    preferred_agents: List[str]
    preferred_workflows: List[str]
    communication_style: str
    complexity_preference: str
    favorite_analysis_types: List[str]
    avoided_agents: List[str]
    created_at: datetime
    updated_at: datetime

class SharedKnowledgeBank:
    """ê³µìœ  ì§€ì‹ ì€í–‰ í•µì‹¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.knowledge_entries: Dict[str, KnowledgeEntry] = {}
        self.collaboration_patterns: Dict[str, CollaborationPattern] = {}
        self.user_preferences: Dict[str, UserPreference] = {}
        self.knowledge_graph = nx.DiGraph()
        
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
        
        # ê¸°ì¡´ ì§€ì‹ ë¡œë“œ
        self._load_existing_knowledge()
        
        logger.info("ğŸ§  Shared Knowledge Bank initialized")
    
    def _load_existing_knowledge(self):
        """ê¸°ì¡´ ì§€ì‹ ë¡œë“œ"""
        try:
            # ì§€ì‹ í•­ëª© ë¡œë“œ
            knowledge_file = KNOWLEDGE_BASE_DIR / "knowledge_entries.json"
            if knowledge_file.exists():
                with open(knowledge_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for entry_data in data:
                        entry = KnowledgeEntry(
                            id=entry_data['id'],
                            type=KnowledgeType(entry_data['type']),
                            title=entry_data['title'],
                            content=entry_data['content'],
                            metadata=entry_data['metadata'],
                            embedding=entry_data.get('embedding'),
                            related_entries=entry_data.get('related_entries', []),
                            created_at=datetime.fromisoformat(entry_data['created_at']),
                            updated_at=datetime.fromisoformat(entry_data['updated_at']),
                            usage_count=entry_data.get('usage_count', 0),
                            relevance_score=entry_data.get('relevance_score', 0.0)
                        )
                        self.knowledge_entries[entry.id] = entry
            
            # í˜‘ì—… íŒ¨í„´ ë¡œë“œ
            patterns_file = KNOWLEDGE_BASE_DIR / "collaboration_patterns.json"
            if patterns_file.exists():
                with open(patterns_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for pattern_data in data:
                        pattern = CollaborationPattern(
                            id=pattern_data['id'],
                            agents=pattern_data['agents'],
                            user_query_type=pattern_data['user_query_type'],
                            success_rate=pattern_data['success_rate'],
                            average_execution_time=pattern_data['average_execution_time'],
                            typical_workflow=pattern_data['typical_workflow'],
                            common_errors=pattern_data['common_errors'],
                            optimization_tips=pattern_data['optimization_tips'],
                            usage_frequency=pattern_data['usage_frequency'],
                            created_at=datetime.fromisoformat(pattern_data['created_at']),
                            updated_at=datetime.fromisoformat(pattern_data['updated_at'])
                        )
                        self.collaboration_patterns[pattern.id] = pattern
            
            # ì‚¬ìš©ì ì„ í˜¸ë„ ë¡œë“œ
            prefs_file = KNOWLEDGE_BASE_DIR / "user_preferences.json"
            if prefs_file.exists():
                with open(prefs_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    for pref_data in data:
                        pref = UserPreference(
                            user_id=pref_data['user_id'],
                            preferred_agents=pref_data['preferred_agents'],
                            preferred_workflows=pref_data['preferred_workflows'],
                            communication_style=pref_data['communication_style'],
                            complexity_preference=pref_data['complexity_preference'],
                            favorite_analysis_types=pref_data['favorite_analysis_types'],
                            avoided_agents=pref_data['avoided_agents'],
                            created_at=datetime.fromisoformat(pref_data['created_at']),
                            updated_at=datetime.fromisoformat(pref_data['updated_at'])
                        )
                        self.user_preferences[pref.user_id] = pref
            
            # ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ
            graph_file = KNOWLEDGE_BASE_DIR / "knowledge_graph.gpickle"
            if graph_file.exists():
                self.knowledge_graph = nx.read_gpickle(graph_file)
            
            logger.info(f"ğŸ“š Loaded {len(self.knowledge_entries)} knowledge entries")
            logger.info(f"ğŸ¤ Loaded {len(self.collaboration_patterns)} collaboration patterns")
            logger.info(f"ğŸ‘¤ Loaded {len(self.user_preferences)} user preferences")
            
        except Exception as e:
            logger.error(f"âŒ Error loading existing knowledge: {e}")
    
    def _save_knowledge(self):
        """ì§€ì‹ ì €ì¥"""
        try:
            # ì§€ì‹ í•­ëª© ì €ì¥
            knowledge_data = []
            for entry in self.knowledge_entries.values():
                knowledge_data.append({
                    'id': entry.id,
                    'type': entry.type.value,
                    'title': entry.title,
                    'content': entry.content,
                    'metadata': entry.metadata,
                    'embedding': entry.embedding,
                    'related_entries': entry.related_entries,
                    'created_at': entry.created_at.isoformat(),
                    'updated_at': entry.updated_at.isoformat(),
                    'usage_count': entry.usage_count,
                    'relevance_score': entry.relevance_score
                })
            
            with open(KNOWLEDGE_BASE_DIR / "knowledge_entries.json", 'w', encoding='utf-8') as f:
                json.dump(knowledge_data, f, indent=2, ensure_ascii=False)
            
            # í˜‘ì—… íŒ¨í„´ ì €ì¥
            patterns_data = []
            for pattern in self.collaboration_patterns.values():
                patterns_data.append({
                    'id': pattern.id,
                    'agents': pattern.agents,
                    'user_query_type': pattern.user_query_type,
                    'success_rate': pattern.success_rate,
                    'average_execution_time': pattern.average_execution_time,
                    'typical_workflow': pattern.typical_workflow,
                    'common_errors': pattern.common_errors,
                    'optimization_tips': pattern.optimization_tips,
                    'usage_frequency': pattern.usage_frequency,
                    'created_at': pattern.created_at.isoformat(),
                    'updated_at': pattern.updated_at.isoformat()
                })
            
            with open(KNOWLEDGE_BASE_DIR / "collaboration_patterns.json", 'w', encoding='utf-8') as f:
                json.dump(patterns_data, f, indent=2, ensure_ascii=False)
            
            # ì‚¬ìš©ì ì„ í˜¸ë„ ì €ì¥
            prefs_data = []
            for pref in self.user_preferences.values():
                prefs_data.append({
                    'user_id': pref.user_id,
                    'preferred_agents': pref.preferred_agents,
                    'preferred_workflows': pref.preferred_workflows,
                    'communication_style': pref.communication_style,
                    'complexity_preference': pref.complexity_preference,
                    'favorite_analysis_types': pref.favorite_analysis_types,
                    'avoided_agents': pref.avoided_agents,
                    'created_at': pref.created_at.isoformat(),
                    'updated_at': pref.updated_at.isoformat()
                })
            
            with open(KNOWLEDGE_BASE_DIR / "user_preferences.json", 'w', encoding='utf-8') as f:
                json.dump(prefs_data, f, indent=2, ensure_ascii=False)
            
            # ì§€ì‹ ê·¸ë˜í”„ ì €ì¥
            nx.write_gpickle(self.knowledge_graph, KNOWLEDGE_BASE_DIR / "knowledge_graph.gpickle")
            
            logger.info("ğŸ’¾ Knowledge base saved successfully")
            
        except Exception as e:
            logger.error(f"âŒ Error saving knowledge: {e}")
    
    async def add_knowledge_entry(self, entry: KnowledgeEntry) -> str:
        """ì§€ì‹ í•­ëª© ì¶”ê°€"""
        try:
            # ì„ë² ë”© ìƒì„±
            if entry.embedding is None:
                text_to_embed = f"{entry.title} {entry.content}"
                entry.embedding = self.embedding_model.encode(text_to_embed).tolist()
            
            # ê´€ë ¨ í•­ëª© ì°¾ê¸°
            related_entries = await self._find_related_entries(entry)
            entry.related_entries = related_entries
            
            # ì§€ì‹ ì €ì¥
            self.knowledge_entries[entry.id] = entry
            
            # ì§€ì‹ ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
            self.knowledge_graph.add_node(entry.id, **asdict(entry))
            for related_id in related_entries:
                if related_id in self.knowledge_entries:
                    self.knowledge_graph.add_edge(entry.id, related_id)
            
            # ì €ì¥
            self._save_knowledge()
            
            logger.info(f"ğŸ“ Added knowledge entry: {entry.title}")
            return entry.id
            
        except Exception as e:
            logger.error(f"âŒ Error adding knowledge entry: {e}")
            raise
    
    async def _find_related_entries(self, entry: KnowledgeEntry, threshold: float = 0.7) -> List[str]:
        """ê´€ë ¨ í•­ëª© ì°¾ê¸°"""
        related_entries = []
        
        if entry.embedding is None:
            return related_entries
        
        try:
            for existing_id, existing_entry in self.knowledge_entries.items():
                if existing_entry.embedding is None:
                    continue
                
                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                similarity = cosine_similarity(
                    [entry.embedding], 
                    [existing_entry.embedding]
                )[0][0]
                
                if similarity >= threshold:
                    related_entries.append(existing_id)
            
            return related_entries[:10]  # ìƒìœ„ 10ê°œë§Œ ë°˜í™˜
            
        except Exception as e:
            logger.error(f"âŒ Error finding related entries: {e}")
            return []
    
    async def search_knowledge(self, query: str, knowledge_type: Optional[KnowledgeType] = None, limit: int = 10) -> List[KnowledgeEntry]:
        """ì§€ì‹ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embedding_model.encode(query).tolist()
            
            # ìœ ì‚¬ë„ ê³„ì‚°
            results = []
            for entry in self.knowledge_entries.values():
                if knowledge_type and entry.type != knowledge_type:
                    continue
                
                if entry.embedding is None:
                    continue
                
                similarity = cosine_similarity(
                    [query_embedding], 
                    [entry.embedding]
                )[0][0]
                
                results.append((entry, similarity))
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
            results.sort(key=lambda x: x[1], reverse=True)
            
            # ìƒìœ„ ê²°ê³¼ ë°˜í™˜
            return [entry for entry, _ in results[:limit]]
            
        except Exception as e:
            logger.error(f"âŒ Error searching knowledge: {e}")
            return []
    
    async def learn_collaboration_pattern(self, agents: List[str], user_query: str, success: bool, execution_time: float, workflow: List[str], errors: List[str] = None) -> str:
        """í˜‘ì—… íŒ¨í„´ í•™ìŠµ"""
        try:
            # íŒ¨í„´ ID ìƒì„±
            pattern_key = f"{'-'.join(sorted(agents))}_{hash(user_query) % 10000}"
            
            if pattern_key in self.collaboration_patterns:
                # ê¸°ì¡´ íŒ¨í„´ ì—…ë°ì´íŠ¸
                pattern = self.collaboration_patterns[pattern_key]
                pattern.usage_frequency += 1
                
                # ì„±ê³µë¥  ì—…ë°ì´íŠ¸
                total_attempts = pattern.usage_frequency
                current_successes = pattern.success_rate * (total_attempts - 1)
                if success:
                    current_successes += 1
                pattern.success_rate = current_successes / total_attempts
                
                # í‰ê·  ì‹¤í–‰ ì‹œê°„ ì—…ë°ì´íŠ¸
                pattern.average_execution_time = (
                    pattern.average_execution_time * (total_attempts - 1) + execution_time
                ) / total_attempts
                
                # ì›Œí¬í”Œë¡œìš° ì—…ë°ì´íŠ¸
                if workflow not in pattern.typical_workflow:
                    pattern.typical_workflow.append(workflow)
                
                # ì˜¤ë¥˜ íŒ¨í„´ ì—…ë°ì´íŠ¸
                if errors:
                    pattern.common_errors.extend(errors)
                    pattern.common_errors = list(set(pattern.common_errors))
                
                pattern.updated_at = datetime.now()
                
            else:
                # ìƒˆ íŒ¨í„´ ìƒì„±
                pattern = CollaborationPattern(
                    id=pattern_key,
                    agents=agents,
                    user_query_type=user_query,
                    success_rate=1.0 if success else 0.0,
                    average_execution_time=execution_time,
                    typical_workflow=[workflow],
                    common_errors=errors or [],
                    optimization_tips=[],
                    usage_frequency=1,
                    created_at=datetime.now(),
                    updated_at=datetime.now()
                )
                
                self.collaboration_patterns[pattern_key] = pattern
            
            # ìµœì í™” íŒ ìƒì„±
            await self._generate_optimization_tips(pattern)
            
            # ì €ì¥
            self._save_knowledge()
            
            logger.info(f"ğŸ¤ Learned collaboration pattern: {pattern_key}")
            return pattern_key
            
        except Exception as e:
            logger.error(f"âŒ Error learning collaboration pattern: {e}")
            raise
    
    async def _generate_optimization_tips(self, pattern: CollaborationPattern):
        """ìµœì í™” íŒ ìƒì„±"""
        tips = []
        
        # ì„±ê³µë¥  ê¸°ë°˜ íŒ
        if pattern.success_rate < 0.5:
            tips.append("ë‚®ì€ ì„±ê³µë¥ ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ì¡°í•©ì„ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ íŒ
        if pattern.average_execution_time > 30:
            tips.append("ì‹¤í–‰ ì‹œê°„ì´ ê¸¸ë¯€ë¡œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        
        # ì˜¤ë¥˜ íŒ¨í„´ ê¸°ë°˜ íŒ
        if pattern.common_errors:
            tips.append(f"ìì£¼ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜: {', '.join(pattern.common_errors[:3])}")
        
        pattern.optimization_tips = tips
    
    async def get_collaboration_recommendations(self, user_query: str, available_agents: List[str]) -> List[Dict[str, Any]]:
        """í˜‘ì—… ì¶”ì²œ"""
        try:
            recommendations = []
            
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = self.embedding_model.encode(user_query).tolist()
            
            # ìœ ì‚¬í•œ íŒ¨í„´ ì°¾ê¸°
            for pattern in self.collaboration_patterns.values():
                # ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                if not any(agent in available_agents for agent in pattern.agents):
                    continue
                
                # ìœ ì‚¬ë„ ê³„ì‚°
                pattern_text = f"{pattern.user_query_type} {' '.join(pattern.agents)}"
                pattern_embedding = self.embedding_model.encode(pattern_text).tolist()
                
                similarity = cosine_similarity(
                    [query_embedding], 
                    [pattern_embedding]
                )[0][0]
                
                if similarity > 0.5:
                    recommendations.append({
                        'pattern_id': pattern.id,
                        'agents': pattern.agents,
                        'success_rate': pattern.success_rate,
                        'average_time': pattern.average_execution_time,
                        'similarity': similarity,
                        'optimization_tips': pattern.optimization_tips
                    })
            
            # ì„±ê³µë¥ ê³¼ ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
            recommendations.sort(
                key=lambda x: (x['success_rate'], x['similarity']), 
                reverse=True
            )
            
            return recommendations[:5]  # ìƒìœ„ 5ê°œ ì¶”ì²œ
            
        except Exception as e:
            logger.error(f"âŒ Error getting collaboration recommendations: {e}")
            return []
    
    async def update_user_preferences(self, user_id: str, preferences: Dict[str, Any]):
        """ì‚¬ìš©ì ì„ í˜¸ë„ ì—…ë°ì´íŠ¸"""
        try:
            if user_id in self.user_preferences:
                pref = self.user_preferences[user_id]
                pref.updated_at = datetime.now()
            else:
                pref = UserPreference(
                    user_id=user_id,
                    preferred_agents=[],
                    preferred_workflows=[],
                    communication_style="standard",
                    complexity_preference="medium",
                    favorite_analysis_types=[],
                    avoided_agents=[],
                    created_at=datetime.now(),
                    updated_at=datetime.now()
                )
                self.user_preferences[user_id] = pref
            
            # ì„ í˜¸ë„ ì—…ë°ì´íŠ¸
            for key, value in preferences.items():
                if hasattr(pref, key):
                    setattr(pref, key, value)
            
            self._save_knowledge()
            
            logger.info(f"ğŸ‘¤ Updated user preferences for: {user_id}")
            
        except Exception as e:
            logger.error(f"âŒ Error updating user preferences: {e}")
            raise
    
    async def get_knowledge_graph_insights(self, node_id: str) -> Dict[str, Any]:
        """ì§€ì‹ ê·¸ë˜í”„ ì¸ì‚¬ì´íŠ¸ ì œê³µ"""
        try:
            if node_id not in self.knowledge_graph:
                return {}
            
            insights = {
                'node_info': dict(self.knowledge_graph.nodes[node_id]),
                'connected_nodes': list(self.knowledge_graph.neighbors(node_id)),
                'centrality': nx.betweenness_centrality(self.knowledge_graph).get(node_id, 0),
                'degree': self.knowledge_graph.degree(node_id),
                'clustering': nx.clustering(self.knowledge_graph.to_undirected()).get(node_id, 0)
            }
            
            return insights
            
        except Exception as e:
            logger.error(f"âŒ Error getting knowledge graph insights: {e}")
            return {}

# A2A ì„œë²„ êµ¬í˜„
class SharedKnowledgeBankExecutor(AgentExecutor):
    """Shared Knowledge Bank A2A ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.knowledge_bank = SharedKnowledgeBank()
        logger.info("ğŸ§  SharedKnowledgeBankExecutor initialized")
    
    async def execute(self, context: RequestContext, task_updater: TaskUpdater) -> Any:
        """A2A ìš”ì²­ ì‹¤í–‰"""
        try:
            # ì‹œì‘ ì—…ë°ì´íŠ¸
            await task_updater.update_status(
                TaskState.working,
                message="ğŸ§  Shared Knowledge Bank ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
            )
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ íŒŒì‹±
            user_message = None
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if hasattr(part, 'root') and hasattr(part.root, 'text'):
                        user_message = part.root.text
                        break
            
            if not user_message:
                await task_updater.update_status(
                    TaskState.failed,
                    message="âŒ ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                )
                return
            
            # ë©”ì‹œì§€ ë¶„ì„ ë° ì ì ˆí•œ ê¸°ëŠ¥ ì‹¤í–‰
            result = await self._process_request(user_message, task_updater)
            
            # ê²°ê³¼ ì „ì†¡
            await task_updater.add_artifact(
                parts=[TextPart(text=json.dumps(result, ensure_ascii=False, indent=2))],
                name="knowledge_bank_result",
                metadata={"content_type": "application/json"}
            )
            
            await task_updater.update_status(
                TaskState.completed,
                message="âœ… Shared Knowledge Bank ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤."
            )
            
        except Exception as e:
            logger.error(f"âŒ SharedKnowledgeBankExecutor error: {e}")
            await task_updater.update_status(
                TaskState.failed,
                message=f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            )
    
    async def _process_request(self, user_message: str, task_updater: TaskUpdater) -> Dict[str, Any]:
        """ìš”ì²­ ì²˜ë¦¬"""
        try:
            # ìš”ì²­ íƒ€ì… ë¶„ì„
            request_lower = user_message.lower()
            
            if "ê²€ìƒ‰" in request_lower or "search" in request_lower:
                # ì§€ì‹ ê²€ìƒ‰
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ” ì§€ì‹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤..."
                )
                
                results = await self.knowledge_bank.search_knowledge(user_message, limit=5)
                
                return {
                    "type": "knowledge_search",
                    "query": user_message,
                    "results": [
                        {
                            "id": entry.id,
                            "title": entry.title,
                            "content": entry.content,
                            "type": entry.type.value,
                            "usage_count": entry.usage_count,
                            "relevance_score": entry.relevance_score
                        } for entry in results
                    ],
                    "total_results": len(results)
                }
            
            elif "ì¶”ì²œ" in request_lower or "recommend" in request_lower:
                # í˜‘ì—… ì¶”ì²œ
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ¤ í˜‘ì—… íŒ¨í„´ ì¶”ì²œì„ ìƒì„±í•©ë‹ˆë‹¤..."
                )
                
                available_agents = [
                    "pandas_agent", "data_cleaning_agent", "visualization_agent",
                    "eda_agent", "ml_modeling_agent", "sql_agent"
                ]
                
                recommendations = await self.knowledge_bank.get_collaboration_recommendations(
                    user_message, available_agents
                )
                
                return {
                    "type": "collaboration_recommendations",
                    "query": user_message,
                    "recommendations": recommendations
                }
            
            elif "í•™ìŠµ" in request_lower or "learn" in request_lower:
                # íŒ¨í„´ í•™ìŠµ (ë°ëª¨ìš©)
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ“š í˜‘ì—… íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤..."
                )
                
                # ë°ëª¨ ë°ì´í„°ë¡œ í•™ìŠµ
                pattern_id = await self.knowledge_bank.learn_collaboration_pattern(
                    agents=["pandas_agent", "visualization_agent"],
                    user_query=user_message,
                    success=True,
                    execution_time=15.5,
                    workflow=["ë°ì´í„° ë¡œë“œ", "ë¶„ì„", "ì‹œê°í™”"],
                    errors=[]
                )
                
                return {
                    "type": "pattern_learning",
                    "pattern_id": pattern_id,
                    "message": "í˜‘ì—… íŒ¨í„´ì´ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤."
                }
            
            elif "í†µê³„" in request_lower or "stats" in request_lower:
                # ì§€ì‹ ì€í–‰ í†µê³„
                await task_updater.update_status(
                    TaskState.working,
                    message="ğŸ“Š ì§€ì‹ ì€í–‰ í†µê³„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤..."
                )
                
                return {
                    "type": "knowledge_bank_stats",
                    "statistics": {
                        "total_knowledge_entries": len(self.knowledge_bank.knowledge_entries),
                        "collaboration_patterns": len(self.knowledge_bank.collaboration_patterns),
                        "user_preferences": len(self.knowledge_bank.user_preferences),
                        "knowledge_graph_nodes": self.knowledge_bank.knowledge_graph.number_of_nodes(),
                        "knowledge_graph_edges": self.knowledge_bank.knowledge_graph.number_of_edges(),
                        "knowledge_types": {
                            ktype.value: len([
                                e for e in self.knowledge_bank.knowledge_entries.values() 
                                if e.type == ktype
                            ])
                            for ktype in KnowledgeType
                        }
                    }
                }
            
            else:
                # ê¸°ë³¸ ì§€ì‹ ê²€ìƒ‰
                results = await self.knowledge_bank.search_knowledge(user_message, limit=3)
                
                return {
                    "type": "general_knowledge_query",
                    "query": user_message,
                    "results": [
                        {
                            "title": entry.title,
                            "content": entry.content[:200] + "..." if len(entry.content) > 200 else entry.content,
                            "type": entry.type.value
                        } for entry in results
                    ],
                    "suggestion": "ë” êµ¬ì²´ì ì¸ ê²€ìƒ‰ì„ ìœ„í•´ 'ê²€ìƒ‰:', 'ì¶”ì²œ:', 'í•™ìŠµ:', 'í†µê³„:' ì¤‘ í•˜ë‚˜ë¥¼ ì•ì— ë¶™ì—¬ì£¼ì„¸ìš”."
                }
            
        except Exception as e:
            logger.error(f"âŒ Error processing request: {e}")
            return {
                "type": "error",
                "message": f"ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            }
    
    async def cancel(self, context: RequestContext, task_updater: TaskUpdater) -> Any:
        """ì‘ì—… ì·¨ì†Œ"""
        await task_updater.update_status(
            TaskState.cancelled,
            message="ğŸ›‘ Shared Knowledge Bank ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤."
        )

# Agent Card ì •ì˜
AGENT_CARD = AgentCard(
    name="Shared Knowledge Bank",
    description="A2A ì—ì´ì „íŠ¸ ê°„ ê³µìœ  ì§€ì‹ ì€í–‰ - í˜‘ì—… íŒ¨í„´ í•™ìŠµ, ì§€ì‹ ê²€ìƒ‰, ì‚¬ìš©ì ì„ í˜¸ë„ ê´€ë¦¬",
    skills=[
        AgentSkill(
            name="knowledge_search",
            description="ì„ë² ë”© ê¸°ë°˜ ì§€ì‹ ê²€ìƒ‰ ë° ê´€ë ¨ ì •ë³´ ì œê³µ"
        ),
        AgentSkill(
            name="collaboration_learning",
            description="ì—ì´ì „íŠ¸ ê°„ í˜‘ì—… íŒ¨í„´ í•™ìŠµ ë° ìµœì í™” ì œì•ˆ"
        ),
        AgentSkill(
            name="preference_management",
            description="ì‚¬ìš©ìë³„ ì„ í˜¸ë„ í•™ìŠµ ë° ê°œì¸í™” ì¶”ì²œ"
        ),
        AgentSkill(
            name="knowledge_graph_analysis",
            description="ì§€ì‹ ê·¸ë˜í”„ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ì œê³µ"
        ),
        AgentSkill(
            name="cross_agent_insights",
            description="í¬ë¡œìŠ¤ ì—ì´ì „íŠ¸ ì¸ì‚¬ì´íŠ¸ ì¶•ì  ë° í™œìš©"
        )
    ],
    capabilities=AgentCapabilities(
        supports_streaming=True,
        supports_cancellation=True,
        supports_artifacts=True
    )
)

# ë©”ì¸ ì‹¤í–‰
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # A2A ì„œë²„ ì„¤ì •
    task_store = InMemoryTaskStore()
    request_handler = DefaultRequestHandler(task_store)
    
    # ì—ì´ì „íŠ¸ ë“±ë¡
    executor = SharedKnowledgeBankExecutor()
    request_handler.register_agent(AGENT_CARD, executor)
    
    # ì•± ìƒì„±
    app = A2AStarletteApplication(
        request_handler=request_handler,
        agent_card=AGENT_CARD
    )
    
    # ì„œë²„ ì‹œì‘
    logger.info("ğŸš€ Starting Shared Knowledge Bank Server on port 8602")
    
    config = uvicorn.Config(
        app=app,
        host="0.0.0.0",
        port=8602,
        log_level="info"
    )
    
    server = uvicorn.Server(config)
    await server.serve()

if __name__ == "__main__":
    asyncio.run(main()) 