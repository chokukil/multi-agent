import asyncio
import logging
import os
import pandas as pd
import re
from datetime import datetime
from typing import Dict, Any, AsyncGenerator
import uvicorn

# A2A SDK ê³µì‹ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš© (ì™„ì „í•œ í‘œì¤€ êµ¬í˜„)
import uuid
from a2a.server.apps import A2AFastAPIApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.agent_execution import AgentExecutor, RequestContext
from a2a.server.events.event_queue import EventQueue
from a2a.server.tasks import InMemoryTaskStore
from a2a.types import (
    AgentCard, AgentSkill, Message, Task, TaskState, TextPart, Role
)
from a2a.utils.message import new_agent_text_message

from langchain_ollama import ChatOllama

# Import core modules
import sys
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, os.path.join(project_root, 'core'))

from utils.logging import setup_logging
from data_manager import DataManager

# --- Logging Setup ---
setup_logging()
logger = logging.getLogger(__name__)

# --- Initialize Global Components ---
try:
    llm = ChatOllama(model="gemma3:latest", temperature=0)
    data_manager = DataManager()
    logger.info("âœ… Global components initialized successfully")
except Exception as e:
    logger.exception(f"ğŸ’¥ Critical error during initialization: {e}")
    exit(1)

class PandasAgentExecutor(AgentExecutor):
    """A2A SDK í‘œì¤€ì„ ì™„ì „íˆ ì¤€ìˆ˜í•˜ëŠ” Pandas ë¶„ì„ AgentExecutor"""
    
    def __init__(self, data_manager: DataManager, llm):
        self.data_manager = data_manager
        self.llm = llm
        logger.info("ğŸ”§ PandasAgentExecutor initialized")

    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """A2A SDK í‘œì¤€ ì‹¤í–‰ ì¸í„°í˜ì´ìŠ¤ - ì‹¤ì‹œê°„ í”¼ë“œë°± ê°•í™”"""
        logger.info("ğŸ¯ A2A AGENT EXECUTE METHOD CALLED!")
        logger.info(f"ğŸ“¥ Request message_id: {getattr(context.message, 'messageId', 'unknown')}")
        logger.info(f"ğŸ“¥ Request user: {getattr(context.message, 'role', 'unknown')}")
        
        try:
            # ë©”ì‹œì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            message_text = ""
            if context.message and context.message.parts:
                for part in context.message.parts:
                    if hasattr(part, 'text') and part.text:
                        message_text += part.text + " "
            
            message_text = message_text.strip()
            logger.info(f"ğŸ“ FULL ANALYSIS REQUEST: {message_text}")
            
            # ë°ì´í„° ë¶„ì„ ìˆ˜í–‰ (A2A í‘œì¤€ ë°©ì‹)
            logger.info("ğŸ” Starting comprehensive data analysis...")
            
            result = await self.analyze_data(message_text)
            logger.info(f"âœ… Analysis completed successfully. Result length: {len(result)} chars")
            
            # A2A í‘œì¤€ ë©”ì‹œì§€ ì‘ë‹µ ìƒì„± ë° ì „ì†¡ (ì‘ë™í•˜ëŠ” íŒ¨í„´ ì ìš©)
            response_message = new_agent_text_message(result)
            await event_queue.put(response_message)
            
            logger.info("ğŸ“¤ Analysis result sent via EventQueue successfully")
            
        except Exception as e:
            logger.error(f"ğŸ’¥ A2A Agent execution failed: {e}", exc_info=True)
            
            # A2A í‘œì¤€ ì˜¤ë¥˜ ë©”ì‹œì§€ ìƒì„± ë° ì „ì†¡
            error_message = new_agent_text_message(f"""âŒ **Analysis Failed**

**Error Details:** {str(e)}

**Troubleshooting:**
1. Check if the dataset is properly loaded
2. Verify the analysis request format
3. Try again with a simpler request

Please contact support if the issue persists.
""")
            await event_queue.put(error_message)

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """A2A SDK í‘œì¤€ ì·¨ì†Œ ì¸í„°í˜ì´ìŠ¤"""
        logger.info(f"ğŸ›‘ Cancelling task")
        # í˜„ì¬ êµ¬í˜„ì—ì„œëŠ” ì·¨ì†Œ ë¡œì§ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ
        pass



    async def analyze_data(self, prompt: str = "Analyze this dataset") -> str:
        """pandas ë°ì´í„° ë¶„ì„ ì‹¤í–‰"""
        logger.info(f"ğŸ¯ ANALYZE_DATA SKILL CALLED")
        logger.debug(f"ğŸ“ Prompt: {prompt}")
        
        try:
            # ë°ì´í„° ID ì¶”ì¶œ
            df_id = self._extract_data_id(prompt)
            available_dfs = self.data_manager.list_dataframes()
            
            logger.info(f"ğŸ’¾ Available dataframes: {available_dfs}")
            
            if not available_dfs:
                return """âŒ **No Data Available**

**Issue:** No dataset has been uploaded yet.

**To use the Pandas Data Analyst:**
1. ğŸ”„ Go to the **Data Loader** page first
2. ğŸ“ Upload a CSV, Excel, or other data file  
3. ğŸ“Š Return here to analyze your uploaded data

**Available datasets:** None (please upload data first)
"""
            
            # ë°ì´í„° ID ìë™ í• ë‹¹
            if not df_id:
                df_id = available_dfs[0]
                logger.info(f"ğŸ”§ Auto-assigned dataframe: '{df_id}'")
            
            # ë°ì´í„°í”„ë ˆì„ ë¡œë“œ
            df = self.data_manager.get_dataframe(df_id)
            if df is None:
                return f"""âŒ **Dataset Not Found: '{df_id}'**

**Available datasets:**
{chr(10).join(f"â€¢ `{df_id}`" for df_id in available_dfs)}

**Solution:** Use one of the available dataset IDs above, or upload new data via the Data Loader page.
"""
            
            # ë°ì´í„° ë¶„ì„ ìˆ˜í–‰
            analysis_result = await self._perform_analysis(df, df_id, prompt)
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ Analysis failed: {e}", exc_info=True)
            return f"Analysis failed: {str(e)}"

    def _extract_data_id(self, prompt: str) -> str:
        """í”„ë¡¬í”„íŠ¸ì—ì„œ ë°ì´í„° ID ì¶”ì¶œ"""
        if not prompt:
            return None
            
        # Pattern 1: Explicit "Data ID: something"
        data_id_match = re.search(r"Data ID:\s*([^\n\r\s]+)", prompt, re.IGNORECASE)
        if data_id_match:
            return data_id_match.group(1).strip().strip("'\"")
        
        # Pattern 2: "dataset with ID 'something'"
        id_pattern2 = re.search(r"dataset\s+with\s+ID\s+['\"]([^'\"]+)['\"]", prompt, re.IGNORECASE)
        if id_pattern2:
            return id_pattern2.group(1).strip()
        
        # Pattern 3: Common dataset names
        common_patterns = [
            r"titanic",
            r"customer_data", 
            r"sales_data",
            r"([a-zA-Z0-9_-]+\.(?:csv|xlsx|json|parquet))"
        ]
        for pattern in common_patterns:
            match = re.search(pattern, prompt, re.IGNORECASE)
            if match:
                return match.group(0).strip()
                
        return None

    async def _perform_analysis(self, df: pd.DataFrame, df_id: str, prompt: str) -> str:
        """ì‹¤ì œ ë°ì´í„° ë¶„ì„ ìˆ˜í–‰ - ìƒì„¸í•œ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        import numpy as np
        from datetime import datetime
        
        logger.info(f"ğŸ” Starting comprehensive analysis for {df_id}")
        
        # 1. ê¸°ë³¸ ë°ì´í„° í”„ë¡œíŒŒì¼ë§
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
        datetime_cols = df.select_dtypes(include=['datetime']).columns.tolist()
        
        # 2. ë°ì´í„° í’ˆì§ˆ ë©”íŠ¸ë¦­
        total_rows = len(df)
        total_cols = len(df.columns)
        missing_data_summary = df.isnull().sum()
        completeness = ((total_rows * total_cols - missing_data_summary.sum()) / (total_rows * total_cols)) * 100
        
        # 3. ìƒì„¸ í†µê³„ ë¶„ì„
        analysis_results = []
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ ë¶„ì„
        if numeric_cols:
            numeric_summary = df[numeric_cols].describe()
            correlations = df[numeric_cols].corr() if len(numeric_cols) > 1 else None
        
        # ë²”ì£¼í˜• ì»¬ëŸ¼ ë¶„ì„  
        categorical_summary = {}
        for col in categorical_cols[:5]:  # ìƒìœ„ 5ê°œ ì»¬ëŸ¼ë§Œ
            value_counts = df[col].value_counts().head(10)
            categorical_summary[col] = {
                'unique_count': df[col].nunique(),
                'top_values': value_counts.to_dict()
            }
        
        # 4. ê³ ê¸‰ ë¶„ì„ ìƒì„±
        advanced_prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë°ì´í„°ì…‹ì— ëŒ€í•´ ìƒì„¸í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:

**ì‚¬ìš©ì ìš”ì²­**: {prompt}

**ë°ì´í„°ì…‹ ì •ë³´**:
- ë°ì´í„°ì…‹ëª…: {df_id}
- ì „ì²´ í¬ê¸°: {total_rows:,}í–‰ Ã— {total_cols}ì—´
- ë°ì´í„° ì™„ì„±ë„: {completeness:.1f}%
- ìˆ«ìí˜• ì»¬ëŸ¼: {len(numeric_cols)}ê°œ ({numeric_cols[:5]})
- ë²”ì£¼í˜• ì»¬ëŸ¼: {len(categorical_cols)}ê°œ ({categorical_cols[:5]})

**ìˆ«ìí˜• ë°ì´í„° ìš”ì•½**:
{numeric_summary.to_string() if numeric_cols else "ìˆ«ìí˜• ë°ì´í„° ì—†ìŒ"}

**ë²”ì£¼í˜• ë°ì´í„° ìš”ì•½**:
{str(categorical_summary) if categorical_summary else "ë²”ì£¼í˜• ë°ì´í„° ì—†ìŒ"}

**ë¶„ì„ ìš”êµ¬ì‚¬í•­**:
1. ğŸ“Š **ë°ì´í„° ê°œìš” ë° êµ¬ì¡° ë¶„ì„**
2. ğŸ” **ë°ì´í„° í’ˆì§ˆ í‰ê°€** (ê²°ì¸¡ê°’, ì´ìƒê°’, ë°ì´í„° íƒ€ì… ì ì ˆì„±)
3. ğŸ“ˆ **ì£¼ìš” í†µê³„ì  íŠ¹ì„±** (ë¶„í¬, ì¤‘ì‹¬ê²½í–¥, ë³€ë™ì„±)
4. ğŸ”— **ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„** (ìƒê´€ê´€ê³„, íŒ¨í„´)
5. ğŸ’¡ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë° ë¹„ì¦ˆë‹ˆìŠ¤ í•¨ì˜**
6. ğŸ“‹ **ì¶”ê°€ ë¶„ì„ ê¶Œì¥ì‚¬í•­**

**ì¶œë ¥ í˜•ì‹**: ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ìƒì„¸ ë³´ê³ ì„œ
**í†¤**: ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ
**ëª©í‘œ**: ì‹¤ë¬´ì§„ì´ ì˜ì‚¬ê²°ì •ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì‹¤ìš©ì  ì¸ì‚¬ì´íŠ¸ ì œê³µ
        """
        
        try:
            # LLMì„ í†µí•œ ì „ë¬¸ ë¶„ì„ ìƒì„±
            logger.info("ğŸ§  Generating AI-powered analysis...")
            response = await asyncio.get_event_loop().run_in_executor(
                None, lambda: self.llm.invoke(advanced_prompt)
            )
            
            # ë¶„ì„ ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            final_result = f"""# ğŸ“Š ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ëŒ€ìƒ**: {df_id}  
**ë¶„ì„ ì¼ì‹œ**: {timestamp}  
**ìš”ì²­ ë‚´ìš©**: {prompt}

---

{response.content}

---

## ğŸ“‹ ë¶„ì„ ë©”íƒ€ë°ì´í„°

| í•­ëª© | ê°’ |
|-----|-----|
| ë°ì´í„°ì…‹ í¬ê¸° | {total_rows:,} í–‰ Ã— {total_cols} ì—´ |
| ë°ì´í„° ì™„ì„±ë„ | {completeness:.1f}% |
| ìˆ«ìí˜• ë³€ìˆ˜ | {len(numeric_cols)}ê°œ |
| ë²”ì£¼í˜• ë³€ìˆ˜ | {len(categorical_cols)}ê°œ |
| ê²°ì¸¡ê°’ ì´ëŸ‰ | {missing_data_summary.sum()} ê°œ |

**ë¶„ì„ ì—”ì§„**: Pandas Data Analyst (A2A Protocol)  
**ë²„ì „**: 1.0.0
"""
            
            logger.info("âœ… Comprehensive analysis completed")
            return final_result
            
        except Exception as e:
            logger.error(f"âŒ Advanced analysis failed, falling back to basic: {e}")
            
            # ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ë¡œ ëŒ€ì²´ (ë” ìƒì„¸í•˜ê²Œ)
            return f"""# ğŸ“Š ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ëŒ€ìƒ**: {df_id}  
**ë¶„ì„ ì¼ì‹œ**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}  
**ìš”ì²­ ë‚´ìš©**: {prompt}

## ğŸ“‹ ë°ì´í„° ê°œìš”

### ê¸°ë³¸ ì •ë³´
- **ë°ì´í„°ì…‹ í¬ê¸°**: {total_rows:,} í–‰ Ã— {total_cols} ì—´
- **ë°ì´í„° ì™„ì„±ë„**: {completeness:.1f}%
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB

### ë³€ìˆ˜ êµ¬ì„±
| ë³€ìˆ˜ íƒ€ì… | ê°œìˆ˜ | ì»¬ëŸ¼ëª… |
|----------|------|--------|
| ìˆ«ìí˜• | {len(numeric_cols)} | {', '.join(numeric_cols[:5])} |
| ë²”ì£¼í˜• | {len(categorical_cols)} | {', '.join(categorical_cols[:5])} |
| ë‚ ì§œí˜• | {len(datetime_cols)} | {', '.join(datetime_cols[:5])} |

## ğŸ” ë°ì´í„° í’ˆì§ˆ ë¶„ì„

### ê²°ì¸¡ê°’ í˜„í™©
{chr(10).join(f"- **{col}**: {count:,}ê°œ ({count/total_rows*100:.1f}%)" for col, count in missing_data_summary.items() if count > 0) or "âœ… ê²°ì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤."}

### ìˆ«ìí˜• ë³€ìˆ˜ ìš”ì•½ í†µê³„
{numeric_summary.round(2).to_markdown() if not numeric_summary.empty else "ìˆ«ìí˜• ë³€ìˆ˜ê°€ ì—†ìŠµë‹ˆë‹¤."}

## ğŸ’¡ ì£¼ìš” ê´€ì°°ì 

1. **ë°ì´í„° í¬ê¸°**: {total_rows:,}ê°œì˜ ê´€ì¸¡ê°’ìœ¼ë¡œ {"ì¶©ë¶„í•œ" if total_rows > 1000 else "ì œí•œì ì¸"} ë¶„ì„ ê°€ëŠ¥
2. **ë°ì´í„° ì™„ì„±ë„**: {completeness:.1f}%ë¡œ {"ìš°ìˆ˜í•œ" if completeness > 95 else "ë³´í†µ" if completeness > 80 else "ê°œì„  í•„ìš”í•œ"} ìˆ˜ì¤€
3. **ë³€ìˆ˜ ë‹¤ì–‘ì„±**: {total_cols}ê°œ ë³€ìˆ˜ë¡œ {"ë‹¤ì–‘í•œ" if total_cols > 10 else "ê¸°ë³¸ì ì¸"} ë¶„ì„ ì°¨ì› ì œê³µ

## ğŸ“ˆ ì¶”ì²œ ë¶„ì„ ë°©í–¥

1. **íƒìƒ‰ì  ë°ì´í„° ë¶„ì„**: ë³€ìˆ˜ë³„ ë¶„í¬ ë° íŒ¨í„´ í™•ì¸
2. **ìƒê´€ê´€ê³„ ë¶„ì„**: ë³€ìˆ˜ ê°„ ì—°ê´€ì„± íƒìƒ‰
3. **ì´ìƒê°’ íƒì§€**: ë°ì´í„° í’ˆì§ˆ ê°œì„ 
4. **ì‹œê°í™”**: ì£¼ìš” íŒ¨í„´ì˜ ì‹œê°ì  í‘œí˜„

---
**ë¶„ì„ ì—”ì§„**: Pandas Data Analyst (A2A Protocol)  
**ìƒíƒœ**: ê¸°ë³¸ ë¶„ì„ ì™„ë£Œ âœ…
"""

def create_agent_card() -> AgentCard:
    """A2A í‘œì¤€ Agent Card ìƒì„±"""
    skill = AgentSkill(
        id="analyze_data",
        name="Data Analysis",
        description="Analyze datasets using pandas and provide comprehensive insights",
        tags=["data", "analysis", "pandas", "statistics"],
        examples=["analyze the titanic dataset", "show me insights about sales data"]
    )
    
    return AgentCard(
        name="Pandas Data Analyst",
        description="Expert data analyst using pandas for comprehensive dataset analysis",
        url="http://localhost:10001",
        version="1.0.0",
        capabilities={
            "streaming": True,
            "pushNotifications": False,
            "stateTransitionHistory": True
        },
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        authentication={"schemes": ["none"]},  # ì¸ì¦ ì—†ìŒ
        skills=[skill],
        provider={
            "organization": "CherryAI",
            "description": "AI-powered data analysis platform",
            "url": "http://localhost:10001"
        }
    )

def create_a2a_server() -> A2AFastAPIApplication:
    """A2A SDKë¥¼ ì‚¬ìš©í•œ ì™„ì „í•œ í‘œì¤€ ì„œë²„ ìƒì„±"""
    
    # Agent Card ìƒì„±
    agent_card = create_agent_card()
    
    # AgentExecutor ìƒì„±
    agent_executor = PandasAgentExecutor(data_manager, llm)
    
    # TaskStore ìƒì„±
    task_store = InMemoryTaskStore()
    
    # A2A í‘œì¤€ RequestHandler ìƒì„±
    http_handler = DefaultRequestHandler(
        agent_executor=agent_executor,
        task_store=task_store
    )
    
    # A2A FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒì„±
    server = A2AFastAPIApplication(
        agent_card=agent_card,
        http_handler=http_handler
    )
    
    logger.info("âœ… A2A ì„œë²„ê°€ í‘œì¤€ SDKë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤")
    return server

if __name__ == "__main__":
    logger.info("ğŸš€ Starting Pandas Data Analyst A2A Server...")
    
    try:
        # A2A í‘œì¤€ ì„œë²„ ìƒì„±
        server = create_a2a_server()
        app = server.build()
        
        # ì„œë²„ ì‹œì‘
        logger.info("ğŸŒ Server starting on http://0.0.0.0:10001")
        uvicorn.run(app, host="0.0.0.0", port=10001)
        
    except Exception as e:
        logger.exception(f"ğŸ’¥ Server startup failed: {e}")
        exit(1) 