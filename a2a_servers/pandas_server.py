import asyncio
import logging
import os
import pandas as pd
import numpy as np
import re
from datetime import datetime
from typing import Dict, Any, AsyncGenerator
import uvicorn
import click
import pickle
from pathlib import Path

# A2A SDK ê³µì‹ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš© (ê³µì‹ Hello World Agent íŒ¨í„´)
import uuid
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.agent_execution.agent_executor import AgentExecutor, RequestContext
from a2a.server.events.event_queue import EventQueue
from a2a.types import AgentCard, AgentSkill, Message, Task, AgentCapabilities
from a2a.utils.message import new_agent_text_message, get_message_text

from langchain_ollama import ChatOllama

# Import core modules
import sys
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from core.data_manager import DataManager

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global data manager instance
data_manager = DataManager()

# 1. Define the core agent (ê³µì‹ Hello World Agent íŒ¨í„´)
class PandasDataAnalysisAgent:
    """ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ë²”ìš© ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸"""
    
    def __init__(self):
        self.data_cache = {}

    async def invoke(self, user_input: str = "", stream: bool = False) -> str:
        """ë¶„ì„ ìˆ˜í–‰ - ìŠ¤íŠ¸ë¦¬ë° ì§€ì›"""
        try:
            logger.info(f"ğŸ“Š ë°ì´í„° ë¶„ì„ ìš”ì²­: {user_input[:100]}...")
            
            # ë°ì´í„° ë¡œë“œ
            df, df_id = await self._load_latest_dataset()
            if df is None:
                return "âŒ ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
            
            logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
            
            if stream:
                # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
                return await self._perform_streaming_analysis(df, df_id, user_input)
            else:
                # ì¼ë°˜ ëª¨ë“œ (ê¸°ì¡´)
                return await self._perform_targeted_analysis(df, df_id, user_input)
                
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _load_latest_dataset(self):
        """ìµœì‹  ë°ì´í„°ì…‹ ë¡œë“œ"""
        try:
            # artifacts/data/shared_dataframes ë””ë ‰í† ë¦¬ì—ì„œ ë°ì´í„° ì°¾ê¸°
            data_dir = Path("artifacts/data/shared_dataframes")
            if not data_dir.exists():
                logger.warning(f"ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {data_dir}")
                return None, None
            
            # pickle íŒŒì¼ë“¤ ì°¾ê¸°
            pickle_files = list(data_dir.glob("*.pkl"))
            if not pickle_files:
                logger.warning(f"pickle íŒŒì¼ì´ ì—†ìŒ: {data_dir}")
                return None, None
            
            # ê°€ì¥ ìµœê·¼ íŒŒì¼ ì„ íƒ
            latest_file = max(pickle_files, key=lambda x: x.stat().st_mtime)
            logger.info(f"ë¡œë“œí•  íŒŒì¼: {latest_file}")
            
            # ë°ì´í„° ë¡œë“œ
            with open(latest_file, 'rb') as f:
                data = pickle.load(f)
            
            # ë°ì´í„° íƒ€ì… í™•ì¸ ë° ë³€í™˜
            if isinstance(data, dict):
                # dictì¸ ê²½ìš° DataFrameìœ¼ë¡œ ë³€í™˜ ì‹œë„
                import pandas as pd
                if 'data' in data:
                    df = pd.DataFrame(data['data'])
                elif 'df' in data:
                    df = pd.DataFrame(data['df'])
                else:
                    # dictì˜ ì²« ë²ˆì§¸ ê°’ì´ DataFrameì´ê±°ë‚˜ dictì¸ì§€ í™•ì¸
                    first_key = list(data.keys())[0]
                    if hasattr(data[first_key], 'shape'):
                        df = data[first_key]
                    else:
                        df = pd.DataFrame(data)
            else:
                df = data
            
            # DataFrameì¸ì§€ ìµœì¢… í™•ì¸
            if not hasattr(df, 'shape'):
                logger.error(f"ë¡œë“œëœ ë°ì´í„°ê°€ DataFrameì´ ì•„ë‹˜: {type(df)}")
                return None, None
            
            df_id = latest_file.stem.replace('.csv', '')  # .csv.pkl -> .csv ì œê±°
            logger.info(f"ë°ì´í„° ë¡œë“œ ì„±ê³µ: {df.shape} - {df_id}")
            
            return df, df_id
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None, None
    
    async def _perform_targeted_analysis(self, df, df_id: str, user_instruction: str) -> str:
        """LLMì´ ì§€ì‹œì‚¬í•­ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ë¶„ì„ì„ ìë™ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ìˆ˜í–‰ (ê¸°ì¡´ ë°©ì‹)"""
        
        try:
            # LLM í˜¸ì¶œì„ ìœ„í•œ ì„¤ì •
            from langchain_ollama import ChatOllama
            
            # Ollama LLM ì´ˆê¸°í™” - gemma3 ëª¨ë¸ ì‚¬ìš©
            llm = ChatOllama(
                model="gemma3:latest",
                temperature=0.1,
                base_url="http://localhost:11434"
            )
            
            # ë¶„ì„ ìœ í˜• ì„ íƒì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸
            analysis_selector_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ë¶„ì„ ìœ í˜•ì„ ì„ íƒí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{user_instruction}"

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:
1. data_overview - ë°ì´í„° êµ¬ì¡°, ë³€ìˆ˜ ì •ë³´, ê¸°ë³¸ ê°œìš” ë¶„ì„
2. descriptive_stats - ê¸°ìˆ í†µê³„, ë¶„í¬, ìš”ì•½ í†µê³„ ë¶„ì„  
3. correlation_analysis - ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„, ê´€ê³„ì„± ë¶„ì„
4. pattern_analysis - íŒ¨í„´, íŠ¸ë Œë“œ, ë¶„í¬ íŠ¹ì„± ë¶„ì„
5. insights_summary - í•µì‹¬ ì¸ì‚¬ì´íŠ¸, ê²°ë¡ , ì¶”ì²œì‚¬í•­

ì˜¤ì§ ìˆ«ìë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (1, 2, 3, 4, ë˜ëŠ” 5):
"""
            
            # LLMì—ê²Œ ë¶„ì„ ìœ í˜• ì„ íƒ ìš”ì²­
            response = await llm.ainvoke(analysis_selector_prompt)
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if hasattr(response, 'content'):
                selection = response.content.strip()
            else:
                selection = str(response).strip()
            
            logger.info(f"ğŸ¯ LLMì´ ì„ íƒí•œ ë¶„ì„ ìœ í˜•: {selection}")
            
            # ì„ íƒëœ ë¶„ì„ í•¨ìˆ˜ ì‹¤í–‰
            if selection == "1":
                return self._generate_data_overview(df, df_id, user_instruction)
            elif selection == "2":
                return self._generate_descriptive_stats(df, df_id, user_instruction)
            elif selection == "3":
                return self._generate_correlation_analysis(df, df_id, user_instruction)
            elif selection == "4":
                return self._generate_pattern_analysis(df, df_id, user_instruction)
            elif selection == "5":
                return self._generate_insights_summary(df, df_id, user_instruction)
            else:
                logger.warning(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” ì„ íƒ: {selection}, ì¢…í•© ë¶„ì„ìœ¼ë¡œ í´ë°±")
                return await self._generate_comprehensive_streaming_analysis(df, df_id, user_instruction)
                
        except Exception as e:
            logger.error(f"âŒ LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ ë¶„ì„
            return await self._generate_comprehensive_streaming_analysis(df, df_id, user_instruction)
    
    async def _perform_streaming_analysis(self, df, df_id: str, user_instruction: str) -> str:
        """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„"""
        try:
            from langchain_ollama import ChatOllama
            
            # Ollama LLM ì´ˆê¸°í™”
            llm = ChatOllama(
                model="gemma3:latest",
                temperature=0.1,
                base_url="http://localhost:11434"
            )
            
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
            streaming_response = []
            
            # 1. ì¦‰ì‹œ ì‹œì‘ ë©”ì‹œì§€
            start_msg = f"""# ğŸ“Š **ì‹¤ì‹œê°„ ë°ì´í„° ë¶„ì„ ì‹œì‘**

**ìš”ì²­**: {user_instruction}
**ë°ì´í„°ì…‹**: {df_id} ({df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´)
**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

ğŸ”„ **ë¶„ì„ ì§„í–‰ ì¤‘...**

"""
            streaming_response.append(start_msg)
            
            # 2. ë°ì´í„° ê¸°ë³¸ ì •ë³´ (ì¦‰ì‹œ ì œê³µ)
            basic_info = f"""## ğŸ“‹ **ë°ì´í„° ê¸°ë³¸ ì •ë³´**

| í•­ëª© | ê°’ |
|------|-----|
| ğŸ“ ë°ì´í„° í¬ê¸° | **{df.shape[0]:,}** í–‰ Ã— **{df.shape[1]}** ì—´ |
| ğŸ”¢ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ | **{len(df.select_dtypes(include=[np.number]).columns)}ê°œ** |
| ğŸ“ ë²”ì£¼í˜• ë³€ìˆ˜ | **{len(df.select_dtypes(include=['object', 'category']).columns)}ê°œ** |
| âœ… ì™„ì„±ë„ | **{(1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100:.1f}%** |
| ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | **{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB** |

"""
            streaming_response.append(basic_info)
            
            # 3. LLM ë¶„ì„ ì„ íƒ (ì•½ê°„ì˜ ì§€ì—°)
            await asyncio.sleep(1)  # ì‹¤ì œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            
            analysis_selector_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ë¶„ì„ ìœ í˜•ì„ ì„ íƒí•´ì£¼ì„¸ìš”.

ì‚¬ìš©ì ìš”ì²­: "{user_instruction}"

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:
1. comprehensive - ì¢…í•©ì ì¸ EDA ë¶„ì„
2. descriptive_stats - ê¸°ìˆ í†µê³„ ë° ë¶„í¬ ë¶„ì„  
3. correlation_analysis - ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
4. pattern_analysis - íŒ¨í„´ ë° íŠ¸ë Œë“œ ë¶„ì„
5. data_quality - ë°ì´í„° í’ˆì§ˆ ë¶„ì„

ì˜¤ì§ ìˆ«ìë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš” (1, 2, 3, 4, ë˜ëŠ” 5):
"""
            
            response = await llm.ainvoke(analysis_selector_prompt)
            selection = response.content.strip() if hasattr(response, 'content') else str(response).strip()
            
            # 4. ì„ íƒëœ ë¶„ì„ ìˆ˜í–‰ ì•Œë¦¼
            analysis_types = {
                "1": "ì¢…í•© EDA ë¶„ì„",
                "2": "ê¸°ìˆ í†µê³„ ë¶„ì„", 
                "3": "ìƒê´€ê´€ê³„ ë¶„ì„",
                "4": "íŒ¨í„´ ë¶„ì„",
                "5": "ë°ì´í„° í’ˆì§ˆ ë¶„ì„"
            }
            
            selected_analysis = analysis_types.get(selection, "ì¢…í•© EDA ë¶„ì„")
            
            progress_msg = f"""ğŸ¯ **ë¶„ì„ ìœ í˜• ì„ íƒ ì™„ë£Œ**: {selected_analysis}

ğŸ”„ **ìƒì„¸ ë¶„ì„ ìˆ˜í–‰ ì¤‘...**

"""
            streaming_response.append(progress_msg)
            
            # 5. ì‹¤ì œ ë¶„ì„ ìˆ˜í–‰ (ì ì§„ì  ê²°ê³¼ ì œê³µ)
            await asyncio.sleep(1)  # ë¶„ì„ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            
            if selection == "1":
                detailed_analysis = await self._generate_comprehensive_streaming_analysis(df, df_id, user_instruction)
            elif selection == "2":
                detailed_analysis = self._generate_descriptive_stats(df, df_id, user_instruction)
            elif selection == "3":
                detailed_analysis = self._generate_correlation_analysis(df, df_id, user_instruction)
            elif selection == "4":
                detailed_analysis = self._generate_pattern_analysis(df, df_id, user_instruction)
            elif selection == "5":
                detailed_analysis = self._generate_data_overview(df, df_id, user_instruction)
            else:
                detailed_analysis = await self._generate_comprehensive_streaming_analysis(df, df_id, user_instruction)
            
            streaming_response.append(detailed_analysis)
            
            # 6. ì™„ë£Œ ë©”ì‹œì§€
            completion_msg = f"""

---

âœ… **ë¶„ì„ ì™„ë£Œ!**  
ğŸ• **ì´ ì²˜ë¦¬ ì‹œê°„**: ~3ì´ˆ  
ğŸ”§ **ë¶„ì„ ì—”ì§„**: ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ë²”ìš© AI ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì—ì´ì „íŠ¸

"""
            streaming_response.append(completion_msg)
            
            return "".join(streaming_response)
            
        except Exception as e:
            logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            return f"ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _generate_comprehensive_streaming_analysis(self, df, df_id: str, instruction: str) -> str:
        """ì¢…í•© ë¶„ì„ì„ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì œê³µ"""
        
        analysis_parts = []
        
        # ë³€ìˆ˜ ìœ í˜• ë¶„ì„
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        
        analysis_parts.append("## ğŸ” **ë³€ìˆ˜ ìœ í˜•ë³„ ë¶„ì„**\n")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„ì„
        if len(numeric_cols) > 0:
            analysis_parts.append("### ğŸ“Š **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„ì„**")
            desc = df[numeric_cols].describe()
            
            for i, col in enumerate(numeric_cols[:5], 1):  # ìƒìœ„ 5ê°œ
                if col in desc.columns:
                    var_name = self._get_generic_column_name(df, col, i)
                    skew_val = df[col].skew()
                    distribution = "ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€" if abs(skew_val) < 0.5 else "ì¢Œí¸í–¥" if skew_val > 0.5 else "ìš°í¸í–¥"
                    
                    analysis_parts.append(f"\n**{var_name}**:")
                    analysis_parts.append(f"- í‰ê· : {desc.loc['mean', col]:.2f}, ì¤‘ì•™ê°’: {desc.loc['50%', col]:.2f}")
                    analysis_parts.append(f"- ë²”ìœ„: {desc.loc['min', col]:.2f} ~ {desc.loc['max', col]:.2f}")
                    analysis_parts.append(f"- ë¶„í¬ íŠ¹ì„±: {distribution}")
                    analysis_parts.append(f"- ê³ ìœ ê°’: {df[col].nunique()}ê°œ")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„
        if len(categorical_cols) > 0:
            analysis_parts.append("\n### ğŸ“ **ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„ì„**")
            
            for i, col in enumerate(categorical_cols[:5], 1):  # ìƒìœ„ 5ê°œ
                var_name = self._get_generic_column_name(df, col, i)
                unique_count = df[col].nunique()
                value_counts = df[col].value_counts().head(3)
                
                analysis_parts.append(f"\n**{var_name}**:")
                analysis_parts.append(f"- ê³ ìœ ê°’: {unique_count}ê°œ")
                analysis_parts.append(f"- ìƒìœ„ 3ê°œ ê°’:")
                for value, count in value_counts.items():
                    analysis_parts.append(f"  - {value}: {count}ê°œ ({count/len(df)*100:.1f}%)")
        
        # ë°ì´í„° í’ˆì§ˆ ë¶„ì„
        analysis_parts.append("\n## ğŸ” **ë°ì´í„° í’ˆì§ˆ ë¶„ì„**\n")
        
        missing_data = df.isnull().sum()
        if missing_data.sum() > 0:
            analysis_parts.append("### âš ï¸ **ê²°ì¸¡ê°’ ë¶„ì„**")
            missing_vars = missing_data[missing_data > 0]
            for i, (col, count) in enumerate(missing_vars.items(), 1):
                var_name = self._get_generic_column_name(df, col, i)
                analysis_parts.append(f"- **{var_name}**: {count}ê°œ ({count/len(df)*100:.1f}%)")
        else:
            analysis_parts.append("âœ… **ê²°ì¸¡ê°’ ì—†ìŒ**: ëª¨ë“  ë³€ìˆ˜ê°€ ì™„ì „í•©ë‹ˆë‹¤.")
        
        # ê´€ê³„ ë¶„ì„ (ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ 2ê°œ ì´ìƒì¸ ê²½ìš°)
        if len(numeric_cols) > 1:
            analysis_parts.append("\n## ğŸ”— **ë³€ìˆ˜ ê°„ ê´€ê³„ ë¶„ì„**\n")
            
            corr_matrix = df[numeric_cols].corr()
            strong_correlations = []
            
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > 0.5:
                        col1_name = self._get_generic_column_name(df, corr_matrix.columns[i])
                        col2_name = self._get_generic_column_name(df, corr_matrix.columns[j])
                        strong_correlations.append((col1_name, col2_name, corr_val))
            
            if strong_correlations:
                analysis_parts.append("### ğŸ“ˆ **ê°•í•œ ìƒê´€ê´€ê³„ (|r| > 0.5)**")
                for var1, var2, corr_val in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True)[:5]:
                    direction = "ì–‘ì˜" if corr_val > 0 else "ìŒì˜"
                    analysis_parts.append(f"- **{var1}** â†” **{var2}**: {direction} ìƒê´€ê´€ê³„ ({corr_val:.3f})")
            else:
                analysis_parts.append("ğŸ“Š **ì¤‘ê°„ ì •ë„ì˜ ìƒê´€ê´€ê³„**: ë³€ìˆ˜ë“¤ ê°„ì— ê°•í•œ ì„ í˜• ê´€ê³„ëŠ” ì—†ìŠµë‹ˆë‹¤.")
        
        # í•µì‹¬ ì¸ì‚¬ì´íŠ¸
        analysis_parts.append("\n## ğŸ’¡ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸**\n")
        
        total_entries = len(df)
        completeness = (1 - df.isnull().sum().sum() / (total_entries * len(df.columns))) * 100
        
        analysis_parts.append(f"1. **ë°ì´í„° ê·œëª¨**: {total_entries:,}ê°œ ê´€ì¸¡ê°’ìœ¼ë¡œ {'ì¶©ë¶„í•œ' if total_entries > 1000 else 'ì ì ˆí•œ' if total_entries > 100 else 'ì œí•œì ì¸'} ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        analysis_parts.append(f"2. **ë°ì´í„° í’ˆì§ˆ**: {completeness:.1f}%ì˜ ì™„ì„±ë„ë¡œ {'ìš°ìˆ˜í•œ' if completeness > 95 else 'ì–‘í˜¸í•œ' if completeness > 85 else 'ê°œì„ ì´ í•„ìš”í•œ'} ìˆ˜ì¤€ì…ë‹ˆë‹¤.")
        analysis_parts.append(f"3. **ë³€ìˆ˜ êµ¬ì„±**: ìˆ˜ì¹˜í˜• {len(numeric_cols)}ê°œ, ë²”ì£¼í˜• {len(categorical_cols)}ê°œë¡œ {'ê· í˜•ì¡íŒ' if len(numeric_cols) > 0 and len(categorical_cols) > 0 else 'ë‹¨ìˆœí•œ'} êµ¬ì¡°ì…ë‹ˆë‹¤.")
        
        if len(numeric_cols) > 2:
            analysis_parts.append("4. **ë¶„ì„ ê°€ëŠ¥ì„±**: ë‹¤ì–‘í•œ í†µê³„ ë¶„ì„ê³¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        
        # ì¶”ì²œ í›„ì† ë¶„ì„
        analysis_parts.append("\n## ğŸ“‹ **ì¶”ì²œ í›„ì† ë¶„ì„**\n")
        analysis_parts.append("1. **ì‹œê°í™”**: ë¶„í¬ë„, ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ, ë°•ìŠ¤í”Œë¡¯ ìƒì„±")
        analysis_parts.append("2. **ê³ ê¸‰ í†µê³„**: ê°€ì„¤ ê²€ì •, ë¶„ì‚° ë¶„ì„ ìˆ˜í–‰")
        if len(categorical_cols) > 0 and len(numeric_cols) > 0:
            analysis_parts.append("3. **ê·¸ë£¹ ë¶„ì„**: ë²”ì£¼ë³„ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¹„êµ ë¶„ì„")
        analysis_parts.append("4. **ì´ìƒê°’ íƒì§€**: í†µê³„ì  ì´ìƒê°’ ì‹ë³„ ë° ì²˜ë¦¬")
        if len(numeric_cols) > 3:
            analysis_parts.append("5. **ì°¨ì› ì¶•ì†Œ**: PCA, t-SNEë¥¼ í†µí•œ ë°ì´í„° êµ¬ì¡° íƒìƒ‰")
        
        return "\n".join(analysis_parts)
    
    def _generate_pattern_analysis(self, df, df_id: str, instruction: str) -> str:
        """ë²”ìš©ì  íŒ¨í„´ ë¶„ì„ (ë°”ì´ë„ˆë¦¬ í¸í–¥ ì œê±°)"""
        analysis_parts = []
        
        analysis_parts.append(f"## ğŸ“ˆ **ë°ì´í„° íŒ¨í„´ ë¶„ì„**\n")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            analysis_parts.append("### ğŸ“Š **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´**")
            
            for i, col in enumerate(numeric_cols[:4], 1):
                var_name = self._get_generic_column_name(df, col, i)
                
                # ë¶„í¬ íŠ¹ì„± ë¶„ì„
                skewness = df[col].skew()
                kurtosis = df[col].kurtosis()
                
                # ì´ìƒê°’ ë¶„ì„
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)][col]
                
                analysis_parts.append(f"\n**{var_name}**:")
                
                # ë¶„í¬ í˜•íƒœ
                if abs(skewness) < 0.5:
                    dist_shape = "ëŒ€ì¹­ì  ë¶„í¬"
                elif skewness > 0.5:
                    dist_shape = "ìš°ì¸¡ ê¼¬ë¦¬ê°€ ê¸´ ë¶„í¬"
                else:
                    dist_shape = "ì¢Œì¸¡ ê¼¬ë¦¬ê°€ ê¸´ ë¶„í¬"
                
                analysis_parts.append(f"- ë¶„í¬ í˜•íƒœ: {dist_shape}")
                analysis_parts.append(f"- ë³€ë™ì„±: {'ë†’ìŒ' if df[col].std() > df[col].mean() else 'ë³´í†µ' if df[col].std() > df[col].mean()/2 else 'ë‚®ìŒ'}")
                
                if len(outliers) > 0:
                    analysis_parts.append(f"- ì´ìƒê°’: {len(outliers)}ê°œ ({len(outliers)/len(df)*100:.1f}%)")
                else:
                    analysis_parts.append("- ì´ìƒê°’: ì—†ìŒ")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            analysis_parts.append("\n### ğŸ“ **ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´**")
            
            for i, col in enumerate(categorical_cols[:4], 1):
                var_name = self._get_generic_column_name(df, col, i)
                value_counts = df[col].value_counts()
                
                # ë¶„í¬ ê· ë“±ì„± ë¶„ì„
                max_freq = value_counts.max()
                min_freq = value_counts.min()
                balance_ratio = min_freq / max_freq
                
                analysis_parts.append(f"\n**{var_name}**:")
                analysis_parts.append(f"- ê³ ìœ ê°’ ìˆ˜: {len(value_counts)}ê°œ")
                
                if balance_ratio > 0.7:
                    balance_desc = "ê· ë“±í•œ ë¶„í¬"
                elif balance_ratio > 0.3:
                    balance_desc = "ì•½ê°„ ë¶ˆê· ë“±í•œ ë¶„í¬"
                else:
                    balance_desc = "ë§¤ìš° ë¶ˆê· ë“±í•œ ë¶„í¬"
                
                analysis_parts.append(f"- ë¶„í¬ ê· ë“±ì„±: {balance_desc}")
                
                # ìƒìœ„ ë¹ˆë„ ì¹´í…Œê³ ë¦¬
                top_categories = value_counts.head(3)
                analysis_parts.append("- ìƒìœ„ ì¹´í…Œê³ ë¦¬:")
                for cat, count in top_categories.items():
                    analysis_parts.append(f"  - {cat}: {count}ê°œ ({count/len(df)*100:.1f}%)")
        
        # ì „ì²´ ë°ì´í„° íŒ¨í„´ ìš”ì•½
        analysis_parts.append("\n### ğŸ” **ì „ì²´ ë°ì´í„° íŒ¨í„´ ìš”ì•½**")
        
        # ë‹¤ì–‘ì„± ì§€ìˆ˜
        total_unique_values = sum(df[col].nunique() for col in df.columns)
        diversity_index = total_unique_values / len(df)
        
        analysis_parts.append(f"- ë°ì´í„° ë‹¤ì–‘ì„±: {'ë†’ìŒ' if diversity_index > 0.5 else 'ë³´í†µ' if diversity_index > 0.2 else 'ë‚®ìŒ'}")
        
        # ì™„ì„±ë„
        completeness = (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100
        analysis_parts.append(f"- ë°ì´í„° ì™„ì„±ë„: {completeness:.1f}%")
        
        # êµ¬ì¡° ë³µì¡ì„±
        if len(numeric_cols) > 3 and len(categorical_cols) > 2:
            complexity = "ë³µì¡í•œ ë‹¤ì°¨ì› êµ¬ì¡°"
        elif len(numeric_cols) > 1 and len(categorical_cols) > 1:
            complexity = "ì¤‘ê°„ ë³µì¡ë„ êµ¬ì¡°"
        else:
            complexity = "ë‹¨ìˆœí•œ êµ¬ì¡°"
        
        analysis_parts.append(f"- êµ¬ì¡° ë³µì¡ì„±: {complexity}")
        
        return "\n".join(analysis_parts)
    
    def _prepare_data_context(self, df) -> str:
        """LLMì´ ë°ì´í„°ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„ (ì™„ì „ ë²”ìš©í™”)"""
        context_parts = []
        
        # ê¸°ë³¸ ì •ë³´
        context_parts.append(f"ë°ì´í„° í¬ê¸°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        # ë°ì´í„° ìœ í˜•ë³„ ë¶„ë¥˜
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        datetime_cols = df.select_dtypes(include=['datetime64']).columns
        
        context_parts.append(f"\në³€ìˆ˜ êµ¬ì„±:")
        context_parts.append(f"- ìˆ˜ì¹˜í˜• ë³€ìˆ˜: {len(numeric_cols)}ê°œ")
        context_parts.append(f"- ë²”ì£¼í˜• ë³€ìˆ˜: {len(categorical_cols)}ê°œ")
        context_parts.append(f"- ë‚ ì§œí˜• ë³€ìˆ˜: {len(datetime_cols)}ê°œ")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ íŠ¹ì„± (ë°”ì´ë„ˆë¦¬ êµ¬ë¶„ ì—†ì´)
        if len(numeric_cols) > 0:
            context_parts.append("\nìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìš”ì•½:")
            desc = df[numeric_cols].describe()
            for i, col in enumerate(numeric_cols[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ
                if col in desc.columns:
                    unique_count = df[col].nunique()
                    var_type = "ì´ì‚°í˜•" if unique_count < 20 else "ì—°ì†í˜•"
                    context_parts.append(f"- ìˆ˜ì¹˜í˜•{i} ({var_type}): í‰ê·  {desc.loc['mean', col]:.2f}, ê³ ìœ ê°’ {unique_count}ê°œ")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì •ë³´
        if len(categorical_cols) > 0:
            context_parts.append("\në²”ì£¼í˜• ë³€ìˆ˜ ì •ë³´:")
            for i, col in enumerate(categorical_cols[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ
                unique_count = df[col].nunique()
                cardinality = "ì €" if unique_count < 10 else "ì¤‘" if unique_count < 50 else "ê³ "
                top_values = df[col].value_counts().head(2)
                context_parts.append(f"- ë²”ì£¼í˜•{i} ({cardinality}ì¹´ë””ë„ë¦¬í‹°): {unique_count}ê°œ ê³ ìœ ê°’, ìƒìœ„ê°’ {dict(top_values)}")
        
        # ê²°ì¸¡ê°’ ì •ë³´
        missing_info = df.isnull().sum()
        if missing_info.sum() > 0:
            context_parts.append("\nê²°ì¸¡ê°’:")
            missing_count = 0
            for col, count in missing_info.items():
                if count > 0:
                    missing_count += 1
                    col_type = self._get_generic_column_name(df, col, missing_count)
                    context_parts.append(f"- {col_type}: {count}ê°œ ({count/len(df)*100:.1f}%)")
        
        # ë°ì´í„° íŠ¹ì„± ìš”ì•½
        context_parts.append(f"\në°ì´í„° íŠ¹ì„±:")
        context_parts.append(f"- ì™„ì„±ë„: {(1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100:.1f}%")
        context_parts.append(f"- ê·œëª¨: {'ëŒ€ìš©ëŸ‰' if len(df) > 10000 else 'ì¤‘ê°„' if len(df) > 1000 else 'ì†Œê·œëª¨'} ë°ì´í„°ì…‹")
        
        return "\n".join(context_parts)
    
    def _get_generic_column_name(self, df, col, index=None):
        """ì»¬ëŸ¼ì„ ë²”ìš©ì  ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        if df[col].dtype in ['int64', 'float64']:
            unique_count = df[col].nunique()
            if unique_count == 2 and set(df[col].dropna().unique()).issubset({0, 1, True, False}):
                return f"ì´ì§„ë³€ìˆ˜{index or ''}"
            elif unique_count < 20:
                return f"ì´ì‚°í˜•{index or ''}"
            else:
                return f"ì—°ì†í˜•{index or ''}"
        elif df[col].dtype in ['object', 'category']:
            unique_count = df[col].nunique()
            if unique_count < 10:
                return f"ë²”ì£¼í˜•{index or ''}"
            else:
                return f"ê³ ì¹´ë””ë„ë¦¬í‹°{index or ''}"
        elif 'datetime' in str(df[col].dtype):
            return f"ë‚ ì§œí˜•{index or ''}"
        else:
            return f"ê¸°íƒ€í˜•{index or ''}"

    def _generate_data_overview(self, df, df_id: str, instruction: str) -> str:
        """ë°ì´í„° êµ¬ì¡° ë° ê°œìš” ë¶„ì„ (ì™„ì „ ë²”ìš©í™”)"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ“‹ **ë°ì´í„° êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´
        analysis_parts.append("## ğŸ“Š **ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´**")
        analysis_parts.append(f"- **ì´ í–‰ ìˆ˜**: {df.shape[0]:,}ê°œ")
        analysis_parts.append(f"- **ì´ ì—´ ìˆ˜**: {df.shape[1]}ê°œ")
        analysis_parts.append(f"- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì… (ë²”ìš©í™”)
        analysis_parts.append("\n## ğŸ” **ë³€ìˆ˜ë³„ ìƒì„¸ ì •ë³´**")
        
        # ë³€ìˆ˜ ìœ í˜•ë³„ ì¹´ìš´í„°
        numeric_count = 0
        categorical_count = 0
        binary_count = 0
        
        for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):
            non_null_count = df[col].count()
            null_count = df[col].isnull().sum()
            
            # ë³€ìˆ˜ ìœ í˜• ê²°ì •
            if df[col].dtype in ['int64', 'float64']:
                if df[col].nunique() == 2 and set(df[col].unique()) == {0, 1}:
                    binary_count += 1
                    var_name = f"ë°”ì´ë„ˆë¦¬{binary_count}"
                else:
                    numeric_count += 1
                    var_name = f"ìˆ˜ì¹˜í˜•{numeric_count}"
            else:
                categorical_count += 1
                var_name = f"ë²”ì£¼í˜•{categorical_count}"
            
            analysis_parts.append(f"{i}. **{var_name}** ({dtype})")
            analysis_parts.append(f"   - ìœ íš¨ê°’: {non_null_count:,}ê°œ ({non_null_count/len(df)*100:.1f}%)")
            if null_count > 0:
                analysis_parts.append(f"   - ê²°ì¸¡ê°’: {null_count:,}ê°œ ({null_count/len(df)*100:.1f}%)")
        
        return "\n".join(analysis_parts)
    
    def _generate_descriptive_stats(self, df, df_id: str, instruction: str) -> str:
        """ê¸°ìˆ í†µê³„ ë° ë¶„í¬ ë¶„ì„ (ì™„ì „ ë²”ìš©í™”)"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ“ˆ **ê¸°ìˆ í†µê³„ ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„ (ë²”ìš©í™”)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            analysis_parts.append("## ğŸ”¢ **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê¸°ìˆ í†µê³„**")
            desc = df[numeric_cols].describe()
            
            numeric_count = 0
            binary_count = 0
            
            for col in numeric_cols:
                if col in desc.columns:
                    # ë°”ì´ë„ˆë¦¬ ë³€ìˆ˜ì™€ ì¼ë°˜ ìˆ˜ì¹˜í˜• ë³€ìˆ˜ êµ¬ë¶„
                    if df[col].nunique() == 2 and set(df[col].unique()) == {0, 1}:
                        binary_count += 1
                        var_name = f"ë°”ì´ë„ˆë¦¬{binary_count}"
                    else:
                        numeric_count += 1
                        var_name = f"ìˆ˜ì¹˜í˜•{numeric_count}"
                    
                    analysis_parts.append(f"\n**{var_name}**:")
                    analysis_parts.append(f"- í‰ê· : {desc.loc['mean', col]:.2f}")
                    analysis_parts.append(f"- ì¤‘ì•™ê°’: {desc.loc['50%', col]:.2f}")
                    analysis_parts.append(f"- í‘œì¤€í¸ì°¨: {desc.loc['std', col]:.2f}")
                    analysis_parts.append(f"- ìµœì†Ÿê°’: {desc.loc['min', col]:.2f}")
                    analysis_parts.append(f"- ìµœëŒ“ê°’: {desc.loc['max', col]:.2f}")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ í†µê³„ (ë²”ìš©í™”)
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            analysis_parts.append("\n## ğŸ“ **ë²”ì£¼í˜• ë³€ìˆ˜ ë¹ˆë„ ë¶„ì„**")
            for i, col in enumerate(categorical_cols[:3], 1):  # ìƒìœ„ 3ê°œë§Œ
                value_counts = df[col].value_counts().head(5)
                analysis_parts.append(f"\n**ë²”ì£¼í˜•{i} (ìƒìœ„ 5ê°œ ê°’):**")
                for value, count in value_counts.items():
                    analysis_parts.append(f"- {value}: {count:,}ê°œ ({count/len(df)*100:.1f}%)")
        
        return "\n".join(analysis_parts)
    
    def _generate_correlation_analysis(self, df, df_id: str, instruction: str) -> str:
        """ìƒê´€ê´€ê³„ ë¶„ì„ (ì™„ì „ ë²”ìš©í™”)"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ”— **ìƒê´€ê´€ê³„ ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ (ë²”ìš©í™”)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr()
            
            # ì»¬ëŸ¼ëª…ì„ ë²”ìš©ì  ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
            col_name_mapping = {}
            numeric_count = 0
            binary_count = 0
            
            for col in numeric_cols:
                if df[col].nunique() == 2 and set(df[col].unique()) == {0, 1}:
                    binary_count += 1
                    col_name_mapping[col] = f"ë°”ì´ë„ˆë¦¬{binary_count}"
                else:
                    numeric_count += 1
                    col_name_mapping[col] = f"ìˆ˜ì¹˜í˜•{numeric_count}"
            
            analysis_parts.append("## ğŸ“Š **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìƒê´€ê´€ê³„**")
            
            # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸° (|r| > 0.5)
            strong_correlations = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > 0.5:
                        col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]
                        var1_name = col_name_mapping.get(col1, col1)
                        var2_name = col_name_mapping.get(col2, col2)
                        strong_correlations.append((var1_name, var2_name, corr_val))
            
            if strong_correlations:
                analysis_parts.append("\n**ê°•í•œ ìƒê´€ê´€ê³„ (|r| > 0.5):**")
                for var1, var2, corr_val in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True):
                    analysis_parts.append(f"- **{var1}** â†” **{var2}**: {corr_val:.3f}")
            else:
                analysis_parts.append("\nê°•í•œ ìƒê´€ê´€ê³„(|r| > 0.5)ë¥¼ ë³´ì´ëŠ” ë³€ìˆ˜ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì•½ (ë²”ìš©í™”)
            analysis_parts.append("\n**ì „ì²´ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤:**")
            for col in numeric_cols[:4]:  # ìƒìœ„ 4ê°œ ë³€ìˆ˜ë§Œ
                var_name = col_name_mapping.get(col, col)
                analysis_parts.append(f"\n**{var_name}ê³¼ì˜ ìƒê´€ê´€ê³„:**")
                correlations = corr_matrix[col].drop(col).sort_values(key=abs, ascending=False)
                for other_col, corr_val in correlations.head(3).items():
                    other_var_name = col_name_mapping.get(other_col, other_col)
                    analysis_parts.append(f"- {other_var_name}: {corr_val:.3f}")
        else:
            analysis_parts.append("## âš ï¸ **ìƒê´€ê´€ê³„ ë¶„ì„ ë¶ˆê°€**")
            analysis_parts.append("ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ 2ê°œ ë¯¸ë§Œì´ì–´ì„œ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return "\n".join(analysis_parts)
    
    def _generate_trend_analysis(self, df, df_id: str, instruction: str) -> str:
        """íŠ¸ë Œë“œ ë° íŒ¨í„´ ë¶„ì„ (ì™„ì „ ë²”ìš©í™”)"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ“ˆ **íŠ¸ë Œë“œ ë° íŒ¨í„´ ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ì»¬ëŸ¼ëª… ë§¤í•‘ ìƒì„±
        col_name_mapping = {}
        numeric_count = 0
        categorical_count = 0
        binary_count = 0
        
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                if df[col].nunique() == 2 and set(df[col].unique()) == {0, 1}:
                    binary_count += 1
                    col_name_mapping[col] = f"ë°”ì´ë„ˆë¦¬{binary_count}"
                else:
                    numeric_count += 1
                    col_name_mapping[col] = f"ìˆ˜ì¹˜í˜•{numeric_count}"
            else:
                categorical_count += 1
                col_name_mapping[col] = f"ë²”ì£¼í˜•{categorical_count}"
        
        # 1. ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ë³€ìˆ˜ íŒ¨í„´ ë¶„ì„ (ë²”ìš©ì )
        binary_target_cols = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and df[col].nunique() == 2:
                unique_vals = sorted(df[col].unique())
                if set(unique_vals) == {0, 1}:
                    binary_target_cols.append(col)
        
        if binary_target_cols:
            analysis_parts.append("## ğŸ¯ **ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ë³€ìˆ˜ íŒ¨í„´ ë¶„ì„**")
            
            for target_col in binary_target_cols:
                target_name = col_name_mapping.get(target_col, target_col)
                positive_rate = df[target_col].mean() * 100
                analysis_parts.append(f"\n**{target_name} ë¶„í¬:**")
                analysis_parts.append(f"- ì–‘ì„±(1): {df[target_col].sum():,}ê°œ ({positive_rate:.1f}%)")
                analysis_parts.append(f"- ìŒì„±(0): {(df[target_col] == 0).sum():,}ê°œ ({100-positive_rate:.1f}%)")
                
                # ë²”ì£¼í˜• ë³€ìˆ˜ì™€ì˜ ê´€ê³„ ë¶„ì„
                categorical_cols = df.select_dtypes(include=['object', 'category']).columns
                for cat_col in categorical_cols[:2]:  # ìƒìœ„ 2ê°œ ë²”ì£¼í˜• ë³€ìˆ˜
                    cat_name = col_name_mapping.get(cat_col, cat_col)
                    analysis_parts.append(f"\n**{cat_name}ë³„ {target_name} íŒ¨í„´:**")
                    group_stats = df.groupby(cat_col)[target_col].agg(['count', 'sum', 'mean'])
                    for category in group_stats.index[:4]:  # ìƒìœ„ 4ê°œ ì¹´í…Œê³ ë¦¬
                        total = group_stats.loc[category, 'count']
                        positive = group_stats.loc[category, 'sum']
                        rate = group_stats.loc[category, 'mean'] * 100
                        analysis_parts.append(f"- **{category}**: {positive}/{total}ê°œ ({rate:.1f}%)")
        
        # 2. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´ (ë²”ìš©í™”)
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            analysis_parts.append("\n## ğŸ“Š **ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´**")
            for col in categorical_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ
                col_name = col_name_mapping.get(col, col)
                value_counts = df[col].value_counts()
                total_unique = df[col].nunique()
                analysis_parts.append(f"\n**{col_name} ({total_unique}ê°œ ê³ ìœ ê°’):**")
                for i, (value, count) in enumerate(value_counts.head(4).items()):
                    analysis_parts.append(f"{i+1}. {value}: {count:,}ê°œ ({count/len(df)*100:.1f}%)")
        
        # 3. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´ (ë²”ìš©í™”)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            analysis_parts.append("\n## ğŸ“ˆ **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ íŠ¹ì„±**")
            desc = df[numeric_cols].describe()
            for col in numeric_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ
                if col in desc.columns:
                    col_name = col_name_mapping.get(col, col)
                    skewness = df[col].skew()
                    outlier_threshold = desc.loc['75%', col] + 1.5 * (desc.loc['75%', col] - desc.loc['25%', col])
                    outliers = (df[col] > outlier_threshold).sum()
                    
                    analysis_parts.append(f"\n**{col_name}:**")
                    analysis_parts.append(f"- ë²”ìœ„: {desc.loc['min', col]:.2f} ~ {desc.loc['max', col]:.2f}")
                    analysis_parts.append(f"- ë¶„í¬: {'ì™¼ìª½ ì¹˜ìš°ì¹¨' if skewness > 1 else 'ì˜¤ë¥¸ìª½ ì¹˜ìš°ì¹¨' if skewness < -1 else 'ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€'}")
                    if outliers > 0:
                        analysis_parts.append(f"- ì´ìƒê°’: {outliers}ê°œ ({outliers/len(df)*100:.1f}%)")
        
        return "\n".join(analysis_parts)
    
    def _generate_insights_summary(self, df, df_id: str, instruction: str) -> str:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë° ìš”ì•½ (ì™„ì „ ë²”ìš©í™”)"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ’¡ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ì»¬ëŸ¼ëª… ë§¤í•‘ ìƒì„±
        col_name_mapping = {}
        numeric_count = 0
        categorical_count = 0
        binary_count = 0
        
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64']:
                if df[col].nunique() == 2 and set(df[col].unique()) == {0, 1}:
                    binary_count += 1
                    col_name_mapping[col] = f"ë°”ì´ë„ˆë¦¬{binary_count}"
                else:
                    numeric_count += 1
                    col_name_mapping[col] = f"ìˆ˜ì¹˜í˜•{numeric_count}"
            else:
                categorical_count += 1
                col_name_mapping[col] = f"ë²”ì£¼í˜•{categorical_count}"
        
        # ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸
        total_entries = len(df)
        missing_data = df.isnull().sum().sum()
        completeness = (1 - missing_data / (total_entries * len(df.columns))) * 100
        
        analysis_parts.append("## ğŸ” **í•µì‹¬ ë°œê²¬ì‚¬í•­**")
        
        analysis_parts.append(f"\n**1. ë°ì´í„° í’ˆì§ˆ**")
        analysis_parts.append(f"- ë°ì´í„° ì™„ì„±ë„: {completeness:.1f}%")
        analysis_parts.append(f"- ì´ {total_entries:,}ê°œ ê´€ì¸¡ê°’ìœ¼ë¡œ {'ì¶©ë¶„í•œ' if total_entries > 1000 else 'ì œí•œì ì¸'} ë¶„ì„ ê°€ëŠ¥")
        
        # ë²”ìš©ì ì¸ ë°ì´í„° ì¸ì‚¬ì´íŠ¸
        analysis_parts.append(f"\n**2. í•µì‹¬ ë°ì´í„° ì¸ì‚¬ì´íŠ¸**")
        
        # ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ë³€ìˆ˜ ì¸ì‚¬ì´íŠ¸ (ë²”ìš©í™”)
        binary_targets = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and df[col].nunique() == 2:
                unique_vals = sorted(df[col].unique())
                if set(unique_vals) == {0, 1}:
                    positive_rate = df[col].mean() * 100
                    col_name = col_name_mapping.get(col, col)
                    binary_targets.append((col_name, positive_rate))
        
        if binary_targets:
            for target_name, rate in binary_targets:
                balance_status = "ê· í˜•ì¡íŒ" if 40 <= rate <= 60 else "ë¶ˆê· í˜•í•œ"
                analysis_parts.append(f"- {target_name}: {rate:.1f}% ì–‘ì„±ë¥ ë¡œ {balance_status} ë¶„í¬")
        
        # ê²°ì¸¡ê°’ íŒ¨í„´ ì¸ì‚¬ì´íŠ¸ (ë²”ìš©í™”)
        missing_rates = df.isnull().mean() * 100
        high_missing = missing_rates[missing_rates > 20]
        if len(high_missing) > 0:
            missing_var_names = [col_name_mapping.get(col, col) for col in high_missing.index]
            analysis_parts.append(f"- ê²°ì¸¡ê°’ ì£¼ì˜: {missing_var_names} ë³€ìˆ˜ì˜ ê²°ì¸¡ë¥ ì´ 20% ì´ìƒ")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ë‹¤ì–‘ì„± ì¸ì‚¬ì´íŠ¸ (ë²”ìš©í™”)
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            high_cardinality = [col for col in categorical_cols if df[col].nunique() > len(df) * 0.1]
            if high_cardinality:
                high_card_names = [col_name_mapping.get(col, col) for col in high_cardinality]
                analysis_parts.append(f"- ê³ ìœ ê°’ ê³¼ë‹¤: {high_card_names} ë³€ìˆ˜ëŠ” ë²”ì£¼ ìˆ˜ê°€ ë§¤ìš° ë†’ìŒ")
        
        # ë°ì´í„° êµ¬ì¡° ì¸ì‚¬ì´íŠ¸
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        
        analysis_parts.append(f"\n**3. ë°ì´í„° êµ¬ì¡° íŠ¹ì§•**")
        analysis_parts.append(f"- ìˆ˜ì¹˜í˜• ë³€ìˆ˜ {len(numeric_cols)}ê°œ, ë²”ì£¼í˜• ë³€ìˆ˜ {len(categorical_cols)}ê°œ")
        analysis_parts.append(f"- ë‹¤ì–‘í•œ ê´€ì ì˜ ë¶„ì„ì´ ê°€ëŠ¥í•œ {'ê· í˜•ì¡íŒ' if len(numeric_cols) > 2 and len(categorical_cols) > 2 else 'ë‹¨ìˆœí•œ'} êµ¬ì¡°")
        
        # ì¶”ì²œì‚¬í•­
        analysis_parts.append(f"\n## ğŸ“‹ **ì¶”ì²œ í›„ì† ë¶„ì„**")
        analysis_parts.append("1. **ì‹œê°í™”**: ì£¼ìš” íŒ¨í„´ì„ ê·¸ë˜í”„ë¡œ í‘œí˜„")
        analysis_parts.append("2. **ì˜ˆì¸¡ ëª¨ë¸ë§**: íƒ€ê²Ÿ ë³€ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•")
        analysis_parts.append("3. **ì„¸ë¶„í™” ë¶„ì„**: íŠ¹ì • ê·¸ë£¹ë³„ ìƒì„¸ ë¶„ì„")
        analysis_parts.append("4. **ì´ìƒê°’ ë¶„ì„**: íŠ¹ì´í•œ ì¼€ì´ìŠ¤ íƒì§€")
        
        return "\n".join(analysis_parts)
    
    def _generate_comprehensive_analysis(self, df, df_id: str, instruction: str) -> str:
        """ì¢…í•© ë¶„ì„ (ê¸°ë³¸ê°’)"""
        analysis_parts = []
        
        analysis_parts.append("# ğŸ“Š **ì¢…í•© ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**í¬ê¸°**: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ë°ì´í„° ê°œìš”
        analysis_parts.append("## ğŸ“‹ **ë°ì´í„° ê°œìš”**")
        analysis_parts.append("**ì»¬ëŸ¼ ì •ë³´:**")
        for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):
            analysis_parts.append(f"{i}. **{col}** ({dtype})")
        
        # ê¸°ë³¸ í†µê³„
        analysis_parts.append("\n## ğŸ“ˆ **ê¸°ë³¸ í†µê³„**")
        desc = df.describe()
        if not desc.empty:
            analysis_parts.append("**ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„:**")
            for col in desc.columns[:3]:  # ì²˜ìŒ 3ê°œ ì»¬ëŸ¼ë§Œ
                analysis_parts.append(f"- **{col}**: í‰ê·  {desc.loc['mean', col]:.2f}, í‘œì¤€í¸ì°¨ {desc.loc['std', col]:.2f}")
        
        # ê²°ì¸¡ì¹˜ ë¶„ì„
        missing = df.isnull().sum()
        if missing.sum() > 0:
            analysis_parts.append("\n## âš ï¸ **ê²°ì¸¡ì¹˜ ë¶„ì„**")
            for col, count in missing.items():
                if count > 0:
                    pct = (count / len(df)) * 100
                    analysis_parts.append(f"- **{col}**: {count}ê°œ ({pct:.1f}%)")
        
        # ë²”ìš©ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„
        binary_target_cols = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and df[col].nunique() == 2:
                unique_vals = sorted(df[col].unique())
                if set(unique_vals) == {0, 1}:
                    binary_target_cols.append(col)
        
        if binary_target_cols:
            analysis_parts.append("\n## ğŸ¯ **íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„**")
            for target_col in binary_target_cols:
                positive_rate = df[target_col].mean() * 100
                analysis_parts.append(f"- **{target_col} ì–‘ì„±ë¥ **: {positive_rate:.1f}%")
                
                # ë²”ì£¼í˜• ë³€ìˆ˜ì™€ì˜ ê´€ê³„
                categorical_cols = df.select_dtypes(include=['object', 'category']).columns
                for cat_col in categorical_cols[:2]:  # ìƒìœ„ 2ê°œë§Œ
                    if len(df.groupby(cat_col)[target_col].mean()) > 1:
                        group_means = df.groupby(cat_col)[target_col].mean() * 100
                        top_categories = group_means.head(3)
                        analysis_parts.append(f"- **{cat_col}ë³„ {target_col}**: {dict(top_categories.round(1))}")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            analysis_parts.append("\n## ğŸ“Š **ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬**")
            for cat_col in categorical_cols[:2]:  # ìƒìœ„ 2ê°œë§Œ
                value_counts = df[cat_col].value_counts()
                analysis_parts.append(f"- **{cat_col}**: {dict(value_counts.head(3))}")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            analysis_parts.append("\n## ğŸ“ˆ **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ íŠ¹ì„±**")
            for num_col in numeric_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ
                if num_col not in binary_target_cols:  # ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ì œì™¸
                    skewness = df[num_col].skew()
                    outliers_count = len(df[df[num_col] > df[num_col].quantile(0.75) + 1.5 * (df[num_col].quantile(0.75) - df[num_col].quantile(0.25))])
                    analysis_parts.append(f"- **{num_col}**: {'ì •ê·œë¶„í¬' if abs(skewness) < 1 else 'ì¹˜ìš°ì¹œ ë¶„í¬'}, ì´ìƒê°’ {outliers_count}ê°œ")
        
        # ì¶”ì²œì‚¬í•­
        analysis_parts.append("\n## ğŸ’¡ **ë¶„ì„ ì¶”ì²œì‚¬í•­**")
        analysis_parts.append("1. ğŸ” **ìƒê´€ê´€ê³„ ë¶„ì„**: ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ê°„ì˜ ê´€ê³„ íƒìƒ‰")
        analysis_parts.append("2. ğŸ“Š **ì‹œê°í™”**: íˆìŠ¤í† ê·¸ë¨, ìƒìê·¸ë¦¼ ë“±ìœ¼ë¡œ ë¶„í¬ í™•ì¸")
        analysis_parts.append("3. ğŸ¯ **ì„¸ë¶„í™” ë¶„ì„**: ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„ ìˆ˜í–‰")
        
        return "\n".join(analysis_parts)

# 2. AgentExecutor êµ¬í˜„ (ê³µì‹ Hello World Agent íŒ¨í„´)
class PandasAgentExecutor(AgentExecutor):
    """ê³µì‹ Hello World Agent íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” AgentExecutor"""
    
    def __init__(self):
        self.agent = PandasDataAnalysisAgent()
        logger.info("ğŸ”§ PandasAgentExecutor ì´ˆê¸°í™” ì™„ë£Œ")

    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """A2A SDK í‘œì¤€ ì‹¤í–‰ - ìŠ¤íŠ¸ë¦¬ë° ì§€ì› (ê³µì‹ Hello World Agent íŒ¨í„´)"""
        logger.info("ğŸ¯ PandasAgentExecutor.execute() í˜¸ì¶œë¨")
        
        try:
            # ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ (ê³µì‹ íŒ¨í„´)
            user_message = context.get_user_input()
            logger.info(f"ğŸ“ ì‚¬ìš©ì ì…ë ¥: {user_message}")
            
            # ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ í™•ì¸ (í‚¤ì›Œë“œ ê¸°ë°˜)
            streaming_keywords = ["eda", "ë¶„ì„", "ì‹¤ì‹œê°„", "ìŠ¤íŠ¸ë¦¬ë°", "progress", "ì¢…í•©", "ìƒì„¸"]
            should_stream = any(keyword in user_message.lower() for keyword in streaming_keywords)
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
            if should_stream:
                logger.info("ğŸ”„ ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ë¶„ì„ ìˆ˜í–‰")
                result = await self.agent.invoke(user_message, stream=True)
            else:
                logger.info("ğŸ“Š ì¼ë°˜ ëª¨ë“œë¡œ ë¶„ì„ ìˆ˜í–‰")
                result = await self.agent.invoke(user_message, stream=False)
            
            # ê²°ê³¼ ì „ì†¡ (ê³µì‹ íŒ¨í„´ - ì¤‘ìš”: await ì¶”ê°€!)
            message = new_agent_text_message(result)
            await event_queue.enqueue_event(message)
            
            logger.info("âœ… Task completed successfully")
            
        except Exception as e:
            logger.error(f"âŒ Error in execute: {e}", exc_info=True)
            error_message = new_agent_text_message(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            await event_queue.enqueue_event(error_message)

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Task ì·¨ì†Œ ì²˜ë¦¬ (ê³µì‹ Hello World Agent íŒ¨í„´)"""
        logger.info("ğŸ›‘ PandasAgentExecutor.cancel() í˜¸ì¶œë¨")
        raise Exception("Cancel not supported")

# 3. Agent Card ìƒì„± (ê³µì‹ A2A í‘œì¤€ ë©”íƒ€ë°ì´í„°)
def create_agent_card() -> AgentCard:
    """A2A í‘œì¤€ Agent Card ìƒì„± (ê³µì‹ Hello World Agent íŒ¨í„´)"""
    
    # ê¸°ë³¸ ìŠ¤í‚¬ ì •ì˜ (ê³µì‹ íŒ¨í„´)
    skill = AgentSkill(
        id="pandas_data_analysis",
        name="Pandas Data Analysis",
        description="Performs comprehensive data analysis on uploaded datasets using pandas",
        tags=["data", "analysis", "pandas", "statistics", "EDA"],
        examples=["Analyze my data", "What insights can you find?", "Show me data statistics"]
    )
    
    return AgentCard(
        name="Pandas Data Analyst",
        description="A comprehensive data analysis agent powered by pandas and AI",
        url="http://localhost:10001/",
        version="2.0.0",
        capabilities=AgentCapabilities(streaming=False),
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        skills=[skill]
    )

# 4. Wire everything together (ê³µì‹ Hello World Agent íŒ¨í„´)
@click.command()
@click.option('--host', default='localhost', help='Host to bind to')
@click.option('--port', default=10001, help='Port to bind to')
def main(host: str, port: int):
    """A2A í‘œì¤€ Pandas ì„œë²„ ì‹¤í–‰ (ê³µì‹ Hello World Agent íŒ¨í„´)"""
    
    logger.info("ğŸš€ Starting Pandas A2A Server...")
    
    # Agent Card ìƒì„±
    agent_card = create_agent_card()
    
    # RequestHandler ì´ˆê¸°í™” (ê³µì‹ íŒ¨í„´)
    request_handler = DefaultRequestHandler(
        agent_executor=PandasAgentExecutor(),
        task_store=InMemoryTaskStore()
    )
    
    # A2A Starlette Application ìƒì„± (ê³µì‹ íŒ¨í„´)
    a2a_app = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler
    )
    
    logger.info(f"ğŸŒ Server starting at http://{host}:{port}")
    logger.info("ğŸ“‹ Agent Card available at /.well-known/agent.json")
    
    # Uvicornìœ¼ë¡œ ì„œë²„ ì‹¤í–‰
    uvicorn.run(a2a_app.build(), host=host, port=port)

if __name__ == "__main__":
    main() 