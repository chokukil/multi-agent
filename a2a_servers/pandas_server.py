import asyncio
import logging
import os
import pandas as pd
import numpy as np
import re
from datetime import datetime
from typing import Dict, Any, AsyncGenerator
import uvicorn
import click

# A2A SDK ê³µì‹ ì»´í¬ë„ŒíŠ¸ ì‚¬ìš© (ê³µì‹ Hello World Agent íŒ¨í„´)
import uuid
from a2a.server.apps import A2AStarletteApplication
from a2a.server.request_handlers import DefaultRequestHandler
from a2a.server.agent_execution.agent_executor import AgentExecutor, RequestContext
from a2a.server.events.event_queue import EventQueue
from a2a.server.tasks.inmemory_task_store import InMemoryTaskStore
from a2a.types import AgentCard, AgentSkill, Message, Task, AgentCapabilities
from a2a.utils.message import new_agent_text_message, get_message_text

from langchain_ollama import ChatOllama

# Import core modules
import sys
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from core.data_manager import DataManager

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global data manager instance
data_manager = DataManager()

# 1. Define the core agent (ê³µì‹ Hello World Agent íŒ¨í„´)
class PandasDataAnalysisAgent:
    """Pandas ë°ì´í„° ë¶„ì„ ì—ì´ì „íŠ¸ (ê³µì‹ Hello World Agent íŒ¨í„´)"""
    
    async def invoke(self, user_input: str = "") -> str:
        """
        ë°ì´í„° ë¶„ì„ ìˆ˜í–‰ (ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ì— ë”°ë¥¸ ë§ì¶¤í˜• ë¶„ì„)
        """
        logger.info(f"ğŸ¯ PandasDataAnalysisAgent.invoke() called with: {user_input}")
        
        try:
            # ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°í”„ë ˆì„ í™•ì¸
            available_dfs = data_manager.list_dataframes()
            logger.info(f"ğŸ’¾ Available dataframes: {available_dfs}")
            
            if not available_dfs:
                result_text = """âŒ **ë°ì´í„° ì—†ìŒ**

**ë¬¸ì œ**: ì•„ì§ ì—…ë¡œë“œëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.

**í•´ê²°ë°©ë²•:**
1. ğŸ”„ **ë°ì´í„° ë¡œë”** í˜ì´ì§€ë¡œ ì´ë™
2. ğŸ“ CSV, Excel ë“±ì˜ ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ  
3. ğŸ“Š ë‹¤ì‹œ ëŒì•„ì™€ì„œ ë°ì´í„° ë¶„ì„ ìš”ì²­

**í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹**: ì—†ìŒ
"""
                return result_text
            
            # ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ ì‚¬ìš©
            df_id = available_dfs[0]
            df = data_manager.get_dataframe(df_id)
            
            if df is None:
                return "âŒ ë°ì´í„°í”„ë ˆì„ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            logger.info(f"ğŸ“Š Analyzing dataframe: {df_id}, shape: {df.shape}")
            
            # ì‚¬ìš©ì ì§€ì‹œì‚¬í•­ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ ë¶„ì„ ìˆ˜í–‰
            return await self._perform_targeted_analysis(df, df_id, user_input)
            
        except Exception as e:
            logger.error(f"âŒ Error in analyze_data: {e}", exc_info=True)
            return f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _perform_targeted_analysis(self, df, df_id: str, user_instruction: str) -> str:
        """LLMì´ ì§€ì‹œì‚¬í•­ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ë¶„ì„ì„ ìë™ìœ¼ë¡œ ì„ íƒí•˜ì—¬ ìˆ˜í–‰"""
        
        # LLMì—ê²Œ ì§€ì‹œì‚¬í•­ì„ í•´ì„í•˜ê³  ì ì ˆí•œ ë¶„ì„ì„ ìš”ì²­
        analysis_director_prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ì—¬ ê°€ì¥ ì ì ˆí•œ ë°ì´í„° ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.

ë°ì´í„°ì…‹ ì •ë³´:
- ì´ë¦„: {df_id}
- í¬ê¸°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´
- ì»¬ëŸ¼: {', '.join(df.columns[:10])}{'...' if len(df.columns) > 10 else ''}

ì‚¬ìš©ì ìš”ì²­: "{user_instruction}"

ìœ„ ìš”ì²­ì— ê°€ì¥ ì í•©í•œ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ , ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

# ğŸ“Š **[ë¶„ì„ ì œëª©]**

**ìš”ì²­**: {user_instruction}
**ë°ì´í„°ì…‹**: {df_id}
**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## [ì ì ˆí•œ ì„¹ì…˜ë“¤]

[ì‹¤ì œ ë°ì´í„°ë¥¼ í™œìš©í•œ êµ¬ì²´ì ì¸ ë¶„ì„ ê²°ê³¼]

ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì •í™•íˆ ì´í•´í•˜ê³ , ë°ì´í„°ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ê°€ì¥ ìœ ìš©í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ìˆë‹¤ë©´ í•´ë‹¹ ë³€ìˆ˜ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ë¶„ì„ì„, ì¼ë°˜ ë°ì´í„°ë¼ë©´ ì ì ˆí•œ EDAë¥¼ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
"""

        try:
            # LLM í˜¸ì¶œì„ ìœ„í•œ ì„¤ì •
            from langchain_ollama import ChatOllama
            
            # Ollama LLM ì´ˆê¸°í™”
            llm = ChatOllama(
                model="qwen2.5:latest",
                temperature=0.1,
                base_url="http://localhost:11434"
            )
            
            # ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
            data_context = self._prepare_data_context(df)
            
            # ìµœì¢… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            final_prompt = f"""{analysis_director_prompt}

ë°ì´í„° ì»¨í…ìŠ¤íŠ¸:
{data_context}

ì‚¬ìš©ìê°€ ìš”ì²­í•œ êµ¬ì²´ì ì¸ ë¶„ì„ì„ ë°ì´í„°ì— ê¸°ë°˜í•˜ì—¬ ìˆ˜í–‰í•´ì£¼ì„¸ìš”."""

            # LLMì—ê²Œ ë¶„ì„ ìš”ì²­
            response = await llm.ainvoke(final_prompt)
            
            # ì‘ë‹µ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            if hasattr(response, 'content'):
                return response.content
            else:
                return str(response)
                
        except Exception as e:
            logger.error(f"âŒ LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
            # í´ë°±: ê¸°ë³¸ ë¶„ì„
            return self._generate_comprehensive_analysis(df, df_id, user_instruction)
    
    def _prepare_data_context(self, df) -> str:
        """LLMì´ ë°ì´í„°ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¤€ë¹„"""
        context_parts = []
        
        # ê¸°ë³¸ ì •ë³´
        context_parts.append(f"ë°ì´í„° í¬ê¸°: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        
        # ì»¬ëŸ¼ ì •ë³´ì™€ ë°ì´í„° íƒ€ì…
        context_parts.append("ì»¬ëŸ¼ ì •ë³´:")
        for col, dtype in zip(df.columns, df.dtypes):
            sample_values = df[col].dropna().head(3).tolist()
            context_parts.append(f"- {col} ({dtype}): ì˜ˆì‹œê°’ {sample_values}")
        
        # ê²°ì¸¡ê°’ ì •ë³´
        missing_info = df.isnull().sum()
        if missing_info.sum() > 0:
            context_parts.append("\nê²°ì¸¡ê°’:")
            for col, count in missing_info.items():
                if count > 0:
                    context_parts.append(f"- {col}: {count}ê°œ ({count/len(df)*100:.1f}%)")
        
        # ìˆ˜ì¹˜í˜• ë°ì´í„° ê¸°ë³¸ í†µê³„
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            context_parts.append("\nìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìš”ì•½:")
            desc = df[numeric_cols].describe()
            for col in numeric_cols[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                if col in desc.columns:
                    context_parts.append(f"- {col}: í‰ê·  {desc.loc['mean', col]:.2f}, ë²”ìœ„ {desc.loc['min', col]:.2f}~{desc.loc['max', col]:.2f}")
        
        # ë²”ì£¼í˜• ë°ì´í„° ì •ë³´
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            context_parts.append("\në²”ì£¼í˜• ë³€ìˆ˜ ì •ë³´:")
            for col in categorical_cols[:3]:  # ì²˜ìŒ 3ê°œë§Œ
                unique_count = df[col].nunique()
                top_values = df[col].value_counts().head(3)
                context_parts.append(f"- {col}: {unique_count}ê°œ ê³ ìœ ê°’, ìƒìœ„ê°’ {dict(top_values)}")
        
        # ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ì»¬ëŸ¼ ìë™ ê°ì§€ (ë²”ìš©ì )
        binary_target_info = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and df[col].nunique() == 2:
                unique_vals = sorted(df[col].unique())
                if set(unique_vals) == {0, 1}:
                    positive_rate = df[col].mean() * 100
                    binary_target_info.append(f"{col}: {positive_rate:.1f}% ì–‘ì„±")
        
        if binary_target_info:
            context_parts.append(f"\në°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ: {', '.join(binary_target_info)}")
        
        return "\n".join(context_parts)

    def _generate_data_overview(self, df, df_id: str, instruction: str) -> str:
        """ë°ì´í„° êµ¬ì¡° ë° ê°œìš” ë¶„ì„"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ“‹ **ë°ì´í„° êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´
        analysis_parts.append("## ğŸ“Š **ë°ì´í„°ì…‹ ê¸°ë³¸ ì •ë³´**")
        analysis_parts.append(f"- **ì´ í–‰ ìˆ˜**: {df.shape[0]:,}ê°œ")
        analysis_parts.append(f"- **ì´ ì—´ ìˆ˜**: {df.shape[1]}ê°œ")
        analysis_parts.append(f"- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
        
        # ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì…
        analysis_parts.append("\n## ğŸ” **ì»¬ëŸ¼ë³„ ìƒì„¸ ì •ë³´**")
        for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):
            non_null_count = df[col].count()
            null_count = df[col].isnull().sum()
            analysis_parts.append(f"{i}. **{col}** ({dtype})")
            analysis_parts.append(f"   - ìœ íš¨ê°’: {non_null_count:,}ê°œ ({non_null_count/len(df)*100:.1f}%)")
            if null_count > 0:
                analysis_parts.append(f"   - ê²°ì¸¡ê°’: {null_count:,}ê°œ ({null_count/len(df)*100:.1f}%)")
        
        return "\n".join(analysis_parts)
    
    def _generate_descriptive_stats(self, df, df_id: str, instruction: str) -> str:
        """ê¸°ìˆ í†µê³„ ë° ë¶„í¬ ë¶„ì„"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ“ˆ **ê¸°ìˆ í†µê³„ ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            analysis_parts.append("## ğŸ”¢ **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ê¸°ìˆ í†µê³„**")
            desc = df[numeric_cols].describe()
            for col in numeric_cols:
                if col in desc.columns:
                    analysis_parts.append(f"\n**{col}**:")
                    analysis_parts.append(f"- í‰ê· : {desc.loc['mean', col]:.2f}")
                    analysis_parts.append(f"- ì¤‘ì•™ê°’: {desc.loc['50%', col]:.2f}")
                    analysis_parts.append(f"- í‘œì¤€í¸ì°¨: {desc.loc['std', col]:.2f}")
                    analysis_parts.append(f"- ìµœì†Ÿê°’: {desc.loc['min', col]:.2f}")
                    analysis_parts.append(f"- ìµœëŒ“ê°’: {desc.loc['max', col]:.2f}")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ í†µê³„
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            analysis_parts.append("\n## ğŸ“ **ë²”ì£¼í˜• ë³€ìˆ˜ ë¹ˆë„ ë¶„ì„**")
            for col in categorical_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ
                value_counts = df[col].value_counts().head(5)
                analysis_parts.append(f"\n**{col} (ìƒìœ„ 5ê°œ ê°’):**")
                for value, count in value_counts.items():
                    analysis_parts.append(f"- {value}: {count:,}ê°œ ({count/len(df)*100:.1f}%)")
        
        return "\n".join(analysis_parts)
    
    def _generate_correlation_analysis(self, df, df_id: str, instruction: str) -> str:
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ”— **ìƒê´€ê´€ê³„ ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 1:
            corr_matrix = df[numeric_cols].corr()
            
            analysis_parts.append("## ğŸ“Š **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ìƒê´€ê´€ê³„**")
            
            # ê°•í•œ ìƒê´€ê´€ê³„ ì°¾ê¸° (|r| > 0.5)
            strong_correlations = []
            for i in range(len(corr_matrix.columns)):
                for j in range(i+1, len(corr_matrix.columns)):
                    corr_val = corr_matrix.iloc[i, j]
                    if abs(corr_val) > 0.5:
                        col1, col2 = corr_matrix.columns[i], corr_matrix.columns[j]
                        strong_correlations.append((col1, col2, corr_val))
            
            if strong_correlations:
                analysis_parts.append("\n**ê°•í•œ ìƒê´€ê´€ê³„ (|r| > 0.5):**")
                for col1, col2, corr_val in sorted(strong_correlations, key=lambda x: abs(x[2]), reverse=True):
                    analysis_parts.append(f"- **{col1}** â†” **{col2}**: {corr_val:.3f}")
            else:
                analysis_parts.append("\nê°•í•œ ìƒê´€ê´€ê³„(|r| > 0.5)ë¥¼ ë³´ì´ëŠ” ë³€ìˆ˜ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì•½
            analysis_parts.append("\n**ì „ì²´ ìƒê´€ê´€ê³„ ë§¤íŠ¸ë¦­ìŠ¤:**")
            for col in numeric_cols[:4]:  # ìƒìœ„ 4ê°œ ë³€ìˆ˜ë§Œ
                analysis_parts.append(f"\n**{col}ê³¼ì˜ ìƒê´€ê´€ê³„:**")
                correlations = corr_matrix[col].drop(col).sort_values(key=abs, ascending=False)
                for other_col, corr_val in correlations.head(3).items():
                    analysis_parts.append(f"- {other_col}: {corr_val:.3f}")
        else:
            analysis_parts.append("## âš ï¸ **ìƒê´€ê´€ê³„ ë¶„ì„ ë¶ˆê°€**")
            analysis_parts.append("ìˆ˜ì¹˜í˜• ë³€ìˆ˜ê°€ 2ê°œ ë¯¸ë§Œì´ì–´ì„œ ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return "\n".join(analysis_parts)
    
    def _generate_trend_analysis(self, df, df_id: str, instruction: str) -> str:
        """íŠ¸ë Œë“œ ë° íŒ¨í„´ ë¶„ì„"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ“ˆ **íŠ¸ë Œë“œ ë° íŒ¨í„´ ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ë²”ìš©ì ì¸ íŒ¨í„´ ë¶„ì„
        
        # 1. ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ë³€ìˆ˜ íŒ¨í„´ ë¶„ì„ (ë²”ìš©ì )
        binary_target_cols = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and df[col].nunique() == 2:
                unique_vals = sorted(df[col].unique())
                if set(unique_vals) == {0, 1}:
                    binary_target_cols.append(col)
        
        if binary_target_cols:
            analysis_parts.append("## ğŸ¯ **ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ë³€ìˆ˜ íŒ¨í„´ ë¶„ì„**")
            
            for target_col in binary_target_cols:
                positive_rate = df[target_col].mean() * 100
                analysis_parts.append(f"\n**{target_col} ë¶„í¬:**")
                analysis_parts.append(f"- ì–‘ì„±(1): {df[target_col].sum():,}ê°œ ({positive_rate:.1f}%)")
                analysis_parts.append(f"- ìŒì„±(0): {(df[target_col] == 0).sum():,}ê°œ ({100-positive_rate:.1f}%)")
                
                # ë²”ì£¼í˜• ë³€ìˆ˜ì™€ì˜ ê´€ê³„ ë¶„ì„
                categorical_cols = df.select_dtypes(include=['object', 'category']).columns
                for cat_col in categorical_cols[:2]:  # ìƒìœ„ 2ê°œ ë²”ì£¼í˜• ë³€ìˆ˜
                    analysis_parts.append(f"\n**{cat_col}ë³„ {target_col} íŒ¨í„´:**")
                    group_stats = df.groupby(cat_col)[target_col].agg(['count', 'sum', 'mean'])
                    for category in group_stats.index[:4]:  # ìƒìœ„ 4ê°œ ì¹´í…Œê³ ë¦¬
                        total = group_stats.loc[category, 'count']
                        positive = group_stats.loc[category, 'sum']
                        rate = group_stats.loc[category, 'mean'] * 100
                        analysis_parts.append(f"- **{category}**: {positive}/{total}ê°œ ({rate:.1f}%)")
        
        # 2. ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            analysis_parts.append("\n## ğŸ“Š **ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´**")
            for col in categorical_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ
                value_counts = df[col].value_counts()
                total_unique = df[col].nunique()
                analysis_parts.append(f"\n**{col} ({total_unique}ê°œ ê³ ìœ ê°’):**")
                for i, (value, count) in enumerate(value_counts.head(4).items()):
                    analysis_parts.append(f"{i+1}. {value}: {count:,}ê°œ ({count/len(df)*100:.1f}%)")
        
        # 3. ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ íŒ¨í„´
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            analysis_parts.append("\n## ğŸ“ˆ **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬ íŠ¹ì„±**")
            desc = df[numeric_cols].describe()
            for col in numeric_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ
                if col in desc.columns:
                    skewness = df[col].skew()
                    outlier_threshold = desc.loc['75%', col] + 1.5 * (desc.loc['75%', col] - desc.loc['25%', col])
                    outliers = (df[col] > outlier_threshold).sum()
                    
                    analysis_parts.append(f"\n**{col}:**")
                    analysis_parts.append(f"- ë²”ìœ„: {desc.loc['min', col]:.2f} ~ {desc.loc['max', col]:.2f}")
                    analysis_parts.append(f"- ë¶„í¬: {'ì™¼ìª½ ì¹˜ìš°ì¹¨' if skewness > 1 else 'ì˜¤ë¥¸ìª½ ì¹˜ìš°ì¹¨' if skewness < -1 else 'ì •ê·œë¶„í¬ì— ê°€ê¹Œì›€'}")
                    if outliers > 0:
                        analysis_parts.append(f"- ì´ìƒê°’: {outliers}ê°œ ({outliers/len(df)*100:.1f}%)")
        
        return "\n".join(analysis_parts)
    
    def _generate_insights_summary(self, df, df_id: str, instruction: str) -> str:
        """í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë° ìš”ì•½"""
        analysis_parts = []
        
        analysis_parts.append(f"# ğŸ’¡ **í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìš”ì•½ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸
        total_entries = len(df)
        missing_data = df.isnull().sum().sum()
        completeness = (1 - missing_data / (total_entries * len(df.columns))) * 100
        
        analysis_parts.append("## ğŸ” **í•µì‹¬ ë°œê²¬ì‚¬í•­**")
        
        analysis_parts.append(f"\n**1. ë°ì´í„° í’ˆì§ˆ**")
        analysis_parts.append(f"- ë°ì´í„° ì™„ì„±ë„: {completeness:.1f}%")
        analysis_parts.append(f"- ì´ {total_entries:,}ê°œ ê´€ì¸¡ê°’ìœ¼ë¡œ {'ì¶©ë¶„í•œ' if total_entries > 1000 else 'ì œí•œì ì¸'} ë¶„ì„ ê°€ëŠ¥")
        
        # ë²”ìš©ì ì¸ ë°ì´í„° ì¸ì‚¬ì´íŠ¸
        analysis_parts.append(f"\n**2. í•µì‹¬ ë°ì´í„° ì¸ì‚¬ì´íŠ¸**")
        
        # ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ë³€ìˆ˜ ì¸ì‚¬ì´íŠ¸
        binary_targets = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and df[col].nunique() == 2:
                unique_vals = sorted(df[col].unique())
                if set(unique_vals) == {0, 1}:
                    positive_rate = df[col].mean() * 100
                    binary_targets.append((col, positive_rate))
        
        if binary_targets:
            for target_col, rate in binary_targets:
                balance_status = "ê· í˜•ì¡íŒ" if 40 <= rate <= 60 else "ë¶ˆê· í˜•í•œ"
                analysis_parts.append(f"- {target_col}: {rate:.1f}% ì–‘ì„±ë¥ ë¡œ {balance_status} ë¶„í¬")
        
        # ê²°ì¸¡ê°’ íŒ¨í„´ ì¸ì‚¬ì´íŠ¸
        missing_rates = df.isnull().mean() * 100
        high_missing = missing_rates[missing_rates > 20]
        if len(high_missing) > 0:
            analysis_parts.append(f"- ê²°ì¸¡ê°’ ì£¼ì˜: {list(high_missing.index)} ì»¬ëŸ¼ì˜ ê²°ì¸¡ë¥ ì´ 20% ì´ìƒ")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ë‹¤ì–‘ì„± ì¸ì‚¬ì´íŠ¸
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            high_cardinality = [col for col in categorical_cols if df[col].nunique() > len(df) * 0.1]
            if high_cardinality:
                analysis_parts.append(f"- ê³ ìœ ê°’ ê³¼ë‹¤: {high_cardinality} ì»¬ëŸ¼ì€ ë²”ì£¼ ìˆ˜ê°€ ë§¤ìš° ë†’ìŒ")
        
        # ë°ì´í„° êµ¬ì¡° ì¸ì‚¬ì´íŠ¸
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        
        analysis_parts.append(f"\n**3. ë°ì´í„° êµ¬ì¡° íŠ¹ì§•**")
        analysis_parts.append(f"- ìˆ˜ì¹˜í˜• ë³€ìˆ˜ {len(numeric_cols)}ê°œ, ë²”ì£¼í˜• ë³€ìˆ˜ {len(categorical_cols)}ê°œ")
        analysis_parts.append(f"- ë‹¤ì–‘í•œ ê´€ì ì˜ ë¶„ì„ì´ ê°€ëŠ¥í•œ {'ê· í˜•ì¡íŒ' if len(numeric_cols) > 2 and len(categorical_cols) > 2 else 'ë‹¨ìˆœí•œ'} êµ¬ì¡°")
        
        # ì¶”ì²œì‚¬í•­
        analysis_parts.append(f"\n## ğŸ“‹ **ì¶”ì²œ í›„ì† ë¶„ì„**")
        analysis_parts.append("1. **ì‹œê°í™”**: ì£¼ìš” íŒ¨í„´ì„ ê·¸ë˜í”„ë¡œ í‘œí˜„")
        analysis_parts.append("2. **ì˜ˆì¸¡ ëª¨ë¸ë§**: íƒ€ê²Ÿ ë³€ìˆ˜ ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶•")
        analysis_parts.append("3. **ì„¸ë¶„í™” ë¶„ì„**: íŠ¹ì • ê·¸ë£¹ë³„ ìƒì„¸ ë¶„ì„")
        analysis_parts.append("4. **ì´ìƒê°’ ë¶„ì„**: íŠ¹ì´í•œ ì¼€ì´ìŠ¤ íƒì§€")
        
        return "\n".join(analysis_parts)
    
    def _generate_comprehensive_analysis(self, df, df_id: str, instruction: str) -> str:
        """ì¢…í•© ë¶„ì„ (ê¸°ë³¸ê°’)"""
        analysis_parts = []
        
        analysis_parts.append("# ğŸ“Š **ì¢…í•© ë°ì´í„° ë¶„ì„ ë³´ê³ ì„œ**\n")
        analysis_parts.append(f"**ìš”ì²­**: {instruction}")
        analysis_parts.append(f"**ë°ì´í„°ì…‹**: {df_id}")
        analysis_parts.append(f"**í¬ê¸°**: {df.shape[0]:,}í–‰ Ã— {df.shape[1]}ì—´")
        analysis_parts.append(f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        # ë°ì´í„° ê°œìš”
        analysis_parts.append("## ğŸ“‹ **ë°ì´í„° ê°œìš”**")
        analysis_parts.append("**ì»¬ëŸ¼ ì •ë³´:**")
        for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):
            analysis_parts.append(f"{i}. **{col}** ({dtype})")
        
        # ê¸°ë³¸ í†µê³„
        analysis_parts.append("\n## ğŸ“ˆ **ê¸°ë³¸ í†µê³„**")
        desc = df.describe()
        if not desc.empty:
            analysis_parts.append("**ìˆ˜ì¹˜í˜• ë³€ìˆ˜ í†µê³„:**")
            for col in desc.columns[:3]:  # ì²˜ìŒ 3ê°œ ì»¬ëŸ¼ë§Œ
                analysis_parts.append(f"- **{col}**: í‰ê·  {desc.loc['mean', col]:.2f}, í‘œì¤€í¸ì°¨ {desc.loc['std', col]:.2f}")
        
        # ê²°ì¸¡ì¹˜ ë¶„ì„
        missing = df.isnull().sum()
        if missing.sum() > 0:
            analysis_parts.append("\n## âš ï¸ **ê²°ì¸¡ì¹˜ ë¶„ì„**")
            for col, count in missing.items():
                if count > 0:
                    pct = (count / len(df)) * 100
                    analysis_parts.append(f"- **{col}**: {count}ê°œ ({pct:.1f}%)")
        
        # ë²”ìš©ì ì¸ íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„
        binary_target_cols = []
        for col in df.columns:
            if df[col].dtype in ['int64', 'float64'] and df[col].nunique() == 2:
                unique_vals = sorted(df[col].unique())
                if set(unique_vals) == {0, 1}:
                    binary_target_cols.append(col)
        
        if binary_target_cols:
            analysis_parts.append("\n## ğŸ¯ **íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„ì„**")
            for target_col in binary_target_cols:
                positive_rate = df[target_col].mean() * 100
                analysis_parts.append(f"- **{target_col} ì–‘ì„±ë¥ **: {positive_rate:.1f}%")
                
                # ë²”ì£¼í˜• ë³€ìˆ˜ì™€ì˜ ê´€ê³„
                categorical_cols = df.select_dtypes(include=['object', 'category']).columns
                for cat_col in categorical_cols[:2]:  # ìƒìœ„ 2ê°œë§Œ
                    if len(df.groupby(cat_col)[target_col].mean()) > 1:
                        group_means = df.groupby(cat_col)[target_col].mean() * 100
                        top_categories = group_means.head(3)
                        analysis_parts.append(f"- **{cat_col}ë³„ {target_col}**: {dict(top_categories.round(1))}")
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        if len(categorical_cols) > 0:
            analysis_parts.append("\n## ğŸ“Š **ë²”ì£¼í˜• ë³€ìˆ˜ ë¶„í¬**")
            for cat_col in categorical_cols[:2]:  # ìƒìœ„ 2ê°œë§Œ
                value_counts = df[cat_col].value_counts()
                analysis_parts.append(f"- **{cat_col}**: {dict(value_counts.head(3))}")
        
        # ìˆ˜ì¹˜í˜• ë³€ìˆ˜ ë¶„í¬
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            analysis_parts.append("\n## ğŸ“ˆ **ìˆ˜ì¹˜í˜• ë³€ìˆ˜ íŠ¹ì„±**")
            for num_col in numeric_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ
                if num_col not in binary_target_cols:  # ë°”ì´ë„ˆë¦¬ íƒ€ê²Ÿ ì œì™¸
                    skewness = df[num_col].skew()
                    outliers_count = len(df[df[num_col] > df[num_col].quantile(0.75) + 1.5 * (df[num_col].quantile(0.75) - df[num_col].quantile(0.25))])
                    analysis_parts.append(f"- **{num_col}**: {'ì •ê·œë¶„í¬' if abs(skewness) < 1 else 'ì¹˜ìš°ì¹œ ë¶„í¬'}, ì´ìƒê°’ {outliers_count}ê°œ")
        
        # ì¶”ì²œì‚¬í•­
        analysis_parts.append("\n## ğŸ’¡ **ë¶„ì„ ì¶”ì²œì‚¬í•­**")
        analysis_parts.append("1. ğŸ” **ìƒê´€ê´€ê³„ ë¶„ì„**: ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ ê°„ì˜ ê´€ê³„ íƒìƒ‰")
        analysis_parts.append("2. ğŸ“Š **ì‹œê°í™”**: íˆìŠ¤í† ê·¸ë¨, ìƒìê·¸ë¦¼ ë“±ìœ¼ë¡œ ë¶„í¬ í™•ì¸")
        analysis_parts.append("3. ğŸ¯ **ì„¸ë¶„í™” ë¶„ì„**: ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ë¶„ì„ ìˆ˜í–‰")
        
        return "\n".join(analysis_parts)

# 2. AgentExecutor êµ¬í˜„ (ê³µì‹ Hello World Agent íŒ¨í„´)
class PandasAgentExecutor(AgentExecutor):
    """ê³µì‹ Hello World Agent íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” AgentExecutor"""
    
    def __init__(self):
        self.agent = PandasDataAnalysisAgent()
        logger.info("ğŸ”§ PandasAgentExecutor ì´ˆê¸°í™” ì™„ë£Œ")

    async def execute(self, context: RequestContext, event_queue: EventQueue) -> None:
        """A2A SDK í‘œì¤€ ì‹¤í–‰ (ê³µì‹ Hello World Agent íŒ¨í„´)"""
        logger.info("ğŸ¯ PandasAgentExecutor.execute() í˜¸ì¶œë¨")
        
        try:
            # ì‚¬ìš©ì ì…ë ¥ ì¶”ì¶œ (ê³µì‹ íŒ¨í„´)
            user_message = context.get_user_input()
            logger.info(f"ğŸ“ ì‚¬ìš©ì ì…ë ¥: {user_message}")
            
            # ì—ì´ì „íŠ¸ ì‹¤í–‰ (ê³µì‹ íŒ¨í„´)
            result = await self.agent.invoke(user_message)
            
            # ê²°ê³¼ ì „ì†¡ (ê³µì‹ íŒ¨í„´ - ì¤‘ìš”: await ì¶”ê°€!)
            message = new_agent_text_message(result)
            await event_queue.enqueue_event(message)
            
            logger.info("âœ… Task completed successfully")
            
        except Exception as e:
            logger.error(f"âŒ Error in execute: {e}", exc_info=True)
            error_message = new_agent_text_message(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
            await event_queue.enqueue_event(error_message)

    async def cancel(self, context: RequestContext, event_queue: EventQueue) -> None:
        """Task ì·¨ì†Œ ì²˜ë¦¬ (ê³µì‹ Hello World Agent íŒ¨í„´)"""
        logger.info("ğŸ›‘ PandasAgentExecutor.cancel() í˜¸ì¶œë¨")
        raise Exception("Cancel not supported")

# 3. Agent Card ìƒì„± (ê³µì‹ A2A í‘œì¤€ ë©”íƒ€ë°ì´í„°)
def create_agent_card() -> AgentCard:
    """A2A í‘œì¤€ Agent Card ìƒì„± (ê³µì‹ Hello World Agent íŒ¨í„´)"""
    
    # ê¸°ë³¸ ìŠ¤í‚¬ ì •ì˜ (ê³µì‹ íŒ¨í„´)
    skill = AgentSkill(
        id="pandas_data_analysis",
        name="Pandas Data Analysis",
        description="Performs comprehensive data analysis on uploaded datasets using pandas",
        tags=["data", "analysis", "pandas", "statistics", "EDA"],
        examples=["Analyze my data", "What insights can you find?", "Show me data statistics"]
    )
    
    return AgentCard(
        name="Pandas Data Analyst",
        description="A comprehensive data analysis agent powered by pandas and AI",
        url="http://localhost:10001/",
        version="2.0.0",
        capabilities=AgentCapabilities(streaming=False),
        defaultInputModes=["text"],
        defaultOutputModes=["text"],
        skills=[skill]
    )

# 4. Wire everything together (ê³µì‹ Hello World Agent íŒ¨í„´)
@click.command()
@click.option('--host', default='localhost', help='Host to bind to')
@click.option('--port', default=10001, help='Port to bind to')
def main(host: str, port: int):
    """A2A í‘œì¤€ Pandas ì„œë²„ ì‹¤í–‰ (ê³µì‹ Hello World Agent íŒ¨í„´)"""
    
    logger.info("ğŸš€ Starting Pandas A2A Server...")
    
    # Agent Card ìƒì„±
    agent_card = create_agent_card()
    
    # RequestHandler ì´ˆê¸°í™” (ê³µì‹ íŒ¨í„´)
    request_handler = DefaultRequestHandler(
        agent_executor=PandasAgentExecutor(),
        task_store=InMemoryTaskStore()
    )
    
    # A2A Starlette Application ìƒì„± (ê³µì‹ íŒ¨í„´)
    a2a_app = A2AStarletteApplication(
        agent_card=agent_card,
        http_handler=request_handler
    )
    
    logger.info(f"ğŸŒ Server starting at http://{host}:{port}")
    logger.info("ğŸ“‹ Agent Card available at /.well-known/agent.json")
    
    # Uvicornìœ¼ë¡œ ì„œë²„ ì‹¤í–‰
    uvicorn.run(a2a_app.build(), host=host, port=port)

if __name__ == "__main__":
    main() 