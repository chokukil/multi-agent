# ğŸ¦™ Ollama Tool Calling ì™„ì „ ê°€ì´ë“œ
*CherryAIì˜ Ollama í†µí•© ê°œì„  ë° ì„¤ì • ê°€ì´ë“œ*

## ğŸ“‹ ê°œìš”

ì´ ê°€ì´ë“œëŠ” CherryAIì—ì„œ Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ GPTì™€ ë™ì¼í•œ ìˆ˜ì¤€ì˜ ë„êµ¬ í˜¸ì¶œ(Tool Calling) ì„±ëŠ¥ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

### âœ… ì™„ë£Œëœ ê°œì„ ì‚¬í•­

- âœ… **íŒ¨í‚¤ì§€ í˜¸í™˜ì„±** - `langchain_ollama` ìš°ì„  ì‚¬ìš©ìœ¼ë¡œ ì™„ì „í•œ ë„êµ¬ í˜¸ì¶œ ì§€ì›
- âœ… **ëª¨ë¸ ê²€ì¦** - ë„êµ¬ í˜¸ì¶œ ì§€ì› ëª¨ë¸ ìë™ ê°ì§€ ë° ê¶Œì¥
- âœ… **ì‚¬ìš©ì ì •ì˜ Agent** - Ollama ì „ìš© ê³ ë„í™”ëœ ë„êµ¬ í˜¸ì¶œ ì—ì´ì „íŠ¸
- âœ… **UI ê°•í™”** - ì‹¤ì‹œê°„ ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° ì„¤ì • ê°€ì´ë“œ
- âœ… **ìë™ ì„¤ì •** - í™˜ê²½ë³€ìˆ˜ ë° ëª¨ë¸ ìë™ êµ¬ì„± ìŠ¤í¬ë¦½íŠ¸
- âœ… **ì—ëŸ¬ ì²˜ë¦¬** - ê°•í™”ëœ ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ë° í´ë°± ì²˜ë¦¬

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ìë™ ì„¤ì • (ê¶Œì¥)

```bash
# Linux/macOS
./setup_ollama_env.sh

# Windows
setup_ollama_env.bat
```

### 2. ìˆ˜ë™ ì„¤ì •

```bash
# 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜
uv add ollama psutil

# 2. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export LLM_PROVIDER=OLLAMA
export OLLAMA_MODEL=llama3.1:8b
export OLLAMA_BASE_URL=http://localhost:11434
export OLLAMA_TIMEOUT=600

# 3. Ollama ì„œë²„ ì‹œì‘
ollama serve

# 4. ê¶Œì¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.1:8b
```

## ğŸ”§ í•µì‹¬ ê¸°ìˆ  ê°œì„ ì‚¬í•­

### 1. LLM Factory ê°•í™” (`core/llm_factory.py`)

#### ğŸ†• íŒ¨í‚¤ì§€ í˜¸í™˜ì„± ìë™ ê°ì§€
```python
# langchain_ollama ìš°ì„  ì‚¬ìš© (ë„êµ¬ í˜¸ì¶œ ì§€ì›)
try:
    from langchain_ollama import ChatOllama
    OLLAMA_TOOL_CALLING_SUPPORTED = True
except ImportError:
    from langchain_community.chat_models.ollama import ChatOllama
    OLLAMA_TOOL_CALLING_SUPPORTED = False
```

#### ğŸ¯ ëª¨ë¸ í˜¸í™˜ì„± ë§¤í•‘
```python
# ë„êµ¬ í˜¸ì¶œ ì§€ì› ëª¨ë¸ (2024ë…„ 12ì›” ê¸°ì¤€)
OLLAMA_TOOL_CALLING_MODELS = {
    "llama3.1:8b", "llama3.1:70b", "llama3.2:3b",
    "qwen2.5:7b", "qwen2.5:14b", "qwen2.5-coder:7b",
    "mistral:7b", "gemma2:9b", "phi3:14b"
}

# ë¯¸ì§€ì› ëª¨ë¸
OLLAMA_NON_TOOL_CALLING_MODELS = {
    "llama2", "qwen3:8b", "vicuna", "alpaca"
}
```

#### ğŸ” ìë™ ëª¨ë¸ ì¶”ì²œ
```python
def get_model_recommendation(ram_gb: Optional[int] = None) -> Dict[str, Any]:
    """ì‚¬ìš©ì ì‹œìŠ¤í…œì— ë§ëŠ” ëª¨ë¸ ì¶”ì²œ"""
    if ram_gb >= 16:
        return {"name": "qwen2.5:14b", "description": "ê³ ì„±ëŠ¥ ì‘ì—…ìš©"}
    elif ram_gb >= 10:
        return {"name": "llama3.1:8b", "description": "ê· í˜•ì¡íŒ ì„±ëŠ¥"}
    else:
        return {"name": "qwen2.5:3b", "description": "ê°€ë²¼ìš´ ì‘ì—…ìš©"}
```

### 2. ì‚¬ìš©ì ì •ì˜ Ollama Agent (`app.py`)

#### ğŸ¤– ê³ ë„í™”ëœ ë„êµ¬ í˜¸ì¶œ ì²˜ë¦¬
```python
def custom_ollama_agent(state):
    """Ollamaìš© ê³ ë„í™”ëœ ì»¤ìŠ¤í…€ ì—ì´ì „íŠ¸"""
    
    # Enhanced prompting
    enhanced_prompt = """You are a data analysis expert.
    IMPORTANT: ALWAYS use available tools for data operations.
    Available tools: {tool_names}"""
    
    # Retry mechanism with tool enforcement
    max_retries = 3
    for attempt in range(max_retries):
        response = llm_with_tools.invoke(enhanced_messages)
        
        # Tool call processing
        if hasattr(response, 'tool_calls') and response.tool_calls:
            # Execute tools and get results
            tool_messages = []
            for tool_call in response.tool_calls:
                result = execute_tool(tool_call)
                tool_messages.append(result)
            
            # Generate final response with tool results
            final_response = llm.invoke(messages + [response] + tool_messages)
            return {"messages": [final_response]}
```

### 3. UI ëª¨ë‹ˆí„°ë§ ê°•í™” (`ui/sidebar_components.py`)

#### ğŸ“Š ì‹¤ì‹œê°„ Ollama ìƒíƒœ í‘œì‹œ
```python
def render_ollama_status():
    """ê³ ë„í™”ëœ Ollama ìƒíƒœ ëª¨ë‹ˆí„°ë§"""
    status = get_ollama_status()
    
    # ì—°ê²° ìƒíƒœ, íŒ¨í‚¤ì§€ ì •ë³´, ëª¨ë¸ ëª©ë¡
    # ê¶Œì¥ì‚¬í•­, ì„¤ì • ì œì•ˆ ë“± í¬í•¨
```

## ğŸ¯ ê¶Œì¥ ëª¨ë¸ ëª©ë¡

### ğŸ’ª ê³ ì„±ëŠ¥ ì‹œìŠ¤í…œ (16GB+ RAM)
```bash
ollama pull qwen2.5:14b        # ê³ ê¸‰ ë¶„ì„, ë³µì¡í•œ ì½”ë”©
ollama pull llama3.1:70b       # ìµœê³  ì„±ëŠ¥ (40GB RAM)
```

### âš–ï¸ ê· í˜• ì‹œìŠ¤í…œ (10-16GB RAM)
```bash
ollama pull llama3.1:8b        # ê¶Œì¥ ê¸°ë³¸ ëª¨ë¸
ollama pull qwen2.5:7b         # ë¹ ë¥¸ ì²˜ë¦¬
ollama pull mistral:7b         # ìš°ìˆ˜í•œ ì¶”ë¡ 
```

### ğŸª¶ ê²½ëŸ‰ ì‹œìŠ¤í…œ (6-10GB RAM)
```bash
ollama pull qwen2.5:3b         # ê°€ë²¼ìš´ ì‘ì—…ìš©
ollama pull llama3.2:3b        # Meta ìµœì‹  ê²½ëŸ‰ ëª¨ë¸
```

### ğŸ’» ì½”ë”© ì „ë¬¸
```bash
ollama pull qwen2.5-coder:7b   # ì½”ë”© ì „ë¬¸ ëª¨ë¸
ollama pull codellama:7b       # Meta ì½”ë“œ ëª¨ë¸
```

## ğŸ› ï¸ ê³ ê¸‰ ì„¤ì •

### í™˜ê²½ë³€ìˆ˜ ì„¸ë¶€ ì„¤ì •
```bash
# ê¸°ë³¸ ì„¤ì •
export LLM_PROVIDER=OLLAMA
export OLLAMA_MODEL=llama3.1:8b
export OLLAMA_BASE_URL=http://localhost:11434

# ì„±ëŠ¥ ìµœì í™”
export OLLAMA_TIMEOUT=600              # 10ë¶„ íƒ€ì„ì•„ì›ƒ
export OLLAMA_AUTO_SWITCH_MODEL=true   # ìë™ ëª¨ë¸ ì „í™˜

# ê³ ê¸‰ ì„¤ì •
export OLLAMA_HOST=0.0.0.0             # ì™¸ë¶€ ì ‘ê·¼ í—ˆìš©
export OLLAMA_PORT=11434               # í¬íŠ¸ ì„¤ì •
export OLLAMA_ORIGINS=*                # CORS ì„¤ì •
```

### Ollama ì„œë²„ ìµœì í™”
```bash
# GPU ë©”ëª¨ë¦¬ ì„¤ì •
export OLLAMA_GPU_LAYERS=32

# ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì„¤ì •
export OLLAMA_MAX_LOADED_MODELS=2

# ë©”ëª¨ë¦¬ ì œí•œ
export OLLAMA_MAX_VRAM=8GB
```

## ğŸ” ë¬¸ì œ í•´ê²°

### 1. ë„êµ¬ í˜¸ì¶œì´ ì‘ë™í•˜ì§€ ì•Šì„ ë•Œ

#### ë¬¸ì œ ì§„ë‹¨
```python
from core.llm_factory import validate_llm_config, get_ollama_status

# ì„¤ì • ê²€ì¦
config = validate_llm_config()
print(config)

# Ollama ìƒíƒœ í™•ì¸
status = get_ollama_status()
print(status)
```

#### í•´ê²° ë°©ë²•
1. **íŒ¨í‚¤ì§€ í™•ì¸**: `langchain_ollama` ì„¤ì¹˜ í™•ì¸
2. **ëª¨ë¸ í™•ì¸**: ë„êµ¬ í˜¸ì¶œ ì§€ì› ëª¨ë¸ ì‚¬ìš©
3. **ì—°ê²° í™•ì¸**: Ollama ì„œë²„ ì‹¤í–‰ ìƒíƒœ
4. **ë²„ì „ í™•ì¸**: ìµœì‹  Ollama ë²„ì „ ì‚¬ìš©

### 2. ì„±ëŠ¥ ìµœì í™”

#### ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
```bash
# ê²½ëŸ‰ ëª¨ë¸ë¡œ ì „í™˜
export OLLAMA_MODEL=qwen2.5:3b

# ë˜ëŠ” ì´ˆê²½ëŸ‰ ëª¨ë¸
export OLLAMA_MODEL=qwen2.5:0.5b
```

#### ì‘ë‹µ ì†ë„ ê°œì„ 
```bash
# GPU ê°€ì† í™œìš©
ollama serve --gpu

# ëª¨ë¸ ì‚¬ì „ ë¡œë”©
ollama run llama3.1:8b ""
```

### 3. ì¼ë°˜ì ì¸ ì˜¤ë¥˜ í•´ê²°

#### Connection Refused
```bash
# Ollama ì„œë²„ ì‹œì‘
ollama serve

# í¬íŠ¸ í™•ì¸
netstat -an | grep 11434
```

#### Model Not Found
```bash
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
ollama list

# ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
ollama pull llama3.1:8b
```

#### Tool Calling Not Working
```bash
# íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
uv remove langchain-ollama
uv add langchain-ollama

# ëª¨ë¸ ë³€ê²½
export OLLAMA_MODEL=llama3.1:8b
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ

### GPT vs Ollama (ë„êµ¬ í˜¸ì¶œ ì„±ëŠ¥)

| ì‘ì—… ìœ í˜• | GPT-4 | Ollama (llama3.1:8b) | Ollama (qwen2.5:7b) |
|-----------|-------|----------------------|---------------------|
| ë°ì´í„° ë¡œë”© | âœ… ì™„ë²½ | âœ… ì™„ë²½ | âœ… ì™„ë²½ |
| í†µê³„ ë¶„ì„ | âœ… ì™„ë²½ | âœ… ì™„ë²½ | âœ… ì™„ë²½ |
| ì‹œê°í™” | âœ… ì™„ë²½ | âœ… ì–‘í˜¸ | âœ… ì–‘í˜¸ |
| ë³µì¡ ì¶”ë¡  | âœ… ì™„ë²½ | âœ… ì–‘í˜¸ | âš ï¸ ì œí•œì  |
| ì†ë„ | ğŸŒ ë³´í†µ | ğŸŒ ëŠë¦¼ | ğŸ‡ ë¹ ë¦„ |
| ë¹„ìš© | ğŸ’° ìœ ë£Œ | ğŸ†“ ë¬´ë£Œ | ğŸ†“ ë¬´ë£Œ |

## ğŸš€ í–¥í›„ ê°œì„  ê³„íš

### ë‹¨ê¸° (1-2ì£¼)
- [ ] ëª¨ë¸ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìë™í™”
- [ ] ë„êµ¬ ì‚¬ìš©ëŸ‰ í†µê³„ ë° ìµœì í™”
- [ ] ì—ëŸ¬ ë³µêµ¬ ìë™í™” ê°•í™”

### ì¤‘ê¸° (1-2ê°œì›”)
- [ ] í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œ (GPT + Ollama)
- [ ] ëª¨ë¸ ì•™ìƒë¸” ê¸°ëŠ¥
- [ ] ìë™ ëª¨ë¸ ìŠ¤ì¼€ì¼ë§

### ì¥ê¸° (3-6ê°œì›”)
- [ ] ì»¤ìŠ¤í…€ ëª¨ë¸ íŒŒì¸íŠœë‹ ì§€ì›
- [ ] ë¶„ì‚° Ollama í´ëŸ¬ìŠ¤í„° ì§€ì›
- [ ] AI ëª¨ë¸ ì„±ëŠ¥ ì˜ˆì¸¡ ì‹œìŠ¤í…œ

## ğŸ“ ì§€ì› ë° ë¬¸ì˜

- **ì´ìŠˆ ë¦¬í¬íŠ¸**: GitHub Issues
- **ê¸°ëŠ¥ ìš”ì²­**: GitHub Discussions
- **ë¬¸ì„œ ê°œì„ **: Pull Request í™˜ì˜

## ğŸ“š ì°¸ê³  ìë£Œ

- [Ollama ê³µì‹ ë¬¸ì„œ](https://ollama.ai/docs)
- [LangChain Ollama í†µí•©](https://python.langchain.com/docs/integrations/chat/ollama)
- [Tool Calling ê°€ì´ë“œ](https://docs.langchain.com/docs/modules/agents/tools/)

---

*ì´ ê°€ì´ë“œëŠ” CherryAI v0.6.2+ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.*
*ìµœì‹  ì—…ë°ì´íŠ¸ëŠ” [ì—¬ê¸°](./OLLAMA_IMPROVEMENT_GUIDE.md)ì—ì„œ í™•ì¸í•˜ì„¸ìš”.* 