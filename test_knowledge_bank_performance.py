#!/usr/bin/env python3
"""
ğŸš€ CherryAI Shared Knowledge Bank ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸

ëŒ€ìš©ëŸ‰ ë°ì´í„°ì™€ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œì˜ ì„±ëŠ¥ì„ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import asyncio
import os
import sys
import logging
import time
import shutil
import random
import statistics
from typing import List, Dict

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(os.path.dirname(__file__))

from core.shared_knowledge_bank import (
    AdvancedSharedKnowledgeBank,
    KnowledgeType,
    SearchStrategy,
    initialize_shared_knowledge_bank
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    
    def __init__(self):
        self.test_dir = "./perf_test_kb"
        self.kb = None
        self.test_data = []
        self.performance_metrics = {}
    
    async def setup(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        print("ğŸ”§ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì¤‘...")
        
        # ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)
        
        # ì§€ì‹ ë±…í¬ ì´ˆê¸°í™”
        self.kb = AdvancedSharedKnowledgeBank(
            persist_directory=self.test_dir,
            embedding_model="paraphrase-multilingual-MiniLM-L12-v2",
            max_chunk_size=300,  # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ìš© ì‘ì€ ì²­í¬
            enable_cache=True
        )
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        self.test_data = self._generate_test_data()
        print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(self.test_data)}ê°œ")
    
    def _generate_test_data(self) -> List[Dict]:
        """í…ŒìŠ¤íŠ¸ìš© ì§€ì‹ ë°ì´í„° ìƒì„±"""
        test_data = []
        
        # ê¸°ë³¸ ë„ë©”ì¸ ì§€ì‹
        base_knowledge = [
            {
                "content": "CherryAIëŠ” A2A í”„ë¡œí† ì½œê³¼ MCP ë„êµ¬ë¥¼ í†µí•©í•œ AI í”Œë«í¼ì…ë‹ˆë‹¤. LLM First ì›ì¹™ì„ ë”°ë¥´ë©° ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°ì„ ì§€ì›í•©ë‹ˆë‹¤.",
                "type": KnowledgeType.DOMAIN_KNOWLEDGE,
                "agent": "system",
                "title": "CherryAI í”Œë«í¼ ê°œìš”"
            },
            {
                "content": "A2AëŠ” Agent-to-Agent í†µì‹  í”„ë¡œí† ì½œë¡œ, ì—ì´ì „íŠ¸ ê°„ ì‹¤ì‹œê°„ ë©”ì‹œì§€ êµí™˜ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. SSE ìŠ¤íŠ¸ë¦¬ë°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.",
                "type": KnowledgeType.DOMAIN_KNOWLEDGE,
                "agent": "a2a_expert",
                "title": "A2A í”„ë¡œí† ì½œ ì†Œê°œ"
            },
            {
                "content": "MCPëŠ” Model Context Protocolë¡œ, AI ëª¨ë¸ê³¼ ì™¸ë¶€ ë„êµ¬ ê°„ì˜ í‘œì¤€ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ëª¨ë“ˆí™”ì™€ í™•ì¥ì„±ì´ í•µì‹¬ì…ë‹ˆë‹¤.",
                "type": KnowledgeType.DOMAIN_KNOWLEDGE,
                "agent": "mcp_expert",
                "title": "MCP í”„ë¡œí† ì½œ ì„¤ëª…"
            },
            {
                "content": "ë°ì´í„° ë¶„ì„ì—ì„œëŠ” pandas, numpy, matplotlibì„ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. íƒìƒ‰ì  ë°ì´í„° ë¶„ì„(EDA)ì´ ì¤‘ìš”í•œ ì²« ë‹¨ê³„ì…ë‹ˆë‹¤.",
                "type": KnowledgeType.BEST_PRACTICE,
                "agent": "data_analyst",
                "title": "ë°ì´í„° ë¶„ì„ ë„êµ¬"
            },
            {
                "content": "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í›ˆë ¨ ì‹œ êµì°¨ ê²€ì¦ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìˆ˜ì…ë‹ˆë‹¤. ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ì •ê·œí™”ë„ ì¤‘ìš”í•©ë‹ˆë‹¤.",
                "type": KnowledgeType.BEST_PRACTICE,
                "agent": "ml_engineer",
                "title": "ML ëª¨ë¸ í›ˆë ¨ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤"
            }
        ]
        
        # ê¸°ë³¸ ë°ì´í„° ì¶”ê°€
        test_data.extend(base_knowledge)
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„± (í™•ì¥ì„± í…ŒìŠ¤íŠ¸ìš©)
        domains = ["AI", "ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤", "ì†Œí”„íŠ¸ì›¨ì–´", "í´ë¼ìš°ë“œ", "ë³´ì•ˆ"]
        agents = ["specialist_1", "specialist_2", "analyst", "engineer", "researcher"]
        
        for i in range(50):  # 50ê°œ ì¶”ê°€ ë°ì´í„°
            domain = random.choice(domains)
            agent = random.choice(agents)
            
            content = f"{domain} ë¶„ì•¼ì—ì„œëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ê³¼ ë°©ë²•ë¡ ì´ í™œìš©ë©ë‹ˆë‹¤. " \
                     f"íŠ¹íˆ {agent}ê°€ ë‹´ë‹¹í•˜ëŠ” ì˜ì—­ì—ì„œëŠ” ì „ë¬¸ì ì¸ ì§€ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤. " \
                     f"ì‹¤ë¬´ ê²½í—˜ê³¼ ì´ë¡ ì  ë°°ê²½ì´ ëª¨ë‘ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. " \
                     f"ìµœì‹  ë™í–¥ê³¼ ê¸°ìˆ  ë°œì „ì„ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•´ì•¼ í•©ë‹ˆë‹¤."
            
            test_data.append({
                "content": content,
                "type": random.choice([KnowledgeType.DOMAIN_KNOWLEDGE, KnowledgeType.BEST_PRACTICE, KnowledgeType.AGENT_MEMORY]),
                "agent": agent,
                "title": f"{domain} ì „ë¬¸ ì§€ì‹ #{i+1}"
            })
        
        return test_data
    
    async def test_bulk_insertion_performance(self):
        """ëŒ€ëŸ‰ ë°ì´í„° ì‚½ì… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ëŒ€ëŸ‰ ë°ì´í„° ì‚½ì… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        
        insertion_times = []
        start_total = time.time()
        
        for i, data in enumerate(self.test_data):
            start_time = time.time()
            
            entry_id = await self.kb.add_knowledge(
                content=data["content"],
                knowledge_type=data["type"],
                source_agent=data["agent"],
                title=data["title"]
            )
            
            elapsed = time.time() - start_time
            insertion_times.append(elapsed)
            
            if (i + 1) % 10 == 0:
                avg_time = statistics.mean(insertion_times[-10:])
                print(f"   ì§„í–‰: {i+1}/{len(self.test_data)} (ìµœê·¼ 10ê°œ í‰ê· : {avg_time:.3f}ì´ˆ)")
        
        total_time = time.time() - start_total
        
        self.performance_metrics["insertion"] = {
            "total_items": len(self.test_data),
            "total_time": total_time,
            "avg_time_per_item": statistics.mean(insertion_times),
            "median_time": statistics.median(insertion_times),
            "max_time": max(insertion_times),
            "min_time": min(insertion_times)
        }
        
        print(f"âœ… ì‚½ì… ì™„ë£Œ: {len(self.test_data)}ê°œ í•­ëª©")
        print(f"   ì´ ì‹œê°„: {total_time:.3f}ì´ˆ")
        print(f"   í‰ê·  ì‹œê°„: {statistics.mean(insertion_times):.3f}ì´ˆ/í•­ëª©")
        print(f"   ì²˜ë¦¬ëŸ‰: {len(self.test_data)/total_time:.1f}í•­ëª©/ì´ˆ")
    
    async def test_search_performance(self):
        """ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        
        search_queries = [
            "CherryAI í”Œë«í¼",
            "A2A í”„ë¡œí† ì½œ",
            "MCP ë„êµ¬",
            "ë°ì´í„° ë¶„ì„",
            "ë¨¸ì‹ ëŸ¬ë‹",
            "AI ê¸°ìˆ ",
            "ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ",
            "í´ë¼ìš°ë“œ ì»´í“¨íŒ…",
            "ë³´ì•ˆ ì‹œìŠ¤í…œ",
            "ì „ë¬¸ ì§€ì‹"
        ]
        
        search_times = []
        result_counts = []
        
        for query in search_queries:
            start_time = time.time()
            
            results = await self.kb.search_knowledge(
                query=query,
                strategy=SearchStrategy.HYBRID,
                max_results=10,
                min_similarity=0.15
            )
            
            elapsed = time.time() - start_time
            search_times.append(elapsed)
            result_counts.append(len(results))
            
            print(f"   '{query}': {len(results)}ê°œ ê²°ê³¼ ({elapsed:.3f}ì´ˆ)")
        
        self.performance_metrics["search"] = {
            "total_queries": len(search_queries),
            "avg_search_time": statistics.mean(search_times),
            "median_search_time": statistics.median(search_times),
            "max_search_time": max(search_times),
            "min_search_time": min(search_times),
            "avg_results": statistics.mean(result_counts),
            "total_results": sum(result_counts)
        }
        
        print(f"âœ… ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   í‰ê·  ê²€ìƒ‰ ì‹œê°„: {statistics.mean(search_times):.3f}ì´ˆ")
        print(f"   í‰ê·  ê²°ê³¼ ìˆ˜: {statistics.mean(result_counts):.1f}ê°œ")
    
    async def test_concurrent_operations(self):
        """ë™ì‹œì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ë™ì‹œì„± í…ŒìŠ¤íŠ¸")
        
        # ë™ì‹œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        search_tasks = []
        queries = ["AI", "ë°ì´í„°", "í”„ë¡œí† ì½œ", "ë¶„ì„", "ê¸°ìˆ "] * 4  # 20ê°œ ë™ì‹œ ê²€ìƒ‰
        
        start_time = time.time()
        
        for query in queries:
            task = self.kb.search_knowledge(
                query=query,
                max_results=5,
                min_similarity=0.15
            )
            search_tasks.append(task)
        
        # ëª¨ë“  ê²€ìƒ‰ ë™ì‹œ ì‹¤í–‰
        results = await asyncio.gather(*search_tasks)
        
        elapsed = time.time() - start_time
        total_results = sum(len(result) for result in results)
        
        self.performance_metrics["concurrency"] = {
            "concurrent_searches": len(search_tasks),
            "total_time": elapsed,
            "avg_time_per_search": elapsed / len(search_tasks),
            "total_results": total_results,
            "throughput": len(search_tasks) / elapsed
        }
        
        print(f"âœ… ë™ì‹œì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print(f"   ë™ì‹œ ê²€ìƒ‰: {len(search_tasks)}ê°œ")
        print(f"   ì´ ì‹œê°„: {elapsed:.3f}ì´ˆ")
        print(f"   ì²˜ë¦¬ëŸ‰: {len(search_tasks)/elapsed:.1f}ê²€ìƒ‰/ì´ˆ")
    
    async def test_memory_usage(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸")
        
        try:
            import psutil
            process = psutil.Process()
            
            # ì´ˆê¸° ë©”ëª¨ë¦¬
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # í†µê³„ ì¡°íšŒ
            stats = await self.kb.get_stats()
            
            # í˜„ì¬ ë©”ëª¨ë¦¬
            current_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_usage = current_memory - initial_memory
            
            self.performance_metrics["memory"] = {
                "initial_memory_mb": initial_memory,
                "current_memory_mb": current_memory,
                "memory_usage_mb": memory_usage,
                "storage_size_mb": stats.storage_size_mb,
                "items_count": stats.total_entries
            }
            
            print(f"âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„ ì™„ë£Œ")
            print(f"   í˜„ì¬ ë©”ëª¨ë¦¬: {current_memory:.1f}MB")
            print(f"   ì €ì¥ì†Œ í¬ê¸°: {stats.storage_size_mb:.1f}MB")
            print(f"   í•­ëª©ë‹¹ ë©”ëª¨ë¦¬: {memory_usage/stats.total_entries:.3f}MB" if stats.total_entries > 0 else "   í•­ëª©ë‹¹ ë©”ëª¨ë¦¬: 0MB")
            
        except ImportError:
            print("âš ï¸ psutil íŒ¨í‚¤ì§€ê°€ ì—†ì–´ì„œ ë©”ëª¨ë¦¬ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
            self.performance_metrics["memory"] = {"error": "psutil not available"}
    
    async def test_scaling_characteristics(self):
        """í™•ì¥ì„± íŠ¹ì„± í…ŒìŠ¤íŠ¸"""
        print("\nğŸ§ª í™•ì¥ì„± íŠ¹ì„± í…ŒìŠ¤íŠ¸")
        
        # ë°ì´í„° í¬ê¸°ë³„ ê²€ìƒ‰ ì„±ëŠ¥
        batch_sizes = [10, 25, 50]  # í˜„ì¬ ë°ì´í„° ê¸°ì¤€
        scaling_results = {}
        
        for batch_size in batch_sizes:
            if batch_size <= len(self.test_data):
                # í•´ë‹¹ í¬ê¸°ê¹Œì§€ì˜ ë°ì´í„°ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                start_time = time.time()
                
                results = await self.kb.search_knowledge(
                    query="ì „ë¬¸ ì§€ì‹",
                    max_results=batch_size,
                    min_similarity=0.1
                )
                
                elapsed = time.time() - start_time
                scaling_results[batch_size] = {
                    "search_time": elapsed,
                    "result_count": len(results)
                }
                
                print(f"   {batch_size}ê°œ ê²°ê³¼ ê²€ìƒ‰: {elapsed:.3f}ì´ˆ")
        
        self.performance_metrics["scaling"] = scaling_results
    
    def print_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ ì¶œë ¥"""
        print("\nğŸ“Š ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¢…í•© ê²°ê³¼")
        print("=" * 50)
        
        # ì‚½ì… ì„±ëŠ¥
        if "insertion" in self.performance_metrics:
            insertion = self.performance_metrics["insertion"]
            print(f"ğŸ”§ ë°ì´í„° ì‚½ì… ì„±ëŠ¥:")
            print(f"   ì²˜ë¦¬ëŸ‰: {insertion['total_items']/insertion['total_time']:.1f} í•­ëª©/ì´ˆ")
            print(f"   í‰ê·  ì§€ì—°: {insertion['avg_time_per_item']*1000:.1f}ms")
        
        # ê²€ìƒ‰ ì„±ëŠ¥
        if "search" in self.performance_metrics:
            search = self.performance_metrics["search"]
            print(f"\nğŸ” ê²€ìƒ‰ ì„±ëŠ¥:")
            print(f"   í‰ê·  ì‘ë‹µì‹œê°„: {search['avg_search_time']*1000:.1f}ms")
            print(f"   í‰ê·  ê²°ê³¼ ìˆ˜: {search['avg_results']:.1f}ê°œ")
        
        # ë™ì‹œì„±
        if "concurrency" in self.performance_metrics:
            concurrency = self.performance_metrics["concurrency"]
            print(f"\nâš¡ ë™ì‹œì„± ì„±ëŠ¥:")
            print(f"   ë™ì‹œ ì²˜ë¦¬ëŸ‰: {concurrency['throughput']:.1f} ìš”ì²­/ì´ˆ")
            print(f"   í‰ê·  ì§€ì—°: {concurrency['avg_time_per_search']*1000:.1f}ms")
        
        # ë©”ëª¨ë¦¬
        if "memory" in self.performance_metrics and "error" not in self.performance_metrics["memory"]:
            memory = self.performance_metrics["memory"]
            print(f"\nğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
            print(f"   í˜„ì¬ ë©”ëª¨ë¦¬: {memory['current_memory_mb']:.1f}MB")
            print(f"   ì €ì¥ì†Œ í¬ê¸°: {memory['storage_size_mb']:.1f}MB")
        
        # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
        self._evaluate_performance_grade()
    
    def _evaluate_performance_grade(self):
        """ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€"""
        print(f"\nğŸ† ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€:")
        
        grade_points = 0
        max_points = 0
        
        # ê²€ìƒ‰ ì‘ë‹µì‹œê°„ í‰ê°€ (50ms ì´í•˜: A+, 100ms ì´í•˜: A, 200ms ì´í•˜: B, ê·¸ ì´ìƒ: C)
        if "search" in self.performance_metrics:
            search_time_ms = self.performance_metrics["search"]["avg_search_time"] * 1000
            max_points += 4
            if search_time_ms <= 50:
                grade_points += 4
                print(f"   ê²€ìƒ‰ ì‘ë‹µì‹œê°„: A+ ({search_time_ms:.1f}ms)")
            elif search_time_ms <= 100:
                grade_points += 3
                print(f"   ê²€ìƒ‰ ì‘ë‹µì‹œê°„: A ({search_time_ms:.1f}ms)")
            elif search_time_ms <= 200:
                grade_points += 2
                print(f"   ê²€ìƒ‰ ì‘ë‹µì‹œê°„: B ({search_time_ms:.1f}ms)")
            else:
                grade_points += 1
                print(f"   ê²€ìƒ‰ ì‘ë‹µì‹œê°„: C ({search_time_ms:.1f}ms)")
        
        # ì‚½ì… ì²˜ë¦¬ëŸ‰ í‰ê°€ (10 items/s ì´ìƒ: A+, 5 items/s ì´ìƒ: A, 2 items/s ì´ìƒ: B, ê·¸ ì´í•˜: C)
        if "insertion" in self.performance_metrics:
            insertion = self.performance_metrics["insertion"]
            throughput = insertion['total_items'] / insertion['total_time']
            max_points += 4
            if throughput >= 10:
                grade_points += 4
                print(f"   ì‚½ì… ì²˜ë¦¬ëŸ‰: A+ ({throughput:.1f} items/s)")
            elif throughput >= 5:
                grade_points += 3
                print(f"   ì‚½ì… ì²˜ë¦¬ëŸ‰: A ({throughput:.1f} items/s)")
            elif throughput >= 2:
                grade_points += 2
                print(f"   ì‚½ì… ì²˜ë¦¬ëŸ‰: B ({throughput:.1f} items/s)")
            else:
                grade_points += 1
                print(f"   ì‚½ì… ì²˜ë¦¬ëŸ‰: C ({throughput:.1f} items/s)")
        
        # ë™ì‹œì„± ì²˜ë¦¬ëŸ‰ í‰ê°€
        if "concurrency" in self.performance_metrics:
            concurrent_throughput = self.performance_metrics["concurrency"]["throughput"]
            max_points += 4
            if concurrent_throughput >= 20:
                grade_points += 4
                print(f"   ë™ì‹œì„± ì²˜ë¦¬ëŸ‰: A+ ({concurrent_throughput:.1f} req/s)")
            elif concurrent_throughput >= 10:
                grade_points += 3
                print(f"   ë™ì‹œì„± ì²˜ë¦¬ëŸ‰: A ({concurrent_throughput:.1f} req/s)")
            elif concurrent_throughput >= 5:
                grade_points += 2
                print(f"   ë™ì‹œì„± ì²˜ë¦¬ëŸ‰: B ({concurrent_throughput:.1f} req/s)")
            else:
                grade_points += 1
                print(f"   ë™ì‹œì„± ì²˜ë¦¬ëŸ‰: C ({concurrent_throughput:.1f} req/s)")
        
        # ì¢…í•© ì ìˆ˜
        if max_points > 0:
            final_score = (grade_points / max_points) * 100
            if final_score >= 90:
                final_grade = "A+"
            elif final_score >= 80:
                final_grade = "A"
            elif final_score >= 70:
                final_grade = "B"
            else:
                final_grade = "C"
            
            print(f"\nğŸ¯ ì¢…í•© ì„±ëŠ¥ ë“±ê¸‰: {final_grade} ({final_score:.1f}ì )")
        
        # ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’¡ ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ì‚¬í•­:")
        if "search" in self.performance_metrics:
            search_time = self.performance_metrics["search"]["avg_search_time"] * 1000
            if search_time > 100:
                print(f"   - ê²€ìƒ‰ ì‘ë‹µì‹œê°„ ê°œì„  í•„ìš” (í˜„ì¬: {search_time:.1f}ms)")
                print(f"   - ì„ë² ë”© ìºì‹œ í¬ê¸° ì¦ê°€ ê³ ë ¤")
                print(f"   - ë” ì‘ì€ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš© ê²€í† ")
    
    async def cleanup(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬"""
        if os.path.exists(self.test_dir):
            shutil.rmtree(self.test_dir)
        print("\nğŸ§¹ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì™„ë£Œ")

async def main():
    """ë©”ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ CherryAI Shared Knowledge Bank ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    benchmark = PerformanceBenchmark()
    
    try:
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        await benchmark.setup()
        await benchmark.test_bulk_insertion_performance()
        await benchmark.test_search_performance()
        await benchmark.test_concurrent_operations()
        await benchmark.test_memory_usage()
        await benchmark.test_scaling_characteristics()
        
        # ê²°ê³¼ ì¶œë ¥
        benchmark.print_performance_summary()
        
    except Exception as e:
        print(f"\nâŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        await benchmark.cleanup()

if __name__ == "__main__":
    asyncio.run(main()) 