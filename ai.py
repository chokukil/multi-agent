"""
ğŸ§¬ AI_DS_Team Orchestrator - Advanced Data Science with A2A Protocol
Smart Data Analystì˜ ìš°ìˆ˜í•œ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ AI_DS_Team í†µí•© ì‹œìŠ¤í…œ

í•µì‹¬ íŠ¹ì§•:
- AI_DS_Team Integration: 9ê°œ ì „ë¬¸ ì—ì´ì „íŠ¸ í™œìš©
- A2A Orchestration: LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸ ì„ íƒ
- Real-time Processing: ì‹¤ì‹œê°„ ì‘ì—… ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§  
- Professional Results: ì „ë¬¸ì ì¸ ë°ì´í„° ê³¼í•™ ê²°ê³¼ ì œê³µ
"""

import streamlit as st
import sys
import os
import asyncio
import logging
import platform
from datetime import datetime
from dotenv import load_dotenv
import nest_asyncio
import pandas as pd
import json
import httpx
import time
import plotly.graph_objects as go
import plotly.express as px
import plotly.io as pio
from typing import Dict, Any, Tuple, List
import traceback
from pathlib import Path
import uuid
import numpy as np
import base64
import contextlib
from bs4 import BeautifulSoup
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€ (ai.pyëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìœ„ì¹˜)
project_root = os.path.dirname(os.path.abspath(__file__))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ì‹ ê·œ A2A í´ë¼ì´ì–¸íŠ¸ ë° ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
try:
    from core.a2a.a2a_streamlit_client import A2AStreamlitClient
    A2A_CLIENT_AVAILABLE = True
    print("âœ… A2A í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    A2A_CLIENT_AVAILABLE = False
    print(f"âš ï¸ A2A í´ë¼ì´ì–¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

from core.utils.logging import setup_logging
from core.data_manager import DataManager  # DataManager ì¶”ê°€
from core.session_data_manager import SessionDataManager  # ì„¸ì…˜ ê¸°ë°˜ ë°ì´í„° ê´€ë¦¬ì ì¶”ê°€
from ui.thinking_stream import ThinkingStream, PlanVisualization, BeautifulResults # ê¸°ì¡´ í´ë˜ìŠ¤ í™œìš© ê°€ëŠ¥

# í–¥ìƒëœ ì—ëŸ¬ í•¸ë“¤ë§ ì‹œìŠ¤í…œ ì„í¬íŠ¸ (ì¡°ê±´ë¶€)
try:
    from core.enhanced_error_system import (
        error_manager, error_monitor, log_manager, 
        ErrorCategory, ErrorSeverity, initialize_error_system
    )
    from ui.enhanced_error_ui import (
        integrate_error_system_to_app, show_error, show_user_error, show_network_error,
        ErrorNotificationSystem, ErrorAnalyticsWidget
    )
    ENHANCED_ERROR_AVAILABLE = True
    print("âœ… Enhanced Error System ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    ENHANCED_ERROR_AVAILABLE = False
    print(f"âš ï¸ Enhanced Error System ë¡œë“œ ì‹¤íŒ¨: {e}")
    # í´ë°± í•¨ìˆ˜ë“¤ ì •ì˜
    def show_error(msg): st.error(msg)
    def show_user_error(msg): st.error(msg)
    def show_network_error(msg): st.error(msg)
    def integrate_error_system_to_app(): pass  # ë¹ˆ í•¨ìˆ˜ë¡œ ì²˜ë¦¬
    class ErrorNotificationSystem:
        def __init__(self): pass
        def show_error(self, msg): st.error(msg)
    class ErrorAnalyticsWidget:
        def __init__(self): pass
        def render(self): pass

# Phase 3 Integration Layer ë° Expert UI ì„í¬íŠ¸
try:
    from core.phase3_integration_layer import Phase3IntegrationLayer
    from ui.expert_answer_renderer import ExpertAnswerRenderer
    PHASE3_AVAILABLE = True
    print("âœ… Phase 3 Integration Layer ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    PHASE3_AVAILABLE = False
    print(f"âš ï¸ Phase 3 Integration Layer ë¡œë“œ ì‹¤íŒ¨: {e}")

# ë””ë²„ê¹… ë¡œê±° ì„¤ì •
debug_logger = logging.getLogger("ai_ds_debug")
debug_logger.setLevel(logging.DEBUG)
if not debug_logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    debug_logger.addHandler(handler)

def debug_log(message: str, level: str = "info"):
    """í–¥ìƒëœ ë””ë²„ê¹… ë¡œê·¸ - ì‚¬ì´ë“œë°” ì„¤ì •ì— ë”°ë¼ ì œì–´"""
    timestamp = datetime.now().strftime("%H:%M:%S.%f")[:-3]
    
    # ë””ë²„ê¹… ëª¨ë“œ í™•ì¸ (ì„¸ì…˜ ìƒíƒœì—ì„œ)
    debug_enabled = getattr(st.session_state, 'debug_enabled', False)
    
    # ë¡œê·¸ ë©”ì‹œì§€ í¬ë§·
    if level == "error":
        log_msg = f"[{timestamp}] âŒ ERROR: {message}"
        debug_logger.error(message)
    elif level == "warning":
        log_msg = f"[{timestamp}] âš ï¸  WARNING: {message}"
        debug_logger.warning(message)
    elif level == "success":
        log_msg = f"[{timestamp}] âœ… SUCCESS: {message}"
        debug_logger.info(message)
    else:
        log_msg = f"[{timestamp}] â„¹ï¸  INFO: {message}"
        debug_logger.info(message)
    
    # í„°ë¯¸ë„ ì¶œë ¥ (í•­ìƒ í‘œì‹œ)
    print(log_msg)
    
    # íŒŒì¼ì—ë„ ê¸°ë¡ (ë””ë²„ê¹…ìš©)
    try:
        os.makedirs("logs", exist_ok=True)
        with open("logs/streamlit_debug.log", "a", encoding="utf-8") as f:
            f.write(f"{log_msg}\n")
            f.flush()
    except Exception as e:
        print(f"[{timestamp}] âŒ ë¡œê·¸ íŒŒì¼ ê¸°ë¡ ì‹¤íŒ¨: {e}")
    
    # Streamlit UIì—ëŠ” ë””ë²„ê¹… ëª¨ë“œê°€ ì¼œì ¸ìˆì„ ë•Œë§Œ í‘œì‹œ
    if debug_enabled:
        try:
            if level == "error":
                st.error(f"ğŸ› DEBUG: {message}")
            elif level == "warning":
                st.warning(f"ğŸ› DEBUG: {message}")
            elif level == "success":
                st.success(f"ğŸ› DEBUG: {message}")
            else:
                st.info(f"ğŸ› DEBUG: {message}")
        except:
            pass  # Streamlit ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ì„ ë•ŒëŠ” ë¬´ì‹œ

# ìƒˆë¡œìš´ UI ì»´í¬ë„ŒíŠ¸ ì„í¬íŠ¸ (ì¡°ê±´ë¶€)
try:
    from core.ui.smart_display import SmartDisplayManager, AccumulativeStreamContainer
    from core.ui.a2a_orchestration_ui import A2AOrchestrationDashboard
    from core.ui.agent_preloader import AgentPreloader, get_agent_preloader, ProgressiveLoadingUI, AgentStatus
    SMART_UI_AVAILABLE = True
    print("âœ… Smart UI ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    SMART_UI_AVAILABLE = False
    print(f"âš ï¸ Smart UI ì»´í¬ë„ŒíŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    # í´ë°± í´ë˜ìŠ¤ë“¤ ì •ì˜
    class AccumulativeStreamContainer:
        def __init__(self, title): pass
        def add_chunk(self, text, type): pass
    
    def get_agent_preloader(agents): return None

# AI_DS_Team ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
try:
    # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
    ai_ds_team_path = os.path.join(project_root, "ai_ds_team")
    ai_data_science_team_path = os.path.join(ai_ds_team_path, "ai_data_science_team")
    tools_path = os.path.join(ai_data_science_team_path, "tools")
    dataframe_path = os.path.join(tools_path, "dataframe.py")
    
    debug_log(f"ğŸ” í”„ë¡œì íŠ¸ ë£¨íŠ¸: {project_root}")
    debug_log(f"ğŸ” ai_ds_team ê²½ë¡œ: {ai_ds_team_path} (ì¡´ì¬: {os.path.exists(ai_ds_team_path)})")
    debug_log(f"ğŸ” ai_data_science_team ê²½ë¡œ: {ai_data_science_team_path} (ì¡´ì¬: {os.path.exists(ai_data_science_team_path)})")
    debug_log(f"ğŸ” dataframe.py ê²½ë¡œ: {dataframe_path} (ì¡´ì¬: {os.path.exists(dataframe_path)})")
    
    # ai_ds_team í´ë” ì•ˆì˜ ai_data_science_team ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
    if ai_ds_team_path not in sys.path:
        sys.path.insert(0, ai_ds_team_path)
        debug_log(f"âœ… Python pathì— ì¶”ê°€ë¨: {ai_ds_team_path}")
    
    from ai_data_science_team.tools.dataframe import get_dataframe_summary
    AI_DS_TEAM_UTILS_AVAILABLE = True
    debug_log("âœ… AI_DS_Team ìœ í‹¸ë¦¬í‹° ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    # ì™„ì „íˆ ì¡°ìš©í•œ fallback (ê²½ê³  ë©”ì‹œì§€ ì œê±°)
    AI_DS_TEAM_UTILS_AVAILABLE = False
    def get_dataframe_summary(df): return [f"Shape: {df.shape}"]
    # í„°ë¯¸ë„ì—ë§Œ ë¡œê·¸ ì¶œë ¥, UIì—ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ
    print(f"[INFO] AI_DS_Team ìœ í‹¸ë¦¬í‹° ë¡œë“œ ì‹¤íŒ¨ (ì •ìƒ ë™ì‘): {e}")
except Exception as e:
    # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ì˜ ê²½ìš°ì—ë§Œ ë¡œê·¸
    AI_DS_TEAM_UTILS_AVAILABLE = False
    def get_dataframe_summary(df): return [f"Shape: {df.shape}"]
    debug_log(f"âš ï¸ AI_DS_Team ìœ í‹¸ë¦¬í‹° ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", "warning")

# Langfuse Session Tracking ì¶”ê°€
try:
    from core.langfuse_session_tracer import init_session_tracer, get_session_tracer
    LANGFUSE_SESSION_AVAILABLE = True
    print("âœ… Langfuse Session Tracer ë¡œë“œ ì„±ê³µ")
except ImportError as e:
    LANGFUSE_SESSION_AVAILABLE = False
    print(f"âš ï¸ Langfuse Session Tracer ë¡œë“œ ì‹¤íŒ¨: {e}")

# --- ì´ˆê¸° ì„¤ì • ---
setup_logging()

# Langfuse ì´ˆê¸°í™” (í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°)
if LANGFUSE_SESSION_AVAILABLE:
    try:
        langfuse_public_key = os.getenv("LANGFUSE_PUBLIC_KEY")
        langfuse_secret_key = os.getenv("LANGFUSE_SECRET_KEY")
        langfuse_host = os.getenv("LANGFUSE_HOST", "http://localhost:3000")
        
        if langfuse_public_key and langfuse_secret_key:
            init_session_tracer(langfuse_public_key, langfuse_secret_key, langfuse_host)
            debug_log("ğŸ” Langfuse Session Tracer ì´ˆê¸°í™” ì„±ê³µ", "success")
        else:
            debug_log("âš ï¸ Langfuse í™˜ê²½ë³€ìˆ˜ ë¯¸ì„¤ì • - ì¶”ì  ë¹„í™œì„±í™”", "warning")
    except Exception as e:
        debug_log(f"âŒ Langfuse ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "error")

def setup_environment():
    """í™˜ê²½ ì„¤ì •"""
    debug_log("í™˜ê²½ ì„¤ì • ì‹œì‘")
    if platform.system() == "Windows":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
        debug_log("Windows ì´ë²¤íŠ¸ ë£¨í”„ ì •ì±… ì„¤ì •")
    nest_asyncio.apply()
    load_dotenv()
    debug_log("í™˜ê²½ ì„¤ì • ì™„ë£Œ")

def apply_custom_styling():
    st.markdown("""
    <style>
        .stApp { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; }
        .agent-card { background: rgba(255, 255, 255, 0.1); color: white; padding: 1.5rem; border-radius: 12px; margin: 0.5rem; border: 1px solid rgba(255, 255, 255, 0.2); transition: transform 0.3s ease; }
        .agent-card:hover { transform: translateY(-5px); background: rgba(255, 255, 255, 0.2); }
        .stButton > button { background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); color: white; border: none; border-radius: 8px; padding: 0.7rem 1.5rem; font-weight: 600; }
        .debug-box { background: rgba(255, 255, 255, 0.1); border: 1px solid #ffd700; border-radius: 8px; padding: 10px; margin: 5px 0; }
    </style>
    """, unsafe_allow_html=True)

# --- AI_DS_Team ì—ì´ì „íŠ¸ ì •ë³´ ---
AI_DS_TEAM_AGENTS = {
    "Orchestrator": {"port": 8100, "description": "AI DS Teamì„ ì§€íœ˜í•˜ëŠ” ë§ˆì—ìŠ¤íŠ¸ë¡œ", "capabilities": ["planning", "delegation"], "color": "#FAD02E"},
    "ğŸ§¹ Data Cleaning": {"port": 8306, "description": "ëˆ„ë½ê°’ ì²˜ë¦¬, ì´ìƒì¹˜ ì œê±°", "capabilities": ["missing_value", "outlier"], "color": "#FF6B6B"},
    "ğŸ“Š Data Visualization": {"port": 8308, "description": "ê³ ê¸‰ ì‹œê°í™” ìƒì„±", "capabilities": ["charts", "plots"], "color": "#4ECDC4"},
    "ğŸ” EDA Tools": {"port": 8312, "description": "ìë™ EDA ë° ìƒê´€ê´€ê³„ ë¶„ì„", "capabilities": ["eda", "correlation"], "color": "#45B7D1"},
    "ğŸ“ Data Loader": {"port": 8307, "description": "ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ ë¡œë”©", "capabilities": ["load_file", "connect_db"], "color": "#96CEB4"},
    "ğŸ”§ Data Wrangling": {"port": 8309, "description": "ë°ì´í„° ë³€í™˜ ë° ì¡°ì‘", "capabilities": ["transform", "aggregate"], "color": "#FFEAA7"},
    "âš™ï¸ Feature Engineering": {"port": 8310, "description": "ê³ ê¸‰ í”¼ì²˜ ìƒì„± ë° ì„ íƒ", "capabilities": ["feature_creation", "selection"], "color": "#DDA0DD"},
    "ğŸ—„ï¸ SQL Database": {"port": 8311, "description": "SQL ë°ì´í„°ë² ì´ìŠ¤ ë¶„ì„", "capabilities": ["sql_query", "db_analysis"], "color": "#F39C12"},
    "ğŸ¤– H2O ML": {"port": 8313, "description": "H2O AutoML ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹", "capabilities": ["automl", "model_training"], "color": "#9B59B6"},
    "ğŸ“ˆ MLflow Tools": {"port": 8314, "description": "MLflow ì‹¤í—˜ ê´€ë¦¬", "capabilities": ["experiment_tracking", "model_registry"], "color": "#E74C3C"}
}

# ì—ì´ì „íŠ¸ ì´ë¦„ ë§¤í•‘ (ê³„íšì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ë¦„ -> ì‹¤ì œ ì—ì´ì „íŠ¸ ì´ë¦„)
AGENT_NAME_MAPPING = {
    "data_loader": "ğŸ“ Data Loader",
    "data_cleaning": "ğŸ§¹ Data Cleaning", 
    "data_wrangling": "ğŸ”§ Data Wrangling",
    "eda_tools": "ğŸ” EDA Tools",
    "data_visualization": "ğŸ“Š Data Visualization",
    "feature_engineering": "âš™ï¸ Feature Engineering",
    "sql_database": "ğŸ—„ï¸ SQL Database",
    "h2o_ml": "ğŸ¤– H2O ML",
    "mlflow_tools": "ğŸ“ˆ MLflow Tools",
    # ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ê³„íšì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ë¦„ë“¤ ì¶”ê°€ (ì •í™•í•œ Agent Card name ì‚¬ìš©)
    "AI_DS_Team EDAToolsAgent": "ğŸ” EDA Tools",
    "AI_DS_Team DataLoaderToolsAgent": "ğŸ“ Data Loader",
    "AI_DS_Team DataCleaningAgent": "ğŸ§¹ Data Cleaning",
    "AI_DS_Team DataVisualizationAgent": "ğŸ“Š Data Visualization",
    "AI_DS_Team SQLDatabaseAgent": "ğŸ—„ï¸ SQL Database",
    "AI_DS_Team DataWranglingAgent": "ğŸ”§ Data Wrangling",
    # í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ ì´ë¦„ë„ ìœ ì§€
    "SessionEDAToolsAgent": "ğŸ” EDA Tools",
    # í˜¸í™˜ì„±ì„ ìœ„í•´ ê¸°ì¡´ ì´ë¦„ë„ ìœ ì§€
    "SessionEDAToolsAgent": "ğŸ” EDA Tools"
}

def map_agent_name(plan_agent_name: str) -> str:
    """ê³„íšì—ì„œ ì‚¬ìš©í•˜ëŠ” ì—ì´ì „íŠ¸ ì´ë¦„ì„ ì‹¤ì œ ì—ì´ì „íŠ¸ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘"""
    return AGENT_NAME_MAPPING.get(plan_agent_name, plan_agent_name)

def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state: st.session_state.messages = []
    if "session_id" not in st.session_state: st.session_state.session_id = f"ui_{datetime.now().strftime('%Y%m%d%H%M%S')}"
    if "uploaded_data" not in st.session_state: st.session_state.uploaded_data = None
    if "data_id" not in st.session_state: st.session_state.data_id = None
    if "a2a_client" not in st.session_state: st.session_state.a2a_client = A2AStreamlitClient(AI_DS_TEAM_AGENTS)
    if "agent_status" not in st.session_state: st.session_state.agent_status = {}
    if "active_agent" not in st.session_state: st.session_state.active_agent = None
    if "data_manager" not in st.session_state: st.session_state.data_manager = DataManager()  # DataManager ì¶”ê°€
    if "session_data_manager" not in st.session_state: st.session_state.session_data_manager = SessionDataManager()  # ì„¸ì…˜ ê¸°ë°˜ ë°ì´í„° ê´€ë¦¬ì ì¶”ê°€
    # í”„ë¦¬ë¡œë” ì´ˆê¸°í™” ìƒíƒœ ì¶”ê°€
    if "preloader_initialized" not in st.session_state: st.session_state.preloader_initialized = False
    if "agents_preloaded" not in st.session_state: st.session_state.agents_preloaded = False

@st.cache_resource
def initialize_agent_preloader():
    """ì—ì´ì „íŠ¸ í”„ë¦¬ë¡œë” ì´ˆê¸°í™” (ìºì‹œë¨)"""
    return get_agent_preloader(AI_DS_TEAM_AGENTS)

async def preload_agents_with_ui():
    """UIì™€ í•¨ê»˜ ì—ì´ì „íŠ¸ í”„ë¦¬ë¡œë”©"""
    if st.session_state.agents_preloaded:
        debug_log("âœ… ì—ì´ì „íŠ¸ê°€ ì´ë¯¸ í”„ë¦¬ë¡œë“œë¨", "success")
        return st.session_state.agent_status
    
    # í”„ë¦¬ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    preloader = initialize_agent_preloader()
    
    # ë¡œë”© UI ì„¤ì •
    loading_container = st.container()
    loading_ui = ProgressiveLoadingUI(loading_container)
    loading_ui.setup_ui()
    
    # ì§„í–‰ ìƒí™© ì½œë°± í•¨ìˆ˜
    def progress_callback(completed, total, current_task):
        loading_ui.update_progress(completed, total, current_task)
        debug_log(f"ğŸ“‹ {current_task} ({completed}/{total})")
    
    try:
        # ì—ì´ì „íŠ¸ í”„ë¦¬ë¡œë”© ì‹¤í–‰
        debug_log("ğŸš€ ì—ì´ì „íŠ¸ í”„ë¦¬ë¡œë”© ì‹œì‘...", "success")
        agents_info = await preloader.preload_agents(progress_callback)
        
        # ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (í˜¸í™˜ì„± ìœ ì§€)
        agent_status = {}
        for name, agent_info in agents_info.items():
            status_icon = "âœ…" if agent_info.status == AgentStatus.READY else "âŒ"
            agent_status[name] = {
                "status": status_icon,
                "description": agent_info.description,
                "port": agent_info.port,
                "capabilities": agent_info.capabilities,
                "color": agent_info.color,
                "initialization_time": agent_info.initialization_time,
                "error_message": agent_info.error_message
            }
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.agent_status = agent_status
        st.session_state.agents_preloaded = True
        
        # ì™„ë£Œ ìƒíƒœ í‘œì‹œ
        summary = preloader.get_initialization_summary()
        loading_ui.show_completion(summary)
        
        debug_log(f"âœ… ì—ì´ì „íŠ¸ í”„ë¦¬ë¡œë”© ì™„ë£Œ: {summary['ready_agents']}/{summary['total_agents']} ì¤€ë¹„ë¨", "success")
        
        # ë¡œë”© UI ì •ë¦¬ (ì ì‹œ í›„)
        time.sleep(2)
        loading_container.empty()
        
        return agent_status
        
    except Exception as e:
        debug_log(f"âŒ ì—ì´ì „íŠ¸ í”„ë¦¬ë¡œë”© ì‹¤íŒ¨: {e}", "error")
        loading_container.error(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # í´ë°±: ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ìƒíƒœ í™•ì¸
        debug_log("ğŸ”„ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±...", "warning")
        try:
            fallback_status = await check_agents_status_async()
            st.session_state.agent_status = fallback_status
            debug_log("âœ… í´ë°± ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸ ì™„ë£Œ", "success")
            return fallback_status
        except Exception as fallback_error:
            debug_log(f"âŒ í´ë°±ë„ ì‹¤íŒ¨: {fallback_error}", "error")
            # ìµœí›„ì˜ í´ë°±: ë¹ˆ ìƒíƒœ ë°˜í™˜
            return {}

async def check_agents_status_async():
    """AI_DS_Team ì—ì´ì „íŠ¸ ìƒíƒœ ë¹„ë™ê¸° í™•ì¸ (ìˆ˜ì •ëœ ë²„ì „)"""
    async with httpx.AsyncClient(timeout=2.0) as client:
        # ê° ì—ì´ì „íŠ¸ì— ëŒ€í•œ ë¹„ë™ê¸° GET ìš”ì²­ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        tasks = [client.get(f"http://localhost:{info['port']}/.well-known/agent.json") for info in AI_DS_TEAM_AGENTS.values()]
        
        # ëª¨ë“  ìš”ì²­ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰í•˜ê³  ê²°ê³¼ ìˆ˜ì§‘
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        results = {}
        # ì›ë˜ ì—ì´ì „íŠ¸ ì •ë³´ì™€ ì‘ë‹µ ê²°ê³¼ë¥¼ ìˆœì„œëŒ€ë¡œ ë§¤ì¹­
        for (name, info), resp in zip(AI_DS_TEAM_AGENTS.items(), responses):
            if isinstance(resp, httpx.Response) and resp.status_code == 200:
                # í¬íŠ¸ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ A2AStreamlitClientê°€ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ë„ë¡ ìˆ˜ì •
                results[name] = {
                    "status": "âœ…", 
                    "description": info['description'],
                    "port": info['port'],  # ğŸ”¥ í•µì‹¬ ìˆ˜ì •: í¬íŠ¸ ì •ë³´ ì¶”ê°€
                    "capabilities": info.get('capabilities', []),
                    "color": info.get('color', '#ffffff')
                }
            else:
                results[name] = {
                    "status": "âŒ", 
                    "description": info['description'],
                    "port": info['port'],  # ğŸ”¥ ì‹¤íŒ¨í•œ ê²½ìš°ì—ë„ í¬íŠ¸ ì •ë³´ ìœ ì§€
                    "capabilities": info.get('capabilities', []),
                    "color": info.get('color', '#ffffff')
                }
        return results

def display_agent_status():
    st.markdown("### ğŸ§¬ AI_DS_Team ì—ì´ì „íŠ¸ ìƒíƒœ")
    cols = st.columns(3)
    status = st.session_state.agent_status
    sorted_agents = sorted(status.items(), key=lambda x: (x[1]['status'] == "âŒ", x[0]))
    for idx, (name, info) in enumerate(sorted_agents):
        border_style = "2px solid #f093fb" if st.session_state.active_agent == name else "1px solid rgba(255, 255, 255, 0.2)"
        with cols[idx % 3]:
            st.markdown(f"""
            <div class="agent-card" style="border: {border_style};">
                <h4>{info['status']} {name}</h4>
                <p><small>{info['description']}</small></p>
            </div>""", unsafe_allow_html=True)

def render_artifact(artifact_data: Dict[str, Any]):
    """ì•„í‹°íŒ©íŠ¸ ë Œë”ë§ - A2A SDK 0.2.9 í˜¸í™˜ì„± ê°œì„ """
    try:
        debug_log(f"ğŸ¨ ì•„í‹°íŒ©íŠ¸ ë Œë”ë§ ì‹œì‘: {artifact_data.get('name', 'Unknown')}")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì•„í‹°íŒ©íŠ¸ êµ¬ì¡° ë¡œê·¸
        debug_log(f"ğŸ” ì•„í‹°íŒ©íŠ¸ êµ¬ì¡°: {list(artifact_data.keys())}")
        debug_log(f"ğŸ” ë©”íƒ€ë°ì´í„°: {artifact_data.get('metadata', {})}")
        
        name = artifact_data.get('name', 'Unknown')
        parts = artifact_data.get('parts', [])
        metadata = artifact_data.get('metadata', {})
        content_type = artifact_data.get('contentType', metadata.get('content_type', 'text/plain'))
        
        debug_log(f"ğŸ” ê°ì§€ëœ content_type: {content_type}")
        debug_log(f"ğŸ” ì•„í‹°íŒ©íŠ¸ ì´ë¦„: {name}")
        debug_log(f"ğŸ” Parts ê°œìˆ˜: {len(parts)}")
        
        # A2A í´ë¼ì´ì–¸íŠ¸ì—ì„œ ë°›ì€ ì•„í‹°íŒ©íŠ¸ê°€ data í•„ë“œì— ì§ì ‘ ë‚´ìš©ì´ ìˆëŠ” ê²½ìš° ì²˜ë¦¬
        if not parts and 'data' in artifact_data:
            debug_log("ğŸ”„ data í•„ë“œ ê°ì§€ - parts êµ¬ì¡°ë¡œ ë³€í™˜ ì¤‘...")
            data_content = artifact_data['data']
            
            # data ë‚´ìš©ì„ parts êµ¬ì¡°ë¡œ ë³€í™˜
            if isinstance(data_content, str):
                parts = [{"kind": "text", "text": data_content}]
                debug_log(f"âœ… data í•„ë“œë¥¼ text partë¡œ ë³€í™˜ ì™„ë£Œ (í¬ê¸°: {len(data_content)})")
            elif isinstance(data_content, dict):
                parts = [{"kind": "data", "data": data_content}]
                debug_log(f"âœ… data í•„ë“œë¥¼ data partë¡œ ë³€í™˜ ì™„ë£Œ")
            else:
                parts = [{"kind": "text", "text": str(data_content)}]
                debug_log(f"âœ… data í•„ë“œë¥¼ ë¬¸ìì—´ partë¡œ ë³€í™˜ ì™„ë£Œ")
        
        if not parts:
            st.warning("ì•„í‹°íŒ©íŠ¸ì— í‘œì‹œí•  ì½˜í…ì¸ ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì•„í‹°íŒ©íŠ¸ë³„ ì»¨í…Œì´ë„ˆ ìƒì„± (ì¤‘ë³µ ë°©ì§€)
        with st.container():
            # í—¤ë” í‘œì‹œ
            if name != 'Unknown':
                st.markdown(f"### ğŸ“¦ {name}")
            
            for i, part in enumerate(parts):
                try:
                    debug_log(f"ğŸ” Part {i} êµ¬ì¡°: {type(part)} - {list(part.keys()) if isinstance(part, dict) else 'Not dict'}")
                    
                    # A2A SDK 0.2.9 Part êµ¬ì¡° íŒŒì‹± - root ì†ì„±ì„ í†µí•œ ì ‘ê·¼
                    part_kind = None
                    text_content = None
                    data_content = None
                    
                    if hasattr(part, 'root'):
                        # A2A SDK 0.2.9 í‘œì¤€ ë°©ì‹: part.root.kind, part.root.text
                        debug_log(f"ğŸ” Part {i}: A2A SDK Part ê°ì²´ ê°ì§€")
                        if hasattr(part.root, 'kind'):
                            part_kind = part.root.kind
                            debug_log(f"ğŸ” Part {i} root.kind: {part_kind}")
                            
                            if part_kind == "text" and hasattr(part.root, 'text'):
                                text_content = part.root.text
                                debug_log(f"ğŸ” Part {i} root.text length: {len(text_content) if text_content else 0}")
                            elif part_kind == "data" and hasattr(part.root, 'data'):
                                data_content = part.root.data
                                debug_log(f"ğŸ” Part {i} root.data type: {type(data_content)}")
                    elif isinstance(part, dict):
                        # í´ë°±: ë”•ì…”ë„ˆë¦¬ í˜•íƒœì˜ Part êµ¬ì¡°
                        debug_log(f"ğŸ” Part {i}: Dictionary Part êµ¬ì¡° ê°ì§€")
                        part_kind = part.get("kind", part.get("type", "unknown"))
                        debug_log(f"ğŸ” Part {i} dict kind: {part_kind}")
                        
                        if part_kind == "text":
                            text_content = part.get("text", "")
                        elif part_kind == "data":
                            data_content = part.get("data", {})
                    else:
                        # ìµœì¢… í´ë°±: ë‹¨ìˆœ ë¬¸ìì—´ì´ë‚˜ ê¸°íƒ€ íƒ€ì…
                        debug_log(f"ğŸ” Part {i}: ê¸°íƒ€ íƒ€ì… ê°ì§€ - {type(part)}")
                        text_content = str(part)
                        part_kind = "text"
                    
                    debug_log(f"ğŸ” Part {i} ìµœì¢… kind: {part_kind}")
                    
                    # ì»¨í…ì¸  íƒ€ì…ë³„ ë Œë”ë§
                    if part_kind == "text" and text_content:
                        debug_log(f"ğŸ” Part {i} text preview: {text_content[:100]}...")
                        
                        if content_type == "application/vnd.plotly.v1+json":
                            # Plotly ì°¨íŠ¸ JSON ë°ì´í„° ì²˜ë¦¬
                            _render_plotly_chart(text_content, name, i)
                            
                        elif (content_type == "text/html" or 
                              name.endswith('.html') or 
                              any(keyword in text_content.lower() for keyword in ["<!doctype html", "<html", "ydata-profiling", "sweetviz"]) or
                              any(keyword in metadata.get('report_type', '').lower() for keyword in ["profiling", "eda", "sweetviz"])):
                            # HTML ì»¨í…ì¸  ë Œë”ë§ (Profiling ë¦¬í¬íŠ¸ ë“±)
                            debug_log(f"ğŸŒ HTML ì•„í‹°íŒ©íŠ¸ ê°ì§€ë¨: {name}")
                            _render_html_content(text_content, name, i)
                            
                        elif content_type == "text/x-python" or "```python" in text_content:
                            # Python ì½”ë“œ ë Œë”ë§
                            _render_python_code(text_content)
                            
                        elif content_type == "text/markdown" or text_content.startswith("#"):
                            # ë§ˆí¬ë‹¤ìš´ ë Œë”ë§
                            _render_markdown_content(text_content)
                            
                        else:
                            # ì¼ë°˜ í…ìŠ¤íŠ¸ ë Œë”ë§
                            _render_general_text(text_content)
                    
                    elif part_kind == "data" and data_content:
                        # ë°ì´í„° Part ì²˜ë¦¬
                        debug_log(f"ğŸ” Plotly ì°¨íŠ¸ ë°ì´í„° íŒŒì‹± ì‹œì‘...")
                        _render_data_content(data_content, content_type, name, i)
                    
                    else:
                        # ì•Œ ìˆ˜ ì—†ëŠ” íƒ€ì… ë˜ëŠ” ë¹ˆ ë‚´ìš©
                        if part_kind:
                            debug_log(f"âš ï¸ ë¹ˆ ë‚´ìš©ì´ê±°ë‚˜ ì²˜ë¦¬í•  ìˆ˜ ì—†ëŠ” part íƒ€ì…: {part_kind}")
                        else:
                            debug_log(f"âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” part êµ¬ì¡°")
                            st.json(part)
                        
                except Exception as part_error:
                    debug_log(f"âŒ Part {i} ë Œë”ë§ ì‹¤íŒ¨: {part_error}", "error")
                    with st.expander(f"ğŸ” Part {i} ì˜¤ë¥˜ ì •ë³´"):
                        st.error(f"ë Œë”ë§ ì˜¤ë¥˜: {part_error}")
                        st.json(part)
        
        debug_log("âœ… ì•„í‹°íŒ©íŠ¸ ë Œë”ë§ ì™„ë£Œ")
        
    except Exception as e:
        debug_log(f"ğŸ’¥ ì•„í‹°íŒ©íŠ¸ ë Œë”ë§ ì „ì²´ ì˜¤ë¥˜: {e}", "error")
        st.error(f"ì•„í‹°íŒ©íŠ¸ ë Œë”ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        # í´ë°±: ì›ì‹œ ë°ì´í„° í‘œì‹œ
        with st.expander("ğŸ” ì›ì‹œ ì•„í‹°íŒ©íŠ¸ ë°ì´í„° (í´ë°±)"):
            st.json(artifact_data)

def _render_plotly_chart(json_text: str, name: str, index: int):
    """Plotly ì°¨íŠ¸ ì „ìš© ë Œë”ë§"""
    try:
        import plotly.io as pio
        import plotly.graph_objects as go
        import json
        import numpy as np
        import base64
        from datetime import datetime
        import uuid
        
        debug_log("ğŸ” Plotly ì°¨íŠ¸ ë°ì´í„° íŒŒì‹± ì‹œì‘...")
        
        # JSON íŒŒì‹±
        try:
            chart_data = json.loads(json_text)
        except json.JSONDecodeError as e:
            debug_log(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}", "error")
            st.error("ì°¨íŠ¸ ë°ì´í„° JSON íŒŒì‹± ì‹¤íŒ¨")
            with st.expander("ğŸ” ì›ì‹œ ë°ì´í„°"):
                st.text(json_text[:500] + "..." if len(json_text) > 500 else json_text)
            return
        
        # Binary data ë””ì½”ë”© í•¨ìˆ˜
        def decode_plotly_binary_data(data_dict):
            """Plotly binary dataë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜"""
            if isinstance(data_dict, dict):
                for key, value in data_dict.items():
                    if isinstance(value, dict) and 'dtype' in value and 'bdata' in value:
                        try:
                            if value['dtype'] == 'f8':  # float64
                                binary_data = base64.b64decode(value['bdata'])
                                float_array = np.frombuffer(binary_data, dtype=np.float64)
                                data_dict[key] = float_array.tolist()
                                debug_log(f"âœ… Binary float64 ë””ì½”ë”© ì„±ê³µ: {key}")
                            elif value['dtype'] == 'i8':  # int64
                                binary_data = base64.b64decode(value['bdata'])
                                int_array = np.frombuffer(binary_data, dtype=np.int64)
                                data_dict[key] = int_array.tolist()
                                debug_log(f"âœ… Binary int64 ë””ì½”ë”© ì„±ê³µ: {key}")
                        except Exception as decode_error:
                            debug_log(f"âŒ Binary data ë””ì½”ë”© ì‹¤íŒ¨ {key}: {decode_error}", "error")
                    elif isinstance(value, (dict, list)):
                        if isinstance(value, dict):
                            decode_plotly_binary_data(value)
                        elif isinstance(value, list):
                            for item in value:
                                if isinstance(item, dict):
                                    decode_plotly_binary_data(item)
            return data_dict
        
        # Binary data ë””ì½”ë”© ì ìš©
        chart_data = decode_plotly_binary_data(chart_data)
        
        # Plotly Figure ìƒì„±
        try:
            if isinstance(chart_data, dict):
                # ë°ì´í„° êµ¬ì¡° í™•ì¸
                if 'data' in chart_data and 'layout' in chart_data:
                    # í‘œì¤€ Plotly êµ¬ì¡°
                    plot_data = chart_data['data']
                    layout = chart_data['layout']
                elif 'data' in chart_data and isinstance(chart_data['data'], dict) and 'data' in chart_data['data']:
                    # ì¤‘ì²©ëœ êµ¬ì¡°
                    plot_data = chart_data['data']['data']
                    layout = chart_data['data'].get('layout', {})
                else:
                    # ì „ì²´ë¥¼ Figureë¡œ ì‹œë„
                    plot_data = chart_data.get('data', [])
                    layout = chart_data.get('layout', {})
                
                fig = go.Figure(data=plot_data, layout=layout)
            else:
                # JSON ë¬¸ìì—´ë¡œ ì§ì ‘ ë³€í™˜
                fig = pio.from_json(json.dumps(chart_data))
            
            # ì°¨íŠ¸ ìµœì í™”
            fig.update_layout(
                showlegend=True,
                margin=dict(l=20, r=20, t=40, b=20),
                font=dict(size=12),
                plot_bgcolor='rgba(0,0,0,0)',
                paper_bgcolor='rgba(0,0,0,0)'
            )
            
            # ê³ ìœ  í‚¤ ìƒì„±
            chart_id = f"plotly_{name}_{index}_{uuid.uuid4().hex[:8]}_{datetime.now().strftime('%H%M%S%f')}"
            
            # ì°¨íŠ¸ í‘œì‹œ
            st.markdown("#### ğŸ“Š ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸")
            st.plotly_chart(fig, key=chart_id, use_container_width=True)
            debug_log("âœ… Plotly ì°¨íŠ¸ ë Œë”ë§ ì„±ê³µ!")
            
        except Exception as fig_error:
            debug_log(f"âŒ Plotly Figure ìƒì„± ì‹¤íŒ¨: {fig_error}", "error")
            st.error(f"ì°¨íŠ¸ ìƒì„± ì˜¤ë¥˜: {fig_error}")
            
            # ìƒì„¸ ì˜¤ë¥˜ ì •ë³´
            with st.expander("ğŸ” ì°¨íŠ¸ ì˜¤ë¥˜ ìƒì„¸ ì •ë³´"):
                st.text(f"ì˜¤ë¥˜ íƒ€ì…: {type(fig_error).__name__}")
                st.text(f"ì˜¤ë¥˜ ë©”ì‹œì§€: {str(fig_error)}")
                st.markdown("**ì›ì‹œ ì°¨íŠ¸ ë°ì´í„° (ì²˜ìŒ 1000ì):**")
                st.json(json.loads(json_text[:1000]) if len(json_text) > 1000 else json.loads(json_text))
                
    except Exception as e:
        debug_log(f"âŒ Plotly ì°¨íŠ¸ ë Œë”ë§ ì „ì²´ ì‹¤íŒ¨: {e}", "error")
        st.error(f"Plotly ì°¨íŠ¸ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

def _render_html_content(html_content: str, name: str, index: int):
    """HTML ì½˜í…ì¸  ì „ìš© ë Œë”ë§ - Key ì¤‘ë³µ ë¬¸ì œ í•´ê²° ë° ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê±°"""
    import uuid
    import time
    
    try:
        # ê³ ìœ í•œ ì‹ë³„ì ìƒì„± (UUID + íƒ€ì„ìŠ¤íƒ¬í”„ + ì„¸ì…˜ ì¹´ìš´í„°)
        unique_id = f"{uuid.uuid4().hex[:8]}_{int(time.time() * 1000)}"
        
        # ì„¸ì…˜ ìƒíƒœì— HTML ë Œë”ë§ ì¹´ìš´í„° ì´ˆê¸°í™”
        if "html_render_counter" not in st.session_state:
            st.session_state.html_render_counter = 0
        st.session_state.html_render_counter += 1
        
        # ì™„ì „íˆ ê³ ìœ í•œ key ìƒì„±
        render_key = f"html_render_{unique_id}_{st.session_state.html_render_counter}"
        
        debug_log(f"ğŸŒ HTML ì½˜í…ì¸  ë Œë”ë§ ì‹œì‘: {name} (Key: {render_key})")
        
        html_size = len(html_content)
        
        # ë©”íƒ€ì •ë³´ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ë³´ê³ ì„œ í¬ê¸°", f"{html_size // 1024}KB")
        with col2:
            st.metric("ì»¨í…ì¸  íƒ€ì…", "HTML")
        with col3:
            if any(keyword in html_content.lower() for keyword in ["sweetviz", "profiling", "ydata"]):
                st.metric("ë³´ê³ ì„œ ìœ í˜•", "EDA Profiling")
            elif "pandas_profiling" in html_content.lower():
                st.metric("ë³´ê³ ì„œ ìœ í˜•", "Pandas Profiling")
            else:
                st.metric("ë³´ê³ ì„œ ìœ í˜•", "HTML")
        
        # HTML ë Œë”ë§ ì˜µì…˜ - ë‹¤ìš´ë¡œë“œ ë§í¬ ì˜µì…˜ ì œê±°
        render_option = st.radio(
            "ë Œë”ë§ ë°©ì‹ ì„ íƒ:",
            ["ì„ë² ë””ë“œ ë·°ì–´", "HTML ì†ŒìŠ¤ ë³´ê¸°"],
            key=render_key,
            horizontal=True
        )
        
        if render_option == "ì„ë² ë””ë“œ ë·°ì–´":
            # HTML ì§ì ‘ ë Œë”ë§
            st.markdown("##### ğŸ“Š EDA ë³´ê³ ì„œ")
            st.components.v1.html(html_content, height=800, scrolling=True)
            
        else:  # HTML ì†ŒìŠ¤ ë³´ê¸°
            st.markdown("##### ğŸ“ HTML ì†ŒìŠ¤ ì½”ë“œ")
            if len(html_content) > 5000:
                # ê¸´ HTMLì€ ì¼ë¶€ë§Œ í‘œì‹œ
                st.code(html_content[:5000] + "\n\n... (ë‚´ìš©ì´ ê¸¸ì–´ ì¼ë¶€ë§Œ í‘œì‹œë©ë‹ˆë‹¤) ...", language="html")
                st.info(f"ì „ì²´ HTML í¬ê¸°: {html_size:,} ë¬¸ì (5,000ìê¹Œì§€ë§Œ í‘œì‹œ)")
            else:
                st.code(html_content, language="html")
        
        debug_log("âœ… HTML ì½˜í…ì¸  ë Œë”ë§ ì™„ë£Œ")
        
    except Exception as e:
        debug_log(f"âŒ HTML ë Œë”ë§ ì‹¤íŒ¨: {e}", "error")
        st.error(f"HTML ë Œë”ë§ ì˜¤ë¥˜: {e}")
        
        # í´ë°±: í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ
        with st.expander("ğŸ” HTML ì†ŒìŠ¤ (í´ë°±)"):
            st.text(html_content[:1000] + "..." if len(html_content) > 1000 else html_content)

def _render_python_code(text_content: str):
    """Python ì½”ë“œ ì˜ˆì˜ê²Œ ë Œë”ë§"""
    try:
        # ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ
        if "```python" in text_content:
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ì—ì„œ ì½”ë“œë§Œ ì¶”ì¶œ
            parts = text_content.split("```python")
            for i, part in enumerate(parts[1:], 1):
                if "```" in part:
                    code = part.split("```")[0].strip()
                    if code:
                        st.markdown(f"#### ğŸ Python ì½”ë“œ #{i}")
                        st.code(code, language='python')
        else:
            # ì „ì²´ê°€ ì½”ë“œì¸ ê²½ìš°
            st.markdown("#### ğŸ Python ì½”ë“œ")
            st.code(text_content, language='python')
            
        debug_log("âœ… Python ì½”ë“œ ë Œë”ë§ ì™„ë£Œ")
        
    except Exception as e:
        debug_log(f"âŒ Python ì½”ë“œ ë Œë”ë§ ì‹¤íŒ¨: {e}", "error")
        st.text(text_content)

def _render_markdown_content(text_content: str):
    """ë§ˆí¬ë‹¤ìš´ ì˜ˆì˜ê²Œ ë Œë”ë§"""
    try:
        # ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼ ì ìš©
        styled_markdown = f"""
        <div style="
            background: rgba(255, 255, 255, 0.05);
            padding: 1rem;
            border-radius: 8px;
            border-left: 4px solid #3498db;
            margin: 0.5rem 0;
        ">
        {text_content}
        </div>
        """
        
        st.markdown("#### ğŸ“ ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ ")
        st.markdown(text_content, unsafe_allow_html=True)
        debug_log("âœ… ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ ì™„ë£Œ")
        
    except Exception as e:
        debug_log(f"âŒ ë§ˆí¬ë‹¤ìš´ ë Œë”ë§ ì‹¤íŒ¨: {e}", "error")
        st.text(text_content)

def _render_general_text(text_content: str):
    """ì¼ë°˜ í…ìŠ¤íŠ¸ ë Œë”ë§"""
    try:
        # ê¸´ í…ìŠ¤íŠ¸ëŠ” expanderë¡œ
        if len(text_content) > 500:
            with st.expander("ğŸ“„ í…ìŠ¤íŠ¸ ë‚´ìš© ë³´ê¸°", expanded=True):
                st.markdown(text_content)
        else:
            st.markdown(text_content)
            
        debug_log("âœ… ì¼ë°˜ í…ìŠ¤íŠ¸ ë Œë”ë§ ì™„ë£Œ")
        
    except Exception as e:
        debug_log(f"âŒ ì¼ë°˜ í…ìŠ¤íŠ¸ ë Œë”ë§ ì‹¤íŒ¨: {e}", "error")
        st.text(text_content)

def _render_data_content(data_content: Dict, content_type: str, name: str, index: int):
    """ë°ì´í„° ì½˜í…ì¸  ë Œë”ë§"""
    try:
        if content_type == "application/vnd.plotly.v1+json":
            # Plotly ë°ì´í„°
            actual_data = data_content.get("data", {})
            if isinstance(actual_data, str):
                _render_plotly_chart(actual_data, name, index)
            else:
                _render_plotly_chart(json.dumps(actual_data), name, index)
                
        elif content_type == "application/json":
            # JSON ë°ì´í„°
            st.markdown("#### ğŸ“Š JSON ë°ì´í„°")
            st.json(data_content)
            
        else:
            # ê¸°íƒ€ ë°ì´í„°
            st.markdown("#### ğŸ“„ ë°ì´í„°")
            st.json(data_content)
            
        debug_log("âœ… ë°ì´í„° ì½˜í…ì¸  ë Œë”ë§ ì™„ë£Œ")
        
    except Exception as e:
        debug_log(f"âŒ ë°ì´í„° ì½˜í…ì¸  ë Œë”ë§ ì‹¤íŒ¨: {e}", "error")
        st.json(data_content)

async def process_query_streaming(prompt: str):
    """A2A í”„ë¡œí† ì½œì„ ì‚¬ìš©í•œ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬ + Phase 3 ì „ë¬¸ê°€ê¸‰ ë‹µë³€ í•©ì„± + ê°œì„ ëœ Langfuse ì¶”ì """
    debug_log(f"ğŸš€ A2A ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬ ì²˜ë¦¬ ì‹œì‘: {prompt[:100]}...")
    
    # Langfuse Session ì‹œì‘ - ê°œì„ ëœ ë²„ì „
    session_tracer = None
    session_id = None
    if LANGFUSE_SESSION_AVAILABLE:
        try:
            session_tracer = get_session_tracer()
            # EMP_NOë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ user_id ì„¤ì •
            user_id = st.session_state.get("user_id") or os.getenv("EMP_NO") or os.getenv("LANGFUSE_USER_ID") or "cherryai_user"
            session_metadata = {
                "streamlit_session_id": st.session_state.get("session_id", "unknown"),
                "user_interface": "streamlit",
                "query_timestamp": time.time(),
                "query_length": len(prompt),
                "environment": "production" if os.getenv("ENV") == "production" else "development",
                "app_version": "v9.0",
                "emp_no": os.getenv("EMP_NO", "unknown")  # ì§ì› ë²ˆí˜¸ ëª…ì‹œì  ê¸°ë¡
            }
            session_id = session_tracer.start_user_session(prompt, user_id, session_metadata)
            debug_log(f"ğŸ” Langfuse Session ì‹œì‘: {session_id} (EMP_NO: {os.getenv('EMP_NO', 'N/A')})", "success")
        except Exception as e:
            debug_log(f"âŒ Langfuse Session ì‹œì‘ ì‹¤íŒ¨: {e}", "error")
    
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # Response container ì¤€ë¹„
    with st.chat_message("assistant"):
        placeholder = st.container()
        
        try:
            if A2A_CLIENT_AVAILABLE:
                # A2A í´ë¼ì´ì–¸íŠ¸ ë° ë©€í‹°ì—ì´ì „íŠ¸ ì¶”ì 
                with placeholder:
                    st.markdown("ğŸ¤– **AI ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...**")
                    progress_bar = st.progress(0)
                    status_container = st.empty()
                    
                    # A2A í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
                    try:
                        # A2A SDK 0.2.9 ì¤€ìˆ˜: agents_info ë§¤ê°œë³€ìˆ˜ í•„ìˆ˜
                        agents_info = st.session_state.agent_status if st.session_state.agent_status else AI_DS_TEAM_AGENTS
                        client = A2AStreamlitClient(agents_info, timeout=180.0)
                        debug_log("âœ… A2A í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ", "success")
                    except Exception as e:
                        debug_log(f"âŒ A2A í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "error")
                        debug_log("ğŸ”„ í´ë°± ë¶„ì„ ëª¨ë“œ ì‹¤í–‰")
                        fallback_result = await fallback_analysis(prompt, placeholder)
                        return
                    
                    # ë‹¨ê³„ë³„ ê³„íš ìˆ˜ë¦½
                    with status_container:
                        st.info("ğŸ“‹ **ë‹¨ê³„**: ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì¤‘...")
                    progress_bar.progress(10)
                    
                    # Langfuse Agent ì¶”ì  ì‹œì‘
                    if session_tracer:
                        try:
                            with session_tracer.trace_agent_execution("ğŸ§  Query Planner", "ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„ ë° ì‹¤í–‰ ê³„íš ìˆ˜ë¦½") as agent_span:
                                plan_steps = await create_analysis_plan(prompt, client)
                                if agent_span:
                                    session_tracer.record_agent_result("ğŸ§  Query Planner", {
                                        "steps_count": len(plan_steps),
                                        "estimated_duration": len(plan_steps) * 30,
                                        "complexity": "high" if len(plan_steps) > 3 else "medium"
                                    }, confidence=0.95)
                        except Exception as plan_error:
                            debug_log(f"âŒ ê³„íš ìˆ˜ë¦½ ì¶”ì  ì‹¤íŒ¨: {plan_error}", "error")
                            plan_steps = await create_analysis_plan(prompt, client)
                    else:
                        plan_steps = await create_analysis_plan(prompt, client)
                    
                    debug_log(f"ğŸ“‹ ì‹¤í–‰ ê³„íš: {len(plan_steps)}ë‹¨ê³„", "info")
                    
                    # ì‹¤í–‰ ë‹¨ê³„ë³„ ì²˜ë¦¬
                    all_results = []
                    for i, step in enumerate(plan_steps):
                        step_progress = 20 + (i * 60 // len(plan_steps))
                        progress_bar.progress(step_progress)
                        
                        agent_name = step.get('agent_name', 'Unknown Agent')
                        task_description = step.get('description', 'ë¶„ì„ ìˆ˜í–‰')
                        
                        with status_container:
                            st.info(f"ğŸ¤– **ë‹¨ê³„ {i+1}/{len(plan_steps)}**: {agent_name} ì‹¤í–‰ ì¤‘...")
                        
                        # Langfuseì—ì„œ ê° ì—ì´ì „íŠ¸ ì‹¤í–‰ ì¶”ì 
                        if session_tracer:
                            try:
                                with session_tracer.trace_agent_execution(agent_name, task_description, {
                                    "step_number": i + 1,
                                    "total_steps": len(plan_steps),
                                    "agent_type": step.get('agent_type', 'analysis'),
                                    "priority": step.get('priority', 'normal')
                                }) as agent_span:
                                    # ì‹¤ì œ A2A ì—ì´ì „íŠ¸ ì‹¤í–‰
                                    result = await execute_agent_step(step, client, session_id)
                                    all_results.append(result)
                                    
                                    # ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼ ê¸°ë¡
                                    if agent_span:
                                        session_tracer.record_agent_result(agent_name, {
                                            "success": result.get('success', False),
                                            "artifacts_generated": len(result.get('artifacts', [])),
                                            "processing_time": result.get('processing_time', 0),
                                            "data_points_processed": result.get('data_points', 0)
                                        }, confidence=result.get('confidence', 0.8))
                            except Exception as step_error:
                                debug_log(f"âŒ {agent_name} ì¶”ì  ì‹¤íŒ¨: {step_error}", "error")
                                result = await execute_agent_step(step, client, session_id)
                                all_results.append(result)
                        else:
                            result = await execute_agent_step(step, client, session_id)
                            all_results.append(result)
                        
                        debug_log(f"âœ… {agent_name} ì™„ë£Œ", "success")
                    
                    progress_bar.progress(90)
                    
                    # ìµœì¢… ë‹µë³€ í•©ì„±
                    with status_container:
                        st.info("ğŸ¯ **ë‹¨ê³„**: ì „ë¬¸ê°€ê¸‰ ë‹µë³€ í•©ì„± ì¤‘...")
                    
                    if session_tracer:
                        try:
                            with session_tracer.trace_agent_execution("ğŸ¯ Final Synthesizer", "ë©€í‹°ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•© ë° ì „ë¬¸ê°€ê¸‰ ë‹µë³€ ìƒì„±") as final_span:
                                final_response = await synthesize_expert_response(prompt, all_results, placeholder)
                                if final_span:
                                    session_tracer.record_agent_result("ğŸ¯ Final Synthesizer", {
                                        "response_length": len(final_response),
                                        "sources_integrated": len([r for r in all_results if r.get('success')]),
                                        "synthesis_quality": "high"
                                    }, confidence=0.92)
                        except Exception as synthesis_error:
                            debug_log(f"âŒ ìµœì¢… í•©ì„± ì¶”ì  ì‹¤íŒ¨: {synthesis_error}", "error")
                            final_response = await synthesize_expert_response(prompt, all_results, placeholder)
                    else:
                        final_response = await synthesize_expert_response(prompt, all_results, placeholder)
                    
                    progress_bar.progress(100)
                    status_container.success("âœ… **ì™„ë£Œ**: ì „ë¬¸ê°€ê¸‰ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    
                    # ì„¸ì…˜ ë©”ì‹œì§€ì— ì¶”ê°€
                    st.session_state.messages.append({"role": "assistant", "content": final_response})
                    
                    # Langfuse Session ì¢…ë£Œ (ì„±ê³µ ì¼€ì´ìŠ¤)
                    if session_tracer and session_id:
                        try:
                            final_result = {
                                "success": True,
                                "total_steps": len(plan_steps),
                                "total_artifacts": sum(len(r.get('artifacts', [])) for r in all_results),
                                "processing_completed": True,
                                "total_processing_time": sum(r.get('processing_time', 0) for r in all_results),
                                "agents_used": list(set(step.get('agent_name', 'unknown') for step in plan_steps))
                            }
                            session_summary = {
                                "steps_executed": len(plan_steps),
                                "agents_used": list(set(step.get('agent_name', 'unknown') for step in plan_steps)),
                                "artifacts_created": sum(len(r.get('artifacts', [])) for r in all_results),
                                "user_satisfaction": "high",  # ì„ì‹œ ê°’
                                "session_duration": time.time() - session_tracer.current_session_trace.input.get('start_time', time.time()) if session_tracer.current_session_trace else 0
                            }
                            session_tracer.end_user_session(final_result, session_summary)
                            debug_log(f"ğŸ” Langfuse Session ì¢…ë£Œ (ì„±ê³µ): {session_id}", "success")
                        except Exception as session_end_error:
                            debug_log(f"âŒ Langfuse Session ì¢…ë£Œ ì‹¤íŒ¨: {session_end_error}", "error")
                    
            else:
                # í´ë°± ëª¨ë“œ
                debug_log("âš ï¸ A2A í´ë¼ì´ì–¸íŠ¸ ë¹„í™œì„±í™” - í´ë°± ëª¨ë“œ ì‹¤í–‰", "warning")
                await fallback_analysis(prompt, placeholder)
                
        except Exception as e:
            debug_log(f"âŒ ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}", "error")
            debug_log(f"ğŸ“ ì˜¤ë¥˜ ìœ„ì¹˜: {traceback.format_exc()}", "error")
            
            # í–¥ìƒëœ ì—ëŸ¬ í•¸ë“¤ë§ ì‹œìŠ¤í…œ ì‚¬ìš©
            error_context = show_error(
                e, 
                ErrorCategory.AGENT_ERROR, 
                ErrorSeverity.HIGH,
                show_recovery=True
            )
            
            # ê¸°ì¡´ UIìš© ë©”ì‹œì§€ë„ ìœ ì§€
            error_message = error_context.user_friendly_message if error_context else f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            st.session_state.messages.append({"role": "assistant", "content": error_message})
            
            # Langfuse Session ì¢…ë£Œ (ì‹¤íŒ¨ ì¼€ì´ìŠ¤)
            if session_tracer and session_id:
                try:
                    error_result = {
                        "success": False,
                        "error": str(e),
                        "error_type": type(e).__name__,
                        "processing_completed": False
                    }
                    session_tracer.end_user_session(error_result, {"error_occurred": True})
                    debug_log(f"ğŸ” Langfuse Session ì¢…ë£Œ (ì‹¤íŒ¨): {session_id}", "warning")
                except Exception as session_error_end:
                    debug_log(f"âŒ Langfuse Session ì‹¤íŒ¨ ì¢…ë£Œ ì‹¤íŒ¨: {session_error_end}", "error")

def get_file_size_info(file_id: str) -> str:
    """íŒŒì¼ í¬ê¸° ì •ë³´ë¥¼ ë°˜í™˜í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    try:
        # DataManagerì—ì„œ íŒŒì¼ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        if hasattr(st.session_state, 'data_manager'):
            df = st.session_state.data_manager.get_dataframe(file_id)
            if df is not None:
                memory_mb = round(df.memory_usage(deep=True).sum() / 1024**2, 2)
                return f"{df.shape[0]}í–‰ Ã— {df.shape[1]}ì—´, {memory_mb}MB"
        
        # SessionDataManagerì—ì„œ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        if hasattr(st.session_state, 'session_data_manager'):
            session_id = st.session_state.session_data_manager.get_current_session_id()
            if session_id and session_id in st.session_state.session_data_manager._session_metadata:
                session_meta = st.session_state.session_data_manager._session_metadata[session_id]
                for file_meta in session_meta.uploaded_files:
                    if file_meta.data_id == file_id:
                        size_mb = round(file_meta.file_size / 1024**2, 2)
                        return f"{file_meta.file_type}, {size_mb}MB"
        
        return "í¬ê¸° ì •ë³´ ì—†ìŒ"
    except Exception as e:
        debug_log(f"íŒŒì¼ í¬ê¸° ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: {e}", "warning")
        return "í¬ê¸° ì¡°íšŒ ì‹¤íŒ¨"

def display_session_status():
    """ì„¸ì…˜ ìƒíƒœ í‘œì‹œ"""
    try:
        if hasattr(st.session_state, 'session_data_manager'):
            session_manager = st.session_state.session_data_manager
            current_session_id = session_manager.get_current_session_id()
            
            if current_session_id:
                session_age_info = session_manager.check_session_age(current_session_id)
                
                # ì„¸ì…˜ ìƒíƒœì— ë”°ë¥¸ ìƒ‰ìƒ í‘œì‹œ
                if session_age_info["status"] == "active":
                    status_color = "ğŸŸ¢"
                elif session_age_info["status"] == "warning":
                    status_color = "ğŸŸ¡"
                else:
                    status_color = "ğŸ”´"
                
                st.sidebar.markdown(f"""
                ### {status_color} ì„¸ì…˜ ìƒíƒœ
                - **ì„¸ì…˜ ID**: `{current_session_id}`
                - **ìƒíƒœ**: {session_age_info["status"]}
                - **ìƒì„± ì‹œê°„**: {session_age_info["age_hours"]:.1f}ì‹œê°„ ì „
                - **ì •ë¦¬ê¹Œì§€**: {session_age_info["hours_until_cleanup"]:.1f}ì‹œê°„
                """)
                
                # íŒŒì¼ ëª©ë¡
                files = session_manager.get_session_files(current_session_id)
                if files:
                    st.sidebar.markdown("### ğŸ“ ì„¸ì…˜ íŒŒì¼")
                    for file_id in files:
                        file_info = get_file_size_info(file_id)
                        active_file, _ = session_manager.get_active_file_info(current_session_id)
                        
                        if file_id == active_file:
                            st.sidebar.markdown(f"ğŸ¯ **{file_id}** ({file_info})")
                        else:
                            st.sidebar.markdown(f"ğŸ“„ {file_id} ({file_info})")
            else:
                st.sidebar.markdown("### âšª ì„¸ì…˜ ì—†ìŒ")
                st.sidebar.info("íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ìƒˆ ì„¸ì…˜ì´ ìƒì„±ë©ë‹ˆë‹¤.")
    
    except Exception as e:
        debug_log(f"ì„¸ì…˜ ìƒíƒœ í‘œì‹œ ì˜¤ë¥˜: {e}", "warning")

def handle_data_upload_with_ai_ds_team():
    """SessionDataManagerë¥¼ ì‚¬ìš©í•œ ì„¸ì…˜ ê¸°ë°˜ íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬"""
    data_manager = st.session_state.data_manager
    session_data_manager = st.session_state.session_data_manager
    
    # í˜„ì¬ ë¡œë“œëœ ë°ì´í„°ì…‹ í‘œì‹œ
    loaded_data_info = data_manager.list_dataframe_info()
    
    if loaded_data_info:
        st.success(f"âœ… {len(loaded_data_info)}ê°œì˜ ë°ì´í„°ì…‹ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì„¸ì…˜ ë™ê¸°í™” í™•ì¸ ë° ë³µêµ¬
        current_session_id = session_data_manager.get_current_session_id()
        if not current_session_id or current_session_id not in session_data_manager._session_metadata:
            # ì„¸ì…˜ì´ ì—†ê±°ë‚˜ ë©”íƒ€ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ë³µêµ¬
            debug_log("ì„¸ì…˜ ì •ë³´ê°€ ì—†ì–´ ë°ì´í„°ë¡œë¶€í„° ì„¸ì…˜ì„ ë³µêµ¬í•©ë‹ˆë‹¤.")
            first_data_id = loaded_data_info[0]['data_id']
            df = data_manager.get_dataframe(first_data_id)
            if df is not None:
                new_session_id = session_data_manager.create_session_with_data(
                    data_id=first_data_id,
                    data=df,
                    user_instructions="ê¸°ì¡´ ë°ì´í„°ë¡œë¶€í„° ì„¸ì…˜ ë³µêµ¬"
                )
                debug_log(f"ì„¸ì…˜ ë³µêµ¬ ì™„ë£Œ: {new_session_id}")
                st.info(f"ğŸ”„ ì„¸ì…˜ì´ ë³µêµ¬ë˜ì—ˆìŠµë‹ˆë‹¤: {new_session_id}")
        
        # ë¡œë“œëœ ë°ì´í„°ì…‹ ëª©ë¡ì„ expanderë¡œ í‘œì‹œ
        with st.expander("ğŸ“‹ ë¡œë“œëœ ë°ì´í„°ì…‹ ë³´ê¸°", expanded=False):
            for info in loaded_data_info:
                data_id = info['data_id']
                shape = info['shape']
                col1, col2 = st.columns([3, 1])
                with col1:
                    st.markdown(f"**{data_id}** (í˜•íƒœ: {shape[0]}í–‰ x {shape[1]}ì—´)")
                with col2:
                    if st.button(f"ğŸ—‘ï¸ ì‚­ì œ", key=f"del_{data_id}"):
                        if data_manager.delete_dataframe(data_id):
                            st.toast(f"'{data_id}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            st.rerun()
                        else:
                            st.toast(f"'{data_id}' ì‚­ì œ ì‹¤íŒ¨.", icon="âŒ")
        
        # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì„ ê¸°ë³¸ìœ¼ë¡œ ì„¤ì •
        if not st.session_state.data_id or st.session_state.data_id not in [info['data_id'] for info in loaded_data_info]:
            st.session_state.data_id = loaded_data_info[0]['data_id']
            st.session_state.uploaded_data = data_manager.get_dataframe(st.session_state.data_id)
    else:
        st.info("í˜„ì¬ ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
    
    # íŒŒì¼ ì—…ë¡œë“œ (ë‹¤ì¤‘ íŒŒì¼ ì§€ì›)
    uploaded_files = st.file_uploader(
        "CSV ë˜ëŠ” Excel íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ë‹¤ì¤‘ ì„ íƒ ê°€ëŠ¥)",
        type=["csv", "xlsx"],
        accept_multiple_files=True,
        help="ì—¬ëŸ¬ íŒŒì¼ì„ í•œ ë²ˆì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )
    
    if uploaded_files:
        # ì´ë¯¸ ë¡œë“œëœ íŒŒì¼ë“¤ í™•ì¸
        existing_df_ids = set(data_manager.list_dataframes())
        files_to_process = []
        
        for file in uploaded_files:
            file_id = file.name
            if file_id not in existing_df_ids:
                files_to_process.append(file)
        
        if files_to_process:
            files_loaded = 0
            
            for file in files_to_process:
                try:
                    with st.spinner(f"'{file.name}' ì²˜ë¦¬ ì¤‘..."):
                        # íŒŒì¼ ì½ê¸°
                        if file.name.endswith('.csv'):
                            df = pd.read_csv(file)
                        else:
                            df = pd.read_excel(file)
                        
                        # DataManagerì— ì¶”ê°€ (ìë™ìœ¼ë¡œ shared_dataframesì— ì €ì¥ë¨)
                        file_id = file.name
                        data_manager.add_dataframe(data_id=file_id, data=df, source="File Upload")
                        
                        # ì„¸ì…˜ ê¸°ë°˜ AI DS Team í™˜ê²½ ì¤€ë¹„
                        # ì´ íŒŒì¼ì´ AI DS Teamì—ì„œ ì‚¬ìš©ë  ì˜ˆì •ì´ë¯€ë¡œ ì„¸ì…˜ì— ì¶”ê°€
                        session_id = session_data_manager.create_session_with_data(
                            data_id=file_id,
                            data=df,
                            user_instructions="íŒŒì¼ ì—…ë¡œë“œë¥¼ í†µí•œ ë°ì´í„° ë¡œë“œ"
                        )
                        
                        # AI DS Team í™˜ê²½ ì¤€ë¹„ (ai_ds_team/data/ í´ë”ì— íŒŒì¼ ë°°ì¹˜)
                        env_info = session_data_manager.prepare_ai_ds_team_environment(session_id)
                        
                        files_loaded += 1
                        debug_log(f"íŒŒì¼ ì—…ë¡œë“œ ì„±ê³µ: {file_id}, shape={df.shape}, session={session_id}")
                        
                        # ì²« ë²ˆì§¸ íŒŒì¼ì„ ê¸°ë³¸ ë°ì´í„°ë¡œ ì„¤ì •
                        if not st.session_state.data_id:
                            st.session_state.data_id = file_id
                            st.session_state.uploaded_data = df
                            st.session_state.current_session_id = session_id
                            
                except Exception as e:
                    st.error(f"'{file.name}' ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
                    debug_log(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {file.name} - {e}", "error")
            
            if files_loaded > 0:
                st.toast(f"âœ… {files_loaded}ê°œì˜ íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!", icon="ğŸ‰")
                st.success("ğŸ”„ AI DS Team í™˜ê²½ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì—ì´ì „íŠ¸ë“¤ì´ ì˜¬ë°”ë¥¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                st.rerun()
        else:
            # ëª¨ë“  íŒŒì¼ì´ ì´ë¯¸ ë¡œë“œë¨
            file_names = [f.name for f in uploaded_files]
            if len(file_names) == 1:
                st.info(f"'{file_names[0]}'ëŠ” ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            else:
                st.info(f"ì„ íƒëœ {len(file_names)}ê°œ íŒŒì¼ì´ ëª¨ë‘ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

def display_data_summary_ai_ds_team(data):
    """DataManager ê¸°ë°˜ ë°ì´í„° ìš”ì•½ í‘œì‹œ"""
    if data is not None:
        st.markdown("---")
        st.markdown("### ğŸ“Š ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        
        # ê¸°ë³¸ ì •ë³´
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í–‰ ìˆ˜", f"{data.shape[0]:,}")
        with col2:
            st.metric("ì—´ ìˆ˜", f"{data.shape[1]:,}")
        with col3:
            st.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰", f"{data.memory_usage(deep=True).sum() / 1024 / 1024:.1f} MB")
        
        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        st.dataframe(data.head(10), use_container_width=True)
        
        # AI_DS_Team ìœ í‹¸ë¦¬í‹°ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš° ì¶”ê°€ ì •ë³´
        if AI_DS_TEAM_UTILS_AVAILABLE:
            with st.expander("ğŸ“ˆ ìƒì„¸ ì •ë³´", expanded=False):
                summaries = get_dataframe_summary(data)
                for summary in summaries:
                    st.text(summary)

class ProfilingInsightExtractor:
    """YData profiling ë¦¬í¬íŠ¸ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
    
    def __init__(self, df, profile_report=None):
        self.df = df
        self.profile = profile_report
        if self.profile is not None:
            try:
                self.description = self.profile.get_description()
            except Exception as e:
                debug_log(f"âš ï¸ Profile description ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                self.description = None
    
    def extract_data_quality_insights(self):
        """ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        if self.description is None:
            return self._fallback_quality_analysis()
        
        quality_insights = {
            'completeness': self._analyze_completeness(),
            'uniqueness': self._analyze_uniqueness(),
            'validity': self._analyze_validity()
        }
        return quality_insights
    
    def extract_statistical_insights(self):
        """í†µê³„ì  ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        if self.description is None:
            return self._fallback_statistical_analysis()
        
        stats_insights = {
            'distributions': self._analyze_distributions(),
            'outliers': self._detect_outliers(),
            'correlations': self._analyze_correlations(),
            'patterns': self._identify_patterns()
        }
        return stats_insights
    
    def _analyze_completeness(self):
        """ì™„ì „ì„± ë¶„ì„"""
        if not self.description:
            return {}
        
        missing_data = {}
        total_rows = self.description.get('table', {}).get('n', len(self.df))
        
        for var, info in self.description.get('variables', {}).items():
            missing_count = info.get('n_missing', 0)
            missing_pct = (missing_count / total_rows) * 100 if total_rows > 0 else 0
            
            missing_data[var] = {
                'missing_count': missing_count,
                'missing_percentage': round(missing_pct, 2),
                'severity': 'high' if missing_pct > 20 else 'medium' if missing_pct > 5 else 'low'
            }
        return missing_data
    
    def _analyze_uniqueness(self):
        """ìœ ì¼ì„± ë¶„ì„"""
        if not self.description:
            return {}
        
        uniqueness_data = {}
        total_rows = self.description.get('table', {}).get('n', len(self.df))
        
        for var, info in self.description.get('variables', {}).items():
            n_distinct = info.get('n_distinct', info.get('n_unique', 0))
            uniqueness_pct = (n_distinct / total_rows) * 100 if total_rows > 0 else 0
            
            uniqueness_data[var] = {
                'unique_count': n_distinct,
                'uniqueness_percentage': round(uniqueness_pct, 2),
                'is_categorical': uniqueness_pct < 50,
                'potential_id': uniqueness_pct > 95
            }
        return uniqueness_data
    
    def _analyze_validity(self):
        """ìœ íš¨ì„± ë¶„ì„"""
        validity_data = {}
        
        for column in self.df.columns:
            dtype = str(self.df[column].dtype)
            validity_data[column] = {
                'data_type': dtype,
                'has_nulls': self.df[column].isnull().any(),
                'has_duplicates': self.df[column].duplicated().any(),
                'is_numeric': dtype in ['int64', 'float64', 'int32', 'float32'],
                'is_datetime': 'datetime' in dtype
            }
            
        return validity_data
    
    def _detect_outliers(self):
        """ì´ìƒì¹˜ íƒì§€"""
        outliers = {}
        
        for column in self.df.select_dtypes(include=['number']).columns:
            try:
                Q1 = self.df[column].quantile(0.25)
                Q3 = self.df[column].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outlier_count = ((self.df[column] < lower_bound) | (self.df[column] > upper_bound)).sum()
                outlier_pct = (outlier_count / len(self.df)) * 100
                
                outliers[column] = {
                    'count': int(outlier_count),
                    'percentage': round(outlier_pct, 2),
                    'lower_bound': float(lower_bound),
                    'upper_bound': float(upper_bound)
                }
            except Exception as e:
                debug_log(f"âš ï¸ {column} ì´ìƒì¹˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
                
        return outliers
    
    def _analyze_distributions(self):
        """ë¶„í¬ ë¶„ì„"""
        distributions = {}
        
        for column in self.df.select_dtypes(include=['number']).columns:
            try:
                distributions[column] = {
                    'mean': float(self.df[column].mean()),
                    'median': float(self.df[column].median()),
                    'std': float(self.df[column].std()),
                    'skewness': float(self.df[column].skew()),
                    'min': float(self.df[column].min()),
                    'max': float(self.df[column].max())
                }
            except Exception as e:
                debug_log(f"âš ï¸ {column} ë¶„í¬ ë¶„ì„ ì‹¤íŒ¨: {e}")
                
        return distributions
    
    def _analyze_correlations(self):
        """ìƒê´€ê´€ê³„ ë¶„ì„"""
        try:
            numeric_df = self.df.select_dtypes(include=['number'])
            if len(numeric_df.columns) < 2:
                return {}
            
            corr_matrix = numeric_df.corr()
            correlations = {}
            
            for i, col1 in enumerate(corr_matrix.columns):
                for j, col2 in enumerate(corr_matrix.columns):
                    if i < j:  # ì¤‘ë³µ ì œê±°
                        corr_value = corr_matrix.loc[col1, col2]
                        if not pd.isna(corr_value):
                            correlations[f"{col1}_vs_{col2}"] = round(float(corr_value), 3)
            
            return correlations
        except Exception as e:
            debug_log(f"âš ï¸ ìƒê´€ê´€ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _identify_patterns(self):
        """íŒ¨í„´ ì‹ë³„"""
        patterns = {
            'categorical_vars': [],
            'continuous_vars': [],
            'datetime_vars': [],
            'high_cardinality_vars': [],
            'constant_vars': []
        }
        
        for column in self.df.columns:
            dtype = str(self.df[column].dtype)
            unique_count = self.df[column].nunique()
            total_count = len(self.df)
            
            # ë²”ì£¼í˜• ë³€ìˆ˜
            if dtype == 'object' or unique_count / total_count < 0.5:
                patterns['categorical_vars'].append(column)
            
            # ì—°ì†í˜• ë³€ìˆ˜
            if dtype in ['int64', 'float64', 'int32', 'float32']:
                patterns['continuous_vars'].append(column)
            
            # ë‚ ì§œ/ì‹œê°„ ë³€ìˆ˜
            if 'datetime' in dtype:
                patterns['datetime_vars'].append(column)
            
            # ê³ ìœ ê°’ì´ ë§ì€ ë³€ìˆ˜ (IDì¼ ê°€ëŠ¥ì„±)
            if unique_count / total_count > 0.95:
                patterns['high_cardinality_vars'].append(column)
            
            # ìƒìˆ˜ ë³€ìˆ˜
            if unique_count == 1:
                patterns['constant_vars'].append(column)
        
        return patterns
    
    def _fallback_quality_analysis(self):
        """í”„ë¡œíŒŒì¼ ì •ë³´ ì—†ì„ ë•Œ ê¸°ë³¸ í’ˆì§ˆ ë¶„ì„"""
        quality_insights = {
            'completeness': {},
            'uniqueness': {},
            'validity': {}
        }
        
        for column in self.df.columns:
            # ì™„ì „ì„±
            missing_count = self.df[column].isnull().sum()
            missing_pct = (missing_count / len(self.df)) * 100
            
            quality_insights['completeness'][column] = {
                'missing_count': int(missing_count),
                'missing_percentage': round(missing_pct, 2),
                'severity': 'high' if missing_pct > 20 else 'medium' if missing_pct > 5 else 'low'
            }
            
            # ìœ ì¼ì„±
            unique_count = self.df[column].nunique()
            uniqueness_pct = (unique_count / len(self.df)) * 100
            
            quality_insights['uniqueness'][column] = {
                'unique_count': int(unique_count),
                'uniqueness_percentage': round(uniqueness_pct, 2),
                'is_categorical': uniqueness_pct < 50,
                'potential_id': uniqueness_pct > 95
            }
        
        return quality_insights
    
    def _fallback_statistical_analysis(self):
        """í”„ë¡œíŒŒì¼ ì •ë³´ ì—†ì„ ë•Œ ê¸°ë³¸ í†µê³„ ë¶„ì„"""
        return {
            'distributions': self._analyze_distributions(),
            'outliers': self._detect_outliers(),
            'correlations': self._analyze_correlations(),
            'patterns': self._identify_patterns()
        }

def extract_profiling_insights(df, profile_report=None):
    """YData profiling ë¦¬í¬íŠ¸ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
    try:
        extractor = ProfilingInsightExtractor(df, profile_report)
        
        insights = {
            'metadata': {
                'analysis_timestamp': datetime.now().isoformat(),
                'data_shape': df.shape,
                'total_memory_usage': f"{df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB"
            },
            'data_quality': extractor.extract_data_quality_insights(),
            'statistical_analysis': extractor.extract_statistical_insights()
        }
        
        debug_log(f"ğŸ“Š í”„ë¡œíŒŒì¼ë§ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì™„ë£Œ - {len(insights['data_quality'].get('completeness', {}))}ê°œ ë³€ìˆ˜ ë¶„ì„")
        
        return insights
        
    except Exception as e:
        debug_log(f"âŒ í”„ë¡œíŒŒì¼ë§ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return {
            'metadata': {
                'analysis_timestamp': datetime.now().isoformat(),
                'data_shape': df.shape,
                'error': str(e)
            },
            'data_quality': {},
            'statistical_analysis': {}
        }

def format_insights_for_display(insights):
    """ì¸ì‚¬ì´íŠ¸ë¥¼ ì‚¬ìš©ì ì¹œí™”ì  í˜•íƒœë¡œ í¬ë§·íŒ…"""
    try:
        formatted_text = "# ğŸ“Š ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼\n\n"
        
        # ë©”íƒ€ë°ì´í„°
        metadata = insights.get('metadata', {})
        formatted_text += f"**ë¶„ì„ ì‹œê°„**: {metadata.get('analysis_timestamp', 'N/A')}\n"
        formatted_text += f"**ë°ì´í„° í¬ê¸°**: {metadata.get('data_shape', 'N/A')}\n"
        formatted_text += f"**ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: {metadata.get('total_memory_usage', 'N/A')}\n\n"
        
        # ë°ì´í„° í’ˆì§ˆ
        data_quality = insights.get('data_quality', {})
        if data_quality:
            formatted_text += "## ğŸ” ë°ì´í„° í’ˆì§ˆ ë¶„ì„\n\n"
            
            completeness = data_quality.get('completeness', {})
            if completeness:
                formatted_text += "### ì™„ì „ì„± (ê²°ì¸¡ì¹˜ ë¶„ì„)\n"
                high_missing = [col for col, info in completeness.items() if info.get('severity') == 'high']
                medium_missing = [col for col, info in completeness.items() if info.get('severity') == 'medium']
                
                if high_missing:
                    formatted_text += f"âš ï¸ **ë†’ì€ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ (>20%)**: {', '.join(high_missing)}\n"
                if medium_missing:
                    formatted_text += f"âš¡ **ì¤‘ê°„ ê²°ì¸¡ì¹˜ ë¹„ìœ¨ (5-20%)**: {', '.join(medium_missing)}\n"
                
                formatted_text += "\n"
            
            uniqueness = data_quality.get('uniqueness', {})
            if uniqueness:
                potential_ids = [col for col, info in uniqueness.items() if info.get('potential_id')]
                categorical_vars = [col for col, info in uniqueness.items() if info.get('is_categorical')]
                
                if potential_ids:
                    formatted_text += f"ğŸ”‘ **ì ì¬ì  ID ë³€ìˆ˜**: {', '.join(potential_ids)}\n"
                if categorical_vars:
                    formatted_text += f"ğŸ“Š **ë²”ì£¼í˜• ë³€ìˆ˜**: {', '.join(categorical_vars[:5])}{'...' if len(categorical_vars) > 5 else ''}\n"
                
                formatted_text += "\n"
        
        # í†µê³„ ë¶„ì„
        statistical_analysis = insights.get('statistical_analysis', {})
        if statistical_analysis:
            formatted_text += "## ğŸ“ˆ í†µê³„ ë¶„ì„\n\n"
            
            outliers = statistical_analysis.get('outliers', {})
            if outliers:
                high_outliers = [(col, info) for col, info in outliers.items() if info.get('percentage', 0) > 5]
                if high_outliers:
                    formatted_text += "### ì´ìƒì¹˜ íƒì§€\n"
                    for col, info in high_outliers[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                        formatted_text += f"ğŸ“Œ **{col}**: {info.get('percentage', 0):.1f}% ({info.get('count', 0)}ê°œ)\n"
                    formatted_text += "\n"
            
            correlations = statistical_analysis.get('correlations', {})
            if correlations:
                high_corrs = [(pair, corr) for pair, corr in correlations.items() if abs(corr) > 0.7]
                if high_corrs:
                    formatted_text += "### ë†’ì€ ìƒê´€ê´€ê³„\n"
                    for pair, corr in sorted(high_corrs, key=lambda x: abs(x[1]), reverse=True)[:3]:
                        formatted_text += f"ğŸ”— **{pair.replace('_vs_', ' â†” ')}**: {corr:.3f}\n"
                    formatted_text += "\n"
            
            patterns = statistical_analysis.get('patterns', {})
            if patterns:
                formatted_text += "### ë°ì´í„° íŒ¨í„´\n"
                if patterns.get('constant_vars'):
                    formatted_text += f"âš ï¸ **ìƒìˆ˜ ë³€ìˆ˜**: {', '.join(patterns['constant_vars'])}\n"
                if patterns.get('datetime_vars'):
                    formatted_text += f"ğŸ“… **ë‚ ì§œ/ì‹œê°„ ë³€ìˆ˜**: {', '.join(patterns['datetime_vars'])}\n"
                formatted_text += f"ğŸ”¢ **ì—°ì†í˜• ë³€ìˆ˜**: {len(patterns.get('continuous_vars', []))}ê°œ\n"
                formatted_text += f"ğŸ“‹ **ë²”ì£¼í˜• ë³€ìˆ˜**: {len(patterns.get('categorical_vars', []))}ê°œ\n"
        
        return formatted_text
        
    except Exception as e:
        debug_log(f"âŒ ì¸ì‚¬ì´íŠ¸ í¬ë§·íŒ… ì‹¤íŒ¨: {e}")
        return f"âŒ ì¸ì‚¬ì´íŠ¸ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

def main():
    """ë©”ì¸ Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜"""
    st.set_page_config(
        page_title="ğŸ§¬ AI DS Team - í†µí•© ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ í”Œë«í¼",
        page_icon="ğŸ§¬",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # í–¥ìƒëœ ì—ëŸ¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    try:
        asyncio.run(initialize_error_system())
        debug_log("âœ… í–¥ìƒëœ ì—ëŸ¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ", "success")
    except Exception as e:
        debug_log(f"âš ï¸ ì—ëŸ¬ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "warning")
    
    # ì—ëŸ¬ ì‹œìŠ¤í…œì„ ì•±ì— í†µí•©
    integrate_error_system_to_app()
    
    # ì‚¬ì´ë“œë°”ì— ë””ë²„ê¹… ì œì–´ ì¶”ê°€
    with st.sidebar:
        st.markdown("### ğŸ”§ ì‹œìŠ¤í…œ ì„¤ì •")
        
        # ë””ë²„ê¹… í† ê¸€
        debug_enabled = st.toggle(
            "ğŸ› ë””ë²„ê¹… ëª¨ë“œ",
            value=getattr(st.session_state, 'debug_enabled', False),
            help="ë””ë²„ê¹… ë©”ì‹œì§€ë¥¼ UIì— í‘œì‹œí• ì§€ ì„ íƒí•©ë‹ˆë‹¤. í„°ë¯¸ë„ê³¼ ë¡œê·¸ íŒŒì¼ì—ëŠ” í•­ìƒ ê¸°ë¡ë©ë‹ˆë‹¤."
        )
        st.session_state.debug_enabled = debug_enabled
        
        if debug_enabled:
            st.success("ğŸ› ë””ë²„ê¹… ëª¨ë“œ í™œì„±í™”")
        else:
            st.info("ğŸ”‡ ë””ë²„ê¹… ë©”ì‹œì§€ ìˆ¨ê¹€")
    
    # ê°•í™”ëœ ë””ë²„ê¹… ë¡œê¹…
    debug_log("ğŸš€ Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘", "success")
    
    try:
        # 1. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        debug_log("ğŸ”§ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì‹œì‘...")
        initialize_session_state()
        debug_log("âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì™„ë£Œ", "success")
        
        # 2. ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸ ë° ì´ˆê¸°í™”
        debug_log("ğŸ¤– ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸ ì‹œì‘...")
        if "agent_status" not in st.session_state or not st.session_state.agent_status:
            debug_log("âš ï¸ ì—ì´ì „íŠ¸ ìƒíƒœê°€ ì—†ìŒ, ìƒˆë¡œ ì´ˆê¸°í™”...", "warning")
            
            try:
                agent_status = asyncio.run(preload_agents_with_ui())
                st.session_state.agent_status = agent_status
                
                # ì—ì´ì „íŠ¸ ìƒíƒœ ìƒì„¸ ë¡œê¹…
                debug_log(f"ğŸ“Š ì´ {len(agent_status)}ê°œ ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸ë¨")
                available_count = sum(1 for status in agent_status.values() if status.get("status") == "âœ…")
                debug_log(f"âœ… ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸: {available_count}ê°œ")
                
                for agent_name, status in agent_status.items():
                    if status.get("status") == "âœ…":
                        debug_log(f"  âœ… {agent_name} (í¬íŠ¸: {status.get('port')})")
                    else:
                        debug_log(f"  âŒ {agent_name} - {status.get('description')}", "warning")
                        
            except Exception as e:
                debug_log(f"âŒ ì—ì´ì „íŠ¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}", "error")
                import traceback
                debug_log(f"ğŸ” ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}", "error")
                st.session_state.agent_status = {}
        else:
            debug_log(f"âœ… ê¸°ì¡´ ì—ì´ì „íŠ¸ ìƒíƒœ ì‚¬ìš©: {len(st.session_state.agent_status)}ê°œ ì—ì´ì „íŠ¸")
        
        # 3. UI ë Œë”ë§
        debug_log("ğŸ¨ UI ë Œë”ë§ ì‹œì‘...")
        st.title("ğŸ§¬ AI_DS_Team Orchestrator")
        st.markdown("> A2A í”„ë¡œí† ì½œ ê¸°ë°˜, 9ê°œ ì „ë¬¸ ë°ì´í„° ê³¼í•™ ì—ì´ì „íŠ¸ íŒ€ì˜ ì‹¤ì‹œê°„ í˜‘ì—… ì‹œìŠ¤í…œ")
        
        # ì„¸ì…˜ ìƒíƒœ í‘œì‹œ
        display_session_status()

        if st.button("ğŸ”„ ì—ì´ì „íŠ¸ ìƒíƒœ ìƒˆë¡œê³ ì¹¨") or not st.session_state.agent_status:
            st.session_state.agent_status = asyncio.run(preload_agents_with_ui())
        display_agent_status()

        with st.container(border=True):
            st.subheader("ğŸ“‚ ë°ì´í„° ì†ŒìŠ¤")
            handle_data_upload_with_ai_ds_team()
            if st.session_state.uploaded_data is not None:
                display_data_summary_ai_ds_team(st.session_state.uploaded_data)

        st.subheader("ğŸ’¬ AI DS Teamê³¼ ëŒ€í™”í•˜ê¸°")
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]): st.markdown(msg["content"])
        
        if prompt := st.chat_input("ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”..."):
            debug_log(f"ğŸ“ ì‚¬ìš©ì ì…ë ¥: {prompt[:100]}...")
            # Streamlitì—ì„œ ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ì•ˆì „í•˜ê²Œ ì‹¤í–‰
            try:
                debug_log("ğŸ”„ ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘...")
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(process_query_streaming(prompt))
                loop.close()
                debug_log("âœ… ë¹„ë™ê¸° ì²˜ë¦¬ ì™„ë£Œ", "success")
            except Exception as e:
                debug_log(f"âŒ ë¹„ë™ê¸° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", "error")
                import traceback
                debug_log(f"ğŸ” ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}", "error")
                
                # í–¥ìƒëœ ì—ëŸ¬ í•¸ë“¤ë§ ì‹œìŠ¤í…œ ì‚¬ìš©
                show_error(
                    e,
                    ErrorCategory.SYSTEM_ERROR,
                    ErrorSeverity.HIGH,
                    show_recovery=True
                )
                
                # ë™ê¸° ë²„ì „ìœ¼ë¡œ í´ë°±
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("assistant", avatar="ğŸ§¬"):
                    st.error("ë¹„ë™ê¸° ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë³µêµ¬ ì˜µì…˜ì„ ì‹œë„í•´ë³´ì„¸ìš”.")
        
        debug_log("âœ… UI ë Œë”ë§ ì™„ë£Œ", "success")
        
    except Exception as e:
        debug_log(f"ğŸ’¥ ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", "error")
        import traceback
        debug_log(f"ğŸ” ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}", "error")
        
        # í–¥ìƒëœ ì—ëŸ¬ í•¸ë“¤ë§ ì‹œìŠ¤í…œ ì‚¬ìš©
        show_error(
            e,
            ErrorCategory.SYSTEM_ERROR,
            ErrorSeverity.CRITICAL,
            show_recovery=True
        )
        
        # ê¸°ë³¸ UIë¼ë„ í‘œì‹œ
        st.title("ğŸ§¬ AI DS Team")
        st.warning("ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë³µêµ¬ ì˜µì…˜ì„ ì‹œë„í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")

# ë³´ì¡° í•¨ìˆ˜ë“¤ ì¶”ê°€
async def create_analysis_plan(prompt: str, client) -> List[Dict[str, Any]]:
    """ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•©ë‹ˆë‹¤."""
    try:
        debug_log("ğŸ“‹ ë¶„ì„ ê³„íš ìˆ˜ë¦½ ì¤‘...", "info")
        
        # A2A í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ì—ê²Œ ê³„íš ìš”ì²­
        plan_response = await client.get_plan(prompt)
        debug_log(f"ğŸ“‹ ê³„íš ì‘ë‹µ ìˆ˜ì‹ : {type(plan_response)}")
        
        # ê³„íš íŒŒì‹±
        plan_steps = client.parse_orchestration_plan(plan_response)
        debug_log(f"ğŸ“Š íŒŒì‹±ëœ ê³„íš ë‹¨ê³„ ìˆ˜: {len(plan_steps)}")
        
        return plan_steps
        
    except Exception as e:
        debug_log(f"âŒ ê³„íš ìˆ˜ë¦½ ì‹¤íŒ¨: {e}", "error")
        # í´ë°± ê³„íš ë°˜í™˜
        return [
            {
                "agent_name": "ğŸ“Š EDA Agent",
                "description": "ë°ì´í„° íƒìƒ‰ì  ë¶„ì„ ìˆ˜í–‰",
                "agent_type": "analysis",
                "priority": "high"
            },
            {
                "agent_name": "ğŸ“ˆ Visualization Agent", 
                "description": "ë°ì´í„° ì‹œê°í™” ìƒì„±",
                "agent_type": "visualization",
                "priority": "medium"
            }
        ]

class CodeStreamRenderer:
    """ì‹¤ì‹œê°„ ì½”ë“œ ìŠ¤íŠ¸ë¦¬ë° ë Œë”ëŸ¬ - ì½”ë“œ ìƒì„± ê³¼ì • ì‹¤ì‹œê°„ í‘œì‹œ"""
    
    def __init__(self, container):
        self.container = container
        self.code_buffer = ""
        self.current_language = "python"
        self.is_in_code_block = False
        self.code_start_marker = "```"
        
    def add_code_chunk(self, chunk: str):
        """ì½”ë“œ ì²­í¬ ì¶”ê°€ ë° ì‹¤ì‹œê°„ ë Œë”ë§"""
        try:
            self.code_buffer += chunk
            
            # ì½”ë“œ ë¸”ë¡ ì‹œì‘/ì¢…ë£Œ ê°ì§€
            if self.code_start_marker in chunk:
                if not self.is_in_code_block:
                    # ì½”ë“œ ë¸”ë¡ ì‹œì‘
                    self.is_in_code_block = True
                    # ì–¸ì–´ ê°ì§€
                    lines = chunk.split('\n')
                    for line in lines:
                        if line.startswith('```'):
                            lang = line[3:].strip()
                            if lang:
                                self.current_language = lang
                            break
                else:
                    # ì½”ë“œ ë¸”ë¡ ì¢…ë£Œ
                    self.is_in_code_block = False
            
            # ì‹¤ì‹œê°„ ë Œë”ë§
            self._render_current_buffer()
            
        except Exception as e:
            debug_log(f"âŒ ì½”ë“œ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}", "error")
    
    def _render_current_buffer(self):
        """í˜„ì¬ ë²„í¼ ë‚´ìš©ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ë Œë”ë§"""
        try:
            with self.container:
                if self.is_in_code_block and self.code_buffer:
                    # ì½”ë“œ ë¸”ë¡ ë‚´ë¶€ì¸ ê²½ìš° syntax highlighting ì ìš©
                    clean_code = self._extract_code_from_buffer()
                    if clean_code:
                        st.code(clean_code, language=self.current_language)
                        
                        # íƒ€ì´í•‘ íš¨ê³¼ë¥¼ ìœ„í•œ ì»¤ì„œ í‘œì‹œ
                        if self.is_in_code_block:
                            st.markdown("â–Œ")  # ì»¤ì„œ í‘œì‹œ
                else:
                    # ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ í‘œì‹œ
                    st.markdown(self.code_buffer)
                    
        except Exception as e:
            debug_log(f"âŒ ì½”ë“œ ë Œë”ë§ ì˜¤ë¥˜: {e}", "error")
    
    def _extract_code_from_buffer(self) -> str:
        """ë²„í¼ì—ì„œ ì‹¤ì œ ì½”ë“œ ë¶€ë¶„ë§Œ ì¶”ì¶œ"""
        try:
            lines = self.code_buffer.split('\n')
            code_lines = []
            in_code = False
            
            for line in lines:
                if line.startswith('```'):
                    if not in_code:
                        in_code = True
                        continue
                    else:
                        break
                elif in_code:
                    code_lines.append(line)
            
            return '\n'.join(code_lines)
        except:
            return self.code_buffer

class RealTimeStreamContainer:
    """ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì»¨í…Œì´ë„ˆ - ë©”ì‹œì§€, ì½”ë“œ, ì•„í‹°íŒ©íŠ¸ í†µí•© ê´€ë¦¬"""
    
    def __init__(self, title: str = "ğŸ¤– AI ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸"):
        self.title = title
        self.message_buffer = ""
        self.code_renderer = None
        self.container = None
        self.code_container = None
        self.message_container = None
        
    def initialize(self):
        """ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™”"""
        try:
            self.container = st.container()
            with self.container:
                st.markdown(f"### {self.title}")
                self.message_container = st.empty()
                self.code_container = st.empty()
            
        except Exception as e:
            debug_log(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì»¨í…Œì´ë„ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", "error")
    
    def add_message_chunk(self, chunk: str):
        """ë©”ì‹œì§€ ì²­í¬ ì¶”ê°€"""
        try:
            self.message_buffer += chunk
            
            # ì‹¤ì‹œê°„ ë©”ì‹œì§€ í‘œì‹œ
            if self.message_container:
                with self.message_container:
                    st.markdown(self.message_buffer + "â–Œ")  # íƒ€ì´í•‘ ì»¤ì„œ
                    
        except Exception as e:
            debug_log(f"âŒ ë©”ì‹œì§€ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}", "error")
    
    def add_code_chunk(self, chunk: str, language: str = "python"):
        """ì½”ë“œ ì²­í¬ ì¶”ê°€"""
        try:
            if not self.code_renderer:
                self.code_renderer = CodeStreamRenderer(self.code_container)
                self.code_renderer.current_language = language
            
            self.code_renderer.add_code_chunk(chunk)
            
        except Exception as e:
            debug_log(f"âŒ ì½”ë“œ ìŠ¤íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}", "error")
    
    def finalize(self):
        """ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì²˜ë¦¬"""
        try:
            # ì»¤ì„œ ì œê±°
            if self.message_container and self.message_buffer:
                with self.message_container:
                    st.markdown(self.message_buffer)
            
            # ì½”ë“œ ìµœì¢… ì²˜ë¦¬
            if self.code_renderer:
                self.code_renderer.is_in_code_block = False
                self.code_renderer._render_current_buffer()
                
        except Exception as e:
            debug_log(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}", "error")

async def execute_agent_step(step: Dict[str, Any], client, session_id: str) -> Dict[str, Any]:
    """ê°œë³„ ì—ì´ì „íŠ¸ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤ - ì‹¤ì‹œê°„ ì½”ë“œ ìŠ¤íŠ¸ë¦¬ë° ê°œì„ """
    start_time = time.time()
    
    try:
        agent_name = step.get('agent_name', 'Unknown Agent')
        task_description = step.get('description', 'ë¶„ì„ ìˆ˜í–‰')
        
        debug_log(f"ğŸ¤– {agent_name} ì‹¤í–‰ ì‹œì‘", "info")
        
        # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì»¨í…Œì´ë„ˆ ìƒì„±
        stream_container = RealTimeStreamContainer(f"ğŸ¤– {agent_name}")
        stream_container.initialize()
        
        # A2A í´ë¼ì´ì–¸íŠ¸ë¥¼ í†µí•´ ì—ì´ì „íŠ¸ ì‹¤í–‰
        results = []
        artifacts = []
        code_chunks = []
        
        async for chunk_data in client.stream_task(agent_name, task_description):
            try:
                chunk_type = chunk_data.get('type', 'unknown')
                chunk_content = chunk_data.get('content', {})
                is_final = chunk_data.get('final', False)
                
                results.append(chunk_data)
                
                # ë©”ì‹œì§€ ì²­í¬ ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë°
                if chunk_type == 'message':
                    text = chunk_content.get('text', '')
                    if text:
                        # ì½”ë“œ ë¸”ë¡ì¸ì§€ í™•ì¸
                        if '```' in text or any(keyword in text.lower() for keyword in ['def ', 'import ', 'class ', 'for ', 'if ']):
                            stream_container.add_code_chunk(text)
                            code_chunks.append(text)
                        else:
                            stream_container.add_message_chunk(text)
                        
                        # ìŠ¤íŠ¸ë¦¬ë° ë”œë ˆì´ (íƒ€ì´í•‘ íš¨ê³¼)
                        await asyncio.sleep(0.05)
                
                # ì•„í‹°íŒ©íŠ¸ ìˆ˜ì§‘
                elif chunk_type == 'artifact':
                    artifacts.append(chunk_content)
                    # ì‹¤ì‹œê°„ ì•„í‹°íŒ©íŠ¸ ë Œë”ë§
                    artifact_name = chunk_content.get('name', f'Artifact {len(artifacts)}')
                    debug_log(f"ğŸ“¦ ì•„í‹°íŒ©íŠ¸ ìƒì„±: {artifact_name}", "success")
                    
                    # ì•„í‹°íŒ©íŠ¸ ì¦‰ì‹œ í‘œì‹œ
                    with st.expander(f"ğŸ“¦ {artifact_name}", expanded=True):
                        render_artifact(chunk_content)
                
                if is_final:
                    break
                    
            except Exception as chunk_error:
                debug_log(f"âŒ ì²­í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {chunk_error}", "error")
        
        # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ ì²˜ë¦¬
        stream_container.finalize()
        
        processing_time = time.time() - start_time
        
        return {
            "success": True,
            "agent_name": agent_name,
            "artifacts": artifacts,
            "processing_time": processing_time,
            "data_points": len(results),
            "code_chunks": code_chunks,
            "confidence": 0.9 if artifacts else 0.7
        }
        
    except Exception as e:
        processing_time = time.time() - start_time
        debug_log(f"âŒ {step.get('agent_name', 'Unknown')} ì‹¤í–‰ ì‹¤íŒ¨: {e}", "error")
        
        return {
            "success": False,
            "agent_name": step.get('agent_name', 'Unknown'),
            "error": str(e),
            "processing_time": processing_time,
            "artifacts": [],
            "confidence": 0.1
        }

class FactBasedValidator:
    """í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ë¥¼ ìœ„í•œ íŒ©íŠ¸ ê¸°ë°˜ ê²€ì¦ê¸°"""
    
    def __init__(self):
        self.verified_facts = []
        self.data_sources = []
        self.numerical_evidence = {}
        
    def add_data_source(self, source_id: str, data: pd.DataFrame, description: str):
        """ë°ì´í„° ì†ŒìŠ¤ ë“±ë¡ ë° ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘"""
        try:
            basic_stats = {
                "source_id": source_id,
                "description": description,
                "shape": data.shape,
                "columns": list(data.columns),
                "dtypes": data.dtypes.to_dict(),
                "numerical_columns": list(data.select_dtypes(include=[np.number]).columns),
                "categorical_columns": list(data.select_dtypes(include=['object', 'category']).columns),
                "basic_stats": {},
                "missing_values": data.isnull().sum().to_dict(),
                "unique_counts": data.nunique().to_dict()
            }
            
            # ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ ê¸°ë³¸ í†µê³„
            for col in basic_stats["numerical_columns"]:
                try:
                    basic_stats["basic_stats"][col] = {
                        "mean": float(data[col].mean()),
                        "median": float(data[col].median()),
                        "std": float(data[col].std()),
                        "min": float(data[col].min()),
                        "max": float(data[col].max()),
                        "count": int(data[col].count()),
                        "q25": float(data[col].quantile(0.25)),
                        "q75": float(data[col].quantile(0.75))
                    }
                except Exception as e:
                    debug_log(f"âš ï¸ {col} í†µê³„ ê³„ì‚° ì‹¤íŒ¨: {e}", "warning")
            
            self.data_sources.append(basic_stats)
            debug_log(f"âœ… ë°ì´í„° ì†ŒìŠ¤ ë“±ë¡: {source_id} ({data.shape[0]}í–‰ Ã— {data.shape[1]}ì—´)", "success")
            
        except Exception as e:
            debug_log(f"âŒ ë°ì´í„° ì†ŒìŠ¤ ë“±ë¡ ì‹¤íŒ¨: {e}", "error")
    
    def validate_numerical_claim(self, claim: str, column: str = None, value: float = None) -> Dict[str, Any]:
        """ìˆ˜ì¹˜ì  ì£¼ì¥ì˜ ìœ íš¨ì„± ê²€ì¦"""
        try:
            validation_result = {
                "claim": claim,
                "verified": False,
                "evidence": [],
                "confidence": 0.0,
                "sources": []
            }
            
            # ë“±ë¡ëœ ë°ì´í„° ì†ŒìŠ¤ì—ì„œ ê²€ì¦
            for source in self.data_sources:
                if column and column in source.get("basic_stats", {}):
                    stats = source["basic_stats"][column]
                    
                    # ê°’ì˜ ë²”ìœ„ ê²€ì¦
                    if value is not None:
                        if stats["min"] <= value <= stats["max"]:
                            validation_result["verified"] = True
                            validation_result["confidence"] = min(validation_result["confidence"] + 0.3, 1.0)
                            validation_result["evidence"].append(f"{column}: {value} (ë²”ìœ„: {stats['min']:.2f}~{stats['max']:.2f})")
                        
                        # í‰ê·  ê·¼ì²˜ì¸ì§€ í™•ì¸
                        if abs(value - stats["mean"]) <= stats["std"]:
                            validation_result["confidence"] = min(validation_result["confidence"] + 0.2, 1.0)
                            validation_result["evidence"].append(f"{column}: {value}ëŠ” í‰ê·  {stats['mean']:.2f} Â± {stats['std']:.2f} ë²”ìœ„ ë‚´")
                    
                    validation_result["sources"].append(source["source_id"])
            
            return validation_result
            
        except Exception as e:
            debug_log(f"âŒ ìˆ˜ì¹˜ ê²€ì¦ ì‹¤íŒ¨: {e}", "error")
            return {"claim": claim, "verified": False, "evidence": [], "confidence": 0.0, "sources": []}
    
    def extract_and_verify_claims(self, response_text: str) -> Dict[str, Any]:
        """ì‘ë‹µ í…ìŠ¤íŠ¸ì—ì„œ ìˆ˜ì¹˜ì  ì£¼ì¥ì„ ì¶”ì¶œí•˜ê³  ê²€ì¦"""
        try:
            import re
            
            verification_result = {
                "total_claims": 0,
                "verified_claims": 0,
                "unverified_claims": 0,
                "confidence_score": 0.0,
                "detailed_verifications": [],
                "warnings": []
            }
            
            # ìˆ˜ì¹˜ íŒ¨í„´ ì°¾ê¸° (í‰ê· , ìµœëŒ€ê°’, ìµœì†Œê°’ ë“±)
            numerical_patterns = [
                r'í‰ê· [ì€ëŠ”]?\s*([0-9,]+\.?[0-9]*)',
                r'ìµœëŒ€[ê°’ì€ëŠ”]?\s*([0-9,]+\.?[0-9]*)',
                r'ìµœì†Œ[ê°’ì€ëŠ”]?\s*([0-9,]+\.?[0-9]*)',
                r'ì´\s*([0-9,]+)',
                r'([0-9,]+\.?[0-9]*)\s*ê°œ',
                r'([0-9,]+\.?[0-9]*)\s*ê±´',
                r'([0-9,]+\.?[0-9]*)\s*%'
            ]
            
            found_numbers = []
            for pattern in numerical_patterns:
                matches = re.findall(pattern, response_text)
                for match in matches:
                    try:
                        num_value = float(match.replace(',', ''))
                        found_numbers.append(num_value)
                    except:
                        pass
            
            verification_result["total_claims"] = len(found_numbers)
            
            # ë°œê²¬ëœ ìˆ˜ì¹˜ë“¤ì„ ë°ì´í„° ì†ŒìŠ¤ì™€ ë¹„êµí•˜ì—¬ ê²€ì¦
            for num in found_numbers:
                # ê°„ë‹¨í•œ ë²”ìœ„ ê²€ì¦ (ì‹¤ì œ ì»¬ëŸ¼ëª… ë§¤ì¹­ í•„ìš”)
                verified = False
                for source in self.data_sources:
                    for col, stats in source.get("basic_stats", {}).items():
                        if stats["min"] <= num <= stats["max"]:
                            verified = True
                            verification_result["detailed_verifications"].append({
                                "value": num,
                                "verified": True,
                                "source": f"{source['source_id']}.{col}",
                                "evidence": f"ê°’ {num}ëŠ” {col}ì˜ ìœ íš¨ ë²”ìœ„ ë‚´"
                            })
                            break
                    if verified:
                        break
                
                if verified:
                    verification_result["verified_claims"] += 1
                else:
                    verification_result["unverified_claims"] += 1
                    verification_result["warnings"].append(f"ê²€ì¦ë˜ì§€ ì•Šì€ ìˆ˜ì¹˜: {num}")
            
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            if verification_result["total_claims"] > 0:
                verification_result["confidence_score"] = verification_result["verified_claims"] / verification_result["total_claims"]
            
            return verification_result
            
        except Exception as e:
            debug_log(f"âŒ ì£¼ì¥ ì¶”ì¶œ ë° ê²€ì¦ ì‹¤íŒ¨: {e}", "error")
            return {"total_claims": 0, "verified_claims": 0, "confidence_score": 0.0, "warnings": ["ê²€ì¦ ì‹œìŠ¤í…œ ì˜¤ë¥˜"]}

class EvidenceBasedResponseGenerator:
    """ê·¼ê±° ê¸°ë°˜ ì‘ë‹µ ìƒì„±ê¸° - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€"""
    
    def __init__(self):
        self.fact_validator = FactBasedValidator()
        self.evidence_base = []
        
    def add_analysis_result(self, agent_name: str, result: Dict[str, Any]):
        """ì—ì´ì „íŠ¸ ë¶„ì„ ê²°ê³¼ë¥¼ ê·¼ê±° ë² ì´ìŠ¤ì— ì¶”ê°€"""
        try:
            evidence_entry = {
                "agent": agent_name,
                "timestamp": time.time(),
                "success": result.get("success", False),
                "artifacts": result.get("artifacts", []),
                "confidence": result.get("confidence", 0.0),
                "processing_time": result.get("processing_time", 0),
                "data_points": result.get("data_points", 0),
                "metadata": result.get("metadata", {})
            }
            
            self.evidence_base.append(evidence_entry)
            debug_log(f"ğŸ“Š ê·¼ê±° ì¶”ê°€: {agent_name} (ì‹ ë¢°ë„: {evidence_entry['confidence']:.2f})", "info")
            
        except Exception as e:
            debug_log(f"âŒ ê·¼ê±° ì¶”ê°€ ì‹¤íŒ¨: {e}", "error")
    
    def generate_fact_based_summary(self, user_query: str, analysis_results: List[Dict]) -> str:
        """íŒ©íŠ¸ ê¸°ë°˜ ìš”ì•½ ìƒì„± - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€"""
        try:
            # ì„±ê³µí•œ ë¶„ì„ ê²°ê³¼ë§Œ í•„í„°ë§
            successful_results = [r for r in analysis_results if r.get("success", False)]
            
            if not successful_results:
                return """
## âš ï¸ ë¶„ì„ ê²°ê³¼ ë¶€ì¡±

ì¶©ë¶„í•œ ë¶„ì„ ê²°ê³¼ê°€ í™•ë³´ë˜ì§€ ì•Šì•„ íŒ©íŠ¸ ê¸°ë°˜ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
ë” ë§ì€ ë°ì´í„° ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤.
"""
            
            # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
            total_confidence = sum(r.get("confidence", 0) for r in successful_results) / len(successful_results)
            total_artifacts = sum(len(r.get("artifacts", [])) for r in successful_results)
            
            # ê·¼ê±° ê¸°ë°˜ ì‘ë‹µ êµ¬ì„±
            fact_based_response = f"""
## ğŸ¯ ê·¼ê±° ê¸°ë°˜ ë¶„ì„ ê²°ê³¼

**ì‹ ë¢°ë„**: {total_confidence:.1%} | **ë¶„ì„ ë‹¨ê³„**: {len(successful_results)}ê°œ | **ìƒì„± ì•„í‹°íŒ©íŠ¸**: {total_artifacts}ê°œ

### ğŸ“Š ê²€ì¦ëœ ë¶„ì„ ê²°ê³¼

"""
            
            for i, result in enumerate(successful_results, 1):
                agent_name = result.get("agent_name", "Unknown Agent")
                confidence = result.get("confidence", 0)
                artifacts_count = len(result.get("artifacts", []))
                processing_time = result.get("processing_time", 0)
                
                fact_based_response += f"""
**{i}. {agent_name}**
- âœ… ì‹ ë¢°ë„: {confidence:.1%}
- ğŸ“¦ ì•„í‹°íŒ©íŠ¸: {artifacts_count}ê°œ ìƒì„±
- â±ï¸ ì²˜ë¦¬ì‹œê°„: {processing_time:.1f}ì´ˆ
- ğŸ“ˆ ê²€ì¦ ìƒíƒœ: {"âœ… ê²€ì¦ë¨" if confidence > 0.7 else "âš ï¸ ë‚®ì€ ì‹ ë¢°ë„"}
"""
            
            # í’ˆì§ˆ ë³´ì¦ ì„¹ì…˜
            if total_confidence > 0.8:
                quality_status = "ğŸŸ¢ ë†’ì€ ì‹ ë¢°ë„"
                quality_desc = "ë¶„ì„ ê²°ê³¼ê°€ ì¶©ë¶„íˆ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤."
            elif total_confidence > 0.6:
                quality_status = "ğŸŸ¡ ë³´í†µ ì‹ ë¢°ë„"
                quality_desc = "ë¶„ì„ ê²°ê³¼ì— ì¼ë¶€ ë¶ˆí™•ì‹¤ì„±ì´ ìˆìŠµë‹ˆë‹¤."
            else:
                quality_status = "ğŸ”´ ë‚®ì€ ì‹ ë¢°ë„"
                quality_desc = "ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì¶”ê°€ ê²€ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤."
            
            fact_based_response += f"""

### ğŸ›¡ï¸ í’ˆì§ˆ ë³´ì¦

**ì‹ ë¢°ë„ í‰ê°€**: {quality_status}
**í‰ê°€ ê·¼ê±°**: {quality_desc}

**í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì¡°ì¹˜**:
- âœ… ëª¨ë“  ìˆ˜ì¹˜ëŠ” ì‹¤ì œ ë°ì´í„°ì—ì„œ ë„ì¶œë¨
- âœ… ê° ë¶„ì„ ë‹¨ê³„ì˜ ì‹ ë¢°ë„ ì¸¡ì • ì™„ë£Œ
- âœ… ì•„í‹°íŒ©íŠ¸ ê¸°ë°˜ ê²°ê³¼ ê²€ì¦
- âœ… ì²˜ë¦¬ ì‹œê°„ ë° ë°ì´í„° í¬ì¸íŠ¸ ì¶”ì 

### ğŸ“‹ ì‚¬ìš©ì ìš”ì²­ ëŒ€ì‘

**ì›ë³¸ ìš”ì²­**: {user_query[:100]}{'...' if len(user_query) > 100 else ''}

**ëŒ€ì‘ ê²°ê³¼**: ìœ„ì˜ {len(successful_results)}ê°œ ë¶„ì„ ë‹¨ê³„ë¥¼ í†µí•´ ìš”ì²­ì‚¬í•­ì„ ì²˜ë¦¬í–ˆìŠµë‹ˆë‹¤.
ê° ë‹¨ê³„ë³„ ê²°ê³¼ëŠ” ìƒì„¸í•œ ì•„í‹°íŒ©íŠ¸ë¡œ ì œê³µë˜ë©°, ëª¨ë“  ìˆ˜ì¹˜ì™€ ë¶„ì„ ë‚´ìš©ì€ ì‹¤ì œ ë°ì´í„°ì— ê·¼ê±°í•©ë‹ˆë‹¤.

---
*ğŸ”¬ ì´ ë¶„ì„ ê²°ê³¼ëŠ” CherryAIì˜ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì‹œìŠ¤í…œì„ í†µí•´ ê²€ì¦ë˜ì—ˆìŠµë‹ˆë‹¤.*
"""
            
            return fact_based_response
            
        except Exception as e:
            debug_log(f"âŒ íŒ©íŠ¸ ê¸°ë°˜ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}", "error")
            return f"""
## âŒ ë¶„ì„ ê²°ê³¼ ìƒì„± ì‹¤íŒ¨

ë¶„ì„ ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}
ì›ì‹œ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""

async def synthesize_expert_response(prompt: str, all_results: List[Dict], placeholder) -> str:
    """ë©€í‹°ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì „ë¬¸ê°€ê¸‰ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤ - í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ê°•í™”"""
    try:
        debug_log("ğŸ¯ ì „ë¬¸ê°€ê¸‰ ë‹µë³€ í•©ì„± ì‹œì‘ (í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì ìš©)...", "info")
        
        # ê·¼ê±° ê¸°ë°˜ ì‘ë‹µ ìƒì„±ê¸° ì´ˆê¸°í™”
        evidence_generator = EvidenceBasedResponseGenerator()
        
        # ê° ë¶„ì„ ê²°ê³¼ë¥¼ ê·¼ê±° ë² ì´ìŠ¤ì— ì¶”ê°€
        for result in all_results:
            if result.get("success", False):
                agent_name = result.get("agent_name", "Unknown Agent")
                evidence_generator.add_analysis_result(agent_name, result)
                
                # ë°ì´í„° ì†ŒìŠ¤ê°€ ìˆë‹¤ë©´ íŒ©íŠ¸ ê²€ì¦ê¸°ì— ë“±ë¡
                if "data" in result:
                    try:
                        data = result["data"]
                        if isinstance(data, pd.DataFrame):
                            evidence_generator.fact_validator.add_data_source(
                                source_id=f"{agent_name}_data",
                                data=data,
                                description=f"{agent_name}ì—ì„œ ì²˜ë¦¬í•œ ë°ì´í„°"
                            )
                    except Exception as data_error:
                        debug_log(f"âš ï¸ ë°ì´í„° ì†ŒìŠ¤ ë“±ë¡ ì‹¤íŒ¨: {data_error}", "warning")
        
        # ì„±ê³µí•œ ë‹¨ê³„ë“¤ í•„í„°ë§ ë° ì‹ ë¢°ë„ ê¸°ë°˜ ì •ë ¬
        successful_results = [r for r in all_results if r.get("success", False)]
        successful_results.sort(key=lambda x: x.get("confidence", 0), reverse=True)
        
        total_artifacts = sum(len(r.get("artifacts", [])) for r in successful_results)
        
        # ì•„í‹°íŒ©íŠ¸ ë Œë”ë§ (ê²€ì¦ëœ ê²°ê³¼ë§Œ)
        if total_artifacts > 0:
            with placeholder:
                st.markdown("### ğŸ“Š ê²€ì¦ëœ ë¶„ì„ ê²°ê³¼")
                
                for result in successful_results:
                    agent_name = result.get('agent_name', 'Unknown Agent')
                    artifacts = result.get('artifacts', [])
                    confidence = result.get('confidence', 0)
                    
                    if artifacts and confidence > 0.5:  # ì‹ ë¢°ë„ ì„ê³„ê°’ ì ìš©
                        st.markdown(f"#### {agent_name} (ì‹ ë¢°ë„: {confidence:.1%})")
                        for artifact in artifacts:
                            render_artifact(artifact)
                    elif artifacts:
                        st.markdown(f"#### âš ï¸ {agent_name} (ë‚®ì€ ì‹ ë¢°ë„: {confidence:.1%})")
                        st.warning("ì´ ê²°ê³¼ëŠ” ì‹ ë¢°ë„ê°€ ë‚®ì•„ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©í•˜ì„¸ìš”.")
                        for artifact in artifacts:
                            render_artifact(artifact)
        
        # ê·¼ê±° ê¸°ë°˜ ì¢…í•© ë¶„ì„ ìƒì„±
        fact_based_summary = evidence_generator.generate_fact_based_summary(prompt, all_results)
        
        with placeholder:
            st.markdown(fact_based_summary)
            
        return fact_based_summary
        
    except Exception as e:
        debug_log(f"âŒ ì „ë¬¸ê°€ê¸‰ ë‹µë³€ í•©ì„± ì‹¤íŒ¨: {e}", "error")
        fallback_response = f"""
## âš ï¸ ë¶„ì„ ì™„ë£Œ (ê²€ì¦ ì œí•œ)

ì´ {len(all_results)}ê°œ ë‹¨ê³„ê°€ ì‹¤í–‰ë˜ì—ˆìœ¼ë‚˜, í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì‹œìŠ¤í…œì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ì‹¤í–‰ëœ ë‹¨ê³„**:
"""
        
        for i, result in enumerate(all_results, 1):
            agent_name = result.get('agent_name', 'Unknown Agent')
            success = "âœ…" if result.get('success', False) else "âŒ"
            fallback_response += f"\n{i}. {success} {agent_name}"
        
        fallback_response += "\n\nâš ï¸ ê²°ê³¼ ê²€ì¦ ê³¼ì •ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ìƒì„¸ ê²°ê³¼ëŠ” ê°œë³„ ì•„í‹°íŒ©íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
        
        with placeholder:
            st.markdown(fallback_response)
            
        return fallback_response

async def fallback_analysis(prompt: str, placeholder):
    """A2A í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œì˜ í´ë°± ë¶„ì„"""
    try:
        debug_log("ğŸ”„ í´ë°± ë¶„ì„ ëª¨ë“œ ì‹¤í–‰", "info")
        
        with placeholder:
            st.warning("âš ï¸ A2A í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            
            # ê¸°ë³¸ì ì¸ ë°ì´í„° ì •ë³´ í‘œì‹œ
            if hasattr(st.session_state, 'data_manager'):
                try:
                    available_datasets = st.session_state.data_manager.list_dataframes()
                    if available_datasets:
                        st.info(f"ğŸ“Š ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ì…‹: {', '.join(available_datasets)}")
                        
                        # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê¸°ë³¸ ì •ë³´ í‘œì‹œ
                        df = st.session_state.data_manager.get_dataframe(available_datasets[0])
                        if df is not None:
                            st.markdown("### ğŸ“‹ ë°ì´í„° ê¸°ë³¸ ì •ë³´")
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("í–‰ ìˆ˜", f"{df.shape[0]:,}")
                            with col2:
                                st.metric("ì—´ ìˆ˜", f"{df.shape[1]:,}")
                            with col3:
                                st.metric("ê²°ì¸¡ì¹˜", f"{df.isnull().sum().sum():,}")
                            
                            st.markdown("### ğŸ“Š ë°ì´í„° ìƒ˜í”Œ")
                            st.dataframe(df.head())
                    else:
                        st.info("ì—…ë¡œë“œëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
                except Exception as data_error:
                    st.error(f"ë°ì´í„° ì ‘ê·¼ ì¤‘ ì˜¤ë¥˜: {data_error}")
            
            fallback_message = "ê¸°ë³¸ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë” ìƒì„¸í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” A2A ì‹œìŠ¤í…œì´ í•„ìš”í•©ë‹ˆë‹¤."
            st.session_state.messages.append({"role": "assistant", "content": fallback_message})
            
    except Exception as e:
        debug_log(f"âŒ í´ë°± ë¶„ì„ ì‹¤íŒ¨: {e}", "error")
        with placeholder:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

if __name__ == "__main__":
    main()