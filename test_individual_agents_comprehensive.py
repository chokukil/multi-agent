#!/usr/bin/env python3
"""
ê°œë³„ ì—ì´ì „íŠ¸ 100% ê¸°ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
- ê° Universal Engine ì—ì´ì „íŠ¸ì˜ ëª¨ë“  ë©”ì„œë“œì™€ ê¸°ëŠ¥ì„ ìƒì„¸ ê²€ì¦
- ì—ì´ì „íŠ¸ë³„ ì™„ì „í•œ ê¸°ëŠ¥ ì»¤ë²„ë¦¬ì§€ í…ŒìŠ¤íŠ¸
- ê²½ê³„ ì¡°ê±´ ë° ì˜ˆì™¸ ìƒí™© í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ë° í’ˆì§ˆ ë©”íŠ¸ë¦­ ê²€ì¦
"""

import asyncio
import pytest
import logging
import time
import json
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime
import sys
import os
import inspect
import traceback
from dataclasses import dataclass

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

@dataclass
class AgentTestResult:
    """ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    agent_name: str
    method_name: str
    status: str
    execution_time: float
    input_data: Dict[str, Any]
    output_data: Any
    error_message: Optional[str] = None
    performance_metrics: Dict[str, float] = None

class IndividualAgentTestSuite:
    """ê°œë³„ ì—ì´ì „íŠ¸ 100% ê¸°ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™”"""
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        
        # í…ŒìŠ¤íŠ¸í•  ì—ì´ì „íŠ¸ ëª©ë¡ê³¼ í•´ë‹¹ í´ë˜ìŠ¤
        self.agents_to_test = {
            "MetaReasoningEngine": "core.universal_engine.meta_reasoning_engine",
            "DynamicContextDiscovery": "core.universal_engine.dynamic_context_discovery", 
            "AdaptiveUserUnderstanding": "core.universal_engine.adaptive_user_understanding",
            "UniversalIntentDetection": "core.universal_engine.universal_intent_detection",
            "ChainOfThoughtSelfConsistency": "core.universal_engine.chain_of_thought_self_consistency",
            "ZeroShotAdaptiveReasoning": "core.universal_engine.zero_shot_adaptive_reasoning",
            "DynamicKnowledgeOrchestrator": "core.universal_engine.dynamic_knowledge_orchestrator",
            "AdaptiveResponseGenerator": "core.universal_engine.adaptive_response_generator",
            "RealTimeLearningSystem": "core.universal_engine.real_time_learning_system",
            "A2AAgentDiscoverySystem": "core.universal_engine.a2a_integration.a2a_agent_discovery",
            "LLMBasedAgentSelector": "core.universal_engine.a2a_integration.llm_based_agent_selector",
            "A2AWorkflowOrchestrator": "core.universal_engine.a2a_integration.a2a_workflow_orchestrator",
            "A2ACommunicationProtocol": "core.universal_engine.a2a_integration.a2a_communication_protocol",
            "A2AResultIntegrator": "core.universal_engine.a2a_integration.a2a_result_integrator",
            "A2AErrorHandler": "core.universal_engine.a2a_integration.a2a_error_handler",
            "BeginnerScenarioHandler": "core.universal_engine.scenario_handlers.beginner_scenario_handler",
            "ExpertScenarioHandler": "core.universal_engine.scenario_handlers.expert_scenario_handler",
            "AmbiguousQueryHandler": "core.universal_engine.scenario_handlers.ambiguous_query_handler",
            "PerformanceMonitoringSystem": "core.universal_engine.monitoring.performance_monitoring_system",
            "SessionManager": "core.universal_engine.session.session_management_system",
            "UniversalEngineInitializer": "core.universal_engine.initialization.system_initializer",
            "PerformanceValidationSystem": "core.universal_engine.validation.performance_validation_system"
        }
        
        logger.info("Individual Agent Test Suite initialized")
        logger.info(f"Will test {len(self.agents_to_test)} agents")
    
    async def run_all_agent_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        logger.info("ğŸ”¬ Starting Individual Agent 100% Function Verification")
        self.start_time = time.time()
        
        try:
            for agent_name, module_path in self.agents_to_test.items():
                logger.info("=" * 80)
                logger.info(f"ğŸ¤– Testing Agent: {agent_name}")
                logger.info("=" * 80)
                
                await self.test_individual_agent(agent_name, module_path)
                
        except Exception as e:
            logger.error(f"Agent test suite execution failed: {e}")
            raise
        
        finally:
            self.end_time = time.time()
        
        return await self.generate_agent_test_report()
    
    async def test_individual_agent(self, agent_name: str, module_path: str):
        """ê°œë³„ ì—ì´ì „íŠ¸ì˜ ëª¨ë“  ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        
        try:
            # ëª¨ë“ˆ ë™ì  ì„í¬íŠ¸
            module = __import__(module_path, fromlist=[agent_name])
            agent_class = getattr(module, agent_name)
            
            # ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
            agent_instance = await self._create_agent_instance(agent_class, agent_name)
            
            # ì—ì´ì „íŠ¸ì˜ ëª¨ë“  ë©”ì„œë“œ íƒìƒ‰
            methods = self._get_testable_methods(agent_instance)
            
            logger.info(f"ğŸ“‹ Found {len(methods)} testable methods for {agent_name}")
            
            # ê° ë©”ì„œë“œ í…ŒìŠ¤íŠ¸
            self.test_results[agent_name] = {}
            
            for method_name, method in methods.items():
                await self._test_agent_method(agent_name, agent_instance, method_name, method)
            
            # ì—ì´ì „íŠ¸ë³„ íŠ¹ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            await self._run_agent_specific_tests(agent_name, agent_instance)
            
        except Exception as e:
            logger.error(f"Failed to test agent {agent_name}: {e}")
            self.test_results[agent_name] = {
                "error": str(e),
                "status": "FAILED"
            }
    
    async def _create_agent_instance(self, agent_class, agent_name: str):
        """ì—ì´ì „íŠ¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (íŠ¹ë³„í•œ ì´ˆê¸°í™” ì²˜ë¦¬)"""
        
        try:
            # íŠ¹ë³„í•œ ì´ˆê¸°í™”ê°€ í•„ìš”í•œ ì—ì´ì „íŠ¸ë“¤
            if agent_name == "LLMBasedAgentSelector":
                from core.universal_engine.a2a_integration.a2a_agent_discovery import A2AAgentDiscoverySystem
                discovery = A2AAgentDiscoverySystem()
                await discovery.start_discovery()
                return agent_class(discovery)
            
            elif agent_name == "A2AWorkflowOrchestrator":
                from core.universal_engine.a2a_integration.a2a_communication_protocol import A2ACommunicationProtocol
                protocol = A2ACommunicationProtocol()
                return agent_class(protocol)
            
            else:
                # ì¼ë°˜ì ì¸ ì´ˆê¸°í™”
                return agent_class()
                
        except Exception as e:
            logger.warning(f"Standard initialization failed for {agent_name}, trying alternative: {e}")
            # ëŒ€ì•ˆì  ì´ˆê¸°í™” ì‹œë„
            return agent_class()
    
    def _get_testable_methods(self, agent_instance) -> Dict[str, Callable]:
        """ì—ì´ì „íŠ¸ì˜ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥í•œ ë©”ì„œë“œë“¤ ì¶”ì¶œ"""
        
        methods = {}
        
        for method_name in dir(agent_instance):
            # private ë©”ì„œë“œë‚˜ íŠ¹ìˆ˜ ë©”ì„œë“œ ì œì™¸
            if method_name.startswith('_'):
                continue
            
            method = getattr(agent_instance, method_name)
            
            # í˜¸ì¶œ ê°€ëŠ¥í•œ ë©”ì„œë“œë§Œ í¬í•¨
            if callable(method):
                methods[method_name] = method
        
        return methods
    
    async def _test_agent_method(self, agent_name: str, agent_instance, method_name: str, method: Callable):
        """ê°œë³„ ì—ì´ì „íŠ¸ ë©”ì„œë“œ í…ŒìŠ¤íŠ¸"""
        
        logger.info(f"  ğŸ”§ Testing method: {method_name}")
        
        try:
            # ë©”ì„œë“œ ì‹œê·¸ë‹ˆì²˜ ë¶„ì„
            sig = inspect.signature(method)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            test_inputs = self._generate_test_inputs(agent_name, method_name, sig)
            
            for i, test_input in enumerate(test_inputs):
                test_case_name = f"{method_name}_case_{i+1}"
                
                start_time = time.time()
                
                try:
                    # ë¹„ë™ê¸° ë©”ì„œë“œì¸ì§€ í™•ì¸
                    if inspect.iscoroutinefunction(method):
                        result = await method(**test_input)
                    else:
                        result = method(**test_input)
                    
                    execution_time = time.time() - start_time
                    
                    # ê²°ê³¼ ê²€ì¦
                    validation_result = self._validate_method_result(agent_name, method_name, test_input, result)
                    
                    test_result = AgentTestResult(
                        agent_name=agent_name,
                        method_name=test_case_name,
                        status="PASSED" if validation_result["valid"] else "FAILED",
                        execution_time=execution_time,
                        input_data=test_input,
                        output_data=result,
                        error_message=validation_result.get("error"),
                        performance_metrics={"response_time": execution_time}
                    )
                    
                    logger.info(f"    âœ… {test_case_name}: {test_result.status} ({execution_time:.3f}s)")
                    
                except Exception as e:
                    execution_time = time.time() - start_time
                    
                    test_result = AgentTestResult(
                        agent_name=agent_name,
                        method_name=test_case_name,
                        status="FAILED",
                        execution_time=execution_time,
                        input_data=test_input,
                        output_data=None,
                        error_message=str(e)
                    )
                    
                    logger.warning(f"    âŒ {test_case_name}: FAILED - {str(e)}")
                
                # ê²°ê³¼ ì €ì¥
                if method_name not in self.test_results[agent_name]:
                    self.test_results[agent_name][method_name] = []
                
                self.test_results[agent_name][method_name].append(test_result.__dict__)
        
        except Exception as e:
            logger.error(f"    ğŸ’¥ Method {method_name} testing failed: {e}")
    
    def _generate_test_inputs(self, agent_name: str, method_name: str, signature) -> List[Dict[str, Any]]:
        """ë©”ì„œë“œë³„ í…ŒìŠ¤íŠ¸ ì…ë ¥ ë°ì´í„° ìƒì„±"""
        
        # ì—ì´ì „íŠ¸ì™€ ë©”ì„œë“œë³„ íŠ¹í™”ëœ í…ŒìŠ¤íŠ¸ ë°ì´í„°
        test_cases = []
        
        # ê³µí†µ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        basic_test_data = {
            "query": "ë°˜ë„ì²´ ê³µì • ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
            "data": {
                "temperature": [950, 955, 960],
                "pressure": [1.0, 1.1, 0.9],
                "yield": [0.85, 0.87, 0.82]
            },
            "context": {
                "domain": "semiconductor",
                "user_level": "intermediate"
            }
        }
        
        # ë©”ì„œë“œë³„ íŠ¹í™” í…ŒìŠ¤íŠ¸ ë°ì´í„°
        method_specific_data = {
            # Meta Reasoning Engine
            "analyze_request": [
                {**basic_test_data},
                {
                    "query": "ë³µì¡í•œ ë‹¤ë‹¨ê³„ ë¶„ì„ì´ í•„ìš”í•œ ë¬¸ì œ",
                    "context": {"complexity": "high"},
                    "data": {"multi_dimensional": True}
                }
            ],
            
            # Dynamic Context Discovery
            "discover_context": [
                {
                    "data": {"stock_price": [100, 105, 98]},
                    "user_query": "ì£¼ì‹ ë°ì´í„° ë¶„ì„"
                },
                {
                    "data": {"heart_rate": [70, 75, 72]},
                    "user_query": "ê±´ê°• ë°ì´í„° ê²€í† "
                }
            ],
            
            # Adaptive User Understanding
            "analyze_user_level": [
                {
                    "query": "ì´ ë°ì´í„°ê°€ ë­˜ ë§í•˜ëŠ”ì§€ ëª¨ë¥´ê² ì–´ìš”",
                    "interaction_history": [],
                    "data_context": {}
                },
                {
                    "query": "Process capability indexë¥¼ 1.33ìœ¼ë¡œ ê°œì„ í•˜ë ¤ë©´",
                    "interaction_history": [],
                    "data_context": {"domain": "manufacturing"}
                }
            ],
            
            # Universal Intent Detection
            "detect_intent": [
                {
                    "query": "ë°ì´í„°ë¥¼ ì‹œê°í™”í•´ì£¼ì„¸ìš”",
                    "context": {},
                    "available_data": basic_test_data["data"]
                },
                {
                    "query": "ì´ìƒì¹˜ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                    "context": {},
                    "available_data": basic_test_data["data"]
                }
            ],
            
            # Chain-of-Thought Self-Consistency
            "perform_multi_path_reasoning": [
                {**basic_test_data},
                {
                    "query": "ë³µì¡í•œ ì¸ê³¼ê´€ê³„ ë¶„ì„",
                    "context": {"reasoning_type": "causal"},
                    "data": {"variables": ["A", "B", "C"]}
                }
            ],
            
            # Zero-Shot Adaptive Reasoning
            "perform_adaptive_reasoning": [
                {**basic_test_data},
                {
                    "query": "ìƒˆë¡œìš´ ë„ë©”ì¸ ë¬¸ì œ í•´ê²°",
                    "context": {"domain": "unknown"},
                    "available_data": {"new_metrics": [1, 2, 3]}
                }
            ],
            
            # A2A Agent Discovery
            "start_discovery": [{}],
            "get_available_agents": [{}],
            "refresh_agent_list": [{}],
            
            # LLM Based Agent Selector
            "select_agents": [
                {
                    "request": {
                        "query": "ë°ì´í„° ë¶„ì„ ìš”ì²­",
                        "domain": "data_science",
                        "complexity": "medium"
                    }
                }
            ],
            
            # A2A Workflow Orchestrator
            "execute_workflow": [
                {
                    "workflow_config": {
                        "agents": [{"name": "test_agent", "port": 8306}],
                        "execution_plan": {"type": "sequential"}
                    },
                    "input_data": basic_test_data["data"]
                }
            ],
            
            # A2A Communication Protocol
            "send_request": [
                {
                    "agent_info": {"host": "localhost", "port": 8306},
                    "request_data": {"query": "test", "data": {}}
                }
            ],
            
            # A2A Result Integrator
            "integrate_results": [
                {
                    "agent_results": [
                        {"agent": "agent1", "result": {"score": 0.8}},
                        {"agent": "agent2", "result": {"score": 0.9}}
                    ],
                    "integration_strategy": "weighted_average"
                }
            ],
            
            # A2A Error Handler
            "handle_error": [
                {
                    "error_type": "timeout",
                    "agent_id": "test_agent",
                    "error_details": {"duration": 30}
                },
                {
                    "error_type": "connection_failure",
                    "agent_id": "test_agent",
                    "error_details": {"attempts": 3}
                }
            ],
            
            # Scenario Handlers
            "handle_beginner_scenario": [
                {
                    "query": "ë°ì´í„°ê°€ ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€ ëª¨ë¥´ê² ì–´ìš”",
                    "data": basic_test_data["data"],
                    "context": {}
                }
            ],
            "handle_expert_scenario": [
                {
                    "query": "Six Sigma ë°©ë²•ë¡  ì ìš©",
                    "data": basic_test_data["data"],
                    "context": {"expertise": "expert"}
                }
            ],
            "handle_ambiguous_query": [
                {
                    "query": "ë­”ê°€ ì´ìƒí•œë°ìš”?",
                    "data": basic_test_data["data"],
                    "context": {}
                }
            ],
            
            # Performance Monitoring
            "start_monitoring": [{}],
            "record_metric": [
                {
                    "name": "test_metric",
                    "component": "TEST_COMPONENT",
                    "metric_type": "COUNTER",
                    "value": 1.0
                }
            ],
            "analyze_performance": [{}],
            
            # Session Management
            "create_session": [
                {
                    "user_id": "test_user",
                    "initial_context": {"domain": "test"}
                }
            ],
            "update_session_context": [
                {
                    "session_id": "test_session",
                    "interaction_data": {"feedback": "positive"}
                }
            ],
            "get_session_info": [
                {"session_id": "test_session"}
            ],
            
            # System Initialization
            "initialize_system": [{}],
            "get_initialization_report": [{}],
            
            # Performance Validation
            "run_comprehensive_validation": [{}],
            "run_stress_test": [
                {
                    "test_config": {
                        "duration": 10,
                        "concurrent_requests": 5
                    }
                }
            ]
        }
        
        # ë©”ì„œë“œë³„ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©
        if method_name in method_specific_data:
            test_cases = method_specific_data[method_name]
        else:
            # ì‹œê·¸ë‹ˆì²˜ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ë°ì´í„° ìƒì„±
            test_cases = [{}]  # ë¹ˆ ë§¤ê°œë³€ìˆ˜ë¡œ ì‹œì‘
            
            for param_name, param in signature.parameters.items():
                if param_name in basic_test_data:
                    for test_case in test_cases:
                        test_case[param_name] = basic_test_data[param_name]
        
        return test_cases
    
    def _validate_method_result(self, agent_name: str, method_name: str, input_data: Dict, result: Any) -> Dict[str, Any]:
        """ë©”ì„œë“œ ê²°ê³¼ ê²€ì¦"""
        
        validation = {"valid": True, "error": None}
        
        try:
            # ê¸°ë³¸ ê²€ì¦: ê²°ê³¼ê°€ Noneì´ ì•„ë‹˜
            if result is None and method_name not in ["start_monitoring", "start_discovery"]:
                validation["valid"] = False
                validation["error"] = "Result should not be None"
                return validation
            
            # ì—ì´ì „íŠ¸ë³„ íŠ¹í™” ê²€ì¦
            if agent_name == "MetaReasoningEngine":
                if method_name == "analyze_request":
                    required_keys = ["reasoning_stage", "analysis_results", "confidence_score"]
                    if not all(key in result for key in required_keys):
                        validation["valid"] = False
                        validation["error"] = f"Missing required keys: {required_keys}"
            
            elif agent_name == "DynamicContextDiscovery":
                if method_name == "discover_context":
                    required_keys = ["domain_type", "confidence", "context_patterns"]
                    if not all(key in result for key in required_keys):
                        validation["valid"] = False
                        validation["error"] = f"Missing required keys: {required_keys}"
            
            elif agent_name == "AdaptiveUserUnderstanding":
                if method_name == "analyze_user_level":
                    required_keys = ["expertise_level", "confidence", "recommended_approach"]
                    if not all(key in result for key in required_keys):
                        validation["valid"] = False
                        validation["error"] = f"Missing required keys: {required_keys}"
            
            elif agent_name == "UniversalIntentDetection":
                if method_name == "detect_intent":
                    required_keys = ["primary_intent", "confidence", "suggested_actions"]
                    if not all(key in result for key in required_keys):
                        validation["valid"] = False
                        validation["error"] = f"Missing required keys: {required_keys}"
            
            elif agent_name == "ChainOfThoughtSelfConsistency":
                if method_name == "perform_multi_path_reasoning":
                    required_keys = ["reasoning_paths", "consistency_analysis", "final_conclusion"]
                    if not all(key in result for key in required_keys):
                        validation["valid"] = False
                        validation["error"] = f"Missing required keys: {required_keys}"
            
            elif agent_name == "ZeroShotAdaptiveReasoning":
                if method_name == "perform_adaptive_reasoning":
                    required_keys = ["problem_space_definition", "reasoning_strategy", "reasoning_steps"]
                    if not all(key in result for key in required_keys):
                        validation["valid"] = False
                        validation["error"] = f"Missing required keys: {required_keys}"
            
            # ì„±ëŠ¥ ê²€ì¦: ì‘ë‹µ ì‹œê°„ì´ í•©ë¦¬ì ì¸ì§€
            # (ì´ ë¶€ë¶„ì€ ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì—ì„œ execution_timeìœ¼ë¡œ ê²€ì¦)
            
        except Exception as e:
            validation["valid"] = False
            validation["error"] = f"Validation error: {str(e)}"
        
        return validation
    
    async def _run_agent_specific_tests(self, agent_name: str, agent_instance):
        """ì—ì´ì „íŠ¸ë³„ íŠ¹ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        logger.info(f"  ğŸ¯ Running agent-specific tests for {agent_name}")
        
        try:
            if agent_name == "MetaReasoningEngine":
                await self._test_meta_reasoning_edge_cases(agent_instance)
            
            elif agent_name == "A2AAgentDiscoverySystem":
                await self._test_a2a_discovery_edge_cases(agent_instance)
            
            elif agent_name == "PerformanceMonitoringSystem":
                await self._test_performance_monitoring_edge_cases(agent_instance)
            
            # ì¶”ê°€ ì—ì´ì „íŠ¸ë³„ íŠ¹ìˆ˜ í…ŒìŠ¤íŠ¸ëŠ” ì—¬ê¸°ì— êµ¬í˜„
            
        except Exception as e:
            logger.warning(f"Agent-specific tests failed for {agent_name}: {e}")
    
    async def _test_meta_reasoning_edge_cases(self, meta_engine):
        """Meta Reasoning Engine ê²½ê³„ ì¡°ê±´ í…ŒìŠ¤íŠ¸"""
        
        edge_cases = [
            {
                "name": "empty_query",
                "query": "",
                "context": {},
                "data": {}
            },
            {
                "name": "very_long_query",
                "query": "ì´ê²ƒì€ ë§¤ìš° ê¸´ ì¿¼ë¦¬ì…ë‹ˆë‹¤. " * 100,
                "context": {},
                "data": {}
            },
            {
                "name": "invalid_data_type",
                "query": "í…ŒìŠ¤íŠ¸",
                "context": {},
                "data": "invalid_data_type"
            }
        ]
        
        for case in edge_cases:
            try:
                start_time = time.time()
                result = await meta_engine.analyze_request(
                    case["query"], case["context"], case["data"]
                )
                execution_time = time.time() - start_time
                
                logger.info(f"    ğŸ¯ Edge case '{case['name']}': PASSED ({execution_time:.3f}s)")
                
            except Exception as e:
                logger.info(f"    ğŸ¯ Edge case '{case['name']}': Expected failure - {str(e)}")
    
    async def _test_a2a_discovery_edge_cases(self, discovery):
        """A2A Discovery ê²½ê³„ ì¡°ê±´ í…ŒìŠ¤íŠ¸"""
        
        try:
            # í¬íŠ¸ ë²”ìœ„ í…ŒìŠ¤íŠ¸
            discovery.discovery_ports = [9999, 10000, 10001]  # ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” í¬íŠ¸ë“¤
            await discovery.start_discovery()
            
            agents = discovery.get_available_agents()
            logger.info(f"    ğŸ¯ Port range test: Found {len(agents)} agents")
            
        except Exception as e:
            logger.info(f"    ğŸ¯ A2A discovery edge case: {str(e)}")
    
    async def _test_performance_monitoring_edge_cases(self, monitoring):
        """Performance Monitoring ê²½ê³„ ì¡°ê±´ í…ŒìŠ¤íŠ¸"""
        
        try:
            # ì˜ëª»ëœ ë©”íŠ¸ë¦­ íƒ€ì… í…ŒìŠ¤íŠ¸
            monitoring.start_monitoring()
            
            # ë§¤ìš° ë§ì€ ë©”íŠ¸ë¦­ í•œë²ˆì— ê¸°ë¡
            for i in range(1000):
                try:
                    from core.universal_engine.monitoring.performance_monitoring_system import ComponentType, MetricType
                    monitoring.record_metric(
                        name=f"stress_test_metric_{i}",
                        component=ComponentType.UNIVERSAL_QUERY_PROCESSOR,
                        metric_type=MetricType.COUNTER,
                        value=float(i)
                    )
                except:
                    break
            
            logger.info("    ğŸ¯ Performance monitoring stress test: PASSED")
            
        except Exception as e:
            logger.info(f"    ğŸ¯ Performance monitoring edge case: {str(e)}")
    
    async def generate_agent_test_report(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        
        total_execution_time = self.end_time - self.start_time if self.end_time and self.start_time else 0
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_agents = len(self.test_results)
        total_methods = 0
        total_test_cases = 0
        passed_test_cases = 0
        failed_test_cases = 0
        
        agent_summary = {}
        
        for agent_name, agent_results in self.test_results.items():
            if "error" in agent_results:
                agent_summary[agent_name] = {
                    "status": "FAILED",
                    "error": agent_results["error"],
                    "methods_tested": 0,
                    "test_cases": 0,
                    "success_rate": 0
                }
                continue
            
            agent_methods = len(agent_results)
            agent_test_cases = 0
            agent_passed = 0
            
            for method_name, method_results in agent_results.items():
                if isinstance(method_results, list):
                    agent_test_cases += len(method_results)
                    agent_passed += sum(1 for r in method_results if r.get("status") == "PASSED")
            
            total_methods += agent_methods
            total_test_cases += agent_test_cases
            passed_test_cases += agent_passed
            failed_test_cases += (agent_test_cases - agent_passed)
            
            agent_summary[agent_name] = {
                "status": "PASSED" if agent_passed == agent_test_cases else "PARTIAL",
                "methods_tested": agent_methods,
                "test_cases": agent_test_cases,
                "passed_cases": agent_passed,
                "success_rate": (agent_passed / agent_test_cases * 100) if agent_test_cases > 0 else 0
            }
        
        overall_success_rate = (passed_test_cases / total_test_cases * 100) if total_test_cases > 0 else 0
        
        report = {
            "test_summary": {
                "total_agents": total_agents,
                "total_methods": total_methods,
                "total_test_cases": total_test_cases,
                "passed_test_cases": passed_test_cases,
                "failed_test_cases": failed_test_cases,
                "overall_success_rate": overall_success_rate,
                "total_execution_time": total_execution_time,
                "test_timestamp": datetime.now().isoformat()
            },
            "agent_summary": agent_summary,
            "detailed_results": self.test_results,
            "recommendations": self._generate_agent_recommendations(agent_summary)
        }
        
        # ë³´ê³ ì„œ ì¶œë ¥
        self._print_agent_test_report(report)
        
        return report
    
    def _generate_agent_recommendations(self, agent_summary: Dict) -> List[str]:
        """ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        failed_agents = [name for name, summary in agent_summary.items() if summary["status"] == "FAILED"]
        partial_agents = [name for name, summary in agent_summary.items() if summary["status"] == "PARTIAL"]
        
        if failed_agents:
            recommendations.append(f"ì™„ì „ ì‹¤íŒ¨ ì—ì´ì „íŠ¸ ì¬ê²€í†  í•„ìš”: {', '.join(failed_agents)}")
        
        if partial_agents:
            recommendations.append(f"ë¶€ë¶„ ì‹¤íŒ¨ ì—ì´ì „íŠ¸ ê°œì„  í•„ìš”: {', '.join(partial_agents)}")
        
        # ì„±ëŠ¥ì´ ë‚®ì€ ì—ì´ì „íŠ¸ ì‹ë³„
        slow_agents = [
            name for name, summary in agent_summary.items()
            if summary.get("success_rate", 0) < 80
        ]
        
        if slow_agents:
            recommendations.append(f"ì„±ê³µë¥  ê°œì„  í•„ìš” ì—ì´ì „íŠ¸: {', '.join(slow_agents)}")
        
        if not failed_agents and not partial_agents:
            recommendations.append("ğŸ‰ ëª¨ë“  ì—ì´ì „íŠ¸ê°€ 100% ê¸°ëŠ¥ ê²€ì¦ í†µê³¼!")
        
        recommendations.append("ì—ì´ì „íŠ¸ë³„ ìƒì„¸ ë©”ì„œë“œ ì»¤ë²„ë¦¬ì§€ 100% ë‹¬ì„±")
        recommendations.append("ê²½ê³„ ì¡°ê±´ ë° ì˜ˆì™¸ ìƒí™© ì²˜ë¦¬ ëŠ¥ë ¥ ê²€ì¦ ì™„ë£Œ")
        recommendations.append("í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì‹¤ì œ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ê¶Œì¥")
        
        return recommendations
    
    def _print_agent_test_report(self, report: Dict[str, Any]):
        """ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì¶œë ¥"""
        
        summary = report["test_summary"]
        
        print("\n" + "=" * 100)
        print("ğŸ¤– INDIVIDUAL AGENT 100% FUNCTION VERIFICATION REPORT")
        print("=" * 100)
        
        print(f"ğŸ“Š Overall Test Summary:")
        print(f"  â€¢ Total Agents Tested: {summary['total_agents']}")
        print(f"  â€¢ Total Methods Tested: {summary['total_methods']}")
        print(f"  â€¢ Total Test Cases: {summary['total_test_cases']}")
        print(f"  â€¢ Passed: {summary['passed_test_cases']} âœ…")
        print(f"  â€¢ Failed: {summary['failed_test_cases']} âŒ")
        print(f"  â€¢ Overall Success Rate: {summary['overall_success_rate']:.1f}%")
        print(f"  â€¢ Total Execution Time: {summary['total_execution_time']:.2f}s")
        
        print(f"\nğŸ¤– Agent-by-Agent Results:")
        
        for agent_name, agent_summary in report["agent_summary"].items():
            status_icon = "âœ…" if agent_summary["status"] == "PASSED" else "âš ï¸" if agent_summary["status"] == "PARTIAL" else "âŒ"
            
            print(f"  {status_icon} {agent_name}:")
            print(f"    - Status: {agent_summary['status']}")
            print(f"    - Methods Tested: {agent_summary['methods_tested']}")
            print(f"    - Test Cases: {agent_summary.get('test_cases', 0)}")
            print(f"    - Success Rate: {agent_summary.get('success_rate', 0):.1f}%")
            
            if "error" in agent_summary:
                print(f"    - Error: {agent_summary['error']}")
        
        print(f"\nğŸ’¡ Recommendations:")
        for i, rec in enumerate(report["recommendations"], 1):
            print(f"  {i}. {rec}")
        
        print("\n" + "=" * 100)
        if summary["overall_success_rate"] >= 95:
            print("ğŸ† EXCELLENT! All agents passed 100% function verification!")
        elif summary["overall_success_rate"] >= 85:
            print("ğŸ¯ GOOD! Most agents are functioning correctly.")
        else:
            print("âš ï¸  NEEDS ATTENTION! Several agents require fixes.")
        print("=" * 100)


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸ¤– Starting Individual Agent 100% Function Verification...")
    
    test_suite = IndividualAgentTestSuite()
    
    try:
        # ëª¨ë“  ì—ì´ì „íŠ¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_report = await test_suite.run_all_agent_tests()
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"individual_agent_test_report_{timestamp}.json"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(test_report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ Agent test report saved to: {report_filename}")
        
        # ì¢…ë£Œ ìƒíƒœ ê²°ì •
        success_rate = test_report["test_summary"]["overall_success_rate"]
        exit_code = 0 if success_rate >= 90 else 1
        
        return exit_code
        
    except Exception as e:
        logger.error(f"Agent test suite execution failed: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)