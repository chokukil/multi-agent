#!/usr/bin/env python3
"""
Universal Engine ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸
- ëª¨ë“  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì˜ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
- A2A í†µí•© í…ŒìŠ¤íŠ¸
- ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
- ì„±ëŠ¥ ë° ê²€ì¦ í…ŒìŠ¤íŠ¸
- End-to-End í†µí•© í…ŒìŠ¤íŠ¸
"""

import asyncio
import pytest
import logging
import time
import json
from typing import Dict, List, Any, Optional
from datetime import datetime
import sys
import os

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

logger = logging.getLogger(__name__)

class UniversalEngineTestSuite:
    """Universal Engine ì¢…í•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self):
        """í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì´ˆê¸°í™”"""
        self.test_results = {
            "core_components": {},
            "a2a_integration": {},
            "scenario_handlers": {},
            "performance_tests": {},
            "e2e_tests": {},
            "cherry_ai_integration": {}
        }
        
        self.start_time = None
        self.end_time = None
        
        logger.info("Universal Engine Test Suite initialized")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        logger.info("ğŸš€ Starting Universal Engine Comprehensive Test Suite")
        self.start_time = time.time()
        
        try:
            # 1. í•µì‹¬ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸
            logger.info("=" * 60)
            logger.info("1ï¸âƒ£ Testing Core Components")
            logger.info("=" * 60)
            await self.test_core_components()
            
            # 2. A2A í†µí•© í…ŒìŠ¤íŠ¸
            logger.info("\n" + "=" * 60)
            logger.info("2ï¸âƒ£ Testing A2A Integration")
            logger.info("=" * 60)
            await self.test_a2a_integration()
            
            # 3. ì‹œë‚˜ë¦¬ì˜¤ í•¸ë“¤ëŸ¬ í…ŒìŠ¤íŠ¸
            logger.info("\n" + "=" * 60)
            logger.info("3ï¸âƒ£ Testing Scenario Handlers")
            logger.info("=" * 60)
            await self.test_scenario_handlers()
            
            # 4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ í…ŒìŠ¤íŠ¸
            logger.info("\n" + "=" * 60)
            logger.info("4ï¸âƒ£ Testing Performance & Monitoring")
            logger.info("=" * 60)
            await self.test_performance_systems()
            
            # 5. CherryAI í†µí•© í…ŒìŠ¤íŠ¸
            logger.info("\n" + "=" * 60)
            logger.info("5ï¸âƒ£ Testing CherryAI Integration")
            logger.info("=" * 60)
            await self.test_cherry_ai_integration()
            
            # 6. End-to-End í†µí•© í…ŒìŠ¤íŠ¸
            logger.info("\n" + "=" * 60)
            logger.info("6ï¸âƒ£ End-to-End Integration Test")
            logger.info("=" * 60)
            await self.test_end_to_end_integration()
            
        except Exception as e:
            logger.error(f"Test suite execution failed: {e}")
            raise
        
        finally:
            self.end_time = time.time()
        
        return await self.generate_test_report()
    
    async def test_core_components(self):
        """í•µì‹¬ ì»´í¬ë„ŒíŠ¸ í…ŒìŠ¤íŠ¸"""
        
        # Meta Reasoning Engine í…ŒìŠ¤íŠ¸
        await self.test_meta_reasoning_engine()
        
        # Dynamic Context Discovery í…ŒìŠ¤íŠ¸
        await self.test_dynamic_context_discovery()
        
        # Adaptive User Understanding í…ŒìŠ¤íŠ¸
        await self.test_adaptive_user_understanding()
        
        # Universal Intent Detection í…ŒìŠ¤íŠ¸
        await self.test_universal_intent_detection()
        
        # Chain-of-Thought Self-Consistency í…ŒìŠ¤íŠ¸
        await self.test_chain_of_thought_self_consistency()
        
        # Zero-Shot Adaptive Reasoning í…ŒìŠ¤íŠ¸
        await self.test_zero_shot_adaptive_reasoning()
    
    async def test_meta_reasoning_engine(self):
        """Meta Reasoning Engine í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ§  Testing Meta Reasoning Engine...")
        
        try:
            from core.universal_engine.meta_reasoning_engine import MetaReasoningEngine
            
            engine = MetaReasoningEngine()
            
            # ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸
            test_query = "ë°˜ë„ì²´ ê³µì •ì—ì„œ ì´ì˜¨ì£¼ì… ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
            test_context = {"domain": "semiconductor", "user_level": "expert"}
            test_data = {"TW": [1.2, 1.5, 1.8], "process": "ion_implant"}
            
            start_time = time.time()
            result = await engine.analyze_request(test_query, test_context, test_data)
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert result is not None, "Meta reasoning result should not be None"
            assert "reasoning_stage" in result, "Result should contain reasoning stage"
            assert "analysis_results" in result, "Result should contain analysis results"
            assert "confidence_score" in result, "Result should contain confidence score"
            
            self.test_results["core_components"]["meta_reasoning_engine"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "details": {
                    "reasoning_stages": len(result.get("reasoning_stages", [])),
                    "confidence_score": result.get("confidence_score", 0),
                    "analysis_quality": result.get("analysis_quality", "unknown")
                }
            }
            
            logger.info(f"âœ… Meta Reasoning Engine test passed in {execution_time:.2f}s")
            
        except Exception as e:
            self.test_results["core_components"]["meta_reasoning_engine"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Meta Reasoning Engine test failed: {e}")
    
    async def test_dynamic_context_discovery(self):
        """Dynamic Context Discovery í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ” Testing Dynamic Context Discovery...")
        
        try:
            from core.universal_engine.dynamic_context_discovery import DynamicContextDiscovery
            
            discovery = DynamicContextDiscovery()
            
            # ë‹¤ì–‘í•œ ë„ë©”ì¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
            test_cases = [
                {
                    "data": {"temperature": [25.1, 25.5], "pressure": [760, 755]},
                    "expected_domain": "manufacturing"
                },
                {
                    "data": {"stock_price": [150.2, 152.1], "volume": [10000, 12000]},
                    "expected_domain": "finance"
                },
                {
                    "data": {"heart_rate": [72, 75], "blood_pressure": [120, 118]},
                    "expected_domain": "healthcare"
                }
            ]
            
            passed_tests = 0
            total_tests = len(test_cases)
            
            for i, test_case in enumerate(test_cases):
                start_time = time.time()
                result = await discovery.discover_context(
                    data=test_case["data"],
                    user_query="ì´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
                )
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ê²€ì¦
                assert result is not None, f"Context discovery result {i} should not be None"
                assert "domain_type" in result, f"Result {i} should contain domain type"
                assert "confidence" in result, f"Result {i} should contain confidence"
                assert "context_patterns" in result, f"Result {i} should contain context patterns"
                
                passed_tests += 1
                logger.info(f"  Test case {i+1}: Domain={result.get('domain_type')}, Confidence={result.get('confidence'):.2f}")
            
            self.test_results["core_components"]["dynamic_context_discovery"] = {
                "status": "PASSED",
                "passed_tests": passed_tests,
                "total_tests": total_tests,
                "success_rate": passed_tests / total_tests
            }
            
            logger.info(f"âœ… Dynamic Context Discovery test passed ({passed_tests}/{total_tests})")
            
        except Exception as e:
            self.test_results["core_components"]["dynamic_context_discovery"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Dynamic Context Discovery test failed: {e}")
    
    async def test_adaptive_user_understanding(self):
        """Adaptive User Understanding í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ‘¤ Testing Adaptive User Understanding...")
        
        try:
            from core.universal_engine.adaptive_user_understanding import AdaptiveUserUnderstanding
            
            understanding = AdaptiveUserUnderstanding()
            
            # ë‹¤ì–‘í•œ ì‚¬ìš©ì ìˆ˜ì¤€ í…ŒìŠ¤íŠ¸
            test_queries = [
                {
                    "query": "ì´ ë°ì´í„°ê°€ ë­˜ ë§í•˜ëŠ”ì§€ ëª¨ë¥´ê² ì–´ìš”",
                    "expected_level": "beginner"
                },
                {
                    "query": "Process capability indexë¥¼ 1.33ìœ¼ë¡œ ê°œì„ í•˜ë ¤ë©´ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ì•¼ í•˜ë‚˜ìš”?",
                    "expected_level": "expert"
                },
                {
                    "query": "ë°ì´í„° ë¶„ì„ ê²°ê³¼ë¥¼ ì¢€ ë” ìì„¸íˆ ì„¤ëª…í•´ì£¼ì‹œê² ì–´ìš”?",
                    "expected_level": "intermediate"
                }
            ]
            
            passed_tests = 0
            total_tests = len(test_queries)
            
            for i, test_case in enumerate(test_queries):
                start_time = time.time()
                result = await understanding.analyze_user_level(
                    query=test_case["query"],
                    interaction_history=[],
                    data_context={}
                )
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ê²€ì¦
                assert result is not None, f"User understanding result {i} should not be None"
                assert "expertise_level" in result, f"Result {i} should contain expertise level"
                assert "confidence" in result, f"Result {i} should contain confidence"
                assert "recommended_approach" in result, f"Result {i} should contain recommended approach"
                
                passed_tests += 1
                logger.info(f"  Query {i+1}: Level={result.get('expertise_level')}, Confidence={result.get('confidence'):.2f}")
            
            self.test_results["core_components"]["adaptive_user_understanding"] = {
                "status": "PASSED",
                "passed_tests": passed_tests,
                "total_tests": total_tests,
                "success_rate": passed_tests / total_tests
            }
            
            logger.info(f"âœ… Adaptive User Understanding test passed ({passed_tests}/{total_tests})")
            
        except Exception as e:
            self.test_results["core_components"]["adaptive_user_understanding"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Adaptive User Understanding test failed: {e}")
    
    async def test_universal_intent_detection(self):
        """Universal Intent Detection í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ¯ Testing Universal Intent Detection...")
        
        try:
            from core.universal_engine.universal_intent_detection import UniversalIntentDetection
            
            detection = UniversalIntentDetection()
            
            # ë‹¤ì–‘í•œ ì˜ë„ í…ŒìŠ¤íŠ¸
            test_queries = [
                {
                    "query": "ë°ì´í„°ë¥¼ ì‹œê°í™”í•´ì£¼ì„¸ìš”",
                    "expected_intent": "visualization"
                },
                {
                    "query": "ì´ìƒì¹˜ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”",
                    "expected_intent": "anomaly_detection"
                },
                {
                    "query": "ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”",
                    "expected_intent": "prediction"
                },
                {
                    "query": "ë°ì´í„°ì˜ íŠ¸ë Œë“œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”",
                    "expected_intent": "trend_analysis"
                }
            ]
            
            passed_tests = 0
            total_tests = len(test_queries)
            
            for i, test_case in enumerate(test_queries):
                start_time = time.time()
                result = await detection.detect_intent(
                    query=test_case["query"],
                    context={},
                    available_data={}
                )
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ê²€ì¦
                assert result is not None, f"Intent detection result {i} should not be None"
                assert "primary_intent" in result, f"Result {i} should contain primary intent"
                assert "confidence" in result, f"Result {i} should contain confidence"
                assert "suggested_actions" in result, f"Result {i} should contain suggested actions"
                
                passed_tests += 1
                logger.info(f"  Query {i+1}: Intent={result.get('primary_intent')}, Confidence={result.get('confidence'):.2f}")
            
            self.test_results["core_components"]["universal_intent_detection"] = {
                "status": "PASSED", 
                "passed_tests": passed_tests,
                "total_tests": total_tests,
                "success_rate": passed_tests / total_tests
            }
            
            logger.info(f"âœ… Universal Intent Detection test passed ({passed_tests}/{total_tests})")
            
        except Exception as e:
            self.test_results["core_components"]["universal_intent_detection"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Universal Intent Detection test failed: {e}")
    
    async def test_chain_of_thought_self_consistency(self):
        """Chain-of-Thought Self-Consistency í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ”— Testing Chain-of-Thought Self-Consistency...")
        
        try:
            from core.universal_engine.chain_of_thought_self_consistency import ChainOfThoughtSelfConsistency
            
            consistency = ChainOfThoughtSelfConsistency()
            
            # ë³µì¡í•œ ì¶”ë¡  ë¬¸ì œ í…ŒìŠ¤íŠ¸
            test_query = "ë°˜ë„ì²´ ì œì¡° ê³µì •ì—ì„œ ìˆ˜ìœ¨ì´ 85%ì—ì„œ 92%ë¡œ ê°œì„ ë˜ì—ˆì„ ë•Œ, ì´ê²ƒì´ ê³µì • ëŠ¥ë ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ë¶„ì„í•´ì£¼ì„¸ìš”"
            test_context = {
                "domain": "semiconductor",
                "analysis_type": "process_improvement"
            }
            test_data = {
                "yield_before": 0.85,
                "yield_after": 0.92,
                "sample_size": 1000
            }
            
            start_time = time.time()
            result = await consistency.perform_multi_path_reasoning(
                query=test_query,
                context=test_context,
                data=test_data
            )
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert result is not None, "Self-consistency result should not be None"
            assert "reasoning_paths" in result, "Result should contain reasoning paths"
            assert "consistency_analysis" in result, "Result should contain consistency analysis"
            assert "final_conclusion" in result, "Result should contain final conclusion"
            assert "confidence_score" in result, "Result should contain confidence score"
            
            self.test_results["core_components"]["chain_of_thought_self_consistency"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "details": {
                    "reasoning_paths": len(result.get("reasoning_paths", [])),
                    "consistency_score": result.get("consistency_analysis", {}).get("consistency_score", 0),
                    "confidence_score": result.get("confidence_score", 0)
                }
            }
            
            logger.info(f"âœ… Chain-of-Thought Self-Consistency test passed in {execution_time:.2f}s")
            
        except Exception as e:
            self.test_results["core_components"]["chain_of_thought_self_consistency"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Chain-of-Thought Self-Consistency test failed: {e}")
    
    async def test_zero_shot_adaptive_reasoning(self):
        """Zero-Shot Adaptive Reasoning í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸª Testing Zero-Shot Adaptive Reasoning...")
        
        try:
            from core.universal_engine.zero_shot_adaptive_reasoning import ZeroShotAdaptiveReasoning
            
            reasoning = ZeroShotAdaptiveReasoning()
            
            # ìƒˆë¡œìš´ ë„ë©”ì¸ ë¬¸ì œ í…ŒìŠ¤íŠ¸ (í…œí”Œë¦¿ ì—†ìŒ)
            test_query = "ìƒˆë¡œìš´ ì—ë„ˆì§€ ì €ì¥ ì‹œìŠ¤í…œì˜ íš¨ìœ¨ì„±ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•´ì£¼ì„¸ìš”"
            test_context = {
                "domain": "energy_storage",  # ìƒˆë¡œìš´ ë„ë©”ì¸
                "problem_type": "efficiency_evaluation"
            }
            test_data = {
                "energy_input": 1000,
                "energy_output": 850,
                "time_duration": 24
            }
            
            start_time = time.time()
            result = await reasoning.perform_adaptive_reasoning(
                query=test_query,
                context=test_context,
                available_data=test_data
            )
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert result is not None, "Zero-shot reasoning result should not be None"
            assert "problem_space_definition" in result, "Result should contain problem space definition"
            assert "reasoning_strategy" in result, "Result should contain reasoning strategy"
            assert "reasoning_steps" in result, "Result should contain reasoning steps"
            assert "uncertainty_assessment" in result, "Result should contain uncertainty assessment"
            
            self.test_results["core_components"]["zero_shot_adaptive_reasoning"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "details": {
                    "strategy_type": result.get("reasoning_strategy", {}).get("strategy_type"),
                    "reasoning_steps": len(result.get("reasoning_steps", [])),
                    "uncertainty_level": result.get("uncertainty_assessment", {}).get("uncertainty_level")
                }
            }
            
            logger.info(f"âœ… Zero-Shot Adaptive Reasoning test passed in {execution_time:.2f}s")
            
        except Exception as e:
            self.test_results["core_components"]["zero_shot_adaptive_reasoning"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Zero-Shot Adaptive Reasoning test failed: {e}")
    
    async def test_a2a_integration(self):
        """A2A í†µí•© ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        
        # A2A Agent Discovery í…ŒìŠ¤íŠ¸
        await self.test_a2a_agent_discovery()
        
        # LLM Based Agent Selector í…ŒìŠ¤íŠ¸
        await self.test_llm_based_agent_selector()
        
        # A2A Workflow Orchestrator í…ŒìŠ¤íŠ¸
        await self.test_a2a_workflow_orchestrator()
        
        # A2A Error Handler í…ŒìŠ¤íŠ¸
        await self.test_a2a_error_handler()
    
    async def test_a2a_agent_discovery(self):
        """A2A Agent Discovery í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ” Testing A2A Agent Discovery...")
        
        try:
            from core.universal_engine.a2a_integration.a2a_agent_discovery import A2AAgentDiscoverySystem
            
            discovery = A2AAgentDiscoverySystem()
            
            # ì—ì´ì „íŠ¸ ë°œê²¬ í…ŒìŠ¤íŠ¸ (ëª¨ì˜ í™˜ê²½)
            start_time = time.time()
            await discovery.start_discovery()
            
            # ì—ì´ì „íŠ¸ ì •ë³´ ì¡°íšŒ
            agents = discovery.get_available_agents()
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦ (ì‹¤ì œ ì—ì´ì „íŠ¸ê°€ ì—†ì–´ë„ ì‹œìŠ¤í…œì€ ë™ì‘í•´ì•¼ í•¨)
            assert agents is not None, "Agents list should not be None"
            assert isinstance(agents, list), "Agents should be a list"
            
            self.test_results["a2a_integration"]["agent_discovery"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "discovered_agents": len(agents),
                "discovery_method": "port_scanning"
            }
            
            logger.info(f"âœ… A2A Agent Discovery test passed in {execution_time:.2f}s")
            logger.info(f"  Discovered {len(agents)} agents")
            
        except Exception as e:
            self.test_results["a2a_integration"]["agent_discovery"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ A2A Agent Discovery test failed: {e}")
    
    async def test_llm_based_agent_selector(self):
        """LLM Based Agent Selector í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ¤– Testing LLM Based Agent Selector...")
        
        try:
            from core.universal_engine.a2a_integration.a2a_agent_discovery import A2AAgentDiscoverySystem
            from core.universal_engine.a2a_integration.llm_based_agent_selector import LLMBasedAgentSelector
            
            # ëª¨ì˜ ì—ì´ì „íŠ¸ ë°œê²¬ ì‹œìŠ¤í…œ
            discovery = A2AAgentDiscoverySystem()
            await discovery.start_discovery()
            
            selector = LLMBasedAgentSelector(discovery)
            
            # ì—ì´ì „íŠ¸ ì„ íƒ í…ŒìŠ¤íŠ¸
            test_request = {
                "query": "ë°ì´í„°ë¥¼ ì‹œê°í™”í•˜ê³  í†µê³„ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”",
                "domain": "data_analysis",
                "data_type": "numerical"
            }
            
            start_time = time.time()
            result = await selector.select_agents(test_request)
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert result is not None, "Agent selection result should not be None"
            assert "selected_agents" in result, "Result should contain selected agents"
            assert "selection_reasoning" in result, "Result should contain selection reasoning"
            assert "execution_plan" in result, "Result should contain execution plan"
            
            self.test_results["a2a_integration"]["agent_selector"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "selected_agents": len(result.get("selected_agents", [])),
                "selection_criteria": result.get("selection_reasoning", {}).get("criteria", "unknown")
            }
            
            logger.info(f"âœ… LLM Based Agent Selector test passed in {execution_time:.2f}s")
            
        except Exception as e:
            self.test_results["a2a_integration"]["agent_selector"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ LLM Based Agent Selector test failed: {e}")
    
    async def test_a2a_workflow_orchestrator(self):
        """A2A Workflow Orchestrator í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ¼ Testing A2A Workflow Orchestrator...")
        
        try:
            from core.universal_engine.a2a_integration.a2a_workflow_orchestrator import A2AWorkflowOrchestrator
            from core.universal_engine.a2a_integration.a2a_communication_protocol import A2ACommunicationProtocol
            
            protocol = A2ACommunicationProtocol()
            orchestrator = A2AWorkflowOrchestrator(protocol)
            
            # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ í…ŒìŠ¤íŠ¸
            test_workflow = {
                "agents": [
                    {"name": "data_loader", "port": 8306},
                    {"name": "statistical_analyzer", "port": 8307},
                    {"name": "visualizer", "port": 8308}
                ],
                "execution_plan": {
                    "type": "sequential",
                    "steps": ["load_data", "analyze", "visualize"]
                }
            }
            
            test_data = {"sample_data": [1, 2, 3, 4, 5]}
            
            start_time = time.time()
            result = await orchestrator.execute_workflow(test_workflow, test_data)
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦ (ì‹¤ì œ ì—ì´ì „íŠ¸ ì—†ì–´ë„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë¡œì§ì€ ë™ì‘)
            assert result is not None, "Workflow execution result should not be None"
            assert "workflow_id" in result, "Result should contain workflow ID"
            assert "execution_status" in result, "Result should contain execution status"
            assert "results" in result, "Result should contain results"
            
            self.test_results["a2a_integration"]["workflow_orchestrator"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "workflow_type": test_workflow["execution_plan"]["type"],
                "agents_count": len(test_workflow["agents"])
            }
            
            logger.info(f"âœ… A2A Workflow Orchestrator test passed in {execution_time:.2f}s")
            
        except Exception as e:
            self.test_results["a2a_integration"]["workflow_orchestrator"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ A2A Workflow Orchestrator test failed: {e}")
    
    async def test_a2a_error_handler(self):
        """A2A Error Handler í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ›¡ï¸ Testing A2A Error Handler...")
        
        try:
            from core.universal_engine.a2a_integration.a2a_error_handler import A2AErrorHandler
            
            error_handler = A2AErrorHandler()
            
            # ë‹¤ì–‘í•œ ì—ëŸ¬ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
            error_scenarios = [
                {
                    "error_type": "agent_timeout",
                    "agent_id": "test_agent_1",
                    "details": {"timeout_duration": 30}
                },
                {
                    "error_type": "connection_failure",
                    "agent_id": "test_agent_2", 
                    "details": {"connection_attempts": 3}
                },
                {
                    "error_type": "data_processing_error",
                    "agent_id": "test_agent_3",
                    "details": {"error_message": "Invalid data format"}
                }
            ]
            
            passed_tests = 0
            total_tests = len(error_scenarios)
            
            for i, scenario in enumerate(error_scenarios):
                start_time = time.time()
                result = await error_handler.handle_error(
                    error_type=scenario["error_type"],
                    agent_id=scenario["agent_id"],
                    error_details=scenario["details"]
                )
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ê²€ì¦
                assert result is not None, f"Error handling result {i} should not be None"
                assert "recovery_action" in result, f"Result {i} should contain recovery action"
                assert "fallback_strategy" in result, f"Result {i} should contain fallback strategy"
                assert "retry_recommended" in result, f"Result {i} should contain retry recommendation"
                
                passed_tests += 1
                logger.info(f"  Scenario {i+1}: {scenario['error_type']} -> {result.get('recovery_action')}")
            
            self.test_results["a2a_integration"]["error_handler"] = {
                "status": "PASSED",
                "passed_tests": passed_tests,
                "total_tests": total_tests,
                "error_scenarios_tested": [s["error_type"] for s in error_scenarios]
            }
            
            logger.info(f"âœ… A2A Error Handler test passed ({passed_tests}/{total_tests})")
            
        except Exception as e:
            self.test_results["a2a_integration"]["error_handler"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ A2A Error Handler test failed: {e}")
    
    async def test_scenario_handlers(self):
        """ì‹œë‚˜ë¦¬ì˜¤ í•¸ë“¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
        
        # Beginner Scenario Handler í…ŒìŠ¤íŠ¸
        await self.test_beginner_scenario_handler()
        
        # Expert Scenario Handler í…ŒìŠ¤íŠ¸  
        await self.test_expert_scenario_handler()
        
        # Ambiguous Query Handler í…ŒìŠ¤íŠ¸
        await self.test_ambiguous_query_handler()
    
    async def test_beginner_scenario_handler(self):
        """Beginner Scenario Handler í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ‘¶ Testing Beginner Scenario Handler...")
        
        try:
            from core.universal_engine.scenario_handlers.beginner_scenario_handler import BeginnerScenarioHandler
            
            handler = BeginnerScenarioHandler()
            
            # ì´ˆë³´ì ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
            beginner_queries = [
                "ì´ ë°ì´í„° íŒŒì¼ì´ ë­˜ ë§í•˜ëŠ”ì§€ ì „í˜€ ëª¨ë¥´ê² ì–´ìš”. ë„ì›€ ì£¼ì„¸ìš”.",
                "ë°ì´í„° ë¶„ì„ì´ ì²˜ìŒì¸ë° ì–´ë–»ê²Œ ì‹œì‘í•´ì•¼ í•˜ë‚˜ìš”?",
                "ì´ ìˆ«ìë“¤ì´ ë¬´ìŠ¨ ì˜ë¯¸ì¸ì§€ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            ]
            
            test_data = {
                "sales": [100, 150, 120, 180, 200],
                "month": ["Jan", "Feb", "Mar", "Apr", "May"]
            }
            
            passed_tests = 0
            total_tests = len(beginner_queries)
            
            for i, query in enumerate(beginner_queries):
                start_time = time.time()
                result = await handler.handle_beginner_scenario(
                    query=query,
                    data=test_data,
                    context={}
                )
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ê²€ì¦
                assert result is not None, f"Beginner scenario result {i} should not be None"
                assert "friendly_explanation" in result, f"Result {i} should contain friendly explanation"
                assert "guided_steps" in result, f"Result {i} should contain guided steps"
                assert "progressive_exploration" in result, f"Result {i} should contain progressive exploration"
                
                passed_tests += 1
                logger.info(f"  Query {i+1}: Generated {len(result.get('guided_steps', []))} guided steps")
            
            self.test_results["scenario_handlers"]["beginner_handler"] = {
                "status": "PASSED",
                "passed_tests": passed_tests,
                "total_tests": total_tests,
                "average_guided_steps": 3  # ì˜ˆì‹œ
            }
            
            logger.info(f"âœ… Beginner Scenario Handler test passed ({passed_tests}/{total_tests})")
            
        except Exception as e:
            self.test_results["scenario_handlers"]["beginner_handler"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Beginner Scenario Handler test failed: {e}")
    
    async def test_expert_scenario_handler(self):
        """Expert Scenario Handler í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ“ Testing Expert Scenario Handler...")
        
        try:
            from core.universal_engine.scenario_handlers.expert_scenario_handler import ExpertScenarioHandler
            
            handler = ExpertScenarioHandler()
            
            # ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
            expert_queries = [
                "ê³µì • ëŠ¥ë ¥ ì§€ìˆ˜ê°€ 1.2ì¸ë° íƒ€ê²Ÿì„ 1.33ìœ¼ë¡œ ì˜¬ë¦¬ë ¤ë©´ ì–´ë–¤ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ì•¼ í•˜ë‚˜ìš”?",
                "Six Sigma ë°©ë²•ë¡ ì„ ì ìš©í•œ ê³µì • ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”.",
                "í†µê³„ì  ê³µì • ê´€ë¦¬(SPC) ì°¨íŠ¸ë¥¼ í•´ì„í•˜ê³  ê°œì„ ì ì„ ì°¾ì•„ì£¼ì„¸ìš”."
            ]
            
            test_data = {
                "process_capability": 1.2,
                "defect_rate": 0.05,
                "sample_size": 1000,
                "control_limits": {"UCL": 10.5, "LCL": 9.5}
            }
            
            passed_tests = 0
            total_tests = len(expert_queries)
            
            for i, query in enumerate(expert_queries):
                start_time = time.time()
                result = await handler.handle_expert_scenario(
                    query=query,
                    data=test_data,
                    context={"domain": "semiconductor", "expertise_level": "expert"}
                )
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ê²€ì¦
                assert result is not None, f"Expert scenario result {i} should not be None"
                assert "technical_analysis" in result, f"Result {i} should contain technical analysis"
                assert "detailed_recommendations" in result, f"Result {i} should contain detailed recommendations"
                assert "advanced_metrics" in result, f"Result {i} should contain advanced metrics"
                
                passed_tests += 1
                logger.info(f"  Query {i+1}: Generated {len(result.get('detailed_recommendations', []))} recommendations")
            
            self.test_results["scenario_handlers"]["expert_handler"] = {
                "status": "PASSED",
                "passed_tests": passed_tests,
                "total_tests": total_tests,
                "technical_depth": "advanced"
            }
            
            logger.info(f"âœ… Expert Scenario Handler test passed ({passed_tests}/{total_tests})")
            
        except Exception as e:
            self.test_results["scenario_handlers"]["expert_handler"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Expert Scenario Handler test failed: {e}")
    
    async def test_ambiguous_query_handler(self):
        """Ambiguous Query Handler í…ŒìŠ¤íŠ¸"""
        
        logger.info("â“ Testing Ambiguous Query Handler...")
        
        try:
            from core.universal_engine.scenario_handlers.ambiguous_query_handler import AmbiguousQueryHandler
            
            handler = AmbiguousQueryHandler()
            
            # ëª¨í˜¸í•œ ì§ˆë¬¸ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
            ambiguous_queries = [
                "ë­”ê°€ ì´ìƒí•œë°ìš”? í‰ì†Œë‘ ë‹¤ë¥¸ ê²ƒ ê°™ì•„ìš”.",
                "ë°ì´í„°ì— ë¬¸ì œê°€ ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
                "ê²°ê³¼ê°€ ì˜ˆìƒê³¼ ë‹¤ë¥´ë„¤ìš”. ì™œ ê·¸ëŸ´ê¹Œìš”?"
            ]
            
            test_data = {
                "values": [95, 98, 102, 89, 156, 101, 97],  # í•˜ë‚˜ì˜ ì´ìƒì¹˜ í¬í•¨
                "timestamps": ["2024-01-01", "2024-01-02", "2024-01-03", "2024-01-04", "2024-01-05", "2024-01-06", "2024-01-07"]
            }
            
            passed_tests = 0
            total_tests = len(ambiguous_queries)
            
            for i, query in enumerate(ambiguous_queries):
                start_time = time.time()
                result = await handler.handle_ambiguous_query(
                    query=query,
                    data=test_data,
                    context={}
                )
                execution_time = time.time() - start_time
                
                # ê²°ê³¼ ê²€ì¦
                assert result is not None, f"Ambiguous query result {i} should not be None"
                assert "clarification_questions" in result, f"Result {i} should contain clarification questions"
                assert "potential_issues" in result, f"Result {i} should contain potential issues"
                assert "exploratory_analysis" in result, f"Result {i} should contain exploratory analysis"
                
                passed_tests += 1
                logger.info(f"  Query {i+1}: Generated {len(result.get('clarification_questions', []))} clarification questions")
            
            self.test_results["scenario_handlers"]["ambiguous_handler"] = {
                "status": "PASSED",
                "passed_tests": passed_tests,
                "total_tests": total_tests,
                "clarification_effectiveness": "high"
            }
            
            logger.info(f"âœ… Ambiguous Query Handler test passed ({passed_tests}/{total_tests})")
            
        except Exception as e:
            self.test_results["scenario_handlers"]["ambiguous_handler"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Ambiguous Query Handler test failed: {e}")
    
    async def test_performance_systems(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ê²€ì¦ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        
        # Performance Monitoring í…ŒìŠ¤íŠ¸
        await self.test_performance_monitoring()
        
        # Session Management í…ŒìŠ¤íŠ¸
        await self.test_session_management()
        
        # System Initialization í…ŒìŠ¤íŠ¸
        await self.test_system_initialization()
        
        # Performance Validation í…ŒìŠ¤íŠ¸
        await self.test_performance_validation()
    
    async def test_performance_monitoring(self):
        """Performance Monitoring í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ“Š Testing Performance Monitoring System...")
        
        try:
            from core.universal_engine.monitoring.performance_monitoring_system import PerformanceMonitoringSystem
            
            monitoring = PerformanceMonitoringSystem()
            
            # ëª¨ë‹ˆí„°ë§ ì‹œì‘
            monitoring.start_monitoring()
            
            # ë©”íŠ¸ë¦­ ê¸°ë¡ í…ŒìŠ¤íŠ¸
            from core.universal_engine.monitoring.performance_monitoring_system import ComponentType, MetricType
            
            test_metrics = [
                {
                    "name": "response_time",
                    "component": ComponentType.META_REASONING_ENGINE,
                    "metric_type": MetricType.HISTOGRAM,
                    "value": 1.5
                },
                {
                    "name": "success_rate",
                    "component": ComponentType.A2A_ORCHESTRATOR,
                    "metric_type": MetricType.GAUGE,
                    "value": 0.95
                },
                {
                    "name": "request_count",
                    "component": ComponentType.UNIVERSAL_QUERY_PROCESSOR,
                    "metric_type": MetricType.COUNTER,
                    "value": 1.0
                }
            ]
            
            # ë©”íŠ¸ë¦­ ê¸°ë¡
            for metric in test_metrics:
                monitoring.record_metric(**metric)
            
            # ì„±ëŠ¥ ë¶„ì„ ì‹¤í–‰
            start_time = time.time()
            analysis = await monitoring.analyze_performance()
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert analysis is not None, "Performance analysis should not be None"
            assert "system_health" in analysis, "Analysis should contain system health"
            assert "bottlenecks" in analysis, "Analysis should contain bottlenecks"
            assert "recommendations" in analysis, "Analysis should contain recommendations"
            
            self.test_results["performance_tests"]["monitoring_system"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "metrics_recorded": len(test_metrics),
                "system_health": analysis.get("system_health", "unknown")
            }
            
            logger.info(f"âœ… Performance Monitoring test passed in {execution_time:.2f}s")
            
        except Exception as e:
            self.test_results["performance_tests"]["monitoring_system"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Performance Monitoring test failed: {e}")
    
    async def test_session_management(self):
        """Session Management í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ” Testing Session Management System...")
        
        try:
            from core.universal_engine.session.session_management_system import SessionManager
            
            session_manager = SessionManager()
            
            # ì„¸ì…˜ ìƒì„± í…ŒìŠ¤íŠ¸
            start_time = time.time()
            session_id = await session_manager.create_session(
                user_id="test_user_001",
                initial_context={
                    "domain": "semiconductor",
                    "expertise_level": "intermediate"
                }
            )
            
            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            await session_manager.update_session_context(
                session_id=session_id,
                interaction_data={
                    "query": "í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬",
                    "response_quality": 0.85,
                    "user_feedback": "positive"
                }
            )
            
            # ì„¸ì…˜ ì •ë³´ ì¡°íšŒ
            session_info = await session_manager.get_session_info(session_id)
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert session_info is not None, "Session info should not be None"
            assert "session_id" in session_info, "Session info should contain session ID"
            assert "user_profile" in session_info, "Session info should contain user profile"
            assert "interaction_history" in session_info, "Session info should contain interaction history"
            
            self.test_results["performance_tests"]["session_management"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "session_id": session_id,
                "profile_updated": "user_profile" in session_info
            }
            
            logger.info(f"âœ… Session Management test passed in {execution_time:.2f}s")
            
        except Exception as e:
            self.test_results["performance_tests"]["session_management"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Session Management test failed: {e}")
    
    async def test_system_initialization(self):
        """System Initialization í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸš€ Testing System Initialization...")
        
        try:
            from core.universal_engine.initialization.system_initializer import UniversalEngineInitializer
            
            # ì´ˆê¸°í™” ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
            initializer = UniversalEngineInitializer()
            
            start_time = time.time()
            system_health = await initializer.initialize_system()
            execution_time = time.time() - start_time
            
            # ì´ˆê¸°í™” ë³´ê³ ì„œ ì¡°íšŒ
            init_report = initializer.get_initialization_report()
            
            # ê²°ê³¼ ê²€ì¦
            assert system_health is not None, "System health should not be None"
            assert "overall_status" in system_health.__dict__, "System health should contain overall status"
            assert "ready_components" in system_health.__dict__, "System health should contain ready components"
            
            assert init_report is not None, "Initialization report should not be None"
            assert "initialization_stage" in init_report, "Report should contain initialization stage"
            
            self.test_results["performance_tests"]["system_initialization"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "overall_status": system_health.overall_status,
                "ready_components": len(system_health.ready_components),
                "failed_components": len(system_health.failed_components)
            }
            
            logger.info(f"âœ… System Initialization test passed in {execution_time:.2f}s")
            logger.info(f"  System Status: {system_health.overall_status}")
            logger.info(f"  Ready Components: {len(system_health.ready_components)}")
            
        except Exception as e:
            self.test_results["performance_tests"]["system_initialization"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ System Initialization test failed: {e}")
    
    async def test_performance_validation(self):
        """Performance Validation í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ¯ Testing Performance Validation System...")
        
        try:
            from core.universal_engine.validation.performance_validation_system import PerformanceValidationSystem
            
            validation_system = PerformanceValidationSystem()
            
            # ì„±ëŠ¥ ê²€ì¦ ì‹¤í–‰
            start_time = time.time()
            validation_results = await validation_system.run_comprehensive_validation()
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert validation_results is not None, "Validation results should not be None"
            assert "overall_score" in validation_results, "Results should contain overall score"
            assert "component_scores" in validation_results, "Results should contain component scores"
            assert "benchmark_results" in validation_results, "Results should contain benchmark results"
            
            self.test_results["performance_tests"]["validation_system"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "overall_score": validation_results.get("overall_score", 0),
                "benchmarks_run": len(validation_results.get("benchmark_results", []))
            }
            
            logger.info(f"âœ… Performance Validation test passed in {execution_time:.2f}s")
            logger.info(f"  Overall Score: {validation_results.get('overall_score', 0):.2f}")
            
        except Exception as e:
            self.test_results["performance_tests"]["validation_system"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ Performance Validation test failed: {e}")
    
    async def test_cherry_ai_integration(self):
        """CherryAI í†µí•© í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ’ Testing CherryAI Integration...")
        
        try:
            from core.universal_engine.cherry_ai_integration.cherry_ai_universal_a2a_integration import CherryAIUniversalA2AIntegration
            
            integration = CherryAIUniversalA2AIntegration()
            
            # CherryAI í†µí•© ë¶„ì„ í…ŒìŠ¤íŠ¸
            test_query = "ë°˜ë„ì²´ ë°ì´í„°ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”"
            test_data = {
                "wafer_id": ["W001", "W002", "W003"],
                "thickness": [2.1, 2.0, 2.2],
                "defect_count": [0, 1, 0]
            }
            
            start_time = time.time()
            result = await integration.execute_universal_analysis(
                query=test_query,
                data=test_data,
                session_context={}
            )
            execution_time = time.time() - start_time
            
            # ê²°ê³¼ ê²€ì¦
            assert result is not None, "CherryAI integration result should not be None"
            assert "analysis_results" in result, "Result should contain analysis results"
            assert "recommendations" in result, "Result should contain recommendations"
            assert "ui_components" in result, "Result should contain UI components"
            
            self.test_results["cherry_ai_integration"]["universal_integration"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "analysis_quality": result.get("analysis_quality", "unknown"),
                "ui_components": len(result.get("ui_components", []))
            }
            
            logger.info(f"âœ… CherryAI Integration test passed in {execution_time:.2f}s")
            
        except Exception as e:
            self.test_results["cherry_ai_integration"]["universal_integration"] = {
                "status": "FAILED", 
                "error": str(e)
            }
            logger.error(f"âŒ CherryAI Integration test failed: {e}")
    
    async def test_end_to_end_integration(self):
        """End-to-End í†µí•© í…ŒìŠ¤íŠ¸"""
        
        logger.info("ğŸ”„ Testing End-to-End Integration...")
        
        try:
            from core.universal_engine.universal_query_processor import UniversalQueryProcessor
            
            # Universal Query Processorë¥¼ í†µí•œ ì „ì²´ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
            processor = UniversalQueryProcessor()
            
            # ë³µí•©ì ì¸ ë¶„ì„ ìš”ì²­ í…ŒìŠ¤íŠ¸
            complex_query = "ë°˜ë„ì²´ ì œì¡° ê³µì • ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìˆ˜ìœ¨ ê°œì„  ë°©ì•ˆì„ ì œì‹œí•´ì£¼ì„¸ìš”. ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            
            complex_data = {
                "process_parameters": {
                    "temperature": [950, 955, 960, 945, 970],
                    "pressure": [1.0, 1.1, 0.9, 1.2, 0.8],
                    "time": [30, 32, 28, 35, 25]
                },
                "yield_data": [0.85, 0.87, 0.82, 0.90, 0.78],
                "defect_types": ["particle", "scratch", "particle", "none", "contamination"]
            }
            
            complex_context = {
                "domain": "semiconductor_manufacturing",
                "analysis_type": "yield_improvement",
                "user_level": "beginner",
                "urgency": "high"
            }
            
            start_time = time.time()
            final_result = await processor.process_query(
                query=complex_query,
                data=complex_data,
                context=complex_context
            )
            execution_time = time.time() - start_time
            
            # ì „ì²´ ì‹œìŠ¤í…œ ê²°ê³¼ ê²€ì¦
            assert final_result is not None, "End-to-end result should not be None"
            assert "success" in final_result, "Result should indicate success status"
            assert "analysis_results" in final_result, "Result should contain analysis results"
            assert "meta_reasoning" in final_result, "Result should contain meta reasoning"
            assert "user_adapted_response" in final_result, "Result should contain user-adapted response"
            
            # í†µí•© í…ŒìŠ¤íŠ¸ ë©”íŠ¸ë¦­ ê³„ì‚°
            total_execution_time = self.end_time - self.start_time if self.end_time and self.start_time else execution_time
            
            passed_components = sum(
                1 for category in self.test_results.values()
                for test in category.values()
                if test.get("status") == "PASSED"
            )
            
            total_components = sum(
                len(category) for category in self.test_results.values()
            )
            
            self.test_results["e2e_tests"]["full_integration"] = {
                "status": "PASSED",
                "execution_time": execution_time,
                "total_test_time": total_execution_time,
                "components_tested": total_components,
                "components_passed": passed_components,
                "success_rate": passed_components / total_components if total_components > 0 else 0,
                "end_to_end_success": final_result.get("success", False)
            }
            
            logger.info(f"âœ… End-to-End Integration test passed in {execution_time:.2f}s")
            logger.info(f"  Overall Success Rate: {(passed_components/total_components*100):.1f}%")
            
        except Exception as e:
            self.test_results["e2e_tests"]["full_integration"] = {
                "status": "FAILED",
                "error": str(e)
            }
            logger.error(f"âŒ End-to-End Integration test failed: {e}")
    
    async def generate_test_report(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±"""
        
        total_execution_time = self.end_time - self.start_time if self.end_time and self.start_time else 0
        
        # ì „ì²´ í†µê³„ ê³„ì‚°
        total_tests = 0
        passed_tests = 0
        failed_tests = 0
        
        for category_name, category_results in self.test_results.items():
            for test_name, test_result in category_results.items():
                total_tests += 1
                if test_result.get("status") == "PASSED":
                    passed_tests += 1
                elif test_result.get("status") == "FAILED":
                    failed_tests += 1
        
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "success_rate": success_rate,
                "total_execution_time": total_execution_time,
                "test_timestamp": datetime.now().isoformat()
            },
            "detailed_results": self.test_results,
            "recommendations": await self._generate_recommendations()
        }
        
        # ë³´ê³ ì„œ ì¶œë ¥
        self._print_test_report(report)
        
        return report
    
    async def _generate_recommendations(self) -> List[str]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ë¶„ì„
        failed_components = []
        for category_name, category_results in self.test_results.items():
            for test_name, test_result in category_results.items():
                if test_result.get("status") == "FAILED":
                    failed_components.append(f"{category_name}.{test_name}")
        
        if failed_components:
            recommendations.append(f"ì‹¤íŒ¨í•œ ì»´í¬ë„ŒíŠ¸ ì¬ê²€í†  í•„ìš”: {', '.join(failed_components)}")
        
        # ì„±ëŠ¥ ê¶Œì¥ì‚¬í•­
        slow_tests = []
        for category_name, category_results in self.test_results.items():
            for test_name, test_result in category_results.items():
                execution_time = test_result.get("execution_time", 0)
                if execution_time > 5.0:  # 5ì´ˆ ì´ìƒ
                    slow_tests.append(f"{category_name}.{test_name}")
        
        if slow_tests:
            recommendations.append(f"ì„±ëŠ¥ ìµœì í™” ê²€í†  í•„ìš”: {', '.join(slow_tests)}")
        
        # ì¼ë°˜ ê¶Œì¥ì‚¬í•­
        if not failed_components:
            recommendations.append("ëª¨ë“  í•µì‹¬ ì»´í¬ë„ŒíŠ¸ê°€ ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤. í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ!")
        
        recommendations.append("ì •ê¸°ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° íšŒê·€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê¶Œì¥")
        recommendations.append("ì‹¤ì œ A2A ì—ì´ì „íŠ¸ í™˜ê²½ì—ì„œì˜ í†µí•© í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ ê¶Œì¥")
        
        return recommendations
    
    def _print_test_report(self, report: Dict[str, Any]):
        """í…ŒìŠ¤íŠ¸ ë³´ê³ ì„œ ì¶œë ¥"""
        
        summary = report["test_summary"]
        
        print("\n" + "=" * 80)
        print("ğŸ‰ UNIVERSAL ENGINE COMPREHENSIVE TEST REPORT")
        print("=" * 80)
        
        print(f"ğŸ“Š Test Summary:")
        print(f"  â€¢ Total Tests: {summary['total_tests']}")
        print(f"  â€¢ Passed: {summary['passed_tests']} âœ…")
        print(f"  â€¢ Failed: {summary['failed_tests']} âŒ")
        print(f"  â€¢ Success Rate: {summary['success_rate']:.1f}%")
        print(f"  â€¢ Total Execution Time: {summary['total_execution_time']:.2f}s")
        
        print(f"\nğŸ“‹ Detailed Results by Category:")
        
        for category_name, category_results in report["detailed_results"].items():
            category_passed = sum(1 for r in category_results.values() if r.get("status") == "PASSED")
            category_total = len(category_results)
            category_rate = (category_passed / category_total * 100) if category_total > 0 else 0
            
            print(f"  â€¢ {category_name.replace('_', ' ').title()}: {category_passed}/{category_total} ({category_rate:.1f}%)")
            
            for test_name, test_result in category_results.items():
                status_icon = "âœ…" if test_result.get("status") == "PASSED" else "âŒ"
                exec_time = test_result.get("execution_time", 0)
                print(f"    - {test_name}: {status_icon} ({exec_time:.2f}s)")
        
        print(f"\nğŸ’¡ Recommendations:")
        for i, rec in enumerate(report["recommendations"], 1):
            print(f"  {i}. {rec}")
        
        print("\n" + "=" * 80)
        if summary["success_rate"] >= 90:
            print("ğŸŠ EXCELLENT! Universal Engine is ready for production!")
        elif summary["success_rate"] >= 75:
            print("âœ¨ GOOD! Minor improvements needed before production.")
        else:
            print("âš ï¸  NEEDS ATTENTION! Significant issues need to be addressed.")
        print("=" * 80)


async def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("ğŸš€ Starting Universal Engine Comprehensive Test Suite...")
    
    test_suite = UniversalEngineTestSuite()
    
    try:
        # ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_report = await test_suite.run_all_tests()
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"universal_engine_test_report_{timestamp}.json"
        
        with open(report_filename, 'w', encoding='utf-8') as f:
            json.dump(test_report, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ Test report saved to: {report_filename}")
        
        # ì¢…ë£Œ ìƒíƒœ ê²°ì •
        success_rate = test_report["test_summary"]["success_rate"]
        exit_code = 0 if success_rate >= 90 else 1
        
        return exit_code
        
    except Exception as e:
        logger.error(f"Test suite execution failed: {e}")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)