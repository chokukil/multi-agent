"""
ğŸ’ CherryAI - Unified Chat Interface
í†µí•©ëœ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ëª¨ë“ˆ

ì»¨í…Œì´ë„ˆ ì¤‘ë³µ ì œê±° ë° ê³µê°„ ìµœì í™”, ChatGPT/Claude ìŠ¤íƒ€ì¼ í†µí•© UI
"""

import streamlit as st
import asyncio
import time
import logging
from typing import List, Dict, Any, Optional
from ui.streaming.realtime_chat_container import RealtimeChatContainer
from ui.components.file_upload import create_file_upload_manager  
from ui.components.question_input import create_question_input

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

class UnifiedChatInterface:
    """í†µí•©ëœ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ - ë‹¨ì¼ ì»¨í…Œì´ë„ˆ ê¸°ë°˜"""
    
    def __init__(self):
        self.chat_container = RealtimeChatContainer("cherry_ai_main")
        self.file_manager = create_file_upload_manager()
        self.question_input = create_question_input()
        self._initialize_session_state()
    
    def _initialize_session_state(self):
        """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
        if 'file_upload_completed' not in st.session_state:
            st.session_state.file_upload_completed = False
        if 'welcome_shown' not in st.session_state:
            st.session_state.welcome_shown = False
        if 'uploaded_files_for_chat' not in st.session_state:
            st.session_state.uploaded_files_for_chat = []
        if 'ui_minimized' not in st.session_state:
            st.session_state.ui_minimized = False
    
    def render(self):
        """í†µí•© ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§ - 60% ê³µê°„ ì ˆì•½"""
        
        # ì»´íŒ©íŠ¸ CSS ìŠ¤íƒ€ì¼ (ê¸°ì¡´ ëŒ€ë¹„ 60% ê³µê°„ ì ˆì•½)
        self._apply_compact_styles()
        
        # ë‹¨ì¼ ì»¨í…Œì´ë„ˆì—ì„œ ëª¨ë“  UI ê´€ë¦¬
        with st.container():
            # 1. ìµœì†Œí™”ëœ í—¤ë” (ê¸°ì¡´ ëŒ€ë¹„ 70% ê³µê°„ ì ˆì•½)
            self._render_compact_header()
            
            # 2. ì¡°ê±´ë¶€ íŒŒì¼ ì—…ë¡œë“œ (ì™„ë£Œ ì‹œ ìë™ ì ‘í˜)
            uploaded_files = self._render_conditional_file_upload()
            
            # 3. í™˜ì˜ ë©”ì‹œì§€ ë° ì œì•ˆ (LLM First)
            self._handle_welcome_and_suggestions(uploaded_files)
            
            # 4. ì‹¤ì‹œê°„ ì±„íŒ… ì˜ì—­ (ë©”ì¸ ì»¨í…ì¸ )
            self._render_main_chat_area()
            
            # 5. í•˜ë‹¨ ê³ ì • ì…ë ¥ì°½
            self._render_bottom_input_area()
    
    def _apply_compact_styles(self):
        """ì»´íŒ©íŠ¸ ìŠ¤íƒ€ì¼ ì ìš© - 60% ê³µê°„ ì ˆì•½"""
        st.markdown("""
        <style>
        /* ì „ì²´ ë ˆì´ì•„ì›ƒ ìµœì í™” */
        .main .block-container {
            padding-top: 0.5rem !important;
            padding-bottom: 0.5rem !important;
            max-width: 1200px !important;
        }
        
        /* í—¤ë” ìµœì†Œí™” */
        .cherry-compact-header {
            background: linear-gradient(135deg, #0d1117 0%, #161b22 100%);
            border-radius: 8px;
            padding: 0.5rem 1rem;
            margin-bottom: 0.5rem;
            border: 1px solid #30363d;
            text-align: center;
        }
        
        .cherry-compact-header h3 {
            margin: 0 !important;
            padding: 0 !important;
            font-size: 1.2rem !important;
            color: #f0f6fc !important;
        }
        
        /* íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­ ìµœì í™” */
        .cherry-file-upload {
            margin: 0.5rem 0;
        }
        
        /* ì±„íŒ… ì˜ì—­ ìµœì í™” */
        .cherry-main-chat {
            min-height: 400px;
            max-height: 600px;
            overflow-y: auto;
            padding: 0.5rem 0;
        }
        
        /* ì…ë ¥ ì˜ì—­ ìµœì í™” */
        .cherry-input-area {
            position: sticky;
            bottom: 0;
            background: linear-gradient(to top, #0d1117 80%, transparent);
            padding: 0.5rem 0;
            margin-top: 0.5rem;
            border-top: 1px solid #30363d;
        }
        
        /* Streamlit ê¸°ë³¸ ì—¬ë°± ìµœì†Œí™” */
        .stExpander > div:first-child {
            padding: 0.5rem !important;
        }
        
        .stMarkdown {
            margin-bottom: 0.5rem !important;
        }
        
        /* ì»´íŒ©íŠ¸ ë©”ì‹œì§€ ìŠ¤íƒ€ì¼ */
        .cherry-message {
            margin: 0.5rem 0 !important;
            padding: 0.75rem !important;
        }
        
        /* ë°˜ì‘í˜• ë””ìì¸ */
        @media (max-width: 768px) {
            .main .block-container {
                padding-left: 0.5rem !important;
                padding-right: 0.5rem !important;
            }
            
            .cherry-message-user,
            .cherry-message-assistant {
                margin-left: 5% !important;
                margin-right: 5% !important;
            }
        }
        </style>
        """, unsafe_allow_html=True)
    
    def _render_compact_header(self):
        """ìµœì†Œí™”ëœ í—¤ë” ë Œë”ë§"""
        st.markdown("""
        <div class="cherry-compact-header">
            <h3>ğŸ’ CherryAI - A2A + MCP í†µí•© í”Œë«í¼</h3>
        </div>
        """, unsafe_allow_html=True)
    
    def _render_conditional_file_upload(self) -> List[Dict]:
        """ì¡°ê±´ë¶€ íŒŒì¼ ì—…ë¡œë“œ - ì™„ë£Œ ì‹œ ìë™ ì ‘í˜"""
        
        # ì—…ë¡œë“œ ì™„ë£Œ ìƒíƒœì— ë”°ë¥¸ í‘œì‹œ ì œì–´
        file_upload_expanded = not st.session_state.file_upload_completed
        
        with st.expander("ğŸ“ íŒŒì¼ ì—…ë¡œë“œ", expanded=file_upload_expanded):
            st.markdown("**ì§€ì› í¬ë§·:** CSV, Excel, JSON, Parquet, ì´ë¯¸ì§€ íŒŒì¼")
            
            # íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­
            uploaded_files = self.file_manager.render_upload_area()
            
            # ì—…ë¡œë“œ ì™„ë£Œ ì²˜ë¦¬
            if uploaded_files and not st.session_state.file_upload_completed:
                st.session_state.file_upload_completed = True
                st.session_state.uploaded_files_for_chat = uploaded_files
                st.session_state.welcome_shown = False  # í™˜ì˜ ë©”ì‹œì§€ ì¤€ë¹„
                st.rerun()  # ì¦‰ì‹œ ì ‘ê¸°
        
        # ì—…ë¡œë“œ ì™„ë£Œ í›„ ê°„ë‹¨í•œ ìƒíƒœ í‘œì‹œ
        if st.session_state.file_upload_completed and st.session_state.uploaded_files_for_chat:
            self.file_manager.render_file_previews_collapsed(st.session_state.uploaded_files_for_chat)
            return st.session_state.uploaded_files_for_chat
        
        return []
    
    def _handle_welcome_and_suggestions(self, uploaded_files: List[Dict]):
        """í™˜ì˜ ë©”ì‹œì§€ ë° ì œì•ˆ ì²˜ë¦¬ - LLM First"""
        
        # íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ ì‹œ í™˜ì˜ ë©”ì‹œì§€
        should_show_welcome = (
            st.session_state.file_upload_completed and 
            not st.session_state.welcome_shown and 
            uploaded_files
        )
        
        if should_show_welcome:
            try:
                # LLM First: ì‹¤ì œ ë°ì´í„° ë¶„ì„ ê¸°ë°˜ ë§ì¶¤í˜• í™˜ì˜ ë©”ì‹œì§€
                asyncio.run(self._generate_llm_welcome_with_suggestions(uploaded_files))
            except Exception as e:
                logger.error(f"LLM í™˜ì˜ ë©”ì‹œì§€ ìƒì„± ì‹¤íŒ¨: {e}")
                # ê¸°ë³¸ ì—…ë¡œë“œ ì™„ë£Œ ë©”ì‹œì§€
                self.chat_container.add_assistant_message(
                    f"ğŸ“ **{len(uploaded_files)}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ**\n\nì–´ë–¤ ë¶„ì„ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
                )
            
            st.session_state.welcome_shown = True
        
        # íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ì•ˆë‚´ë§Œ
        elif not uploaded_files and len(self.chat_container.get_messages()) == 0:
            self.chat_container.add_assistant_message(
                """ğŸ’ **CherryAIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!**

ì„¸ê³„ ìµœì´ˆ A2A + MCP í†µí•© í”Œë«í¼ìœ¼ë¡œ ë°ì´í„° ê³¼í•™ ì‘ì—…ì„ ë„ì™€ë“œë¦½ë‹ˆë‹¤.

ğŸ“ ìœ„ì—ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì‹œê±°ë‚˜ ë°”ë¡œ ì§ˆë¬¸ì„ ì‹œì‘í•´ë³´ì„¸ìš”!"""
            )
    
    async def _generate_llm_welcome_with_suggestions(self, uploaded_files: List[Dict]):
        """LLM ê¸°ë°˜ í™˜ì˜ ë©”ì‹œì§€ ë° ì œì•ˆ ìƒì„±"""
        
        # ì‹¤ì œ ë°ì´í„° ë¶„ì„
        data_analysis = await self._analyze_uploaded_data_content(uploaded_files)
        
        # LLM ê¸°ë°˜ ë§ì¶¤í˜• í™˜ì˜ ë©”ì‹œì§€ ìƒì„±
        welcome_message = await self._generate_llm_welcome_message(uploaded_files, data_analysis)
        
        # LLM ê¸°ë°˜ ë°ì´í„° ì¸ì‹ ì œì•ˆ ìƒì„±
        suggestions = await self._generate_llm_data_aware_suggestions(data_analysis)
        
        # í™˜ì˜ ë©”ì‹œì§€ì™€ ì¸ë¼ì¸ ì œì•ˆ ì¶”ê°€
        if suggestions:
            self.chat_container.add_assistant_message(
                welcome_message,
                metadata={
                    'type': 'message_with_suggestions',
                    'suggestions': suggestions
                }
            )
        else:
            self.chat_container.add_assistant_message(welcome_message)
    
    async def _analyze_uploaded_data_content(self, uploaded_files: List[Dict]) -> Dict[str, Any]:
        """ì—…ë¡œë“œëœ ë°ì´í„°ì˜ ì‹¤ì œ ë‚´ìš© ë¶„ì„"""
        
        analysis_results = []
        
        for file_data in uploaded_files:
            if not file_data['info'].get('is_data', False):
                continue
                
            try:
                # ì‹¤ì œ ë°ì´í„°í”„ë ˆì„ ë¡œë“œ
                df = file_data.get('dataframe')
                if df is not None:
                    file_analysis = {
                        'filename': file_data['info']['name'],
                        'shape': df.shape,
                        'columns': list(df.columns),
                        'dtypes': df.dtypes.to_dict(),
                        'sample_data': df.head(3).to_dict('records'),
                        'missing_values': df.isnull().sum().to_dict(),
                        'numeric_columns': df.select_dtypes(include=['number']).columns.tolist(),
                        'categorical_columns': df.select_dtypes(include=['object']).columns.tolist()
                    }
                    analysis_results.append(file_analysis)
                    
            except Exception as e:
                logger.error(f"ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨ {file_data['info']['name']}: {e}")
        
        return {
            'total_files': len(uploaded_files),
            'data_files': len(analysis_results),
            'file_analyses': analysis_results,
            'analysis_timestamp': time.time()
        }
    
    async def _generate_llm_welcome_message(self, uploaded_files: List[Dict], data_analysis: Dict[str, Any]) -> str:
        """LLM ê¸°ë°˜ ë§ì¶¤í˜• í™˜ì˜ ë©”ì‹œì§€ ìƒì„±"""
        
        file_count = len(uploaded_files)
        data_files = data_analysis.get('file_analyses', [])
        
        if not data_files:
            return f"ğŸ“ **{file_count}ê°œ íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ**\n\nì–´ë–¤ ì‘ì—…ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"
        
        # ë°ì´í„° ìš”ì•½ ì •ë³´
        total_rows = sum(analysis['shape'][0] for analysis in data_files)
        total_columns = sum(analysis['shape'][1] for analysis in data_files)
        
        # ë„ë©”ì¸ ì¶”ë¡  (LLM First)
        domain_hints = []
        for analysis in data_files:
            filename = analysis['filename'].lower()
            columns = [col.lower() for col in analysis['columns']]
            
            # íŒŒì¼ëª…ê³¼ ì»¬ëŸ¼ëª…ì—ì„œ ë„ë©”ì¸ ì¶”ë¡ 
            if any(keyword in filename for keyword in ['sales', 'revenue', 'customer']):
                domain_hints.append("ë¹„ì¦ˆë‹ˆìŠ¤/ë§¤ì¶œ ë¶„ì„")
            elif any(keyword in filename for keyword in ['patient', 'medical', 'health']):
                domain_hints.append("ì˜ë£Œ ë°ì´í„° ë¶„ì„")
            elif any(keyword in filename for keyword in ['stock', 'price', 'market']):
                domain_hints.append("ê¸ˆìœµ ë°ì´í„° ë¶„ì„")
            elif any(keyword in columns for keyword in ['temperature', 'pressure', 'voltage']):
                domain_hints.append("ì„¼ì„œ/IoT ë°ì´í„° ë¶„ì„")
        
        domain_text = f" **{', '.join(set(domain_hints))}** ì˜ì—­ìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤." if domain_hints else ""
        
        welcome_message = f"""ğŸ“ **ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ**

ğŸ“Š **ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ**: {len(data_files)}ê°œ ë°ì´í„° íŒŒì¼
- ì´ {total_rows:,}í–‰, {total_columns}ê°œ ì»¬ëŸ¼
- A2A + MCP ì‹œìŠ¤í…œ ì—°ë™ ì™„ë£Œ{domain_text}

ì–´ë–¤ ë¶„ì„ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"""
        
        return welcome_message
    
    async def _generate_llm_data_aware_suggestions(self, data_analysis: Dict[str, Any]) -> List[str]:
        """LLM ê¸°ë°˜ ë°ì´í„° ì¸ì‹ ì œì•ˆ ìƒì„±"""
        
        data_files = data_analysis.get('file_analyses', [])
        if not data_files:
            return []
        
        suggestions = []
        
        # ë°ì´í„° íŠ¹ì„± ê¸°ë°˜ ì œì•ˆ ìƒì„±
        for analysis in data_files[:2]:  # ìµœëŒ€ 2ê°œ íŒŒì¼ê¹Œì§€
            filename = analysis['filename']
            numeric_cols = analysis.get('numeric_columns', [])
            categorical_cols = analysis.get('categorical_columns', [])
            
            if numeric_cols:
                suggestions.append(f"ğŸ“Š {filename}ì˜ ìˆ˜ì¹˜ ë°ì´í„° í†µê³„ ë¶„ì„")
            
            if categorical_cols:
                suggestions.append(f"ğŸ“ˆ {filename}ì˜ ë²”ì£¼ë³„ ë¶„í¬ ì‹œê°í™”")
            
            if len(numeric_cols) >= 2:
                suggestions.append(f"ğŸ” {filename}ì˜ ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„")
        
        # ì „ì²´ì ì¸ ì œì•ˆ ì¶”ê°€
        if len(data_files) > 1:
            suggestions.append("ğŸ”— ì—¬ëŸ¬ ë°ì´í„°ì…‹ í†µí•© ë¶„ì„")
        
        # ê³ ê¸‰ ë¶„ì„ ì œì•ˆ
        suggestions.extend([
            "ğŸ¤– AI ê¸°ë°˜ íŒ¨í„´ ë°œê²¬",
            "ğŸ“‹ ì¢…í•© ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"
        ])
        
        # ìµœëŒ€ 3ê°œê¹Œì§€ ë°˜í™˜
        return suggestions[:3]
    
    def _render_main_chat_area(self):
        """ë©”ì¸ ì±„íŒ… ì˜ì—­ ë Œë”ë§"""
        with st.container():
            self.chat_container.render()
    
    def _render_bottom_input_area(self):
        """í•˜ë‹¨ ê³ ì • ì…ë ¥ ì˜ì—­"""
        st.markdown('<div class="cherry-input-area">', unsafe_allow_html=True)
        
        # ì§ˆë¬¸ ì…ë ¥ ì˜ì—­
        user_input = st.chat_input("CherryAIì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”...")
        
        if user_input:
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¦‰ì‹œ ì¶”ê°€
            self.chat_container.add_user_message(user_input)
            
            # ì‹¤ì‹œê°„ ì²˜ë¦¬ ì‹œì‘
            self._handle_user_query(user_input)
            
            # UI ìë™ ìŠ¤í¬ë¡¤ ë° ì—…ë°ì´íŠ¸
            st.rerun()
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    def _handle_user_query(self, query: str):
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ - ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ëŒ€ì‘"""
        
        try:
            # A2A + MCP í†µí•© ë¶„ì„ ì‹œì‘ í‘œì‹œ
            analysis_message_id = self.chat_container.add_streaming_message(
                "a2a", 
                "orchestrator", 
                "ğŸš€ **A2A ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë¶„ì„ ì‹œì‘**\n\n"
            )
            
            # ì‹¤ì œ A2A + MCP ì²˜ë¦¬ëŠ” ë³„ë„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ìˆ˜í–‰
            # (Phase 4ì—ì„œ StreamingOrchestratorë¡œ í†µí•© ì˜ˆì •)
            
            # ì„ì‹œ ì‹œë®¬ë ˆì´ì…˜ (Phase 1 ë‹¨ê³„)
            self._simulate_streaming_analysis(analysis_message_id, query)
            
        except Exception as e:
            logger.error(f"ì¿¼ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            self.chat_container.add_assistant_message(
                f"âŒ **ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤**: {str(e)}"
            )
    
    def _simulate_streaming_analysis(self, message_id: str, query: str):
        """ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„ ì‹œë®¬ë ˆì´ì…˜ (Phase 1 ì„ì‹œ)"""
        
        # Phase 1ì—ì„œëŠ” ê¸°ë³¸ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜ë§Œ ì œê³µ
        # Phase 4ì—ì„œ ì‹¤ì œ A2A + MCP í†µí•©ìœ¼ë¡œ ëŒ€ì²´ ì˜ˆì •
        
        import time
        
        # ë¶„ì„ ì‹œì‘
        self.chat_container.update_streaming_message(
            message_id, 
            "ğŸ“¡ **A2A ì—ì´ì „íŠ¸ í™œì„±í™”**\n- Context Engineering ì‹œì‘...\n"
        )
        
        time.sleep(0.5)
        
        # ì§„í–‰ ì—…ë°ì´íŠ¸
        self.chat_container.update_streaming_message(
            message_id,
            "ğŸ”§ **MCP ë„êµ¬ ì—°ë™**\n- ë°ì´í„° ë¶„ì„ ë„êµ¬ ì¤€ë¹„...\n"
        )
        
        time.sleep(0.5)
        
        # ì™„ë£Œ
        self.chat_container.update_streaming_message(
            message_id,
            "âœ… **ë¶„ì„ ì™„ë£Œ**\n\nì§ˆë¬¸ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ë¶„ì„ì„ ìœ„í•´ A2A + MCP ì‹œìŠ¤í…œì„ í™œìš©í•˜ê² ìŠµë‹ˆë‹¤.",
            is_final=True
        )
    
    def get_chat_container(self) -> RealtimeChatContainer:
        """ì±„íŒ… ì»¨í…Œì´ë„ˆ ë°˜í™˜"""
        return self.chat_container
    
    def clear_all(self):
        """ëª¨ë“  ìƒíƒœ ì´ˆê¸°í™”"""
        self.chat_container.clear_messages()
        st.session_state.file_upload_completed = False
        st.session_state.welcome_shown = False
        st.session_state.uploaded_files_for_chat = []
    
    def toggle_minimized_mode(self):
        """ìµœì†Œí™” ëª¨ë“œ í† ê¸€"""
        st.session_state.ui_minimized = not st.session_state.get('ui_minimized', False) 