# File: ui/sidebar_components.py
# Location: ./ui/sidebar_components.py

import streamlit as st
import os
import json
from pathlib import Path
from typing import Dict, List, Tuple
import pandas as pd
import asyncio
import logging
from datetime import datetime

def render_data_upload_section():
    """Îç∞Ïù¥ÌÑ∞ ÏóÖÎ°úÎìú ÏÑπÏÖò Î†åÎçîÎßÅ"""
    from core import data_manager, data_lineage_tracker
    
    st.markdown("### ÔøΩÔøΩ Í∞ïÌôîÎêú Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù")
    
    # SSOT Îç∞Ïù¥ÌÑ∞ ÏÉÅÌÉú ÌëúÏãú
    current_status = data_manager.get_status_message()
    
    if data_manager.is_data_loaded():
        st.success(current_status)
        
        # Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥ ÌëúÏãú
        with st.expander("üìà ÌòÑÏû¨ Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥", expanded=False):
            info = data_manager.get_data_info()
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Ìñâ Ïàò", f"{info['row_count']:,}")
                st.metric("Î©îÎ™®Î¶¨", f"{info['memory_mb']:.2f}MB")
            
            with col2:
                st.metric("Ïó¥ Ïàò", f"{info['col_count']:,}")
                st.metric("Ï∂úÏ≤ò", info['source'])
            
            st.write("**Ïª¨Îüº Ï†ïÎ≥¥:**")
            cols_display = ', '.join(info['columns'][:8])
            if len(info['columns']) > 8:
                cols_display += f" ... (+{len(info['columns']) - 8}Í∞ú)"
            st.text(cols_display)
            
            st.write("**ÌÜµÍ≥Ñ:**")
            st.text(f"ÏàòÏπòÌòï: {len(info['numeric_cols'])}Í∞ú, Î≤îÏ£ºÌòï: {len(info['categorical_cols'])}Í∞ú, Í≤∞Ï∏°Í∞í: {info['null_count']:,}Í∞ú")
        
        # Îç∞Ïù¥ÌÑ∞ Í¥ÄÎ¶¨ ÎèÑÍµ¨Îì§
        col1, col2 = st.columns(2)
        with col1:
            if st.button("üîÑ ÏÉàÎ°úÍ≥†Ïπ®", use_container_width=True):
                if hasattr(st.session_state, 'uploaded_data') and st.session_state.uploaded_data is not None:
                    success = data_manager.set_data(st.session_state.uploaded_data, "ÏÑ∏ÏÖò Îç∞Ïù¥ÌÑ∞ ÏÉàÎ°úÍ≥†Ïπ®")
                    if success:
                        st.success("‚úÖ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏÉàÎ°úÍ≥†Ïπ®ÎêòÏóàÏäµÎãàÎã§!")
                        st.rerun()
        
        with col2:
            if st.button("‚úÖ ÏùºÍ¥ÄÏÑ± Í≤ÄÏ¶ù", use_container_width=True):
                is_valid, message = data_manager.validate_data_consistency()
                if is_valid:
                    st.success(f"‚úÖ {message}")
                else:
                    st.error(f"‚ùå {message}")
    else:
        st.warning(current_status)
    
    # sandbox/datasets Ìè¥Îçî ÏÉùÏÑ±
    DATASETS_DIR = "./sandbox/datasets"
    os.makedirs(DATASETS_DIR, exist_ok=True)
    
    uploaded_csv = st.file_uploader("üìÇ CSV ÌååÏùº ÏóÖÎ°úÎìú", type=["csv"], help="Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑùÏùÑ ÏúÑÌïú CSV ÌååÏùºÏùÑ ÏóÖÎ°úÎìúÌïòÏÑ∏Ïöî")
    if uploaded_csv:
        try:
            # ÌååÏùºÏùÑ datasets Ìè¥ÎçîÏóê Ï†ÄÏû•
            file_path = os.path.join(DATASETS_DIR, uploaded_csv.name)
            with open(file_path, "wb") as f:
                f.write(uploaded_csv.getvalue())
                
            # Ï†ÄÏû•Îêú ÌååÏùºÏùÑ DataFrameÏúºÎ°ú Î°úÎìú
            df = pd.read_csv(file_path)
            
            # Í∞ïÌôîÎêú SSOTÏóê Îç∞Ïù¥ÌÑ∞ ÏÑ§Ï†ï
            success = data_manager.set_data(df, f"ÏóÖÎ°úÎìúÎêú ÌååÏùº: {uploaded_csv.name}")
            
            if success:
                # Îç∞Ïù¥ÌÑ∞ Í≥ÑÎ≥¥ Ï∂îÏ†Å ÏãúÏûë
                original_hash = data_lineage_tracker.set_original_data(df)
                
                # ÏÑ∏ÏÖò ÏÉÅÌÉúÏóêÎèÑ Î∞±ÏóÖ Ï†ÄÏû•
                st.session_state.uploaded_data = df
                st.session_state.original_data_hash = original_hash
                
                st.success(f"‚úÖ Í∞ïÌôîÎêú SSOT ÏÑ§Ï†ï ÏôÑÎ£å: {uploaded_csv.name}")
                
                # ÏóÖÎç∞Ïù¥Ìä∏Îêú Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥ ÌëúÏãú
                with st.expander("üìà ÏóÖÎ°úÎìúÎêú Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥", expanded=True):
                    info = data_manager.get_data_info()
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Ìñâ Ïàò", f"{info['row_count']:,}")
                    with col2:
                        st.metric("Ïó¥ Ïàò", f"{info['col_count']:,}")
                    with col3:
                        st.metric("Î©îÎ™®Î¶¨", f"{info['memory_mb']:.2f}MB")
                    
                    st.write("**ÌååÏùº Í≤ΩÎ°ú**: `" + file_path + "`")
                    st.write("**Ïª¨Îüº Î™©Î°ù**: " + ', '.join(info['columns'][:5]) + ("..." if len(info['columns']) > 5 else ""))
                    
                    # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÎØ∏Î¶¨Î≥¥Í∏∞
                    st.write("**ÎØ∏Î¶¨Î≥¥Í∏∞:**")
                    st.dataframe(df.head(3), use_container_width=True)
                
                # ÏãúÏä§ÌÖú Ï¥àÍ∏∞Ìôî ÏïàÎÇ¥
                if st.session_state.get("graph_initialized", False):
                    st.info("üí° Î™®Îì† ExecutorÍ∞Ä ÎèôÏùºÌïú Îç∞Ïù¥ÌÑ∞Ïóê Ï†ëÍ∑ºÌï† Ïàò ÏûàÏäµÎãàÎã§!")
            else:
                st.error("‚ùå Í∞ïÌôîÎêú SSOT Îç∞Ïù¥ÌÑ∞ ÏÑ§Ï†ï Ïã§Ìå®")
                
        except Exception as e:
            # SSOT Ï¥àÍ∏∞Ìôî
            data_manager.clear_data()
            if hasattr(st.session_state, 'uploaded_data'):
                del st.session_state.uploaded_data
            st.error(f"CSV ÏóÖÎ°úÎìú Ïã§Ìå®: {e}")
    
    # Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú ÏòµÏÖò
    if data_manager.is_data_loaded():
        if st.button("üóëÔ∏è Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú", use_container_width=True, type="secondary"):
            data_manager.clear_data()
            if hasattr(st.session_state, 'uploaded_data'):
                del st.session_state.uploaded_data
            st.success("‚úÖ Îç∞Ïù¥ÌÑ∞Í∞Ä ÏÇ≠Ï†úÎêòÏóàÏäµÎãàÎã§.")
            st.rerun()

def render_system_settings():
    """ÏãúÏä§ÌÖú ÏÑ§Ï†ï ÏÑπÏÖò Î†åÎçîÎßÅ"""
    with st.expander("‚öôÔ∏è System Settings", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            new_recursion_limit = st.number_input(
                "Recursion Limit",
                min_value=5,
                max_value=50,
                value=st.session_state.get("recursion_limit", 30),
                step=1,
                help="Maximum number of recursive calls to prevent infinite loops"
            )
        
        with col2:
            new_timeout_seconds = st.number_input(
                "Query Timeout (seconds)",
                min_value=30,
                max_value=600,
                value=st.session_state.get("timeout_seconds", 180),
                step=30,
                help="Maximum time to wait for response (30-600 seconds)"
            )
        
        if st.button("Apply Settings", use_container_width=True):
            st.session_state.recursion_limit = new_recursion_limit
            st.session_state.timeout_seconds = new_timeout_seconds
            st.success(f"‚úÖ Settings updated - Recursion: {new_recursion_limit}, Timeout: {new_timeout_seconds}s")
            st.rerun()

def render_executor_creation_form():
    """Executor ÏÉùÏÑ± Ìèº Î†åÎçîÎßÅ"""
    st.subheader("‚ûï Create New Executor")
    
    # Enhanced agent creation with detailed options
    with st.form("create_executor_form"):
        # Executor name
        executor_name = st.text_input(
            "Executor Name", 
            placeholder="e.g., Data Analyst",
            help="Choose a descriptive name for the executor"
        )
        
        # Enhanced role templates
        role_templates = {
            "Custom": "",
            "EDA Specialist": """You are an Exploratory Data Analysis Expert who uncovers hidden patterns and insights in data. 
You focus on understanding data structure, distributions, relationships, and anomalies.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Perform ALL your analysis on `df` in the SAME Python tool execution.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
            "Visualization Expert": """You are a Data Visualization Expert who creates compelling and insightful charts, graphs, and dashboards.
You excel at choosing the right visualization for the data and message.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Create visualizations using matplotlib, seaborn, or plotly.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
            "ML Engineer": """You are a Machine Learning Engineer who builds, trains, and evaluates predictive models.
You handle the full ML pipeline from data preparation to model deployment.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Use sklearn or other ML libraries for modeling.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
            "Data Preprocessor": """You are a Data Preprocessing Expert who cleans, transforms, and prepares data for analysis.
You handle missing values, outliers, encoding, and feature scaling.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Document all transformations clearly.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
            "Statistical Analyst": """You are a Statistical Analysis Expert who performs rigorous statistical tests and modeling.
You derive meaningful insights through hypothesis testing and statistical inference.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Use scipy.stats or statsmodels for analysis.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
            "Report Writer": """You are a Report Writing Expert who creates comprehensive analysis reports.
You summarize findings and communicate insights to stakeholders.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool if data analysis is needed.
2. Focus on clear, actionable insights.
3. End with 'TASK COMPLETED: [Summary]' when finished."""
        }
        
        selected_role = st.selectbox(
            "Select Role Template",
            list(role_templates.keys()),
            help="Choose a predefined role or create custom"
        )
        
        if selected_role == "Custom":
            prompt_text = st.text_area(
                "Role Description",
                height=150,
                placeholder="Describe the executor's role... Include 'TASK COMPLETED:' instruction at the end."
            )
        else:
            prompt_text = st.text_area(
                "Role Description",
                value=role_templates[selected_role],
                height=150
            )
        
        # Tool selection
        st.subheader("üîß Tool Selection")
        
        # Python tool (always included for data science tasks)
        use_python = st.checkbox(
            "Enhanced Python Tool (SSOT)", 
            value=True,
            help="Í∞ïÌôîÎêú SSOT Í∏∞Î∞ò Îç∞Ïù¥ÌÑ∞ Î∂ÑÏÑù ÌôòÍ≤Ω"
        )
        
        # MCP Tools Selection (simplified for now)
        st.info("üì¶ MCP tools can be added in future versions")
        
        # Submit button
        submitted = st.form_submit_button("‚ú® Create Executor", type="primary", use_container_width=True)
        
        if submitted:
            if not executor_name:
                st.error("‚ùå Please enter an executor name")
            elif not prompt_text:
                st.error("‚ùå Please enter a role description")
            elif executor_name in st.session_state.get("executors", {}):
                st.error(f"‚ùå Executor '{executor_name}' already exists")
            else:
                # Initialize executors dict if not exists
                if "executors" not in st.session_state:
                    st.session_state.executors = {}
                
                # Create executor configuration
                executor_config = {
                    "prompt": prompt_text,
                    "tools": ["python_repl_ast"] if use_python else [],
                    "created_at": datetime.now().isoformat()
                }
                
                st.session_state.executors[executor_name] = executor_config
                st.success(f"‚úÖ Executor '{executor_name}' created successfully!")
                
                logging.info(f"Created executor: {executor_name}")
                st.rerun()

def render_saved_systems():
    """Ï†ÄÏû•Îêú ÏãúÏä§ÌÖú Í¥ÄÎ¶¨ ÏÑπÏÖò Î†åÎçîÎßÅ"""
    st.markdown("### üìÇ Saved Multi-Agent Systems")
    
    saved_configs = load_multi_agent_configs()
    
    if saved_configs:
        selected_config = st.selectbox(
            "Load a saved system",
            ["None"] + [f"{config['name']} ({config['executors_count']} executors)" for config in saved_configs],
            help="Select a saved multi-agent system to load"
        )
        
        if selected_config != "None":
            config_name = selected_config.split(" (")[0]
            config_to_load = next((c for c in saved_configs if c['name'] == config_name), None)
            
            if config_to_load:
                col1, col2 = st.columns(2)
                
                with col1:
                    if st.button("üì• Load", use_container_width=True):
                        config_data = load_multi_agent_config(config_to_load['file'])
                        if config_data:
                            st.session_state.executors = config_data.get("executors", {})
                            st.session_state.current_system_name = config_data.get("name", "")
                            st.success(f"‚úÖ Loaded '{config_name}' successfully!")
                            st.rerun()
                
                with col2:
                    if st.button("üóëÔ∏è Delete", use_container_width=True):
                        if delete_multi_agent_config(config_to_load['file']):
                            st.success(f"‚úÖ Deleted '{config_name}'")
                            st.rerun()
    else:
        st.info("No saved systems yet. Create and save your first one!")

def render_quick_templates():
    """Îπ†Î•∏ ÏãúÏûë ÌÖúÌîåÎ¶ø Î†åÎçîÎßÅ"""
    with st.expander("üöÄ Quick Start Templates", expanded=True):
        st.markdown("### Pre-configured Systems")
        
        if st.button("üî¨ Data Science Team", use_container_width=True):
            # Í∏∞Î≥∏ Data Science ÌåÄ Íµ¨ÏÑ±
            st.session_state.executors = {
                "Data_Preprocessor": {
                    "prompt": """You are a Data Preprocessing Expert who cleans, transforms, and prepares data for analysis.
You handle missing values, outliers, encoding, and feature scaling.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Document all transformations clearly.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
                    "tools": ["python_repl_ast"],
                    "created_at": datetime.now().isoformat()
                },
                "EDA_Specialist": {
                    "prompt": """You are an Exploratory Data Analysis Expert who uncovers hidden patterns and insights in data. 
You focus on understanding data structure, distributions, relationships, and anomalies.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Perform ALL your analysis on `df` in the SAME Python tool execution.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
                    "tools": ["python_repl_ast"],
                    "created_at": datetime.now().isoformat()
                },
                "Visualization_Expert": {
                    "prompt": """You are a Data Visualization Expert who creates compelling and insightful charts, graphs, and dashboards.
You excel at choosing the right visualization for the data and message.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Create visualizations using matplotlib, seaborn, or plotly.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
                    "tools": ["python_repl_ast"],
                    "created_at": datetime.now().isoformat()
                },
                "ML_Engineer": {
                    "prompt": """You are a Machine Learning Engineer who builds, trains, and evaluates predictive models.
You handle the full ML pipeline from data preparation to model deployment.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Use sklearn or other ML libraries for modeling.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
                    "tools": ["python_repl_ast"],
                    "created_at": datetime.now().isoformat()
                },
                "Statistical_Analyst": {
                    "prompt": """You are a Statistical Analysis Expert who performs rigorous statistical tests and modeling.
You derive meaningful insights through hypothesis testing and statistical inference.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool.
2. Use scipy.stats or statsmodels for analysis.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
                    "tools": ["python_repl_ast"],
                    "created_at": datetime.now().isoformat()
                },
                "Report_Writer": {
                    "prompt": """You are a Report Writing Expert who creates comprehensive analysis reports.
You summarize findings and communicate insights to stakeholders.

CRITICAL DATA HANDLING RULE: You MUST use the `python_repl_ast` tool for ALL data operations.
1. ALWAYS start by calling `df = get_current_data()` within the Python tool if data analysis is needed.
2. Focus on clear, actionable insights.
3. End with 'TASK COMPLETED: [Summary]' when finished.""",
                    "tools": ["python_repl_ast"],
                    "created_at": datetime.now().isoformat()
                }
            }
            
            st.success("‚úÖ Data Science Team template loaded!")
            st.rerun()

# Helper functions
def load_multi_agent_configs():
    """Load all saved multi-agent configurations"""
    config_dir = Path("multi-agent-configs")
    configs = []
    
    if config_dir.exists():
        for json_file in config_dir.glob("*.json"):
            try:
                with open(json_file, encoding="utf-8") as f:
                    config = json.load(f)
                    configs.append({
                        "name": config.get("name", json_file.stem),
                        "file": json_file.stem,
                        "created_at": config.get("created_at", "Unknown"),
                        "executors_count": len(config.get("executors", {})),
                        "description": config.get("description", "")
                    })
            except Exception as e:
                logging.error(f"Failed to load config {json_file}: {e}")
    
    return sorted(configs, key=lambda x: x.get("created_at", ""), reverse=True)

def load_multi_agent_config(filename: str):
    """Load a specific multi-agent configuration"""
    config_dir = Path("multi-agent-configs")
    config_file = config_dir / f"{filename}.json"
    
    if config_file.exists():
        with open(config_file, encoding="utf-8") as f:
            return json.load(f)
    return None

def delete_multi_agent_config(filename: str):
    """Delete a multi-agent configuration"""
    config_dir = Path("multi-agent-configs")
    config_file = config_dir / f"{filename}.json"
    
    if config_file.exists():
        config_file.unlink()
        return True
    return False

def save_multi_agent_config(name: str, config: dict):
    """Save multi-agent configuration to file"""
    config_dir = Path("multi-agent-configs")
    config_dir.mkdir(exist_ok=True)
    
    config_file = config_dir / f"{name}.json"
    
    # Save configuration
    save_data = {
        "name": name,
        "created_at": datetime.now().isoformat(),
        "executors": config.get("executors", {}),
        "description": config.get("description", "")
    }
    
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)
    
    return config_file