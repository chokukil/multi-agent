# File: ui/chat_interface.py
# Location: ./ui/chat_interface.py

import streamlit as st
import asyncio
import time
import logging
from typing import Dict, Tuple
from core import data_manager

MAX_HISTORY_LENGTH = 20  # ìµœëŒ€ ëŒ€í™” ê¸°ë¡ ìˆ˜ (ì‚¬ìš©ì ì…ë ¥ + ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ = 1í„´)

def render_chat_interface():
    """ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§ - ì‹¤ì‹œê°„ ë„êµ¬ í™œë™ í‘œì‹œ í¬í•¨"""
    st.markdown("### ğŸ’¬ Enhanced Multi-Agent Chat with Real-time Tool Monitoring")
    
    # ë°ì´í„° ìƒíƒœ í‘œì‹œ
    if data_manager.is_data_loaded():
        st.info(f"ğŸ“Š **Data Ready**: {data_manager.get_status_message()}")
    else:
        st.warning("ğŸ“‚ **No Data**: Upload CSV in sidebar for data analysis")
    
    # ëŒ€í™” ê¸°ë¡ ì¶œë ¥
    print_chat_history()
    
    # ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
    user_query = st.session_state.pop("user_query", None) or st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
    
    if user_query:
        # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
        st.chat_message("user", avatar="ğŸ§‘").write(user_query)
        st.session_state.history.append({"role": "user", "content": user_query})
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì²˜ë¦¬
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            # ì‘ë‹µ ì»¨í…Œì´ë„ˆ
            response_container = st.container()
            
            # ì‹¤ì‹œê°„ ë„êµ¬ í™œë™ ëª¨ë‹ˆí„°ë§ ì„¹ì…˜
            tool_activity_container = st.container()
            
            with response_container:
                st.markdown("### ğŸ“ Final Response:")
                text_placeholder = st.empty()
                text_placeholder.markdown("*ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...*")
            
            with tool_activity_container:
                tool_expander = st.expander("ğŸ”¬ ì‹¤í–‰ ê³¼ì • (Tool Activity)", expanded=True)
                with tool_expander:
                    tool_activity_placeholder = st.empty()
                    tool_activity_placeholder.markdown("*ë„êµ¬ ì‹¤í–‰ì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘...*")
            
            # ì¿¼ë¦¬ ì²˜ë¦¬
            resp, final_text, tool_activity_content = (
                st.session_state.event_loop.run_until_complete(
                    process_query_with_enhanced_streaming(
                        user_query,
                        text_placeholder,
                        tool_activity_placeholder,
                        st.session_state.get("timeout_seconds", 180)
                    )
                )
            )
            
            # ì™„ë£Œ í›„ ì •ë¦¬
            if tool_activity_content:
                with tool_activity_container:
                    with st.expander("ğŸ”¬ ì‹¤í–‰ ê³¼ì • (Tool Activity) - ì™„ë£Œ", expanded=False):
                        st.markdown(tool_activity_content)

        # ì‘ë‹µì„ ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        if isinstance(resp, dict) and "error" in resp:
            # ì˜¤ë¥˜ íƒ€ì…ì— ë”°ë¥¸ ë©”ì‹œì§€ ë¶„ë¥˜
            error_msg = resp.get("error", "An unknown error occurred.")
            
            if "timeout" in error_msg.lower():
                assistant_response = {
                    "role": "assistant",
                    "content": f"""## â° ì²˜ë¦¬ ì‹œê°„ ì´ˆê³¼

ìš”ì²­ ì²˜ë¦¬ê°€ ì œí•œ ì‹œê°„({st.session_state.get("timeout_seconds", 180)}ì´ˆ)ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤.

### ğŸ’¡ í•´ê²° ë°©ë²•:
- ë” ê°„ë‹¨í•œ ë¶„ì„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìš”ì²­í•´ ë³´ì„¸ìš”
- ì‚¬ì´ë“œë°”ì—ì„œ timeout ì„¤ì •ì„ ëŠ˜ë ¤ë³´ì„¸ìš”
- ë°ì´í„° í¬ê¸°ê°€ í° ê²½ìš° ìƒ˜í”Œë§ì„ ê³ ë ¤í•´ ë³´ì„¸ìš”

### ğŸ”§ ì‹œìŠ¤í…œ ìƒíƒœ:
- MCP ì„œë²„: {'âœ… ì—°ê²°ë¨' if st.session_state.get('mcp_client') else 'âŒ ì—°ê²° ì•ˆë¨'}
- Plan-Execute: {'âœ… ì´ˆê¸°í™”ë¨' if st.session_state.get('plan_execute_graph') else 'âŒ ì´ˆê¸°í™” ì•ˆë¨'}
""",
                    "error": True,
                    "error_type": "timeout"
                }
            elif "mcp" in error_msg.lower():
                assistant_response = {
                    "role": "assistant", 
                    "content": f"""## ğŸ”§ MCP ë„êµ¬ ì—°ê²° ì˜¤ë¥˜

MCP (Model Context Protocol) ë„êµ¬ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

### ğŸ’¡ í•´ê²° ë°©ë²•:
1. MCP ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
2. `system_start.bat`ë¥¼ ë‹¤ì‹œ ì‹¤í–‰
3. ê¸°ë³¸ ë„êµ¬ë§Œìœ¼ë¡œ ë¶„ì„ ì§„í–‰ ê°€ëŠ¥

### ğŸ“Š í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ê¸°ëŠ¥:
- âœ… ê¸°ë³¸ ë°ì´í„° ë¶„ì„ (Python)
- âœ… ì‹œê°í™” (matplotlib, plotly)
- âœ… í†µê³„ ë¶„ì„ (pandas, numpy)
- âŒ ê³ ê¸‰ MCP ë„êµ¬ë“¤

ì˜¤ë¥˜ ìƒì„¸: {error_msg}
""",
                    "error": True,
                    "error_type": "mcp"
                }
            else:
                assistant_response = {
                    "role": "assistant",
                    "content": f"""## âŒ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ

ë¶„ì„ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

### ğŸ” ì˜¤ë¥˜ ì •ë³´:
```
{error_msg}
```

### ğŸ’¡ í•´ê²° ë°©ë²•:
1. ìš”ì²­ì„ ë‹¤ì‹œ ì‘ì„±í•´ ë³´ì„¸ìš”
2. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ìš”ì²­í•´ ë³´ì„¸ìš”
3. ë°ì´í„°ê°€ ì˜¬ë°”ë¥´ê²Œ ì—…ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”

ì‹œìŠ¤í…œì´ ì—¬ì „íˆ ì‘ë™ ì¤‘ì´ë¯€ë¡œ ë‹¤ë¥¸ ìš”ì²­ì„ ì‹œë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
""",
                    "error": True,
                    "error_type": "general"
                }
        else:
            assistant_response = {
                "role": "assistant",
                "content": final_text,
                "tool_activity": tool_activity_content
            }
        st.session_state.history.append(assistant_response)
        
        # ëŒ€í™” ê¸°ë¡ ê¸¸ì´ ì œí•œ (user + assistant = 2 messages per turn)
        while len(st.session_state.history) > MAX_HISTORY_LENGTH * 2:
            st.session_state.history.pop(0)

        st.rerun()

def print_chat_history():
    """ëŒ€í™” ê¸°ë¡ ì¶œë ¥"""
    if "history" not in st.session_state:
        st.session_state.history = []
    
    for message in st.session_state.history:
        if message["role"] == "user":
            st.chat_message("user", avatar="ğŸ§‘").write(message["content"])
        elif message["role"] == "assistant":
            with st.chat_message("assistant", avatar="ğŸ¤–"):
                st.write(message["content"])
                
                # ì—ëŸ¬ ë©”ì‹œì§€ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ë„êµ¬ í™œë™ í‘œì‹œ
                if not message.get("error") and message.get("tool_activity"):
                    with st.expander("ğŸ”¬ ì‹¤í–‰ ê³¼ì • (Tool Activity)", expanded=False):
                        st.markdown(message["tool_activity"])

async def process_query_with_enhanced_streaming(
    query: str,
    text_placeholder,
    tool_activity_placeholder,
    timeout_seconds: int = 180
) -> Tuple[Dict, str, str]:
    """í–¥ìƒëœ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì¿¼ë¦¬ ì²˜ë¦¬"""
    from core.utils.streaming import astream_graph, get_plan_execute_streaming_callback
    
    logging.info(f"Processing query with enhanced streaming: {query}")
    
    start_time = time.time()
    
    try:
        if st.session_state.plan_execute_graph:
            # ìƒˆë¡œìš´ ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì„¤ì •
            streaming_callback = get_plan_execute_streaming_callback(tool_activity_placeholder)
            
            # ìµœì¢… ì‘ë‹µ ìˆ˜ì§‘ì„ ìœ„í•œ ë³€ìˆ˜
            final_text_parts = []
            
            def response_callback(msg):
                """ìµœì¢… ì‘ë‹µ ìˆ˜ì§‘ ì½œë°±"""
                node = msg.get("node", "")
                content = msg.get("content")
                
                # ğŸ†• Final Responder ì‘ë‹µ ì²˜ë¦¬ ê°•í™”
                if node == "final_responder":
                    if hasattr(content, "content"):
                        final_response = content.content
                    elif isinstance(content, str):
                        final_response = content
                    else:
                        final_response = str(content)
                    
                    final_text_parts.append(final_response)
                    text_placeholder.markdown(final_response)
                    logging.info(f"âœ… Final response displayed: {len(final_response)} characters")
            
            # í†µí•© ì½œë°±
            def combined_callback(msg):
                streaming_callback(msg)
                response_callback(msg)
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            from langchain_core.messages import HumanMessage
            from langchain_core.runnables import RunnableConfig
            
            # ë””ë²„ê¹…: ì„¸ì…˜ ì •ë³´ í™•ì¸
            session_id = st.session_state.get('thread_id', 'default-session')
            user_id = st.session_state.get('user_id', 'default-user')
            logging.info(f"ğŸ” Chat Interface - Session ID: {session_id}")
            logging.info(f"ğŸ” Chat Interface - User ID: {user_id}")
            logging.info(f"ğŸ” Chat Interface - st.session_state keys: {list(st.session_state.keys())}")
            
            config = RunnableConfig(
                recursion_limit=st.session_state.get("recursion_limit", 30),
                configurable={"thread_id": st.session_state.thread_id}
            )
            
            initial_state = {
                "messages": [HumanMessage(content=query)],
                "session_id": session_id,
                "user_id": user_id
            }
            logging.info(f"ğŸ” Chat Interface - Initial state: {initial_state}")
            
            response = await asyncio.wait_for(
                astream_graph(
                    st.session_state.plan_execute_graph,
                    initial_state,
                    callback=combined_callback,
                    config=config
                ),
                timeout=timeout_seconds
            )
            
            # ìµœì¢… í…ìŠ¤íŠ¸ ì¶”ì¶œ - ê°œì„ ëœ ë¡œì§
            final_text = "".join(final_text_parts)
            
            # --- ğŸ›Ÿ UI ìµœì¢… ì•ˆì „ë§ ---
            # ë§Œì•½ ëª¨ë“  ê³¼ì •ì´ ëë‚¬ëŠ”ë°ë„ final_textê°€ ë¹„ì–´ìˆë‹¤ë©´, 
            # UI ë‹¨ì—ì„œ ìµœì†Œí•œì˜ ì‘ë‹µì„ ë³´ì¥í•©ë‹ˆë‹¤.
            if not final_text.strip():
                logging.warning("No final text was generated from the graph. Displaying UI fallback message.")
                final_text = """
### âœ… ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ

ì‹œìŠ¤í…œì´ ëª¨ë“  ë¶„ì„ ë‹¨ê³„ë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. 
í•˜ì§€ë§Œ ìµœì¢… ìš”ì•½ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí•œ ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.

**ìƒì„±ëœ ê²°ê³¼ëŠ” ìš°ì¸¡ì˜ 'ì•„í‹°íŒ©íŠ¸' íŒ¨ë„ ë˜ëŠ” ìœ„ì˜ 'ì‹¤í–‰ ê³¼ì •'ì—ì„œ ì§ì ‘ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

ë§Œì•½ ê²°ê³¼ê°€ ë§Œì¡±ìŠ¤ëŸ½ì§€ ì•Šë‹¤ë©´, ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë‹¤ì‹œ ì‹œë„í•´ ë³´ì„¸ìš”.
"""
                text_placeholder.markdown(final_text)

            tool_activity_content = tool_activity_placeholder.markdown_content if hasattr(tool_activity_placeholder, 'markdown_content') else ""
            
            duration = time.time() - start_time
            logging.info(f"Query processed in {duration:.2f}s")
            
            return response, final_text, tool_activity_content
            
        else:
            error_msg = "ğŸš« Plan-Execute system not initialized"
            return {"error": error_msg}, error_msg, ""
            
    except asyncio.TimeoutError:
        error_msg = f"â° Query timed out after {timeout_seconds} seconds"
        logging.error(error_msg)
        return {"error": error_msg}, error_msg, ""
        
    except Exception as e:
        error_msg = f"âŒ Error processing query: {str(e)}"
        logging.error(f"Query processing error: {e}", exc_info=True)
        return {"error": error_msg}, error_msg, ""

async def process_query_with_plan_execute(
    query: str,
    plan_placeholder,
    progress_placeholder,
    text_placeholder,
    tool_placeholder,
    timeout_seconds: int = 180
) -> Tuple[Dict, Dict, str, str, str]:
    """Plan-Execute íŒ¨í„´ìœ¼ë¡œ ì¿¼ë¦¬ ì²˜ë¦¬ (ë ˆê±°ì‹œ í˜¸í™˜ì„±)"""
    from core import astream_graph, get_streaming_callback
    
    logging.info(f"Processing query with Plan-Execute pattern: {query}")
    
    start_time = time.time()
    plan_info = {}
    progress_info = ""
    
    try:
        if st.session_state.plan_execute_graph:
            # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì„¤ì •
            streaming_callback, accumulated_text, accumulated_tool = (
                get_streaming_callback(text_placeholder, tool_placeholder)
            )
            
            # Plan í‘œì‹œ ì½œë°±
            def plan_callback(msg):
                if msg.get("node") == "planner" and "plan" in msg.get("content", {}):
                    plan_info.update(msg["content"]["plan"])
                    plan_placeholder.json(plan_info)
                    
            # Progress í‘œì‹œ ì½œë°±
            def progress_callback(msg):
                if msg.get("node") == "replanner":
                    content = msg.get("content", {})
                    if "progress" in content:
                        progress_placeholder.progress(
                            content["progress"]["completed"] / content["progress"]["total"],
                            text=content["progress"]["text"]
                        )
            
            # í†µí•© ì½œë°±
            def combined_callback(msg):
                streaming_callback(msg)
                plan_callback(msg)
                progress_callback(msg)
            
            # ê·¸ë˜í”„ ì‹¤í–‰
            from langchain_core.messages import HumanMessage
            from langchain_core.runnables import RunnableConfig
            
            # ë””ë²„ê¹…: ì„¸ì…˜ ì •ë³´ í™•ì¸
            session_id = st.session_state.get('thread_id', 'default-session')
            user_id = st.session_state.get('user_id', 'default-user')
            logging.info(f"ğŸ” Legacy Chat Interface - Session ID: {session_id}")
            logging.info(f"ğŸ” Legacy Chat Interface - User ID: {user_id}")
            
            config = RunnableConfig(
                recursion_limit=30,
                configurable={"thread_id": st.session_state.thread_id}
            )
            
            initial_state = {
                "messages": [HumanMessage(content=query)],
                "session_id": session_id,
                "user_id": user_id
            }
            logging.info(f"ğŸ” Legacy Chat Interface - Initial state: {initial_state}")
            
            response = await asyncio.wait_for(
                astream_graph(
                    st.session_state.plan_execute_graph,
                    initial_state,
                    callback=combined_callback,
                    config=config
                ),
                timeout=timeout_seconds
            )
            
            final_text = "".join(accumulated_text)
            final_tool = "".join(accumulated_tool)
            
            # ìµœì¢… ì‘ë‹µì´ ì—†ë‹¤ë©´ responseì—ì„œ ì¶”ì¶œ
            if not final_text and response:
                if isinstance(response, dict) and "messages" in response:
                    for msg in reversed(response["messages"]):
                        if hasattr(msg, "name") and msg.name == "Final_Responder":
                            final_text = msg.content
                            break
            
            duration = time.time() - start_time
            logging.info(f"Query processed in {duration:.2f}s")
            
            return response, plan_info, progress_info, final_text, final_tool
            
        else:
            error_msg = "ğŸš« Plan-Execute system not initialized"
            return {"error": error_msg}, {}, "", error_msg, ""
            
    except asyncio.TimeoutError:
        error_msg = f"â±ï¸ Request timed out after {timeout_seconds} seconds"
        return {"error": error_msg}, plan_info, progress_info, error_msg, ""
        
    except Exception as e:
        error_msg = f"âŒ Error: {str(e)}"
        logging.error(f"Query processing error: {e}")
        return {"error": error_msg}, plan_info, progress_info, error_msg, ""

def render_system_status():
    """ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ"""
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        executors_count = len(st.session_state.get("executors", {}))
        st.metric("Executors", executors_count)
    
    with col2:
        messages_count = len(st.session_state.get("history", []))
        st.metric("Messages", messages_count)
    
    with col3:
        status = "âœ… Ready" if st.session_state.get("graph_initialized") else "âš ï¸ Not Initialized"
        st.metric("System", status)
    
    with col4:
        has_data = "âœ… Loaded" if data_manager.is_data_loaded() else "âŒ Empty"
        st.metric("Data", has_data)