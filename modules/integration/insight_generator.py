"""
ë©€í‹° ì—ì´ì „íŠ¸ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ í†µí•©ëœ A2A ì—ì´ì „íŠ¸ ê²°ê³¼ì—ì„œ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ ,
íŒ¨í„´ ë¶„ì„ ë° íŠ¸ë Œë“œ ì‹ë³„ì„ í†µí•´ ë¹„ì¦ˆë‹ˆìŠ¤ ê°€ì¹˜ë¥¼ ì°½ì¶œí•˜ëŠ” ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ìë™ ì¶”ì¶œ
- ë°ì´í„° íŒ¨í„´ ë° íŠ¸ë Œë“œ ë¶„ì„
- ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ í‰ê°€
- ìƒê´€ê´€ê³„ ë° ì´ìƒì¹˜ ê°ì§€
"""

import json
import logging
import re
import statistics
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple, Union
from enum import Enum
import pandas as pd

from .result_integrator import IntegrationResult
from .agent_result_collector import AgentResult

logger = logging.getLogger(__name__)

class InsightType(Enum):
    """ì¸ì‚¬ì´íŠ¸ ìœ í˜•"""
    TREND = "trend"                      # íŠ¸ë Œë“œ ë¶„ì„
    PATTERN = "pattern"                  # íŒ¨í„´ ì‹ë³„
    CORRELATION = "correlation"          # ìƒê´€ê´€ê³„
    ANOMALY = "anomaly"                 # ì´ìƒì¹˜/ì˜ˆì™¸
    DISTRIBUTION = "distribution"        # ë¶„í¬ ë¶„ì„
    COMPARISON = "comparison"            # ë¹„êµ ë¶„ì„
    STATISTICAL = "statistical"          # í†µê³„ì  ë°œê²¬
    BUSINESS_IMPACT = "business_impact"   # ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥

class InsightPriority(Enum):
    """ì¸ì‚¬ì´íŠ¸ ìš°ì„ ìˆœìœ„"""
    CRITICAL = "critical"    # ì¦‰ì‹œ ì£¼ëª© í•„ìš”
    HIGH = "high"           # ë†’ì€ ìš°ì„ ìˆœìœ„
    MEDIUM = "medium"       # ë³´í†µ ìš°ì„ ìˆœìœ„
    LOW = "low"            # ë‚®ì€ ìš°ì„ ìˆœìœ„
    INFORMATIONAL = "informational"  # ì°¸ê³ ìš©

@dataclass
class Insight:
    """ê°œë³„ ì¸ì‚¬ì´íŠ¸"""
    insight_id: str
    title: str
    description: str
    insight_type: InsightType
    priority: InsightPriority
    
    # ì‹ ë¢°ë„ ë° ì˜í–¥ë„
    confidence: float  # 0.0 ~ 1.0
    impact_score: float  # 0.0 ~ 1.0
    
    # ì§€ì› ë°ì´í„°
    supporting_data: Dict[str, Any] = field(default_factory=dict)
    evidence: List[str] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    source_agents: List[str] = field(default_factory=list)
    data_sources: List[str] = field(default_factory=list)
    generated_at: datetime = field(default_factory=datetime.now)
    
    # ì•¡ì…˜ ì•„ì´í…œ
    recommended_actions: List[str] = field(default_factory=list)
    follow_up_questions: List[str] = field(default_factory=list)

@dataclass
class InsightAnalysis:
    """ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ê²°ê³¼"""
    session_id: str
    total_insights: int
    insights: List[Insight] = field(default_factory=list)
    
    # ìš”ì•½ í†µê³„
    insights_by_type: Dict[InsightType, int] = field(default_factory=dict)
    insights_by_priority: Dict[InsightPriority, int] = field(default_factory=dict)
    
    # í’ˆì§ˆ ì§€í‘œ
    average_confidence: float = 0.0
    average_impact: float = 0.0
    overall_quality_score: float = 0.0
    
    # ì£¼ìš” ë°œê²¬ì‚¬í•­
    key_findings: List[str] = field(default_factory=list)
    top_insights: List[Insight] = field(default_factory=list)
    
    # ì¶”ê°€ ì •ë³´
    processing_notes: List[str] = field(default_factory=list)
    generation_time: datetime = field(default_factory=datetime.now)

class InsightGenerator:
    """ë©€í‹° ì—ì´ì „íŠ¸ ì¸ì‚¬ì´íŠ¸ ìƒì„±ê¸°"""
    
    def __init__(self):
        # ì„ê³„ê°’ ì„¤ì •
        self.thresholds = {
            'correlation_significance': 0.7,      # ìƒê´€ê´€ê³„ ìœ ì˜ì„±
            'trend_confidence': 0.6,              # íŠ¸ë Œë“œ ì‹ ë¢°ë„
            'anomaly_threshold': 2.0,             # ì´ìƒì¹˜ ì„ê³„ê°’ (í‘œì¤€í¸ì°¨)
            'pattern_frequency': 0.3,             # íŒ¨í„´ ë¹ˆë„
            'statistical_significance': 0.05      # í†µê³„ì  ìœ ì˜ì„±
        }
        
        # í‚¤ì›Œë“œ íŒ¨í„´
        self.trend_keywords = [
            'ì¦ê°€', 'increase', 'ê°ì†Œ', 'decrease', 'ìƒìŠ¹', 'rise',
            'í•˜ë½', 'decline', 'ê°œì„ ', 'improve', 'ì•…í™”', 'worsen'
        ]
        
        self.pattern_keywords = [
            'íŒ¨í„´', 'pattern', 'ê·œì¹™', 'rule', 'ì£¼ê¸°', 'cycle',
            'ë°˜ë³µ', 'repeat', 'ê²½í–¥', 'tendency', 'íŠ¹ì„±', 'characteristic'
        ]
        
        self.comparison_keywords = [
            'ë¹„êµ', 'compare', 'ì°¨ì´', 'difference', 'ëŒ€ë¹„', 'vs',
            'ë†’ì€', 'higher', 'ë‚®ì€', 'lower', 'ë§ì€', 'more', 'ì ì€', 'less'
        ]
        
        # í†µê³„ íŒ¨í„´
        self.statistical_patterns = [
            r'(\d+(?:\.\d+)?)\s*%',                    # ë°±ë¶„ìœ¨
            r'í‰ê· .*?(\d+(?:\.\d+)?)',                  # í‰ê· ê°’
            r'ìµœëŒ€.*?(\d+(?:\.\d+)?)',                  # ìµœëŒ€ê°’
            r'ìµœì†Œ.*?(\d+(?:\.\d+)?)',                  # ìµœì†Œê°’
            r'í‘œì¤€í¸ì°¨.*?(\d+(?:\.\d+)?)',              # í‘œì¤€í¸ì°¨
            r'(\d+(?:\.\d+)?)\s*ë°°',                   # ë°°ìˆ˜ ê´€ê³„
        ]
    
    def generate_insights(self, 
                         integration_result: IntegrationResult,
                         agent_results: Dict[str, AgentResult] = None) -> InsightAnalysis:
        """í†µí•© ê²°ê³¼ì—ì„œ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        logger.info(f"ğŸ§  ì¸ì‚¬ì´íŠ¸ ìƒì„± ì‹œì‘ - ì„¸ì…˜ {integration_result.session_id}")
        
        analysis = InsightAnalysis(
            session_id=integration_result.session_id,
            total_insights=0
        )
        
        try:
            insights = []
            
            # 1. í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
            if integration_result.integrated_text:
                text_insights = self._extract_text_insights(
                    integration_result.integrated_text,
                    integration_result.contributing_agents
                )
                insights.extend(text_insights)
            
            # 2. ì•„í‹°íŒ©íŠ¸ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
            if integration_result.integrated_artifacts:
                artifact_insights = self._extract_artifact_insights(
                    integration_result.integrated_artifacts,
                    integration_result.contributing_agents
                )
                insights.extend(artifact_insights)
            
            # 3. í¬ë¡œìŠ¤ ë¶„ì„ ì¸ì‚¬ì´íŠ¸
            if agent_results:
                cross_insights = self._extract_cross_analysis_insights(
                    agent_results, integration_result
                )
                insights.extend(cross_insights)
            
            # 4. ì¸ì‚¬ì´íŠ¸ í’ˆì§ˆ í‰ê°€ ë° í•„í„°ë§
            qualified_insights = self._evaluate_and_filter_insights(insights)
            
            # 5. ì¸ì‚¬ì´íŠ¸ ìš°ì„ ìˆœìœ„ ê²°ì •
            prioritized_insights = self._prioritize_insights(qualified_insights)
            
            analysis.insights = prioritized_insights
            analysis.total_insights = len(prioritized_insights)
            
            # 6. ë¶„ì„ ê²°ê³¼ ì§‘ê³„
            self._aggregate_analysis(analysis)
            
            # 7. ì£¼ìš” ë°œê²¬ì‚¬í•­ ìƒì„±
            analysis.key_findings = self._generate_key_findings(prioritized_insights)
            analysis.top_insights = prioritized_insights[:5]  # ìƒìœ„ 5ê°œ
            
            logger.info(f"âœ… ì¸ì‚¬ì´íŠ¸ ìƒì„± ì™„ë£Œ - {analysis.total_insights}ê°œ ì¸ì‚¬ì´íŠ¸, "
                       f"í’ˆì§ˆì ìˆ˜: {analysis.overall_quality_score:.3f}")
            
        except Exception as e:
            logger.error(f"âŒ ì¸ì‚¬ì´íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            analysis.processing_notes.append(f"ìƒì„± ì˜¤ë¥˜: {str(e)}")
        
        return analysis
    
    def _extract_text_insights(self, 
                             text: str, 
                             source_agents: List[str]) -> List[Insight]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        
        insights = []
        
        try:
            # 1. íŠ¸ë Œë“œ ì¸ì‚¬ì´íŠ¸
            trend_insights = self._identify_trends(text, source_agents)
            insights.extend(trend_insights)
            
            # 2. íŒ¨í„´ ì¸ì‚¬ì´íŠ¸
            pattern_insights = self._identify_patterns(text, source_agents)
            insights.extend(pattern_insights)
            
            # 3. ë¹„êµ ì¸ì‚¬ì´íŠ¸
            comparison_insights = self._identify_comparisons(text, source_agents)
            insights.extend(comparison_insights)
            
            # 4. í†µê³„ì  ì¸ì‚¬ì´íŠ¸
            statistical_insights = self._identify_statistical_insights(text, source_agents)
            insights.extend(statistical_insights)
            
        except Exception as e:
            logger.warning(f"í…ìŠ¤íŠ¸ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _extract_artifact_insights(self, 
                                 artifacts: List[Dict[str, Any]], 
                                 source_agents: List[str]) -> List[Insight]:
        """ì•„í‹°íŒ©íŠ¸ì—ì„œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        
        insights = []
        
        try:
            for artifact in artifacts:
                art_type = artifact.get('type', 'unknown')
                
                if art_type == 'plotly_chart':
                    chart_insights = self._analyze_chart_insights(artifact, source_agents)
                    insights.extend(chart_insights)
                
                elif art_type == 'dataframe':
                    df_insights = self._analyze_dataframe_insights(artifact, source_agents)
                    insights.extend(df_insights)
                
                elif art_type == 'statistical_summary':
                    stats_insights = self._analyze_statistical_summary_insights(artifact, source_agents)
                    insights.extend(stats_insights)
            
        except Exception as e:
            logger.warning(f"ì•„í‹°íŒ©íŠ¸ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _extract_cross_analysis_insights(self, 
                                       agent_results: Dict[str, AgentResult],
                                       integration_result: IntegrationResult) -> List[Insight]:
        """í¬ë¡œìŠ¤ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        
        insights = []
        
        try:
            # 1. ì—ì´ì „íŠ¸ ê°„ ì¼ê´€ì„± ë¶„ì„
            consistency_insights = self._analyze_agent_consistency(
                agent_results, integration_result.contributing_agents
            )
            insights.extend(consistency_insights)
            
            # 2. ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸
            quality_insights = self._analyze_data_quality_insights(
                agent_results, integration_result.contributing_agents
            )
            insights.extend(quality_insights)
            
            # 3. ì»¤ë²„ë¦¬ì§€ ë¶„ì„
            coverage_insights = self._analyze_coverage_insights(
                agent_results, integration_result
            )
            insights.extend(coverage_insights)
            
        except Exception as e:
            logger.warning(f"í¬ë¡œìŠ¤ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _identify_trends(self, text: str, source_agents: List[str]) -> List[Insight]:
        """íŠ¸ë Œë“œ ì‹ë³„"""
        
        insights = []
        text_lower = text.lower()
        
        try:
            for keyword in self.trend_keywords:
                if keyword in text_lower:
                    # í‚¤ì›Œë“œ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
                    trend_context = self._extract_context_around_keyword(text, keyword)
                    
                    if trend_context:
                        # ìˆ˜ì¹˜ ì •ë³´ ì¶”ì¶œ
                        numbers = re.findall(r'\d+(?:\.\d+)?%?', trend_context)
                        
                        confidence = 0.6
                        impact = 0.5
                        
                        # ìˆ˜ì¹˜ê°€ ìˆìœ¼ë©´ ì‹ ë¢°ë„ í–¥ìƒ
                        if numbers:
                            confidence += 0.2
                            impact += 0.2
                        
                        insight = Insight(
                            insight_id=f"trend_{len(insights)}_{keyword}",
                            title=f"{keyword.capitalize()} íŠ¸ë Œë“œ ë°œê²¬",
                            description=trend_context,
                            insight_type=InsightType.TREND,
                            priority=InsightPriority.MEDIUM,
                            confidence=min(1.0, confidence),
                            impact_score=min(1.0, impact),
                            supporting_data={"keyword": keyword, "numbers": numbers},
                            evidence=[trend_context],
                            source_agents=source_agents,
                            recommended_actions=[f"{keyword} íŠ¸ë Œë“œì— ëŒ€í•œ ì¶”ê°€ ë¶„ì„ ìˆ˜í–‰"]
                        )
                        
                        insights.append(insight)
        
        except Exception as e:
            logger.warning(f"íŠ¸ë Œë“œ ì‹ë³„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights[:3]  # ìµœëŒ€ 3ê°œ
    
    def _identify_patterns(self, text: str, source_agents: List[str]) -> List[Insight]:
        """íŒ¨í„´ ì‹ë³„"""
        
        insights = []
        
        try:
            # ë°˜ë³µë˜ëŠ” êµ¬ì¡° íŒ¨í„´ ì°¾ê¸°
            sentences = re.split(r'[.!?]+', text)
            sentence_patterns = defaultdict(list)
            
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 20:
                    # ë¬¸ì¥ êµ¬ì¡° íŒ¨í„´ (ê°„ë‹¨í•œ ë°©ì‹)
                    words = sentence.lower().split()
                    if len(words) >= 3:
                        pattern = f"{words[0]}...{words[-1]}"
                        sentence_patterns[pattern].append(sentence)
            
            # ë°˜ë³µë˜ëŠ” íŒ¨í„´ ì‹ë³„
            for pattern, sentences in sentence_patterns.items():
                if len(sentences) >= 2:  # 2ë²ˆ ì´ìƒ ë°˜ë³µ
                    insight = Insight(
                        insight_id=f"pattern_{len(insights)}_{hash(pattern)}",
                        title=f"ë°˜ë³µ íŒ¨í„´ ë°œê²¬: {pattern}",
                        description=f"ë‹¤ìŒ êµ¬ì¡°ê°€ {len(sentences)}ë²ˆ ë°˜ë³µë¨: {sentences[0][:100]}...",
                        insight_type=InsightType.PATTERN,
                        priority=InsightPriority.LOW,
                        confidence=0.5 + min(0.4, len(sentences) * 0.1),
                        impact_score=0.4,
                        supporting_data={"pattern": pattern, "occurrences": len(sentences)},
                        evidence=sentences[:3],  # ìµœëŒ€ 3ê°œ ì˜ˆì‹œ
                        source_agents=source_agents,
                        recommended_actions=["íŒ¨í„´ì˜ ì›ì¸ê³¼ ì˜ë¯¸ ë¶„ì„"]
                    )
                    
                    insights.append(insight)
        
        except Exception as e:
            logger.warning(f"íŒ¨í„´ ì‹ë³„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights[:2]  # ìµœëŒ€ 2ê°œ
    
    def _identify_comparisons(self, text: str, source_agents: List[str]) -> List[Insight]:
        """ë¹„êµ ë¶„ì„ ì‹ë³„"""
        
        insights = []
        
        try:
            for keyword in self.comparison_keywords:
                if keyword in text.lower():
                    comparison_context = self._extract_context_around_keyword(text, keyword)
                    
                    if comparison_context:
                        # ë¹„êµ ëŒ€ìƒê³¼ ìˆ˜ì¹˜ ì¶”ì¶œ
                        numbers = re.findall(r'\d+(?:\.\d+)?%?', comparison_context)
                        
                        if len(numbers) >= 2:  # ë¹„êµí•  ìˆ˜ì¹˜ê°€ ìˆëŠ” ê²½ìš°
                            try:
                                # ìˆ˜ì¹˜ ê°„ ì°¨ì´ ê³„ì‚°
                                nums = [float(n.replace('%', '')) for n in numbers[:2]]
                                difference = abs(nums[0] - nums[1])
                                
                                impact = min(1.0, difference / max(nums) if max(nums) > 0 else 0.5)
                                
                                insight = Insight(
                                    insight_id=f"comparison_{len(insights)}_{keyword}",
                                    title=f"ìœ ì˜ë¯¸í•œ ì°¨ì´ ë°œê²¬: {keyword}",
                                    description=comparison_context,
                                    insight_type=InsightType.COMPARISON,
                                    priority=InsightPriority.MEDIUM if impact > 0.3 else InsightPriority.LOW,
                                    confidence=0.7,
                                    impact_score=impact,
                                    supporting_data={
                                        "keyword": keyword,
                                        "numbers": numbers,
                                        "difference": difference
                                    },
                                    evidence=[comparison_context],
                                    source_agents=source_agents,
                                    recommended_actions=["ì°¨ì´ì˜ ì›ì¸ ë¶„ì„", "ë¹„êµ ê¸°ì¤€ ì¬ê²€í† "]
                                )
                                
                                insights.append(insight)
                                
                            except ValueError:
                                continue
        
        except Exception as e:
            logger.warning(f"ë¹„êµ ë¶„ì„ ì‹ë³„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights[:3]  # ìµœëŒ€ 3ê°œ
    
    def _identify_statistical_insights(self, text: str, source_agents: List[str]) -> List[Insight]:
        """í†µê³„ì  ì¸ì‚¬ì´íŠ¸ ì‹ë³„"""
        
        insights = []
        
        try:
            for pattern in self.statistical_patterns:
                matches = re.findall(pattern, text, re.IGNORECASE)
                
                if matches:
                    for match in matches[:2]:  # ìµœëŒ€ 2ê°œ
                        try:
                            value = float(match) if isinstance(match, str) else float(match[0])
                            
                            # í†µê³„ì  ì˜ë¯¸ í‰ê°€
                            stat_type = self._classify_statistical_value(pattern, value)
                            
                            insight = Insight(
                                insight_id=f"statistical_{len(insights)}_{stat_type}",
                                title=f"í†µê³„ì  ë°œê²¬: {stat_type}",
                                description=f"{stat_type} ê°’: {value}",
                                insight_type=InsightType.STATISTICAL,
                                priority=InsightPriority.MEDIUM,
                                confidence=0.8,
                                impact_score=self._evaluate_statistical_impact(stat_type, value),
                                supporting_data={"type": stat_type, "value": value},
                                evidence=[f"{stat_type}: {value}"],
                                source_agents=source_agents,
                                recommended_actions=[f"{stat_type} ê°’ì˜ ì˜ë¯¸ì™€ ì›ì¸ ë¶„ì„"]
                            )
                            
                            insights.append(insight)
                            
                        except ValueError:
                            continue
        
        except Exception as e:
            logger.warning(f"í†µê³„ì  ì¸ì‚¬ì´íŠ¸ ì‹ë³„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights[:3]  # ìµœëŒ€ 3ê°œ
    
    def _analyze_chart_insights(self, 
                              chart_artifact: Dict[str, Any], 
                              source_agents: List[str]) -> List[Insight]:
        """ì°¨íŠ¸ ì•„í‹°íŒ©íŠ¸ ë¶„ì„"""
        
        insights = []
        
        try:
            chart_data = chart_artifact.get('data', {})
            chart_layout = chart_artifact.get('layout', {})
            
            # ì°¨íŠ¸ ì œëª©ì—ì„œ ì¸ì‚¬ì´íŠ¸ ë‹¨ì„œ ì¶”ì¶œ
            title = chart_layout.get('title', {}).get('text', '')
            
            if title:
                insight = Insight(
                    insight_id=f"chart_insight_{hash(title)}",
                    title=f"ì‹œê°í™” ì¸ì‚¬ì´íŠ¸: {title}",
                    description=f"ì°¨íŠ¸ '{title}'ì—ì„œ ì¤‘ìš”í•œ íŒ¨í„´ì´ë‚˜ íŠ¸ë Œë“œê°€ ì‹ë³„ë¨",
                    insight_type=InsightType.PATTERN,
                    priority=InsightPriority.MEDIUM,
                    confidence=0.6,
                    impact_score=0.7,
                    supporting_data={"chart_title": title, "chart_type": chart_artifact.get('type')},
                    evidence=[f"ì°¨íŠ¸ ì œëª©: {title}"],
                    source_agents=source_agents,
                    recommended_actions=["ì°¨íŠ¸ ë°ì´í„°ì˜ ìƒì„¸ ë¶„ì„", "íŠ¸ë Œë“œ ì§€ì†ì„± ê²€í† "]
                )
                
                insights.append(insight)
            
            # ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ ë¶„ì„
            if isinstance(chart_data, list) and chart_data:
                data_points = len(chart_data[0].get('y', [])) if chart_data[0].get('y') else 0
                
                if data_points > 100:
                    insight = Insight(
                        insight_id=f"chart_volume_{data_points}",
                        title="ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ë¶„ì„",
                        description=f"ì°¨íŠ¸ì— {data_points}ê°œì˜ ë°ì´í„° í¬ì¸íŠ¸ê°€ í¬í•¨ë¨",
                        insight_type=InsightType.STATISTICAL,
                        priority=InsightPriority.LOW,
                        confidence=0.9,
                        impact_score=0.4,
                        supporting_data={"data_points": data_points},
                        evidence=[f"ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜: {data_points}"],
                        source_agents=source_agents,
                        recommended_actions=["ë°ì´í„° ìƒ˜í”Œë§ ì „ëµ ê²€í† ", "ì„±ëŠ¥ ìµœì í™” ê³ ë ¤"]
                    )
                    
                    insights.append(insight)
        
        except Exception as e:
            logger.warning(f"ì°¨íŠ¸ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _analyze_dataframe_insights(self, 
                                  df_artifact: Dict[str, Any], 
                                  source_agents: List[str]) -> List[Insight]:
        """ë°ì´í„°í”„ë ˆì„ ì•„í‹°íŒ©íŠ¸ ë¶„ì„"""
        
        insights = []
        
        try:
            df_data = df_artifact.get('data', [])
            df_columns = df_artifact.get('columns', [])
            
            if df_data and df_columns:
                row_count = len(df_data)
                col_count = len(df_columns)
                
                # ë°ì´í„° ê·œëª¨ ì¸ì‚¬ì´íŠ¸
                if row_count > 1000:
                    priority = InsightPriority.MEDIUM if row_count > 10000 else InsightPriority.LOW
                    
                    insight = Insight(
                        insight_id=f"df_scale_{row_count}_{col_count}",
                        title=f"ëŒ€ê·œëª¨ ë°ì´í„°ì…‹: {row_count:,}í–‰ x {col_count}ì—´",
                        description=f"ë¶„ì„ ëŒ€ìƒ ë°ì´í„°ê°€ {row_count:,}ê°œ ë ˆì½”ë“œ, {col_count}ê°œ í•„ë“œë¡œ êµ¬ì„±ë¨",
                        insight_type=InsightType.STATISTICAL,
                        priority=priority,
                        confidence=1.0,
                        impact_score=min(1.0, row_count / 10000),
                        supporting_data={"rows": row_count, "columns": col_count},
                        evidence=[f"ë°ì´í„° ê·œëª¨: {row_count:,} x {col_count}"],
                        source_agents=source_agents,
                        recommended_actions=["ë°ì´í„° í’ˆì§ˆ ê²€ì¦", "ì²˜ë¦¬ ì„±ëŠ¥ ìµœì í™”"]
                    )
                    
                    insights.append(insight)
                
                # ì»¬ëŸ¼ íƒ€ì… ë‹¤ì–‘ì„± ë¶„ì„
                if col_count > 10:
                    insight = Insight(
                        insight_id=f"df_complexity_{col_count}",
                        title=f"ë‹¤ì°¨ì› ë°ì´í„° ë¶„ì„: {col_count}ê°œ ë³€ìˆ˜",
                        description=f"ë‹¤ì–‘í•œ {col_count}ê°œ ë³€ìˆ˜ì— ëŒ€í•œ ì¢…í•©ì  ë¶„ì„ ìˆ˜í–‰ë¨",
                        insight_type=InsightType.PATTERN,
                        priority=InsightPriority.MEDIUM,
                        confidence=0.8,
                        impact_score=0.6,
                        supporting_data={"column_count": col_count, "columns": df_columns},
                        evidence=[f"ë¶„ì„ ë³€ìˆ˜: {', '.join(df_columns[:5])}..."],
                        source_agents=source_agents,
                        recommended_actions=["ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„", "ì°¨ì› ì¶•ì†Œ ê²€í† "]
                    )
                    
                    insights.append(insight)
        
        except Exception as e:
            logger.warning(f"ë°ì´í„°í”„ë ˆì„ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _analyze_statistical_summary_insights(self, 
                                            stats_artifact: Dict[str, Any], 
                                            source_agents: List[str]) -> List[Insight]:
        """í†µê³„ ìš”ì•½ ì•„í‹°íŒ©íŠ¸ ë¶„ì„"""
        
        insights = []
        
        try:
            stats_data = stats_artifact.get('content', {})
            
            # í‰ê· , í‘œì¤€í¸ì°¨ ë“±ì—ì„œ ì´ìƒì¹˜ ê°ì§€
            for metric, value in stats_data.items():
                if isinstance(value, (int, float)):
                    if metric.lower() in ['std', 'standard_deviation', 'í‘œì¤€í¸ì°¨']:
                        if value > 0:  # í‘œì¤€í¸ì°¨ê°€ ìˆëŠ” ê²½ìš°
                            insight = Insight(
                                insight_id=f"stats_variability_{metric}",
                                title=f"ë°ì´í„° ë³€ë™ì„±: {metric}",
                                description=f"{metric}: {value:.3f} - ë°ì´í„°ì˜ ë³€ë™ì„± ì •ë„",
                                insight_type=InsightType.STATISTICAL,
                                priority=InsightPriority.LOW,
                                confidence=0.9,
                                impact_score=0.3,
                                supporting_data={metric: value},
                                evidence=[f"{metric}: {value:.3f}"],
                                source_agents=source_agents,
                                recommended_actions=["ë³€ë™ì„± ì›ì¸ ë¶„ì„", "ì´ìƒì¹˜ ê²€ì¶œ"]
                            )
                            
                            insights.append(insight)
        
        except Exception as e:
            logger.warning(f"í†µê³„ ìš”ì•½ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _analyze_agent_consistency(self, 
                                 agent_results: Dict[str, AgentResult],
                                 contributing_agents: List[str]) -> List[Insight]:
        """ì—ì´ì „íŠ¸ ê°„ ì¼ê´€ì„± ë¶„ì„"""
        
        insights = []
        
        try:
            # ì‹¤í–‰ ì‹œê°„ ì¼ê´€ì„±
            execution_times = []
            for agent_id in contributing_agents:
                if agent_id in agent_results:
                    execution_times.append(agent_results[agent_id].execution_duration)
            
            if len(execution_times) >= 2:
                time_std = statistics.stdev(execution_times)
                time_mean = statistics.mean(execution_times)
                
                cv = time_std / time_mean if time_mean > 0 else 0  # ë³€ë™ê³„ìˆ˜
                
                if cv > 0.5:  # ë³€ë™ê³„ìˆ˜ê°€ 50% ì´ìƒ
                    insight = Insight(
                        insight_id="agent_time_inconsistency",
                        title="ì—ì´ì „íŠ¸ ì„±ëŠ¥ í¸ì°¨ ë°œê²¬",
                        description=f"ì—ì´ì „íŠ¸ ê°„ ì‹¤í–‰ ì‹œê°„ í¸ì°¨ê°€ í¼ (ë³€ë™ê³„ìˆ˜: {cv:.2f})",
                        insight_type=InsightType.ANOMALY,
                        priority=InsightPriority.MEDIUM,
                        confidence=0.8,
                        impact_score=0.5,
                        supporting_data={
                            "execution_times": execution_times,
                            "coefficient_of_variation": cv
                        },
                        evidence=[f"ì‹¤í–‰ ì‹œê°„ ë²”ìœ„: {min(execution_times):.1f}~{max(execution_times):.1f}ì´ˆ"],
                        source_agents=contributing_agents,
                        recommended_actions=["ì„±ëŠ¥ ë³‘ëª© ì§€ì  ë¶„ì„", "ì—ì´ì „íŠ¸ ìµœì í™”"]
                    )
                    
                    insights.append(insight)
        
        except Exception as e:
            logger.warning(f"ì—ì´ì „íŠ¸ ì¼ê´€ì„± ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _analyze_data_quality_insights(self, 
                                     agent_results: Dict[str, AgentResult],
                                     contributing_agents: List[str]) -> List[Insight]:
        """ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ ë¶„ì„"""
        
        insights = []
        
        try:
            # ì•„í‹°íŒ©íŠ¸ í’ˆì§ˆ ë¶„ì„
            total_artifacts = 0
            valid_artifacts = 0
            
            for agent_id in contributing_agents:
                if agent_id in agent_results:
                    result = agent_results[agent_id]
                    total_artifacts += len(result.artifacts)
                    
                    for artifact in result.artifacts:
                        if artifact.get('content') or artifact.get('data'):
                            valid_artifacts += 1
            
            if total_artifacts > 0:
                quality_ratio = valid_artifacts / total_artifacts
                
                if quality_ratio < 0.8:  # 80% ë¯¸ë§Œ
                    insight = Insight(
                        insight_id="data_quality_issue",
                        title="ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ê°ì§€",
                        description=f"ì•„í‹°íŒ©íŠ¸ ì¤‘ {quality_ratio:.1%}ë§Œ ìœ íš¨í•œ ë°ì´í„° í¬í•¨",
                        insight_type=InsightType.ANOMALY,
                        priority=InsightPriority.HIGH,
                        confidence=0.9,
                        impact_score=1.0 - quality_ratio,
                        supporting_data={
                            "total_artifacts": total_artifacts,
                            "valid_artifacts": valid_artifacts,
                            "quality_ratio": quality_ratio
                        },
                        evidence=[f"ìœ íš¨ ì•„í‹°íŒ©íŠ¸: {valid_artifacts}/{total_artifacts}"],
                        source_agents=contributing_agents,
                        recommended_actions=["ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ê°•í™”", "ì—ì´ì „íŠ¸ ì˜¤ë¥˜ ì²˜ë¦¬ ê°œì„ "]
                    )
                    
                    insights.append(insight)
        
        except Exception as e:
            logger.warning(f"ë°ì´í„° í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _analyze_coverage_insights(self, 
                                 agent_results: Dict[str, AgentResult],
                                 integration_result: IntegrationResult) -> List[Insight]:
        """ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì¸ì‚¬ì´íŠ¸"""
        
        insights = []
        
        try:
            total_agents = len(agent_results)
            contributing_agents = len(integration_result.contributing_agents)
            
            coverage_rate = contributing_agents / total_agents if total_agents > 0 else 0
            
            if coverage_rate < 0.8:  # 80% ë¯¸ë§Œ
                insight = Insight(
                    insight_id="low_coverage",
                    title="ë¶„ì„ ì»¤ë²„ë¦¬ì§€ ë¶€ì¡±",
                    description=f"ì „ì²´ ì—ì´ì „íŠ¸ ì¤‘ {coverage_rate:.1%}ë§Œ ë¶„ì„ì— ê¸°ì—¬",
                    insight_type=InsightType.BUSINESS_IMPACT,
                    priority=InsightPriority.HIGH,
                    confidence=1.0,
                    impact_score=1.0 - coverage_rate,
                    supporting_data={
                        "total_agents": total_agents,
                        "contributing_agents": contributing_agents,
                        "coverage_rate": coverage_rate
                    },
                    evidence=[f"ê¸°ì—¬ ì—ì´ì „íŠ¸: {contributing_agents}/{total_agents}"],
                    source_agents=integration_result.contributing_agents,
                    recommended_actions=["ë¹„ê¸°ì—¬ ì—ì´ì „íŠ¸ ë¬¸ì œ í•´ê²°", "ë¶„ì„ ë²”ìœ„ í™•ëŒ€"]
                )
                
                insights.append(insight)
        
        except Exception as e:
            logger.warning(f"ì»¤ë²„ë¦¬ì§€ ë¶„ì„ ì¸ì‚¬ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return insights
    
    def _evaluate_and_filter_insights(self, insights: List[Insight]) -> List[Insight]:
        """ì¸ì‚¬ì´íŠ¸ í’ˆì§ˆ í‰ê°€ ë° í•„í„°ë§"""
        
        qualified_insights = []
        
        for insight in insights:
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            quality_score = (
                insight.confidence * 0.4 +
                insight.impact_score * 0.3 +
                (len(insight.evidence) / 5) * 0.2 +  # ìµœëŒ€ 5ê°œ ì¦ê±°
                (len(insight.supporting_data) / 10) * 0.1  # ìµœëŒ€ 10ê°œ ì§€ì› ë°ì´í„°
            )
            
            # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€
            if quality_score >= 0.3:
                qualified_insights.append(insight)
        
        return qualified_insights
    
    def _prioritize_insights(self, insights: List[Insight]) -> List[Insight]:
        """ì¸ì‚¬ì´íŠ¸ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        
        # ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°
        priority_scores = {
            InsightPriority.CRITICAL: 5,
            InsightPriority.HIGH: 4,
            InsightPriority.MEDIUM: 3,
            InsightPriority.LOW: 2,
            InsightPriority.INFORMATIONAL: 1
        }
        
        def calculate_priority_score(insight: Insight) -> float:
            base_score = priority_scores.get(insight.priority, 1)
            
            # ì‹ ë¢°ë„ì™€ ì˜í–¥ë„ë¡œ ê°€ì¤‘
            weighted_score = base_score * (
                0.6 * insight.confidence +
                0.4 * insight.impact_score
            )
            
            return weighted_score
        
        # ìš°ì„ ìˆœìœ„ ì ìˆ˜ë¡œ ì •ë ¬
        prioritized = sorted(insights, key=calculate_priority_score, reverse=True)
        
        return prioritized
    
    def _aggregate_analysis(self, analysis: InsightAnalysis):
        """ë¶„ì„ ê²°ê³¼ ì§‘ê³„"""
        
        if not analysis.insights:
            return
        
        # ìœ í˜•ë³„/ìš°ì„ ìˆœìœ„ë³„ ì§‘ê³„
        for insight in analysis.insights:
            analysis.insights_by_type[insight.insight_type] = \
                analysis.insights_by_type.get(insight.insight_type, 0) + 1
            
            analysis.insights_by_priority[insight.priority] = \
                analysis.insights_by_priority.get(insight.priority, 0) + 1
        
        # í‰ê·  ì§€í‘œ ê³„ì‚°
        confidences = [insight.confidence for insight in analysis.insights]
        impacts = [insight.impact_score for insight in analysis.insights]
        
        analysis.average_confidence = sum(confidences) / len(confidences)
        analysis.average_impact = sum(impacts) / len(impacts)
        
        # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        analysis.overall_quality_score = (
            analysis.average_confidence * 0.5 +
            analysis.average_impact * 0.3 +
            min(1.0, analysis.total_insights / 10) * 0.2
        )
    
    def _generate_key_findings(self, insights: List[Insight]) -> List[str]:
        """ì£¼ìš” ë°œê²¬ì‚¬í•­ ìƒì„±"""
        
        key_findings = []
        
        try:
            # 1. ê°€ì¥ ë†’ì€ ìš°ì„ ìˆœìœ„ ì¸ì‚¬ì´íŠ¸ë“¤
            high_priority = [i for i in insights 
                           if i.priority in [InsightPriority.CRITICAL, InsightPriority.HIGH]]
            
            if high_priority:
                key_findings.append(f"ğŸš¨ {len(high_priority)}ê°œì˜ ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ì´ ìˆìŠµë‹ˆë‹¤")
            
            # 2. ì¸ì‚¬ì´íŠ¸ ìœ í˜•ë³„ ìš”ì•½
            type_counts = {}
            for insight in insights:
                type_counts[insight.insight_type] = type_counts.get(insight.insight_type, 0) + 1
            
            if type_counts:
                most_common_type = max(type_counts.items(), key=lambda x: x[1])
                key_findings.append(f"ğŸ“Š ì£¼ìš” ë¶„ì„ ì˜ì—­: {most_common_type[0].value} ({most_common_type[1]}ê±´)")
            
            # 3. í‰ê·  ì‹ ë¢°ë„ê°€ ë†’ì€ ê²½ìš°
            avg_confidence = sum(i.confidence for i in insights) / len(insights)
            if avg_confidence > 0.8:
                key_findings.append(f"âœ… ë†’ì€ ì‹ ë¢°ë„ì˜ ë¶„ì„ ê²°ê³¼ (í‰ê·  {avg_confidence:.1%})")
            
            # 4. íŠ¹ë³„í•œ íŒ¨í„´ì´ë‚˜ ì´ìƒì¹˜
            anomalies = [i for i in insights if i.insight_type == InsightType.ANOMALY]
            if anomalies:
                key_findings.append(f"âš ï¸ {len(anomalies)}ê°œì˜ ì´ìƒì¹˜ ë˜ëŠ” ì˜ˆì™¸ ìƒí™© ê°ì§€")
        
        except Exception as e:
            logger.warning(f"ì£¼ìš” ë°œê²¬ì‚¬í•­ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            key_findings.append("ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—¬ëŸ¬ ì¸ì‚¬ì´íŠ¸ê°€ ë„ì¶œë˜ì—ˆìŠµë‹ˆë‹¤")
        
        return key_findings[:5]  # ìµœëŒ€ 5ê°œ
    
    def _extract_context_around_keyword(self, text: str, keyword: str, context_length: int = 150) -> str:
        """í‚¤ì›Œë“œ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        text_lower = text.lower()
        keyword_lower = keyword.lower()
        
        idx = text_lower.find(keyword_lower)
        if idx == -1:
            return ""
        
        start = max(0, idx - context_length)
        end = min(len(text), idx + len(keyword) + context_length)
        
        return text[start:end].strip()
    
    def _classify_statistical_value(self, pattern: str, value: float) -> str:
        """í†µê³„ ê°’ ë¶„ë¥˜"""
        
        if '%' in pattern:
            return "ë°±ë¶„ìœ¨"
        elif 'í‰ê· ' in pattern:
            return "í‰ê· ê°’"
        elif 'ìµœëŒ€' in pattern:
            return "ìµœëŒ€ê°’"
        elif 'ìµœì†Œ' in pattern:
            return "ìµœì†Œê°’"
        elif 'í‘œì¤€í¸ì°¨' in pattern:
            return "í‘œì¤€í¸ì°¨"
        elif 'ë°°' in pattern:
            return "ë°°ìˆ˜ê´€ê³„"
        else:
            return "ìˆ˜ì¹˜ê°’"
    
    def _evaluate_statistical_impact(self, stat_type: str, value: float) -> float:
        """í†µê³„ ê°’ì˜ ì˜í–¥ë„ í‰ê°€"""
        
        if stat_type == "ë°±ë¶„ìœ¨":
            if value > 90 or value < 10:
                return 0.8  # ê·¹ê°’
            elif value > 70 or value < 30:
                return 0.6  # ë†’ì€ í¸í–¥
            else:
                return 0.4
        
        elif stat_type == "ë°°ìˆ˜ê´€ê³„":
            if value > 5:
                return 0.9  # 5ë°° ì´ìƒ ì°¨ì´
            elif value > 2:
                return 0.7  # 2ë°° ì´ìƒ ì°¨ì´
            else:
                return 0.4
        
        elif stat_type == "í‘œì¤€í¸ì°¨":
            # ìƒëŒ€ì  í‰ê°€ëŠ” ì–´ë ¤ìš°ë¯€ë¡œ ì¤‘ê°„ ê°’
            return 0.5
        
        else:
            return 0.5  # ê¸°ë³¸ê°’