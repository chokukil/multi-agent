"""
ë©€í‹° ì—ì´ì „íŠ¸ ê²°ê³¼ ì¶©ëŒ ê°ì§€ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ì—¬ëŸ¬ A2A ì—ì´ì „íŠ¸ ê°„ì˜ ìƒì¶©ë˜ëŠ” ê²°ê³¼ë¥¼ ê°ì§€í•˜ê³ ,
ì¶©ëŒ ìœ í˜•ì„ ë¶„ë¥˜í•˜ë©° í•´ê²° ì „ëµì„ ì œì•ˆí•˜ëŠ” ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì—ì´ì „íŠ¸ ê°„ ìƒì¶© ê²°ê³¼ ì‹ë³„ ë° ë¶„ë¥˜
- ì¶©ëŒ ìœ í˜•ë³„ ìš°ì„ ìˆœìœ„ ê²°ì •
- ë°ì´í„° ë¶ˆì¼ì¹˜ ë° ëª¨ìˆœ ê°ì§€
- ì¶©ëŒ í•´ê²° ì „ëµ ë° ê¶Œì¥ì‚¬í•­ ì œê³µ
"""

import json
import logging
import re
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Tuple
from enum import Enum
import difflib
from collections import defaultdict

from .agent_result_collector import AgentResult, CollectionSession
from .result_validator import QualityMetrics

logger = logging.getLogger(__name__)

class ConflictType(Enum):
    """ì¶©ëŒ ìœ í˜•"""
    DATA_CONTRADICTION = "data_contradiction"        # ë°ì´í„° ëª¨ìˆœ
    STATISTICAL_DISCREPANCY = "statistical_discrepancy"  # í†µê³„ ë¶ˆì¼ì¹˜
    CONCLUSION_DIVERGENCE = "conclusion_divergence"  # ê²°ë¡  ë¶„ê¸°
    FORMAT_INCONSISTENCY = "format_inconsistency"    # í˜•ì‹ ë¶ˆì¼ì¹˜
    QUALITY_VARIANCE = "quality_variance"            # í’ˆì§ˆ í¸ì°¨
    TEMPORAL_INCONSISTENCY = "temporal_inconsistency"  # ì‹œê°„ì  ë¶ˆì¼ì¹˜
    SCOPE_MISMATCH = "scope_mismatch"               # ë²”ìœ„ ë¶ˆì¼ì¹˜

class ConflictSeverity(Enum):
    """ì¶©ëŒ ì‹¬ê°ë„"""
    CRITICAL = "critical"    # ì¹˜ëª…ì  (ì¦‰ì‹œ í•´ê²° í•„ìš”)
    HIGH = "high"           # ë†’ìŒ (ìš°ì„  í•´ê²°)
    MEDIUM = "medium"       # ë³´í†µ (ê²€í†  í•„ìš”)
    LOW = "low"            # ë‚®ìŒ (ì°¸ê³ )

class ResolutionStrategy(Enum):
    """í•´ê²° ì „ëµ"""
    QUALITY_BASED = "quality_based"          # í’ˆì§ˆ ê¸°ì¤€ ì„ íƒ
    CONSENSUS_BASED = "consensus_based"      # í•©ì˜ ê¸°ì¤€ ì„ íƒ
    MERGE_RESULTS = "merge_results"          # ê²°ê³¼ ë³‘í•©
    RERUN_ANALYSIS = "rerun_analysis"        # ì¬ë¶„ì„ ì‹¤í–‰
    MANUAL_REVIEW = "manual_review"          # ìˆ˜ë™ ê²€í† 
    EXCLUDE_OUTLIER = "exclude_outlier"      # ì´ìƒì¹˜ ì œì™¸

@dataclass
class ConflictInstance:
    """ì¶©ëŒ ì¸ìŠ¤í„´ìŠ¤"""
    conflict_id: str
    conflict_type: ConflictType
    severity: ConflictSeverity
    
    # ê´€ë ¨ ì—ì´ì „íŠ¸
    involved_agents: List[str]
    primary_agent: Optional[str] = None
    
    # ì¶©ëŒ ìƒì„¸ ì •ë³´
    description: str = ""
    conflicting_data: Dict[str, Any] = field(default_factory=dict)
    evidence: List[str] = field(default_factory=list)
    
    # í•´ê²° ì •ë³´
    suggested_strategy: ResolutionStrategy = ResolutionStrategy.MANUAL_REVIEW
    resolution_confidence: float = 0.0
    alternative_strategies: List[ResolutionStrategy] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    detected_at: datetime = field(default_factory=datetime.now)
    detection_method: str = ""
    
    @property
    def agent_count(self) -> int:
        return len(self.involved_agents)

@dataclass
class ConflictAnalysis:
    """ì¶©ëŒ ë¶„ì„ ê²°ê³¼"""
    session_id: str
    total_conflicts: int
    conflicts_by_type: Dict[ConflictType, int]
    conflicts_by_severity: Dict[ConflictSeverity, int]
    
    # ìƒì„¸ ì¶©ëŒ ëª©ë¡
    conflicts: List[ConflictInstance] = field(default_factory=list)
    
    # ì „ì²´ ë¶„ì„
    overall_consistency_score: float = 0.0
    reliability_impact: float = 0.0
    recommended_actions: List[str] = field(default_factory=list)
    
    # í†µê³„
    agent_conflict_matrix: Dict[Tuple[str, str], int] = field(default_factory=dict)
    most_conflicted_agents: List[str] = field(default_factory=list)

class ConflictDetector:
    """ë©€í‹° ì—ì´ì „íŠ¸ ê²°ê³¼ ì¶©ëŒ ê°ì§€ê¸°"""
    
    def __init__(self):
        # ê°ì§€ ì„ê³„ê°’
        self.thresholds = {
            'statistical_difference': 0.15,    # 15% ì´ìƒ ì°¨ì´
            'quality_variance': 0.3,           # í’ˆì§ˆ ì ìˆ˜ 30% ì´ìƒ ì°¨ì´
            'text_similarity': 0.7,            # í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ 70% ë¯¸ë§Œ
            'execution_time_variance': 5.0     # ì‹¤í–‰ ì‹œê°„ 5ë°° ì´ìƒ ì°¨ì´
        }
        
        # í‚¤ì›Œë“œ íŒ¨í„´
        self.conclusion_keywords = [
            'conclusion', 'ê²°ë¡ ', 'summary', 'ìš”ì•½', 'result', 'ê²°ê³¼',
            'finding', 'ë°œê²¬', 'insight', 'ì¸ì‚¬ì´íŠ¸', 'recommendation', 'ê¶Œì¥'
        ]
        
        self.statistical_keywords = [
            'mean', 'average', 'í‰ê· ', 'median', 'ì¤‘ì•™ê°’', 'std', 'í‘œì¤€í¸ì°¨',
            'count', 'ê°œìˆ˜', 'total', 'ì´í•©', 'percentage', 'ë¹„ìœ¨'
        ]
        
        # ì¶©ëŒ ìš°ì„ ìˆœìœ„ (ë†’ì„ìˆ˜ë¡ ìš°ì„ )
        self.severity_priorities = {
            ConflictSeverity.CRITICAL: 4,
            ConflictSeverity.HIGH: 3,
            ConflictSeverity.MEDIUM: 2,
            ConflictSeverity.LOW: 1
        }
    
    def detect_conflicts(self, 
                        session: CollectionSession,
                        quality_metrics: Dict[str, QualityMetrics] = None) -> ConflictAnalysis:
        """ì„¸ì…˜ ë‚´ ëª¨ë“  ì¶©ëŒ ê°ì§€"""
        
        logger.info(f"ğŸ” ì¶©ëŒ ê°ì§€ ì‹œì‘ - ì„¸ì…˜ {session.session_id}, "
                   f"ì—ì´ì „íŠ¸ {len(session.collected_results)}ê°œ")
        
        analysis = ConflictAnalysis(
            session_id=session.session_id,
            total_conflicts=0,
            conflicts_by_type=defaultdict(int),
            conflicts_by_severity=defaultdict(int)
        )
        
        if len(session.collected_results) < 2:
            logger.info("ì¶©ëŒ ê°ì§€ì— í•„ìš”í•œ ìµœì†Œ ì—ì´ì „íŠ¸ ìˆ˜(2ê°œ) ë¯¸ë§Œ")
            return analysis
        
        try:
            results = list(session.collected_results.values())
            
            # 1. ë°ì´í„° ëª¨ìˆœ ê°ì§€
            conflicts = self._detect_data_contradictions(results)
            analysis.conflicts.extend(conflicts)
            
            # 2. í†µê³„ ë¶ˆì¼ì¹˜ ê°ì§€
            conflicts = self._detect_statistical_discrepancies(results)
            analysis.conflicts.extend(conflicts)
            
            # 3. ê²°ë¡  ë¶„ê¸° ê°ì§€
            conflicts = self._detect_conclusion_divergence(results)
            analysis.conflicts.extend(conflicts)
            
            # 4. í˜•ì‹ ë¶ˆì¼ì¹˜ ê°ì§€
            conflicts = self._detect_format_inconsistencies(results)
            analysis.conflicts.extend(conflicts)
            
            # 5. í’ˆì§ˆ í¸ì°¨ ê°ì§€
            if quality_metrics:
                conflicts = self._detect_quality_variance(results, quality_metrics)
                analysis.conflicts.extend(conflicts)
            
            # 6. ì‹œê°„ì  ë¶ˆì¼ì¹˜ ê°ì§€
            conflicts = self._detect_temporal_inconsistencies(results)
            analysis.conflicts.extend(conflicts)
            
            # 7. ë²”ìœ„ ë¶ˆì¼ì¹˜ ê°ì§€
            conflicts = self._detect_scope_mismatches(results)
            analysis.conflicts.extend(conflicts)
            
            # ë¶„ì„ ê²°ê³¼ ì§‘ê³„
            self._aggregate_analysis(analysis)
            
            logger.info(f"âœ… ì¶©ëŒ ê°ì§€ ì™„ë£Œ - {analysis.total_conflicts}ê°œ ì¶©ëŒ ë°œê²¬")
            
        except Exception as e:
            logger.error(f"âŒ ì¶©ëŒ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            analysis.recommended_actions.append(f"ì¶©ëŒ ê°ì§€ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        
        return analysis
    
    def resolve_conflicts(self, 
                         analysis: ConflictAnalysis,
                         results: Dict[str, AgentResult]) -> Dict[str, Any]:
        """ì¶©ëŒ í•´ê²° ë°©ì•ˆ ì œì‹œ"""
        
        logger.info(f"ğŸ”§ ì¶©ëŒ í•´ê²° ë°©ì•ˆ ìƒì„± - {analysis.total_conflicts}ê°œ ì¶©ëŒ")
        
        resolution_plan = {
            "summary": {
                "total_conflicts": analysis.total_conflicts,
                "resolvable_conflicts": 0,
                "manual_review_required": 0
            },
            "resolutions": [],
            "priority_order": [],
            "implementation_steps": []
        }
        
        try:
            # ì‹¬ê°ë„ë³„ ì •ë ¬
            sorted_conflicts = sorted(
                analysis.conflicts,
                key=lambda c: self.severity_priorities[c.severity],
                reverse=True
            )
            
            for conflict in sorted_conflicts:
                resolution = self._generate_resolution(conflict, results)
                resolution_plan["resolutions"].append(resolution)
                
                if resolution["strategy"] != ResolutionStrategy.MANUAL_REVIEW.value:
                    resolution_plan["summary"]["resolvable_conflicts"] += 1
                else:
                    resolution_plan["summary"]["manual_review_required"] += 1
                
                resolution_plan["priority_order"].append(conflict.conflict_id)
            
            # êµ¬í˜„ ë‹¨ê³„ ìƒì„±
            resolution_plan["implementation_steps"] = self._generate_implementation_steps(
                resolution_plan["resolutions"]
            )
            
        except Exception as e:
            logger.error(f"âŒ ì¶©ëŒ í•´ê²° ë°©ì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            resolution_plan["error"] = str(e)
        
        return resolution_plan
    
    def _detect_data_contradictions(self, results: List[AgentResult]) -> List[ConflictInstance]:
        """ë°ì´í„° ëª¨ìˆœ ê°ì§€"""
        
        conflicts = []
        
        # ì•„í‹°íŒ©íŠ¸ ë°ì´í„° ë¹„êµ
        for i, result1 in enumerate(results):
            for j, result2 in enumerate(results[i+1:], i+1):
                
                # ë™ì¼í•œ íƒ€ì…ì˜ ì•„í‹°íŒ©íŠ¸ ë¹„êµ
                artifacts1 = {art['type']: art for art in result1.artifacts if 'type' in art}
                artifacts2 = {art['type']: art for art in result2.artifacts if 'type' in art}
                
                common_types = set(artifacts1.keys()) & set(artifacts2.keys())
                
                for art_type in common_types:
                    contradiction = self._compare_artifacts(
                        artifacts1[art_type], artifacts2[art_type],
                        result1.agent_id, result2.agent_id
                    )
                    
                    if contradiction:
                        conflict_id = f"data_contradiction_{result1.agent_id}_{result2.agent_id}_{art_type}"
                        
                        conflict = ConflictInstance(
                            conflict_id=conflict_id,
                            conflict_type=ConflictType.DATA_CONTRADICTION,
                            severity=ConflictSeverity.HIGH,
                            involved_agents=[result1.agent_id, result2.agent_id],
                            description=f"{art_type} ë°ì´í„°ì—ì„œ ëª¨ìˆœ ë°œê²¬",
                            conflicting_data=contradiction,
                            evidence=[f"ì—ì´ì „íŠ¸ {result1.agent_id}ì™€ {result2.agent_id}ì˜ {art_type} ë°ì´í„° ë¶ˆì¼ì¹˜"],
                            suggested_strategy=ResolutionStrategy.QUALITY_BASED,
                            detection_method="artifact_comparison"
                        )
                        
                        conflicts.append(conflict)
        
        return conflicts
    
    def _detect_statistical_discrepancies(self, results: List[AgentResult]) -> List[ConflictInstance]:
        """í†µê³„ ë¶ˆì¼ì¹˜ ê°ì§€"""
        
        conflicts = []
        
        # í…ìŠ¤íŠ¸ì—ì„œ ìˆ«ì íŒ¨í„´ ì¶”ì¶œ
        number_pattern = r'\b\d+(?:\.\d+)?%?\b'
        
        for i, result1 in enumerate(results):
            numbers1 = re.findall(number_pattern, result1.processed_text)
            
            for j, result2 in enumerate(results[i+1:], i+1):
                numbers2 = re.findall(number_pattern, result2.processed_text)
                
                # ìœ ì‚¬í•œ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë°œê²¬ëœ ìˆ«ìë“¤ ë¹„êµ
                discrepancies = self._find_numerical_discrepancies(
                    numbers1, numbers2, result1.processed_text, result2.processed_text
                )
                
                if discrepancies:
                    conflict_id = f"statistical_discrepancy_{result1.agent_id}_{result2.agent_id}"
                    
                    conflict = ConflictInstance(
                        conflict_id=conflict_id,
                        conflict_type=ConflictType.STATISTICAL_DISCREPANCY,
                        severity=ConflictSeverity.MEDIUM,
                        involved_agents=[result1.agent_id, result2.agent_id],
                        description="í†µê³„ ìˆ˜ì¹˜ ë¶ˆì¼ì¹˜ ë°œê²¬",
                        conflicting_data={"discrepancies": discrepancies},
                        evidence=[f"ì—ì´ì „íŠ¸ ê°„ í†µê³„ ìˆ˜ì¹˜ ì°¨ì´: {len(discrepancies)}ê°œ í•­ëª©"],
                        suggested_strategy=ResolutionStrategy.CONSENSUS_BASED,
                        detection_method="numerical_comparison"
                    )
                    
                    conflicts.append(conflict)
        
        return conflicts
    
    def _detect_conclusion_divergence(self, results: List[AgentResult]) -> List[ConflictInstance]:
        """ê²°ë¡  ë¶„ê¸° ê°ì§€"""
        
        conflicts = []
        
        # ê° ê²°ê³¼ì—ì„œ ê²°ë¡  ë¶€ë¶„ ì¶”ì¶œ
        conclusions = {}
        for result in results:
            conclusion = self._extract_conclusion(result.processed_text)
            if conclusion:
                conclusions[result.agent_id] = conclusion
        
        if len(conclusions) < 2:
            return conflicts
        
        # ê²°ë¡  ê°„ ìœ ì‚¬ë„ ë¹„êµ
        agent_ids = list(conclusions.keys())
        for i, agent1 in enumerate(agent_ids):
            for j, agent2 in enumerate(agent_ids[i+1:], i+1):
                
                similarity = self._calculate_text_similarity(
                    conclusions[agent1], conclusions[agent2]
                )
                
                if similarity < self.thresholds['text_similarity']:
                    conflict_id = f"conclusion_divergence_{agent1}_{agent2}"
                    
                    severity = ConflictSeverity.HIGH if similarity < 0.5 else ConflictSeverity.MEDIUM
                    
                    conflict = ConflictInstance(
                        conflict_id=conflict_id,
                        conflict_type=ConflictType.CONCLUSION_DIVERGENCE,
                        severity=severity,
                        involved_agents=[agent1, agent2],
                        description=f"ê²°ë¡  ìœ ì‚¬ë„ ë‚®ìŒ ({similarity:.2f})",
                        conflicting_data={
                            "similarity_score": similarity,
                            "conclusions": {agent1: conclusions[agent1], agent2: conclusions[agent2]}
                        },
                        evidence=[f"ê²°ë¡  ìœ ì‚¬ë„ {similarity:.1%} (ì„ê³„ê°’ {self.thresholds['text_similarity']:.1%})"],
                        suggested_strategy=ResolutionStrategy.MANUAL_REVIEW,
                        detection_method="text_similarity_analysis"
                    )
                    
                    conflicts.append(conflict)
        
        return conflicts
    
    def _detect_format_inconsistencies(self, results: List[AgentResult]) -> List[ConflictInstance]:
        """í˜•ì‹ ë¶ˆì¼ì¹˜ ê°ì§€"""
        
        conflicts = []
        
        # ì•„í‹°íŒ©íŠ¸ í˜•ì‹ ë¶„ì„
        format_patterns = {}
        for result in results:
            patterns = []
            for artifact in result.artifacts:
                art_type = artifact.get('type', 'unknown')
                patterns.append(art_type)
            
            format_patterns[result.agent_id] = set(patterns)
        
        if len(format_patterns) < 2:
            return conflicts
        
        # í˜•ì‹ ì°¨ì´ ê°ì§€
        agent_ids = list(format_patterns.keys())
        expected_formats = set()
        for patterns in format_patterns.values():
            expected_formats.update(patterns)
        
        for agent_id, patterns in format_patterns.items():
            missing_formats = expected_formats - patterns
            
            if missing_formats:
                conflict_id = f"format_inconsistency_{agent_id}"
                
                conflict = ConflictInstance(
                    conflict_id=conflict_id,
                    conflict_type=ConflictType.FORMAT_INCONSISTENCY,
                    severity=ConflictSeverity.LOW,
                    involved_agents=[agent_id],
                    description=f"ëˆ„ë½ëœ ì•„í‹°íŒ©íŠ¸ í˜•ì‹: {missing_formats}",
                    conflicting_data={
                        "missing_formats": list(missing_formats),
                        "available_formats": list(patterns)
                    },
                    evidence=[f"ì˜ˆìƒ í˜•ì‹ {len(expected_formats)}ê°œ ì¤‘ {len(missing_formats)}ê°œ ëˆ„ë½"],
                    suggested_strategy=ResolutionStrategy.RERUN_ANALYSIS,
                    detection_method="format_pattern_analysis"
                )
                
                conflicts.append(conflict)
        
        return conflicts
    
    def _detect_quality_variance(self, 
                               results: List[AgentResult],
                               quality_metrics: Dict[str, QualityMetrics]) -> List[ConflictInstance]:
        """í’ˆì§ˆ í¸ì°¨ ê°ì§€"""
        
        conflicts = []
        
        # í’ˆì§ˆ ì ìˆ˜ ìˆ˜ì§‘
        quality_scores = {}
        for result in results:
            if result.agent_id in quality_metrics:
                quality_scores[result.agent_id] = quality_metrics[result.agent_id].overall_score
        
        if len(quality_scores) < 2:
            return conflicts
        
        # í’ˆì§ˆ í¸ì°¨ ë¶„ì„
        scores = list(quality_scores.values())
        avg_score = sum(scores) / len(scores)
        max_diff = max(scores) - min(scores)
        
        if max_diff > self.thresholds['quality_variance']:
            # í’ˆì§ˆì´ í˜„ì €íˆ ë‚®ì€ ì—ì´ì „íŠ¸ ì‹ë³„
            low_quality_agents = [
                agent_id for agent_id, score in quality_scores.items()
                if score < avg_score - self.thresholds['quality_variance'] / 2
            ]
            
            if low_quality_agents:
                conflict_id = f"quality_variance_{'_'.join(low_quality_agents)}"
                
                conflict = ConflictInstance(
                    conflict_id=conflict_id,
                    conflict_type=ConflictType.QUALITY_VARIANCE,
                    severity=ConflictSeverity.MEDIUM,
                    involved_agents=low_quality_agents,
                    description=f"í’ˆì§ˆ í¸ì°¨ í¼ (ìµœëŒ€ ì°¨ì´: {max_diff:.2f})",
                    conflicting_data={
                        "quality_scores": quality_scores,
                        "average_score": avg_score,
                        "max_difference": max_diff
                    },
                    evidence=[f"í’ˆì§ˆ ì ìˆ˜ í¸ì°¨ {max_diff:.1%} (ì„ê³„ê°’ {self.thresholds['quality_variance']:.1%})"],
                    suggested_strategy=ResolutionStrategy.EXCLUDE_OUTLIER,
                    detection_method="quality_variance_analysis"
                )
                
                conflicts.append(conflict)
        
        return conflicts
    
    def _detect_temporal_inconsistencies(self, results: List[AgentResult]) -> List[ConflictInstance]:
        """ì‹œê°„ì  ë¶ˆì¼ì¹˜ ê°ì§€"""
        
        conflicts = []
        
        # ì‹¤í–‰ ì‹œê°„ ë¶„ì„
        execution_times = {result.agent_id: result.execution_duration for result in results}
        
        if len(execution_times) < 2:
            return conflicts
        
        times = list(execution_times.values())
        avg_time = sum(times) / len(times)
        max_time = max(times)
        min_time = min(times)
        
        # ì‹¤í–‰ ì‹œê°„ í¸ì°¨ê°€ í° ê²½ìš°
        if max_time > min_time * self.thresholds['execution_time_variance']:
            
            # ë¹„ì •ìƒì ìœ¼ë¡œ ë¹ ë¥´ê±°ë‚˜ ëŠë¦° ì—ì´ì „íŠ¸ ì‹ë³„
            outlier_agents = []
            for agent_id, time in execution_times.items():
                if time < avg_time / 3 or time > avg_time * 3:
                    outlier_agents.append(agent_id)
            
            if outlier_agents:
                conflict_id = f"temporal_inconsistency_{'_'.join(outlier_agents)}"
                
                conflict = ConflictInstance(
                    conflict_id=conflict_id,
                    conflict_type=ConflictType.TEMPORAL_INCONSISTENCY,
                    severity=ConflictSeverity.LOW,
                    involved_agents=outlier_agents,
                    description=f"ì‹¤í–‰ ì‹œê°„ í¸ì°¨ í¼ (ë¹„ìœ¨: {max_time/min_time:.1f}ë°°)",
                    conflicting_data={
                        "execution_times": execution_times,
                        "average_time": avg_time,
                        "time_ratio": max_time / min_time
                    },
                    evidence=[f"ì‹¤í–‰ ì‹œê°„ í¸ì°¨ {max_time/min_time:.1f}ë°° (ì„ê³„ê°’ {self.thresholds['execution_time_variance']:.1f}ë°°)"],
                    suggested_strategy=ResolutionStrategy.MANUAL_REVIEW,
                    detection_method="execution_time_analysis"
                )
                
                conflicts.append(conflict)
        
        return conflicts
    
    def _detect_scope_mismatches(self, results: List[AgentResult]) -> List[ConflictInstance]:
        """ë²”ìœ„ ë¶ˆì¼ì¹˜ ê°ì§€"""
        
        conflicts = []
        
        # ê° ê²°ê³¼ì˜ ë²”ìœ„/ê¹Šì´ ë¶„ì„
        scope_analysis = {}
        for result in results:
            analysis = {
                "text_length": len(result.processed_text),
                "artifact_count": len(result.artifacts),
                "artifact_types": set(art.get('type', 'unknown') for art in result.artifacts),
                "keyword_coverage": self._analyze_keyword_coverage(result.processed_text)
            }
            scope_analysis[result.agent_id] = analysis
        
        if len(scope_analysis) < 2:
            return conflicts
        
        # ë²”ìœ„ ì°¨ì´ ê°ì§€
        text_lengths = [analysis["text_length"] for analysis in scope_analysis.values()]
        artifact_counts = [analysis["artifact_count"] for analysis in scope_analysis.values()]
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ í¸ì°¨
        if max(text_lengths) > min(text_lengths) * 3:
            outlier_agents = []
            avg_length = sum(text_lengths) / len(text_lengths)
            
            for agent_id, analysis in scope_analysis.items():
                if analysis["text_length"] < avg_length / 2:
                    outlier_agents.append(agent_id)
            
            if outlier_agents:
                conflict_id = f"scope_mismatch_text_{'_'.join(outlier_agents)}"
                
                conflict = ConflictInstance(
                    conflict_id=conflict_id,
                    conflict_type=ConflictType.SCOPE_MISMATCH,
                    severity=ConflictSeverity.MEDIUM,
                    involved_agents=outlier_agents,
                    description="ë¶„ì„ ë²”ìœ„ ë¶ˆì¼ì¹˜ (í…ìŠ¤íŠ¸ ê¸¸ì´)",
                    conflicting_data={
                        "text_lengths": {aid: analysis["text_length"] 
                                       for aid, analysis in scope_analysis.items()},
                        "average_length": avg_length
                    },
                    evidence=[f"í…ìŠ¤íŠ¸ ê¸¸ì´ í¸ì°¨ {max(text_lengths)/min(text_lengths):.1f}ë°°"],
                    suggested_strategy=ResolutionStrategy.RERUN_ANALYSIS,
                    detection_method="scope_analysis"
                )
                
                conflicts.append(conflict)
        
        return conflicts
    
    def _compare_artifacts(self, 
                          artifact1: Dict[str, Any], 
                          artifact2: Dict[str, Any],
                          agent1_id: str,
                          agent2_id: str) -> Optional[Dict[str, Any]]:
        """ì•„í‹°íŒ©íŠ¸ ë¹„êµ ë° ëª¨ìˆœ ê°ì§€"""
        
        contradictions = {}
        
        try:
            art_type = artifact1.get('type', '')
            
            if art_type == 'dataframe':
                # ë°ì´í„°í”„ë ˆì„ ë¹„êµ
                data1 = artifact1.get('data', [])
                data2 = artifact2.get('data', [])
                
                if len(data1) != len(data2):
                    contradictions['row_count'] = {
                        agent1_id: len(data1),
                        agent2_id: len(data2)
                    }
                
                cols1 = set(artifact1.get('columns', []))
                cols2 = set(artifact2.get('columns', []))
                
                if cols1 != cols2:
                    contradictions['columns'] = {
                        agent1_id: list(cols1),
                        agent2_id: list(cols2),
                        'missing_in_1': list(cols2 - cols1),
                        'missing_in_2': list(cols1 - cols2)
                    }
            
            elif art_type == 'plotly_chart':
                # ì°¨íŠ¸ ë¹„êµ
                layout1 = artifact1.get('layout', {})
                layout2 = artifact2.get('layout', {})
                
                title1 = layout1.get('title', {}).get('text', '')
                title2 = layout2.get('title', {}).get('text', '')
                
                if title1 and title2 and title1 != title2:
                    contradictions['chart_title'] = {
                        agent1_id: title1,
                        agent2_id: title2
                    }
            
        except Exception as e:
            logger.warning(f"ì•„í‹°íŒ©íŠ¸ ë¹„êµ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return contradictions if contradictions else None
    
    def _find_numerical_discrepancies(self, 
                                    numbers1: List[str],
                                    numbers2: List[str],
                                    text1: str,
                                    text2: str) -> List[Dict[str, Any]]:
        """ìˆ˜ì¹˜ ë¶ˆì¼ì¹˜ ê°ì§€"""
        
        discrepancies = []
        
        try:
            # ìˆ«ìë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€í™˜
            values1 = []
            for num_str in numbers1:
                try:
                    if '%' in num_str:
                        values1.append(float(num_str.replace('%', '')) / 100)
                    else:
                        values1.append(float(num_str))
                except ValueError:
                    continue
            
            values2 = []
            for num_str in numbers2:
                try:
                    if '%' in num_str:
                        values2.append(float(num_str.replace('%', '')) / 100)
                    else:
                        values2.append(float(num_str))
                except ValueError:
                    continue
            
            # ìœ ì‚¬í•œ í¬ê¸°ì˜ ìˆ«ìë“¤ ë¹„êµ
            for val1 in values1:
                for val2 in values2:
                    if abs(val1) > 0 and abs(val2) > 0:
                        diff_ratio = abs(val1 - val2) / max(abs(val1), abs(val2))
                        
                        if diff_ratio > self.thresholds['statistical_difference']:
                            discrepancies.append({
                                "value1": val1,
                                "value2": val2,
                                "difference_ratio": diff_ratio,
                                "context1": self._get_number_context(str(val1), text1),
                                "context2": self._get_number_context(str(val2), text2)
                            })
            
        except Exception as e:
            logger.warning(f"ìˆ˜ì¹˜ ë¶ˆì¼ì¹˜ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return discrepancies
    
    def _extract_conclusion(self, text: str) -> Optional[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê²°ë¡  ë¶€ë¶„ ì¶”ì¶œ"""
        
        text_lower = text.lower()
        
        for keyword in self.conclusion_keywords:
            if keyword in text_lower:
                # í‚¤ì›Œë“œ ì´í›„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                start_idx = text_lower.find(keyword)
                if start_idx != -1:
                    # ë‹¤ìŒ ë¬¸ë‹¨ì´ë‚˜ ì¶©ë¶„í•œ ê¸¸ì´ê¹Œì§€ ì¶”ì¶œ
                    conclusion_part = text[start_idx:start_idx + 500]
                    return conclusion_part.strip()
        
        # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ë§ˆì§€ë§‰ ë¬¸ë‹¨ ë°˜í™˜
        paragraphs = text.split('\n\n')
        if paragraphs:
            return paragraphs[-1].strip()
        
        return None
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        
        try:
            # ê°„ë‹¨í•œ í† í° ê¸°ë°˜ ìœ ì‚¬ë„
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            if not words1 and not words2:
                return 1.0
            
            intersection = words1 & words2
            union = words1 | words2
            
            return len(intersection) / len(union) if union else 0.0
            
        except Exception:
            return 0.0
    
    def _analyze_keyword_coverage(self, text: str) -> Dict[str, int]:
        """í‚¤ì›Œë“œ ì»¤ë²„ë¦¬ì§€ ë¶„ì„"""
        
        text_lower = text.lower()
        coverage = {}
        
        # í†µê³„ í‚¤ì›Œë“œ
        coverage['statistical'] = sum(1 for kw in self.statistical_keywords if kw in text_lower)
        
        # ê²°ë¡  í‚¤ì›Œë“œ
        coverage['conclusion'] = sum(1 for kw in self.conclusion_keywords if kw in text_lower)
        
        return coverage
    
    def _get_number_context(self, number: str, text: str, context_length: int = 50) -> str:
        """ìˆ«ì ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        
        idx = text.find(number)
        if idx == -1:
            return ""
        
        start = max(0, idx - context_length)
        end = min(len(text), idx + len(number) + context_length)
        
        return text[start:end].strip()
    
    def _aggregate_analysis(self, analysis: ConflictAnalysis):
        """ë¶„ì„ ê²°ê³¼ ì§‘ê³„"""
        
        analysis.total_conflicts = len(analysis.conflicts)
        
        # ìœ í˜•ë³„/ì‹¬ê°ë„ë³„ ì§‘ê³„
        for conflict in analysis.conflicts:
            analysis.conflicts_by_type[conflict.conflict_type] += 1
            analysis.conflicts_by_severity[conflict.severity] += 1
        
        # ì „ì²´ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
        if analysis.total_conflicts == 0:
            analysis.overall_consistency_score = 1.0
        else:
            # ì‹¬ê°ë„ ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ê³„ì‚°
            severity_weights = {
                ConflictSeverity.CRITICAL: 1.0,
                ConflictSeverity.HIGH: 0.8,
                ConflictSeverity.MEDIUM: 0.5,
                ConflictSeverity.LOW: 0.2
            }
            
            weighted_conflicts = sum(
                severity_weights.get(conflict.severity, 0.5)
                for conflict in analysis.conflicts
            )
            
            max_possible_score = len(analysis.conflicts)
            analysis.overall_consistency_score = max(0.0, 1.0 - weighted_conflicts / max_possible_score)
        
        # ê¶Œì¥ ì¡°ì¹˜ ìƒì„±
        if analysis.total_conflicts == 0:
            analysis.recommended_actions.append("ëª¨ë“  ì—ì´ì „íŠ¸ ê²°ê³¼ê°€ ì¼ê´€ì„±ì„ ë³´ì…ë‹ˆë‹¤.")
        else:
            analysis.recommended_actions.extend([
                f"ì´ {analysis.total_conflicts}ê°œ ì¶©ëŒ í•´ê²° í•„ìš”",
                f"ë†’ì€ ìš°ì„ ìˆœìœ„ ì¶©ëŒ: {analysis.conflicts_by_severity.get(ConflictSeverity.HIGH, 0)}ê°œ",
                f"ì¹˜ëª…ì  ì¶©ëŒ: {analysis.conflicts_by_severity.get(ConflictSeverity.CRITICAL, 0)}ê°œ"
            ])
    
    def _generate_resolution(self, 
                           conflict: ConflictInstance, 
                           results: Dict[str, AgentResult]) -> Dict[str, Any]:
        """ê°œë³„ ì¶©ëŒ í•´ê²° ë°©ì•ˆ ìƒì„±"""
        
        resolution = {
            "conflict_id": conflict.conflict_id,
            "strategy": conflict.suggested_strategy.value,
            "confidence": conflict.resolution_confidence,
            "steps": [],
            "expected_outcome": "",
            "risk_level": "low"
        }
        
        try:
            if conflict.suggested_strategy == ResolutionStrategy.QUALITY_BASED:
                # í’ˆì§ˆ ê¸°ë°˜ í•´ê²°
                resolution["steps"] = [
                    "ê° ì—ì´ì „íŠ¸ì˜ í’ˆì§ˆ ì ìˆ˜ ë¹„êµ",
                    "ê°€ì¥ ë†’ì€ í’ˆì§ˆì˜ ê²°ê³¼ ì„ íƒ",
                    "ì„ íƒëœ ê²°ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ë‹µë³€ êµ¬ì„±"
                ]
                resolution["expected_outcome"] = "ê°€ì¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ ì±„íƒ"
                
            elif conflict.suggested_strategy == ResolutionStrategy.CONSENSUS_BASED:
                # í•©ì˜ ê¸°ë°˜ í•´ê²°
                resolution["steps"] = [
                    "ê³µí†µ ê²°ê³¼ ìš”ì†Œ ì‹ë³„",
                    "ì°¨ì´ì ì— ëŒ€í•œ ê°€ì¤‘ í‰ê·  ê³„ì‚°",
                    "í•©ì˜ëœ ê²°ê³¼ë¡œ í†µí•©"
                ]
                resolution["expected_outcome"] = "ì—ì´ì „íŠ¸ ê°„ í•©ì˜ëœ ê²°ê³¼ ë„ì¶œ"
                
            elif conflict.suggested_strategy == ResolutionStrategy.EXCLUDE_OUTLIER:
                # ì´ìƒì¹˜ ì œì™¸
                resolution["steps"] = [
                    "ì´ìƒì¹˜ ì—ì´ì „íŠ¸ ì‹ë³„",
                    "ë‚˜ë¨¸ì§€ ì—ì´ì „íŠ¸ ê²°ê³¼ë¡œ ì¬ë¶„ì„",
                    "ì •ìƒ ë²”ìœ„ ê²°ê³¼ë§Œ í™œìš©"
                ]
                resolution["expected_outcome"] = "ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼ ì§‘í•© í™•ë³´"
                resolution["risk_level"] = "medium"
                
            else:
                # ìˆ˜ë™ ê²€í†  í•„ìš”
                resolution["steps"] = [
                    "ì¶©ëŒ ìƒì„¸ ë‚´ìš© ê²€í† ",
                    "ë„ë©”ì¸ ì „ë¬¸ê°€ ì˜ê²¬ ìˆ˜ë ´",
                    "ìˆ˜ë™ìœ¼ë¡œ ìµœì  í•´ê²° ë°©ì•ˆ ê²°ì •"
                ]
                resolution["expected_outcome"] = "ì „ë¬¸ê°€ íŒë‹¨ ê¸°ë°˜ í•´ê²°"
                resolution["risk_level"] = "high"
        
        except Exception as e:
            logger.error(f"í•´ê²° ë°©ì•ˆ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            resolution["error"] = str(e)
        
        return resolution
    
    def _generate_implementation_steps(self, resolutions: List[Dict[str, Any]]) -> List[str]:
        """í†µí•© êµ¬í˜„ ë‹¨ê³„ ìƒì„±"""
        
        steps = []
        
        try:
            # ìë™ í•´ê²° ê°€ëŠ¥í•œ ì¶©ëŒë“¤
            auto_resolvable = [r for r in resolutions 
                             if r["strategy"] != ResolutionStrategy.MANUAL_REVIEW.value]
            
            if auto_resolvable:
                steps.append("1. ìë™ í•´ê²° ê°€ëŠ¥í•œ ì¶©ëŒë“¤ ì²˜ë¦¬")
                steps.extend([f"   - {r['conflict_id']}: {r['strategy']}" 
                            for r in auto_resolvable])
            
            # ìˆ˜ë™ ê²€í†  í•„ìš”í•œ ì¶©ëŒë“¤
            manual_review = [r for r in resolutions 
                           if r["strategy"] == ResolutionStrategy.MANUAL_REVIEW.value]
            
            if manual_review:
                steps.append("2. ìˆ˜ë™ ê²€í†  í•„ìš”í•œ ì¶©ëŒë“¤")
                steps.extend([f"   - {r['conflict_id']}: ì „ë¬¸ê°€ ê²€í†  í•„ìš”" 
                            for r in manual_review])
            
            steps.append("3. í•´ê²°ëœ ê²°ê³¼ë¡œ ìµœì¢… ë‹µë³€ ì¬êµ¬ì„±")
            steps.append("4. í†µí•© ê²°ê³¼ í’ˆì§ˆ ê²€ì¦")
            
        except Exception as e:
            logger.error(f"êµ¬í˜„ ë‹¨ê³„ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            steps.append(f"ì˜¤ë¥˜ë¡œ ì¸í•œ ìˆ˜ë™ ì²˜ë¦¬ í•„ìš”: {str(e)}")
        
        return steps