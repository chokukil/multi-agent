"""
ë©€í‹° ì—ì´ì „íŠ¸ ê²°ê³¼ ê²€ì¦ ë° í’ˆì§ˆ í‰ê°€ ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ A2A ì—ì´ì „íŠ¸ë“¤ì˜ ê²°ê³¼ ë°ì´í„°ì— ëŒ€í•œ ë¬´ê²°ì„± ê²€ì‚¬,
ì™„ì„±ë„ í‰ê°€, ì‹ ë¢°ë„ ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ë°ì´í„° ë¬´ê²°ì„± ê²€ì‚¬ ë° í˜•ì‹ ê²€ì¦
- ê²°ê³¼ ì™„ì„±ë„ ë° ì‹ ë¢°ë„ í‰ê°€
- ëˆ„ë½ëœ ì •ë³´ ì‹ë³„ ë° ë³´ì™„ ì œì•ˆ
- í’ˆì§ˆ ê¸°ë°˜ ê²°ê³¼ ìš°ì„ ìˆœìœ„ ê²°ì •
"""

import json
import logging
import re
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Tuple
from enum import Enum

from .agent_result_collector import AgentResult, CollectionSession

logger = logging.getLogger(__name__)

class ValidationLevel(Enum):
    """ê²€ì¦ ìˆ˜ì¤€"""
    BASIC = "basic"          # ê¸°ë³¸ ê²€ì¦ (í˜•ì‹, êµ¬ì¡°)
    STANDARD = "standard"    # í‘œì¤€ ê²€ì¦ (ë‚´ìš©, ì™„ì„±ë„)
    COMPREHENSIVE = "comprehensive"  # ì¢…í•© ê²€ì¦ (í’ˆì§ˆ, ì‹ ë¢°ë„)

class ValidationStatus(Enum):
    """ê²€ì¦ ìƒíƒœ"""
    PASSED = "passed"
    WARNING = "warning"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class ValidationResult:
    """ê²€ì¦ ê²°ê³¼"""
    status: ValidationStatus
    score: float  # 0.0 ~ 1.0
    message: str
    details: Dict[str, Any]
    recommendations: List[str]

@dataclass
class QualityMetrics:
    """í’ˆì§ˆ ì§€í‘œ"""
    # ë°ì´í„° í’ˆì§ˆ
    data_integrity_score: float = 0.0
    format_validity_score: float = 0.0
    content_richness_score: float = 0.0
    
    # ì™„ì„±ë„ ì§€í‘œ
    completeness_score: float = 0.0
    coverage_score: float = 0.0
    depth_score: float = 0.0
    
    # ì‹ ë¢°ë„ ì§€í‘œ
    consistency_score: float = 0.0
    accuracy_score: float = 0.0
    reliability_score: float = 0.0
    
    # ì¢…í•© ì ìˆ˜
    overall_score: float = 0.0
    
    def calculate_overall_score(self):
        """ì¢…í•© ì ìˆ˜ ê³„ì‚°"""
        scores = [
            self.data_integrity_score,
            self.format_validity_score,
            self.content_richness_score,
            self.completeness_score,
            self.coverage_score,
            self.depth_score,
            self.consistency_score,
            self.accuracy_score,
            self.reliability_score
        ]
        
        # ìœ íš¨í•œ ì ìˆ˜ë§Œìœ¼ë¡œ í‰ê·  ê³„ì‚°
        valid_scores = [s for s in scores if s > 0.0]
        self.overall_score = sum(valid_scores) / len(valid_scores) if valid_scores else 0.0

class ResultValidator:
    """ë©€í‹° ì—ì´ì „íŠ¸ ê²°ê³¼ ê²€ì¦ê¸°"""
    
    def __init__(self):
        # ê²€ì¦ ê·œì¹™ ì„¤ì •
        self.min_text_length = 10
        self.max_text_length = 50000
        self.expected_artifact_types = {
            'plotly_chart', 'dataframe', 'image', 'code', 'text'
        }
        
        # í’ˆì§ˆ ì„ê³„ê°’
        self.quality_thresholds = {
            'excellent': 0.9,
            'good': 0.7,
            'acceptable': 0.5,
            'poor': 0.3
        }
        
        # í‚¤ì›Œë“œ íŒ¨í„´
        self.error_patterns = [
            r'error', r'exception', r'failed', r'failure',
            r'timeout', r'connection\s+refused', r'not\s+found'
        ]
        
        self.success_indicators = [
            r'successfully', r'completed', r'finished',
            r'analysis', r'result', r'conclusion'
        ]
    
    def validate_agent_result(self, 
                            result: AgentResult,
                            level: ValidationLevel = ValidationLevel.STANDARD) -> Tuple[ValidationResult, QualityMetrics]:
        """ë‹¨ì¼ ì—ì´ì „íŠ¸ ê²°ê³¼ ê²€ì¦"""
        
        logger.info(f"ğŸ” ì—ì´ì „íŠ¸ ê²°ê³¼ ê²€ì¦ ì‹œì‘ - {result.agent_name} ({level.value})")
        
        metrics = QualityMetrics()
        validations = []
        
        try:
            # ê¸°ë³¸ ê²€ì¦
            if level.value in ['basic', 'standard', 'comprehensive']:
                validations.extend(self._validate_basic_structure(result, metrics))
            
            # í‘œì¤€ ê²€ì¦
            if level.value in ['standard', 'comprehensive']:
                validations.extend(self._validate_content_quality(result, metrics))
                validations.extend(self._validate_completeness(result, metrics))
            
            # ì¢…í•© ê²€ì¦
            if level.value == 'comprehensive':
                validations.extend(self._validate_reliability(result, metrics))
                validations.extend(self._validate_consistency(result, metrics))
            
            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            metrics.calculate_overall_score()
            
            # ì „ì²´ ê²€ì¦ ê²°ê³¼ ì¢…í•©
            overall_validation = self._aggregate_validations(validations, metrics)
            
            logger.info(f"âœ… ê²€ì¦ ì™„ë£Œ - {result.agent_name}, "
                       f"ì ìˆ˜: {metrics.overall_score:.3f}, "
                       f"ìƒíƒœ: {overall_validation.status.value}")
            
            return overall_validation, metrics
            
        except Exception as e:
            logger.error(f"âŒ ê²€ì¦ ì¤‘ ì˜¤ë¥˜ - {result.agent_name}: {e}")
            
            error_validation = ValidationResult(
                status=ValidationStatus.FAILED,
                score=0.0,
                message=f"ê²€ì¦ ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                details={"error": str(e)},
                recommendations=["ê²°ê³¼ ë°ì´í„°ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ê³  ì¬ì‹¤í–‰í•´ì£¼ì„¸ìš”."]
            )
            
            return error_validation, metrics
    
    def validate_session_results(self, 
                                session: CollectionSession,
                                level: ValidationLevel = ValidationLevel.STANDARD) -> Dict[str, Tuple[ValidationResult, QualityMetrics]]:
        """ì„¸ì…˜ ë‚´ ëª¨ë“  ê²°ê³¼ ê²€ì¦"""
        
        logger.info(f"ğŸ” ì„¸ì…˜ ê²°ê³¼ ê²€ì¦ ì‹œì‘ - {session.session_id}, "
                   f"ì—ì´ì „íŠ¸: {len(session.collected_results)}ê°œ")
        
        validation_results = {}
        
        for agent_id, result in session.collected_results.items():
            try:
                validation, metrics = self.validate_agent_result(result, level)
                validation_results[agent_id] = (validation, metrics)
                
            except Exception as e:
                logger.error(f"âŒ ì—ì´ì „íŠ¸ {agent_id} ê²€ì¦ ì‹¤íŒ¨: {e}")
                
                error_validation = ValidationResult(
                    status=ValidationStatus.FAILED,
                    score=0.0,
                    message=f"ê²€ì¦ ì‹¤íŒ¨: {str(e)}",
                    details={"error": str(e)},
                    recommendations=[]
                )
                
                validation_results[agent_id] = (error_validation, QualityMetrics())
        
        logger.info(f"âœ… ì„¸ì…˜ ê²€ì¦ ì™„ë£Œ - {session.session_id}")
        
        return validation_results
    
    def identify_missing_information(self, 
                                   session: CollectionSession,
                                   expected_elements: List[str] = None) -> Dict[str, List[str]]:
        """ëˆ„ë½ëœ ì •ë³´ ì‹ë³„"""
        
        expected_elements = expected_elements or [
            'data_analysis', 'visualization', 'summary', 'insights'
        ]
        
        missing_info = {}
        
        for agent_id, result in session.collected_results.items():
            agent_missing = []
            
            # ê¸°ë³¸ ìš”ì†Œ í™•ì¸
            if not result.processed_text or len(result.processed_text) < self.min_text_length:
                agent_missing.append("ì¶©ë¶„í•œ í…ìŠ¤íŠ¸ ì„¤ëª…")
            
            # ì•„í‹°íŒ©íŠ¸ í™•ì¸
            if not result.artifacts:
                agent_missing.append("ë°ì´í„° ì‹œê°í™” ë˜ëŠ” ë¶„ì„ ê²°ê³¼ë¬¼")
            
            # ì˜ˆìƒ ìš”ì†Œ í™•ì¸
            text_lower = result.processed_text.lower()
            for element in expected_elements:
                if element.replace('_', ' ') not in text_lower:
                    agent_missing.append(f"{element} ê´€ë ¨ ë‚´ìš©")
            
            if agent_missing:
                missing_info[agent_id] = agent_missing
        
        return missing_info
    
    def generate_quality_report(self, 
                              validation_results: Dict[str, Tuple[ValidationResult, QualityMetrics]]) -> Dict[str, Any]:
        """í’ˆì§ˆ ë³´ê³ ì„œ ìƒì„±"""
        
        total_agents = len(validation_results)
        if total_agents == 0:
            return {"error": "ê²€ì¦í•  ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."}
        
        # ìƒíƒœë³„ í†µê³„
        status_counts = {}
        quality_scores = []
        
        for validation, metrics in validation_results.values():
            status = validation.status.value
            status_counts[status] = status_counts.get(status, 0) + 1
            quality_scores.append(metrics.overall_score)
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜
        avg_quality = sum(quality_scores) / len(quality_scores)
        
        # í’ˆì§ˆ ë“±ê¸‰ ê²°ì •
        if avg_quality >= self.quality_thresholds['excellent']:
            quality_grade = "Excellent"
        elif avg_quality >= self.quality_thresholds['good']:
            quality_grade = "Good"
        elif avg_quality >= self.quality_thresholds['acceptable']:
            quality_grade = "Acceptable"
        else:
            quality_grade = "Poor"
        
        # ê¶Œì¥ì‚¬í•­ ìˆ˜ì§‘
        all_recommendations = []
        for validation, _ in validation_results.values():
            all_recommendations.extend(validation.recommendations)
        
        # ì¤‘ë³µ ì œê±° ë° ìš°ì„ ìˆœìœ„ ì •ë ¬
        unique_recommendations = list(set(all_recommendations))
        
        return {
            "summary": {
                "total_agents": total_agents,
                "average_quality_score": round(avg_quality, 3),
                "quality_grade": quality_grade,
                "status_distribution": status_counts
            },
            "quality_metrics": {
                "scores": quality_scores,
                "min_score": min(quality_scores),
                "max_score": max(quality_scores),
                "std_deviation": self._calculate_std_dev(quality_scores)
            },
            "recommendations": unique_recommendations[:10],  # ìƒìœ„ 10ê°œ
            "detailed_results": {
                agent_id: {
                    "status": validation.status.value,
                    "score": metrics.overall_score,
                    "message": validation.message
                }
                for agent_id, (validation, metrics) in validation_results.items()
            }
        }
    
    def _validate_basic_structure(self, result: AgentResult, metrics: QualityMetrics) -> List[ValidationResult]:
        """ê¸°ë³¸ êµ¬ì¡° ê²€ì¦"""
        
        validations = []
        
        # 1. ì‘ë‹µ ì¡´ì¬ ì—¬ë¶€
        if not result.raw_response and not result.processed_text:
            validations.append(ValidationResult(
                status=ValidationStatus.FAILED,
                score=0.0,
                message="ì‘ë‹µ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤",
                details={"issue": "no_response"},
                recommendations=["ì—ì´ì „íŠ¸ê°€ ì ì ˆí•œ ì‘ë‹µì„ ìƒì„±í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."]
            ))
            metrics.data_integrity_score = 0.0
        else:
            metrics.data_integrity_score = 0.8
        
        # 2. í…ìŠ¤íŠ¸ ê¸¸ì´ ê²€ì¦
        text_length = len(result.processed_text)
        if text_length < self.min_text_length:
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=0.3,
                message=f"ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({text_length}ì)",
                details={"text_length": text_length, "min_required": self.min_text_length},
                recommendations=["ë” ìƒì„¸í•œ ë¶„ì„ì´ë‚˜ ì„¤ëª…ì„ ìš”ì²­í•˜ì„¸ìš”."]
            ))
            metrics.content_richness_score = 0.3
        elif text_length > self.max_text_length:
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=0.7,
                message=f"ì‘ë‹µì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({text_length}ì)",
                details={"text_length": text_length, "max_recommended": self.max_text_length},
                recommendations=["ì‘ë‹µì„ ìš”ì•½í•˜ê±°ë‚˜ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì„¸ìš”."]
            ))
            metrics.content_richness_score = 0.7
        else:
            metrics.content_richness_score = 0.9
        
        # 3. í˜•ì‹ ìœ íš¨ì„± ê²€ì¦
        format_score = 0.5
        
        # JSON í˜•ì‹ ì²´í¬ (ì•„í‹°íŒ©íŠ¸ê°€ ìˆëŠ” ê²½ìš°)
        if result.artifacts:
            valid_artifacts = 0
            for artifact in result.artifacts:
                if artifact.get('type') in self.expected_artifact_types:
                    valid_artifacts += 1
            
            if valid_artifacts == len(result.artifacts):
                format_score = 1.0
            elif valid_artifacts > 0:
                format_score = 0.7
        
        metrics.format_validity_score = format_score
        
        if format_score < 0.7:
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=format_score,
                message="ì¼ë¶€ ì•„í‹°íŒ©íŠ¸ì˜ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤",
                details={"valid_artifacts": valid_artifacts, "total_artifacts": len(result.artifacts)},
                recommendations=["ì•„í‹°íŒ©íŠ¸ ìƒì„± ê³¼ì •ì„ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”."]
            ))
        
        return validations
    
    def _validate_content_quality(self, result: AgentResult, metrics: QualityMetrics) -> List[ValidationResult]:
        """ë‚´ìš© í’ˆì§ˆ ê²€ì¦"""
        
        validations = []
        text = result.processed_text.lower()
        
        # 1. ì—ëŸ¬ íŒ¨í„´ ê°ì§€
        error_count = 0
        for pattern in self.error_patterns:
            error_count += len(re.findall(pattern, text, re.IGNORECASE))
        
        if error_count > 3:
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=0.3,
                message=f"ì—ëŸ¬ ê´€ë ¨ ë‚´ìš©ì´ ë§ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ ({error_count}ê°œ)",
                details={"error_count": error_count},
                recommendations=["ì—ì´ì „íŠ¸ ì‹¤í–‰ ê³¼ì •ì—ì„œ ë°œìƒí•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ì„¸ìš”."]
            ))
            metrics.accuracy_score = 0.3
        elif error_count > 0:
            metrics.accuracy_score = 0.7
        else:
            metrics.accuracy_score = 0.9
        
        # 2. ì„±ê³µ ì§€í‘œ í™•ì¸
        success_count = 0
        for pattern in self.success_indicators:
            success_count += len(re.findall(pattern, text, re.IGNORECASE))
        
        if success_count == 0:
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=0.4,
                message="ë¶„ì„ ì™„ë£Œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œê°€ ë¶€ì¡±í•©ë‹ˆë‹¤",
                details={"success_indicators": success_count},
                recommendations=["ë¶„ì„ ê²°ê³¼ì™€ ê²°ë¡ ì„ ëª…í™•íˆ ì œì‹œí•˜ì„¸ìš”."]
            ))
        
        return validations
    
    def _validate_completeness(self, result: AgentResult, metrics: QualityMetrics) -> List[ValidationResult]:
        """ì™„ì„±ë„ ê²€ì¦"""
        
        validations = []
        
        # 1. ê¸°ë³¸ ìš”ì†Œ ì™„ì„±ë„
        completeness_factors = []
        
        # í…ìŠ¤íŠ¸ ì„¤ëª…
        if result.processed_text and len(result.processed_text) > 50:
            completeness_factors.append(1.0)
        else:
            completeness_factors.append(0.3)
        
        # ì•„í‹°íŒ©íŠ¸
        if result.artifacts:
            completeness_factors.append(1.0)
        else:
            completeness_factors.append(0.0)
        
        # ë©”íƒ€ë°ì´í„°
        if result.meta:
            completeness_factors.append(0.8)
        else:
            completeness_factors.append(0.2)
        
        # ì‹¤í–‰ ì„±ê³µ
        if result.error_message:
            completeness_factors.append(0.2)
        else:
            completeness_factors.append(1.0)
        
        completeness_score = sum(completeness_factors) / len(completeness_factors)
        metrics.completeness_score = completeness_score
        
        if completeness_score < 0.5:
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=completeness_score,
                message=f"ê²°ê³¼ ì™„ì„±ë„ê°€ ë‚®ìŠµë‹ˆë‹¤ ({completeness_score:.1%})",
                details={"completeness_score": completeness_score},
                recommendations=["ëˆ„ë½ëœ ë¶„ì„ ìš”ì†Œë“¤ì„ ë³´ì™„í•˜ì„¸ìš”."]
            ))
        
        # 2. ì»¤ë²„ë¦¬ì§€ í‰ê°€
        text = result.processed_text.lower()
        coverage_keywords = ['data', 'analysis', 'result', 'chart', 'table', 'insight']
        found_keywords = sum(1 for kw in coverage_keywords if kw in text)
        
        coverage_score = found_keywords / len(coverage_keywords)
        metrics.coverage_score = coverage_score
        
        if coverage_score < 0.3:
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=coverage_score,
                message="ë¶„ì„ ë²”ìœ„ê°€ ì œí•œì ì…ë‹ˆë‹¤",
                details={"coverage_score": coverage_score},
                recommendations=["ë” í¬ê´„ì ì¸ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì„¸ìš”."]
            ))
        
        return validations
    
    def _validate_reliability(self, result: AgentResult, metrics: QualityMetrics) -> List[ValidationResult]:
        """ì‹ ë¢°ë„ ê²€ì¦"""
        
        validations = []
        
        # 1. ì‹¤í–‰ ì‹œê°„ ê¸°ë°˜ ì‹ ë¢°ë„
        exec_time = result.execution_duration
        if exec_time < 1.0:  # ë„ˆë¬´ ë¹ ë¥¸ ì‹¤í–‰
            reliability_score = 0.4
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=0.4,
                message=f"ì‹¤í–‰ ì‹œê°„ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({exec_time:.1f}ì´ˆ)",
                details={"execution_time": exec_time},
                recommendations=["ì¶©ë¶„í•œ ë¶„ì„ì´ ì´ë£¨ì–´ì¡ŒëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”."]
            ))
        elif exec_time > 300.0:  # ë„ˆë¬´ ê¸´ ì‹¤í–‰
            reliability_score = 0.6
            validations.append(ValidationResult(
                status=ValidationStatus.WARNING,
                score=0.6,
                message=f"ì‹¤í–‰ ì‹œê°„ì´ ë„ˆë¬´ ê¹ë‹ˆë‹¤ ({exec_time:.1f}ì´ˆ)",
                details={"execution_time": exec_time},
                recommendations=["ì²˜ë¦¬ ê³¼ì •ì„ ìµœì í™”í•˜ê±°ë‚˜ íƒ€ì„ì•„ì›ƒì„ ì¡°ì •í•˜ì„¸ìš”."]
            ))
        else:
            reliability_score = 0.9
        
        metrics.reliability_score = reliability_score
        
        # 2. ë°ì´í„° ì¼ê´€ì„±
        if result.artifacts:
            consistent_artifacts = 0
            for artifact in result.artifacts:
                if artifact.get('metadata') and artifact.get('content'):
                    consistent_artifacts += 1
            
            consistency_ratio = consistent_artifacts / len(result.artifacts)
            metrics.consistency_score = consistency_ratio
            
            if consistency_ratio < 0.7:
                validations.append(ValidationResult(
                    status=ValidationStatus.WARNING,
                    score=consistency_ratio,
                    message="ì•„í‹°íŒ©íŠ¸ ë°ì´í„° ì¼ê´€ì„±ì´ ë¶€ì¡±í•©ë‹ˆë‹¤",
                    details={"consistency_ratio": consistency_ratio},
                    recommendations=["ì•„í‹°íŒ©íŠ¸ ìƒì„± ê³¼ì •ì˜ ì¼ê´€ì„±ì„ í™•ë³´í•˜ì„¸ìš”."]
                ))
        else:
            metrics.consistency_score = 0.5
        
        return validations
    
    def _validate_consistency(self, result: AgentResult, metrics: QualityMetrics) -> List[ValidationResult]:
        """ì¼ê´€ì„± ê²€ì¦"""
        
        validations = []
        
        # í…ìŠ¤íŠ¸-ì•„í‹°íŒ©íŠ¸ ì¼ê´€ì„± í™•ì¸
        if result.artifacts and result.processed_text:
            text_mentions_charts = any(word in result.processed_text.lower() 
                                     for word in ['chart', 'graph', 'plot', 'visualization'])
            has_chart_artifacts = any(art.get('type') == 'plotly_chart' 
                                    for art in result.artifacts)
            
            text_mentions_tables = any(word in result.processed_text.lower() 
                                     for word in ['table', 'dataframe', 'data'])
            has_table_artifacts = any(art.get('type') == 'dataframe' 
                                    for art in result.artifacts)
            
            consistency_issues = []
            if text_mentions_charts and not has_chart_artifacts:
                consistency_issues.append("í…ìŠ¤íŠ¸ì—ì„œ ì°¨íŠ¸ë¥¼ ì–¸ê¸‰í–ˆì§€ë§Œ ì°¨íŠ¸ ì•„í‹°íŒ©íŠ¸ê°€ ì—†ìŒ")
            if text_mentions_tables and not has_table_artifacts:
                consistency_issues.append("í…ìŠ¤íŠ¸ì—ì„œ í…Œì´ë¸”ì„ ì–¸ê¸‰í–ˆì§€ë§Œ í…Œì´ë¸” ì•„í‹°íŒ©íŠ¸ê°€ ì—†ìŒ")
            
            if consistency_issues:
                validations.append(ValidationResult(
                    status=ValidationStatus.WARNING,
                    score=0.6,
                    message="í…ìŠ¤íŠ¸ì™€ ì•„í‹°íŒ©íŠ¸ ê°„ ì¼ê´€ì„± ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤",
                    details={"issues": consistency_issues},
                    recommendations=["í…ìŠ¤íŠ¸ ì„¤ëª…ê³¼ ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸ì˜ ì¼ì¹˜ì„±ì„ í™•ì¸í•˜ì„¸ìš”."]
                ))
        
        return validations
    
    def _aggregate_validations(self, validations: List[ValidationResult], metrics: QualityMetrics) -> ValidationResult:
        """ê²€ì¦ ê²°ê³¼ ì§‘ê³„"""
        
        if not validations:
            return ValidationResult(
                status=ValidationStatus.PASSED,
                score=metrics.overall_score,
                message="ëª¨ë“  ê²€ì¦ì„ í†µê³¼í–ˆìŠµë‹ˆë‹¤",
                details={"metrics": metrics.__dict__},
                recommendations=[]
            )
        
        # ìƒíƒœ ìš°ì„ ìˆœìœ„: FAILED > WARNING > PASSED
        has_failed = any(v.status == ValidationStatus.FAILED for v in validations)
        has_warning = any(v.status == ValidationStatus.WARNING for v in validations)
        
        if has_failed:
            overall_status = ValidationStatus.FAILED
        elif has_warning:
            overall_status = ValidationStatus.WARNING
        else:
            overall_status = ValidationStatus.PASSED
        
        # ë©”ì‹œì§€ ë° ê¶Œì¥ì‚¬í•­ ìˆ˜ì§‘
        messages = [v.message for v in validations]
        all_recommendations = []
        for v in validations:
            all_recommendations.extend(v.recommendations)
        
        return ValidationResult(
            status=overall_status,
            score=metrics.overall_score,
            message=f"{len(validations)}ê°œ ê²€ì¦ í•­ëª© ì¤‘ ë¬¸ì œ ë°œê²¬: " + "; ".join(messages),
            details={"validation_count": len(validations), "metrics": metrics.__dict__},
            recommendations=list(set(all_recommendations))  # ì¤‘ë³µ ì œê±°
        )
    
    def _calculate_std_dev(self, scores: List[float]) -> float:
        """í‘œì¤€í¸ì°¨ ê³„ì‚°"""
        if len(scores) <= 1:
            return 0.0
        
        mean = sum(scores) / len(scores)
        variance = sum((x - mean) ** 2 for x in scores) / (len(scores) - 1)
        return variance ** 0.5