"""
ë©€í‹° ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•© ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ ì—¬ëŸ¬ A2A ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¥¼ í†µí•©í•˜ì—¬ 
ì¤‘ë³µì„ ì œê±°í•˜ê³  ì¼ê´€ì„± ìˆëŠ” ìµœì¢… ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì‹œìŠ¤í…œì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•© ì•Œê³ ë¦¬ì¦˜
- ì¤‘ë³µ ì •ë³´ ì œê±° ë° ì¼ê´€ì„± í™•ë³´
- ì‚¬ìš©ì ì§ˆë¬¸ê³¼ì˜ ì—°ê´€ì„± ë¶„ì„
- ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê²°ê³¼ ì„ íƒ
"""

import json
import logging
import re
from collections import defaultdict, Counter
from dataclasses import dataclass, field
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Tuple
from enum import Enum

from .agent_result_collector import AgentResult, CollectionSession
from .result_validator import QualityMetrics, ValidationResult
from .conflict_detector import ConflictAnalysis, ConflictInstance, ResolutionStrategy

logger = logging.getLogger(__name__)

class IntegrationStrategy(Enum):
    """í†µí•© ì „ëµ"""
    QUALITY_WEIGHTED = "quality_weighted"      # í’ˆì§ˆ ê°€ì¤‘ í†µí•©
    CONSENSUS_BASED = "consensus_based"        # í•©ì˜ ê¸°ë°˜ í†µí•©
    BEST_RESULT = "best_result"               # ìµœê³  ê²°ê³¼ ì„ íƒ
    COMPREHENSIVE = "comprehensive"            # ì¢…í•© í†µí•©
    CONFLICT_AWARE = "conflict_aware"          # ì¶©ëŒ ì¸ì‹ í†µí•©

class ContentType(Enum):
    """ì½˜í…ì¸  ìœ í˜•"""
    STATISTICAL = "statistical"               # í†µê³„ ì •ë³´
    VISUALIZATION = "visualization"           # ì‹œê°í™”
    TEXTUAL = "textual"                      # í…ìŠ¤íŠ¸ ì„¤ëª…
    INSIGHT = "insight"                      # ì¸ì‚¬ì´íŠ¸
    RECOMMENDATION = "recommendation"         # ê¶Œì¥ì‚¬í•­

@dataclass
class IntegratedContent:
    """í†µí•©ëœ ì½˜í…ì¸ """
    content_type: ContentType
    content: Any
    confidence: float
    sources: List[str]  # ì†ŒìŠ¤ ì—ì´ì „íŠ¸ IDë“¤
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class IntegrationResult:
    """í†µí•© ê²°ê³¼"""
    session_id: str
    query: str
    strategy: IntegrationStrategy
    
    # í†µí•©ëœ ì½˜í…ì¸ 
    integrated_text: str = ""
    integrated_artifacts: List[Dict[str, Any]] = field(default_factory=list)
    integrated_insights: List[str] = field(default_factory=list)
    
    # í’ˆì§ˆ ì§€í‘œ
    overall_confidence: float = 0.0
    integration_quality: float = 0.0
    coverage_score: float = 0.0
    
    # ì†ŒìŠ¤ ì •ë³´
    contributing_agents: List[str] = field(default_factory=list)
    excluded_agents: List[str] = field(default_factory=list)
    
    # ë©”íƒ€ë°ì´í„°
    integration_time: datetime = field(default_factory=datetime.now)
    processing_notes: List[str] = field(default_factory=list)
    
    # ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ
    success: bool = True
    error_message: Optional[str] = None

class MultiAgentResultIntegrator:
    """ë©€í‹° ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•©ê¸°"""
    
    def __init__(self):
        # í†µí•© ì„¤ì •
        self.min_confidence_threshold = 0.3
        self.duplicate_similarity_threshold = 0.85
        self.consensus_threshold = 0.6
        
        # ê°€ì¤‘ì¹˜ ì„¤ì •
        self.quality_weight = 0.4
        self.confidence_weight = 0.3
        self.consistency_weight = 0.3
        
        # í‚¤ì›Œë“œ íŒ¨í„´
        self.insight_keywords = [
            'ë¶„ì„', 'analysis', 'ê²°ë¡ ', 'conclusion', 'ë°œê²¬', 'finding',
            'ì¸ì‚¬ì´íŠ¸', 'insight', 'íŒ¨í„´', 'pattern', 'íŠ¸ë Œë“œ', 'trend'
        ]
        
        self.recommendation_keywords = [
            'ê¶Œì¥', 'recommend', 'ì œì•ˆ', 'suggest', 'ê°œì„ ', 'improve',
            'ë‹¤ìŒë‹¨ê³„', 'next step', 'ì•¡ì…˜', 'action', 'ì¡°ì¹˜', 'measure'
        ]
    
    def integrate_results(self,
                         session: CollectionSession,
                         quality_metrics: Dict[str, QualityMetrics] = None,
                         conflict_analysis: ConflictAnalysis = None,
                         strategy: IntegrationStrategy = IntegrationStrategy.COMPREHENSIVE) -> IntegrationResult:
        """ë©€í‹° ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•©"""
        
        logger.info(f"ğŸ”„ ê²°ê³¼ í†µí•© ì‹œì‘ - ì„¸ì…˜ {session.session_id}, "
                   f"ì—ì´ì „íŠ¸ {len(session.collected_results)}ê°œ, "
                   f"ì „ëµ: {strategy.value}")
        
        integration_result = IntegrationResult(
            session_id=session.session_id,
            query=session.query,
            strategy=strategy
        )
        
        try:
            # 1. ìœ íš¨í•œ ê²°ê³¼ í•„í„°ë§
            valid_results = self._filter_valid_results(
                session.collected_results, quality_metrics
            )
            
            if not valid_results:
                integration_result.success = False
                integration_result.error_message = "í†µí•©í•  ìœ íš¨í•œ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤"
                return integration_result
            
            integration_result.contributing_agents = list(valid_results.keys())
            integration_result.excluded_agents = [
                aid for aid in session.collected_results.keys()
                if aid not in valid_results
            ]
            
            # 2. ì „ëµë³„ í†µí•© ì‹¤í–‰
            if strategy == IntegrationStrategy.QUALITY_WEIGHTED:
                self._integrate_quality_weighted(integration_result, valid_results, quality_metrics)
            
            elif strategy == IntegrationStrategy.CONSENSUS_BASED:
                self._integrate_consensus_based(integration_result, valid_results)
            
            elif strategy == IntegrationStrategy.BEST_RESULT:
                self._integrate_best_result(integration_result, valid_results, quality_metrics)
            
            elif strategy == IntegrationStrategy.COMPREHENSIVE:
                self._integrate_comprehensive(integration_result, valid_results, quality_metrics)
            
            elif strategy == IntegrationStrategy.CONFLICT_AWARE:
                self._integrate_conflict_aware(integration_result, valid_results, conflict_analysis)
            
            # 3. í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
            self._calculate_integration_metrics(integration_result, valid_results, quality_metrics)
            
            # 4. í›„ì²˜ë¦¬
            self._post_process_integration(integration_result)
            
            logger.info(f"âœ… ê²°ê³¼ í†µí•© ì™„ë£Œ - í’ˆì§ˆ: {integration_result.integration_quality:.3f}, "
                       f"ì‹ ë¢°ë„: {integration_result.overall_confidence:.3f}")
            
        except Exception as e:
            logger.error(f"âŒ ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜: {e}")
            integration_result.success = False
            integration_result.error_message = str(e)
        
        return integration_result
    
    def _filter_valid_results(self,
                             results: Dict[str, AgentResult],
                             quality_metrics: Dict[str, QualityMetrics] = None) -> Dict[str, AgentResult]:
        """ìœ íš¨í•œ ê²°ê³¼ í•„í„°ë§"""
        
        valid_results = {}
        
        for agent_id, result in results.items():
            # ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬
            if not result.processed_text and not result.artifacts:
                logger.warning(f"ì—ì´ì „íŠ¸ {agent_id}: ë¹ˆ ê²°ê³¼ë¡œ ì œì™¸")
                continue
            
            # ì—ëŸ¬ ìƒíƒœ í™•ì¸
            if result.error_message:
                logger.warning(f"ì—ì´ì „íŠ¸ {agent_id}: ì—ëŸ¬ ìƒíƒœë¡œ ì œì™¸ - {result.error_message}")
                continue
            
            # í’ˆì§ˆ ê¸°ì¤€ í™•ì¸
            if quality_metrics and agent_id in quality_metrics:
                quality_score = quality_metrics[agent_id].overall_score
                if quality_score < self.min_confidence_threshold:
                    logger.warning(f"ì—ì´ì „íŠ¸ {agent_id}: í’ˆì§ˆ ê¸°ì¤€ ë¯¸ë‹¬ë¡œ ì œì™¸ ({quality_score:.3f})")
                    continue
            
            valid_results[agent_id] = result
        
        logger.info(f"ìœ íš¨í•œ ê²°ê³¼: {len(valid_results)}/{len(results)}ê°œ")
        return valid_results
    
    def _integrate_quality_weighted(self,
                                  integration_result: IntegrationResult,
                                  results: Dict[str, AgentResult],
                                  quality_metrics: Dict[str, QualityMetrics] = None):
        """í’ˆì§ˆ ê°€ì¤‘ í†µí•©"""
        
        logger.info("í’ˆì§ˆ ê°€ì¤‘ í†µí•© ì‹¤í–‰")
        
        # í’ˆì§ˆ ê°€ì¤‘ì¹˜ ê³„ì‚°
        weights = {}
        total_weight = 0.0
        
        for agent_id in results.keys():
            if quality_metrics and agent_id in quality_metrics:
                weight = quality_metrics[agent_id].overall_score
            else:
                weight = 0.5  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            
            weights[agent_id] = weight
            total_weight += weight
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        if total_weight > 0:
            weights = {aid: w / total_weight for aid, w in weights.items()}
        
        # í…ìŠ¤íŠ¸ í†µí•© (ê°€ì¤‘ ì„ íƒ)
        text_segments = []
        for agent_id, result in results.items():
            if result.processed_text:
                weight = weights.get(agent_id, 0.0)
                # ê°€ì¤‘ì¹˜ê°€ ë†’ì€ ê²°ê³¼ ìš°ì„  í¬í•¨
                if weight > 0.2:  # 20% ì´ìƒ ê°€ì¤‘ì¹˜
                    text_segments.append({
                        'text': result.processed_text,
                        'weight': weight,
                        'agent_id': agent_id
                    })
        
        # ê°€ì¤‘ì¹˜ ìˆœìœ¼ë¡œ ì •ë ¬
        text_segments.sort(key=lambda x: x['weight'], reverse=True)
        
        # í†µí•© í…ìŠ¤íŠ¸ ìƒì„±
        integrated_parts = []
        for segment in text_segments:
            # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ìœ ì‚¬ë„ ê²€ì‚¬
            is_duplicate = False
            for existing_part in integrated_parts:
                if self._calculate_text_similarity(segment['text'], existing_part) > self.duplicate_similarity_threshold:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                integrated_parts.append(segment['text'])
        
        integration_result.integrated_text = '\n\n'.join(integrated_parts)
        
        # ì•„í‹°íŒ©íŠ¸ í†µí•© (í’ˆì§ˆ ê¸°ì¤€)
        self._integrate_artifacts_by_quality(integration_result, results, weights)
        
        integration_result.processing_notes.append(f"í’ˆì§ˆ ê°€ì¤‘ í†µí•© ì™„ë£Œ - {len(text_segments)}ê°œ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸")
    
    def _integrate_consensus_based(self,
                                 integration_result: IntegrationResult,
                                 results: Dict[str, AgentResult]):
        """í•©ì˜ ê¸°ë°˜ í†µí•©"""
        
        logger.info("í•©ì˜ ê¸°ë°˜ í†µí•© ì‹¤í–‰")
        
        # ê³µí†µ ìš”ì†Œ ì¶”ì¶œ
        common_elements = self._extract_common_elements(results)
        
        # í•©ì˜ëœ ë‚´ìš© ìš°ì„  í¬í•¨
        consensus_text_parts = []
        
        # ê³µí†µ í‚¤ì›Œë“œ/ì£¼ì œ ê¸°ë°˜ ë¬¸ì¥ ì¶”ì¶œ
        for common_element in common_elements:
            sentences = []
            for result in results.values():
                # í•´ë‹¹ ìš”ì†Œë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ë“¤ ì°¾ê¸°
                text_sentences = self._extract_sentences_with_element(
                    result.processed_text, common_element
                )
                sentences.extend(text_sentences)
            
            if sentences:
                # ê°€ì¥ ëŒ€í‘œì ì¸ ë¬¸ì¥ ì„ íƒ
                representative_sentence = self._select_representative_sentence(sentences)
                if representative_sentence:
                    consensus_text_parts.append(representative_sentence)
        
        # ê°œë³„ íŠ¹ìƒ‰ìˆëŠ” ë‚´ìš© ì¶”ê°€
        unique_parts = []
        for agent_id, result in results.items():
            unique_content = self._extract_unique_content(
                result.processed_text, common_elements
            )
            if unique_content:
                unique_parts.append({
                    'content': unique_content,
                    'agent_id': agent_id
                })
        
        # ìµœì¢… í…ìŠ¤íŠ¸ êµ¬ì„±
        final_parts = consensus_text_parts
        
        # ê³ ìœ  ë‚´ìš© ì¤‘ ê°€ì¹˜ ìˆëŠ” ê²ƒë“¤ ì¶”ê°€
        for unique_part in unique_parts[:3]:  # ìµœëŒ€ 3ê°œ
            final_parts.append(unique_part['content'])
        
        integration_result.integrated_text = '\n\n'.join(final_parts)
        
        # í•©ì˜ëœ ì•„í‹°íŒ©íŠ¸ ì„ íƒ
        self._integrate_artifacts_by_consensus(integration_result, results)
        
        integration_result.processing_notes.append(f"í•©ì˜ ê¸°ë°˜ í†µí•© ì™„ë£Œ - ê³µí†µìš”ì†Œ {len(common_elements)}ê°œ")
    
    def _integrate_best_result(self,
                             integration_result: IntegrationResult,
                             results: Dict[str, AgentResult],
                             quality_metrics: Dict[str, QualityMetrics] = None):
        """ìµœê³  ê²°ê³¼ ì„ íƒ"""
        
        logger.info("ìµœê³  ê²°ê³¼ ì„ íƒ í†µí•© ì‹¤í–‰")
        
        # ìµœê³  í’ˆì§ˆ ê²°ê³¼ ì„ íƒ
        best_agent_id = None
        best_score = -1.0
        
        for agent_id in results.keys():
            score = 0.0
            
            if quality_metrics and agent_id in quality_metrics:
                score = quality_metrics[agent_id].overall_score
            else:
                # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
                result = results[agent_id]
                score = (
                    (0.4 if result.processed_text else 0.0) +
                    (0.3 * min(1.0, len(result.artifacts) / 3)) +
                    (0.3 if not result.error_message else 0.0)
                )
            
            if score > best_score:
                best_score = score
                best_agent_id = agent_id
        
        if best_agent_id:
            best_result = results[best_agent_id]
            integration_result.integrated_text = best_result.processed_text
            integration_result.integrated_artifacts = best_result.artifacts.copy()
            
            # ë‹¤ë¥¸ ê²°ê³¼ì—ì„œ ë³´ì™„ ì •ë³´ ì¶”ê°€
            supplementary_info = []
            for agent_id, result in results.items():
                if agent_id != best_agent_id and result.processed_text:
                    # ìµœê³  ê²°ê³¼ì— ì—†ëŠ” ìœ ë‹ˆí¬í•œ ì •ë³´ ì°¾ê¸°
                    unique_info = self._find_unique_information(
                        result.processed_text, best_result.processed_text
                    )
                    if unique_info:
                        supplementary_info.append(unique_info)
            
            if supplementary_info:
                integration_result.integrated_text += "\n\n**ì¶”ê°€ ì •ë³´:**\n" + '\n'.join(supplementary_info)
            
            integration_result.processing_notes.append(f"ìµœê³  ê²°ê³¼ ì„ íƒ - ì—ì´ì „íŠ¸ {best_agent_id} (ì ìˆ˜: {best_score:.3f})")
        
    def _integrate_comprehensive(self,
                               integration_result: IntegrationResult,
                               results: Dict[str, AgentResult],
                               quality_metrics: Dict[str, QualityMetrics] = None):
        """ì¢…í•© í†µí•©"""
        
        logger.info("ì¢…í•© í†µí•© ì‹¤í–‰")
        
        # ì—¬ëŸ¬ í†µí•© ì „ëµ ì¡°í•©
        
        # 1. í’ˆì§ˆ ê¸°ë°˜ í•„í„°ë§
        high_quality_results = {}
        medium_quality_results = {}
        
        for agent_id, result in results.items():
            quality_score = 0.5
            if quality_metrics and agent_id in quality_metrics:
                quality_score = quality_metrics[agent_id].overall_score
            
            if quality_score >= 0.7:
                high_quality_results[agent_id] = result
            elif quality_score >= 0.4:
                medium_quality_results[agent_id] = result
        
        # 2. ê³ í’ˆì§ˆ ê²°ê³¼ ìš°ì„  í†µí•©
        primary_text_parts = []
        if high_quality_results:
            for result in high_quality_results.values():
                if result.processed_text:
                    primary_text_parts.append(result.processed_text)
        
        # 3. ì¤‘í’ˆì§ˆ ê²°ê³¼ì—ì„œ ë³´ì™„ ì •ë³´ ì¶”ì¶œ
        supplementary_parts = []
        for result in medium_quality_results.values():
            if result.processed_text:
                # ê¸°ì¡´ ë‚´ìš©ê³¼ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ì •ë³´ ì°¾ê¸°
                unique_content = result.processed_text
                for primary_part in primary_text_parts:
                    if self._calculate_text_similarity(unique_content, primary_part) > 0.7:
                        unique_content = ""
                        break
                
                if unique_content and len(unique_content) > 50:
                    supplementary_parts.append(unique_content)
        
        # 4. í…ìŠ¤íŠ¸ êµ¬ì¡°í™”
        final_text_parts = []
        
        # ì£¼ìš” ë¶„ì„ ê²°ê³¼
        if primary_text_parts:
            final_text_parts.append("## ğŸ” ì£¼ìš” ë¶„ì„ ê²°ê³¼\n")
            final_text_parts.extend(primary_text_parts)
        
        # ë³´ì¶© ì •ë³´
        if supplementary_parts:
            final_text_parts.append("\n## ğŸ“ ì¶”ê°€ ì •ë³´\n")
            final_text_parts.extend(supplementary_parts[:2])  # ìµœëŒ€ 2ê°œ
        
        integration_result.integrated_text = '\n\n'.join(final_text_parts)
        
        # 5. ì•„í‹°íŒ©íŠ¸ ì¢…í•© í†µí•©
        self._integrate_artifacts_comprehensive(integration_result, results, quality_metrics)
        
        # 6. ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ
        integration_result.integrated_insights = self._extract_integrated_insights(results)
        
        integration_result.processing_notes.append(
            f"ì¢…í•© í†µí•© ì™„ë£Œ - ê³ í’ˆì§ˆ {len(high_quality_results)}ê°œ, ì¤‘í’ˆì§ˆ {len(medium_quality_results)}ê°œ"
        )
    
    def _integrate_conflict_aware(self,
                                integration_result: IntegrationResult,
                                results: Dict[str, AgentResult],
                                conflict_analysis: ConflictAnalysis = None):
        """ì¶©ëŒ ì¸ì‹ í†µí•©"""
        
        logger.info("ì¶©ëŒ ì¸ì‹ í†µí•© ì‹¤í–‰")
        
        if not conflict_analysis or not conflict_analysis.conflicts:
            # ì¶©ëŒì´ ì—†ìœ¼ë©´ ì¢…í•© í†µí•©ìœ¼ë¡œ fallback
            self._integrate_comprehensive(integration_result, results)
            integration_result.processing_notes.append("ì¶©ëŒ ì—†ìŒ - ì¢…í•© í†µí•©ìœ¼ë¡œ ì§„í–‰")
            return
        
        # ì¶©ëŒ í•´ê²° ê¸°ë°˜ í†µí•©
        resolved_results = {}
        conflicted_agents = set()
        
        # ì¶©ëŒ ê´€ë ¨ ì—ì´ì „íŠ¸ ì‹ë³„
        for conflict in conflict_analysis.conflicts:
            conflicted_agents.update(conflict.involved_agents)
        
        # ì¶©ëŒ ì—†ëŠ” ê²°ê³¼ë“¤ ìš°ì„  ì±„íƒ
        for agent_id, result in results.items():
            if agent_id not in conflicted_agents:
                resolved_results[agent_id] = result
        
        # ì¶©ëŒ í•´ê²° ì „ëµ ì ìš©
        for conflict in conflict_analysis.conflicts:
            if conflict.suggested_strategy == ResolutionStrategy.QUALITY_BASED:
                # í’ˆì§ˆ ê¸°ë°˜ ì„ íƒ
                best_agent = self._select_best_quality_agent(
                    conflict.involved_agents, results
                )
                if best_agent and best_agent in results:
                    resolved_results[best_agent] = results[best_agent]
            
            elif conflict.suggested_strategy == ResolutionStrategy.CONSENSUS_BASED:
                # í•©ì˜ ë‚´ìš©ë§Œ ì±„íƒ
                consensus_content = self._extract_consensus_from_conflict(
                    conflict.involved_agents, results
                )
                if consensus_content:
                    # ê°€ìƒ ê²°ê³¼ ìƒì„±
                    consensus_result = AgentResult(
                        agent_id="consensus",
                        agent_name="Consensus",
                        endpoint="internal",
                        status=results[conflict.involved_agents[0]].status
                    )
                    consensus_result.processed_text = consensus_content
                    resolved_results["consensus"] = consensus_result
        
        # í•´ê²°ëœ ê²°ê³¼ë“¤ë¡œ í†µí•©
        if resolved_results:
            self._integrate_comprehensive(integration_result, resolved_results)
            
            # ì¶©ëŒ ì •ë³´ ì¶”ê°€
            if conflict_analysis.conflicts:
                conflict_note = f"\n\n**âš ï¸ ì¶©ëŒ í•´ê²° ì •ë³´:**\n"
                conflict_note += f"- ì´ {len(conflict_analysis.conflicts)}ê°œ ì¶©ëŒ ê°ì§€ ë° í•´ê²°\n"
                conflict_note += f"- ì¶©ëŒ ê´€ë ¨ ì—ì´ì „íŠ¸: {', '.join(conflicted_agents)}\n"
                
                integration_result.integrated_text += conflict_note
        
        integration_result.processing_notes.append(
            f"ì¶©ëŒ ì¸ì‹ í†µí•© ì™„ë£Œ - {len(conflict_analysis.conflicts)}ê°œ ì¶©ëŒ ì²˜ë¦¬"
        )
    
    def _integrate_artifacts_by_quality(self,
                                      integration_result: IntegrationResult,
                                      results: Dict[str, AgentResult],
                                      weights: Dict[str, float]):
        """í’ˆì§ˆ ê¸°ë°˜ ì•„í‹°íŒ©íŠ¸ í†µí•©"""
        
        # ì•„í‹°íŒ©íŠ¸ íƒ€ì…ë³„ ìµœê³  í’ˆì§ˆ ì„ íƒ
        artifact_by_type = defaultdict(list)
        
        for agent_id, result in results.items():
            weight = weights.get(agent_id, 0.0)
            
            for artifact in result.artifacts:
                art_type = artifact.get('type', 'unknown')
                artifact_by_type[art_type].append({
                    'artifact': artifact,
                    'weight': weight,
                    'agent_id': agent_id
                })
        
        # íƒ€ì…ë³„ ìµœê³  í’ˆì§ˆ ì•„í‹°íŒ©íŠ¸ ì„ íƒ
        for art_type, artifacts in artifact_by_type.items():
            # ê°€ì¤‘ì¹˜ ìˆœ ì •ë ¬
            artifacts.sort(key=lambda x: x['weight'], reverse=True)
            
            # ìµœê³  í’ˆì§ˆ ì•„í‹°íŒ©íŠ¸ ì„ íƒ
            best_artifact = artifacts[0]['artifact'].copy()
            best_artifact['source_agent'] = artifacts[0]['agent_id']
            best_artifact['quality_weight'] = artifacts[0]['weight']
            
            integration_result.integrated_artifacts.append(best_artifact)
    
    def _integrate_artifacts_by_consensus(self,
                                        integration_result: IntegrationResult,
                                        results: Dict[str, AgentResult]):
        """í•©ì˜ ê¸°ë°˜ ì•„í‹°íŒ©íŠ¸ í†µí•©"""
        
        # ì•„í‹°íŒ©íŠ¸ íƒ€ì…ë³„ ë¹ˆë„ ê³„ì‚°
        artifact_type_count = Counter()
        artifact_by_type = defaultdict(list)
        
        for agent_id, result in results.items():
            for artifact in result.artifacts:
                art_type = artifact.get('type', 'unknown')
                artifact_type_count[art_type] += 1
                artifact_by_type[art_type].append({
                    'artifact': artifact,
                    'agent_id': agent_id
                })
        
        # í•©ì˜ ì„ê³„ê°’ ì´ìƒì¸ ì•„í‹°íŒ©íŠ¸ íƒ€ì…ë§Œ í¬í•¨
        min_consensus = max(1, int(len(results) * self.consensus_threshold))
        
        for art_type, count in artifact_type_count.items():
            if count >= min_consensus:
                # í•´ë‹¹ íƒ€ì…ì˜ ëŒ€í‘œ ì•„í‹°íŒ©íŠ¸ ì„ íƒ
                artifacts = artifact_by_type[art_type]
                representative = artifacts[0]['artifact'].copy()  # ì²« ë²ˆì§¸ë¥¼ ëŒ€í‘œë¡œ
                representative['consensus_count'] = count
                representative['total_agents'] = len(results)
                
                integration_result.integrated_artifacts.append(representative)
    
    def _integrate_artifacts_comprehensive(self,
                                         integration_result: IntegrationResult,
                                         results: Dict[str, AgentResult],
                                         quality_metrics: Dict[str, QualityMetrics] = None):
        """ì¢…í•© ì•„í‹°íŒ©íŠ¸ í†µí•©"""
        
        # ì•„í‹°íŒ©íŠ¸ ìˆ˜ì§‘ ë° ì¤‘ë³µ ì œê±°
        unique_artifacts = {}
        
        for agent_id, result in results.items():
            quality_score = 0.5
            if quality_metrics and agent_id in quality_metrics:
                quality_score = quality_metrics[agent_id].overall_score
            
            for artifact in result.artifacts:
                art_type = artifact.get('type', 'unknown')
                art_key = f"{art_type}_{self._calculate_artifact_hash(artifact)}"
                
                if art_key not in unique_artifacts or quality_score > unique_artifacts[art_key]['quality']:
                    artifact_copy = artifact.copy()
                    artifact_copy['source_agent'] = agent_id
                    artifact_copy['quality_score'] = quality_score
                    
                    unique_artifacts[art_key] = {
                        'artifact': artifact_copy,
                        'quality': quality_score
                    }
        
        # í’ˆì§ˆ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶”ê°€
        sorted_artifacts = sorted(
            unique_artifacts.values(),
            key=lambda x: x['quality'],
            reverse=True
        )
        
        integration_result.integrated_artifacts = [
            item['artifact'] for item in sorted_artifacts
        ]
    
    def _calculate_integration_metrics(self,
                                     integration_result: IntegrationResult,
                                     results: Dict[str, AgentResult],
                                     quality_metrics: Dict[str, QualityMetrics] = None):
        """í†µí•© í’ˆì§ˆ ì§€í‘œ ê³„ì‚°"""
        
        # ì „ì²´ ì‹ ë¢°ë„ ê³„ì‚°
        confidence_scores = []
        for agent_id in integration_result.contributing_agents:
            if quality_metrics and agent_id in quality_metrics:
                confidence_scores.append(quality_metrics[agent_id].overall_score)
            else:
                confidence_scores.append(0.5)
        
        if confidence_scores:
            integration_result.overall_confidence = sum(confidence_scores) / len(confidence_scores)
        
        # í†µí•© í’ˆì§ˆ ê³„ì‚°
        quality_factors = []
        
        # í…ìŠ¤íŠ¸ í’ˆì§ˆ
        if integration_result.integrated_text:
            text_quality = min(1.0, len(integration_result.integrated_text) / 1000)
            quality_factors.append(text_quality)
        
        # ì•„í‹°íŒ©íŠ¸ í’ˆì§ˆ
        if integration_result.integrated_artifacts:
            artifact_quality = min(1.0, len(integration_result.integrated_artifacts) / 5)
            quality_factors.append(artifact_quality)
        
        # ì»¤ë²„ë¦¬ì§€ ì ìˆ˜
        total_agents = len(results)
        contributing_agents = len(integration_result.contributing_agents)
        integration_result.coverage_score = contributing_agents / total_agents if total_agents > 0 else 0.0
        
        if quality_factors:
            integration_result.integration_quality = sum(quality_factors) / len(quality_factors)
        
        # ì„±ê³µ ì—¬ë¶€ ê²°ì •
        integration_result.success = (
            integration_result.integration_quality > 0.3 and
            integration_result.overall_confidence > 0.3 and
            integration_result.coverage_score > 0.5
        )
    
    def _post_process_integration(self, integration_result: IntegrationResult):
        """í†µí•© ê²°ê³¼ í›„ì²˜ë¦¬"""
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        if integration_result.integrated_text:
            # ì¤‘ë³µ ë¬¸ë‹¨ ì œê±°
            paragraphs = integration_result.integrated_text.split('\n\n')
            unique_paragraphs = []
            
            for paragraph in paragraphs:
                is_duplicate = False
                for existing in unique_paragraphs:
                    if self._calculate_text_similarity(paragraph, existing) > 0.9:
                        is_duplicate = True
                        break
                
                if not is_duplicate and len(paragraph.strip()) > 10:
                    unique_paragraphs.append(paragraph)
            
            integration_result.integrated_text = '\n\n'.join(unique_paragraphs)
        
        # ì•„í‹°íŒ©íŠ¸ ë©”íƒ€ë°ì´í„° ì •ë¦¬
        for artifact in integration_result.integrated_artifacts:
            if 'metadata' not in artifact:
                artifact['metadata'] = {}
            
            artifact['metadata']['integration_time'] = integration_result.integration_time.isoformat()
            artifact['metadata']['integration_strategy'] = integration_result.strategy.value
    
    def _extract_common_elements(self, results: Dict[str, AgentResult]) -> List[str]:
        """ê³µí†µ ìš”ì†Œ ì¶”ì¶œ"""
        
        common_elements = []
        
        # ëª¨ë“  í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        all_words = []
        for result in results.values():
            if result.processed_text:
                # ë‹¨ì–´ í† í°í™” (ê°„ë‹¨í•œ ë°©ì‹)
                words = re.findall(r'\b\w+\b', result.processed_text.lower())
                all_words.extend(words)
        
        # ë¹ˆë„ ê¸°ë°˜ ê³µí†µ í‚¤ì›Œë“œ ì‹ë³„
        word_counts = Counter(all_words)
        
        # ìµœì†Œ ì ˆë°˜ ì´ìƒì˜ ê²°ê³¼ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” í‚¤ì›Œë“œ
        min_frequency = max(2, len(results) // 2)
        
        for word, count in word_counts.items():
            if count >= min_frequency and len(word) > 3:
                common_elements.append(word)
        
        return common_elements[:10]  # ìƒìœ„ 10ê°œ
    
    def _extract_sentences_with_element(self, text: str, element: str) -> List[str]:
        """íŠ¹ì • ìš”ì†Œë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ ì¶”ì¶œ"""
        
        sentences = re.split(r'[.!?]+', text)
        matching_sentences = []
        
        for sentence in sentences:
            if element.lower() in sentence.lower() and len(sentence.strip()) > 20:
                matching_sentences.append(sentence.strip())
        
        return matching_sentences
    
    def _select_representative_sentence(self, sentences: List[str]) -> Optional[str]:
        """ëŒ€í‘œ ë¬¸ì¥ ì„ íƒ"""
        
        if not sentences:
            return None
        
        # ê°€ì¥ ê¸´ ë¬¸ì¥ì„ ëŒ€í‘œë¡œ ì„ íƒ (ë” ë§ì€ ì •ë³´ í¬í•¨ ê°€ëŠ¥ì„±)
        return max(sentences, key=len)
    
    def _extract_unique_content(self, text: str, common_elements: List[str]) -> str:
        """ê³ ìœ  ì½˜í…ì¸  ì¶”ì¶œ"""
        
        # ê³µí†µ ìš”ì†Œë¥¼ ì œì™¸í•œ ë…íŠ¹í•œ ë‚´ìš© ì°¾ê¸°
        sentences = re.split(r'[.!?]+', text)
        unique_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 30:
                # ê³µí†µ ìš”ì†Œê°€ ë§ì´ í¬í•¨ë˜ì§€ ì•Šì€ ë¬¸ì¥
                common_count = sum(1 for element in common_elements 
                                 if element.lower() in sentence.lower())
                
                if common_count < len(common_elements) * 0.3:  # 30% ë¯¸ë§Œ
                    unique_sentences.append(sentence)
        
        return '. '.join(unique_sentences[:3])  # ìµœëŒ€ 3ë¬¸ì¥
    
    def _find_unique_information(self, source_text: str, reference_text: str) -> str:
        """ì°¸ì¡° í…ìŠ¤íŠ¸ì— ì—†ëŠ” ê³ ìœ  ì •ë³´ ì°¾ê¸°"""
        
        source_sentences = re.split(r'[.!?]+', source_text)
        unique_info = []
        
        for sentence in source_sentences:
            sentence = sentence.strip()
            if len(sentence) > 20:
                # ì°¸ì¡° í…ìŠ¤íŠ¸ì™€ ìœ ì‚¬ë„ê°€ ë‚®ì€ ë¬¸ì¥ ì°¾ê¸°
                similarity = self._calculate_text_similarity(sentence, reference_text)
                if similarity < 0.3:
                    unique_info.append(sentence)
        
        return '. '.join(unique_info[:2])  # ìµœëŒ€ 2ë¬¸ì¥
    
    def _extract_integrated_insights(self, results: Dict[str, AgentResult]) -> List[str]:
        """í†µí•© ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ"""
        
        insights = []
        
        # ê° ê²°ê³¼ì—ì„œ ì¸ì‚¬ì´íŠ¸ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ
        for result in results.values():
            if result.processed_text:
                text_lower = result.processed_text.lower()
                
                for keyword in self.insight_keywords:
                    if keyword in text_lower:
                        # í‚¤ì›Œë“œ ì£¼ë³€ ë¬¸ì¥ ì¶”ì¶œ
                        sentences = re.split(r'[.!?]+', result.processed_text)
                        for sentence in sentences:
                            if keyword in sentence.lower() and len(sentence) > 30:
                                insights.append(sentence.strip())
        
        # ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í•„í„°ë§
        unique_insights = []
        for insight in insights:
            is_duplicate = False
            for existing in unique_insights:
                if self._calculate_text_similarity(insight, existing) > 0.8:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_insights.append(insight)
        
        return unique_insights[:5]  # ìµœëŒ€ 5ê°œ
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        
        if not text1 or not text2:
            return 0.0
        
        try:
            # ë‹¨ì–´ ì§‘í•© ê¸°ë°˜ Jaccard ìœ ì‚¬ë„
            words1 = set(text1.lower().split())
            words2 = set(text2.lower().split())
            
            if not words1 and not words2:
                return 1.0
            
            intersection = words1 & words2
            union = words1 | words2
            
            return len(intersection) / len(union) if union else 0.0
            
        except Exception:
            return 0.0
    
    def _calculate_artifact_hash(self, artifact: Dict[str, Any]) -> str:
        """ì•„í‹°íŒ©íŠ¸ í•´ì‹œ ê³„ì‚° (ì¤‘ë³µ ê°ì§€ìš©)"""
        
        try:
            # ì£¼ìš” í•„ë“œë§Œìœ¼ë¡œ í•´ì‹œ ìƒì„±
            key_fields = ['type', 'title', 'columns', 'shape']
            hash_data = {}
            
            for field in key_fields:
                if field in artifact:
                    hash_data[field] = artifact[field]
            
            return str(hash(json.dumps(hash_data, sort_keys=True)))
            
        except Exception:
            return str(hash(str(artifact)))
    
    def _select_best_quality_agent(self, 
                                 agent_ids: List[str], 
                                 results: Dict[str, AgentResult]) -> Optional[str]:
        """í’ˆì§ˆ ê¸°ë°˜ ìµœê³  ì—ì´ì „íŠ¸ ì„ íƒ"""
        
        best_agent = None
        best_score = -1.0
        
        for agent_id in agent_ids:
            if agent_id in results:
                result = results[agent_id]
                
                # ê°„ë‹¨í•œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                score = 0.0
                if result.processed_text:
                    score += 0.5
                if result.artifacts:
                    score += 0.3
                if not result.error_message:
                    score += 0.2
                
                if score > best_score:
                    best_score = score
                    best_agent = agent_id
        
        return best_agent
    
    def _extract_consensus_from_conflict(self, 
                                       agent_ids: List[str], 
                                       results: Dict[str, AgentResult]) -> str:
        """ì¶©ëŒì—ì„œ í•©ì˜ ë‚´ìš© ì¶”ì¶œ"""
        
        if len(agent_ids) < 2:
            return ""
        
        # ê´€ë ¨ ê²°ê³¼ë“¤ì˜ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        texts = []
        for agent_id in agent_ids:
            if agent_id in results and results[agent_id].processed_text:
                texts.append(results[agent_id].processed_text)
        
        if len(texts) < 2:
            return ""
        
        # ê³µí†µ í‚¤ì›Œë“œ ê¸°ë°˜ í•©ì˜ ë‚´ìš© ì¶”ì¶œ
        common_elements = self._extract_common_elements({
            f"agent_{i}": AgentResult(
                agent_id=f"agent_{i}",
                agent_name=f"Agent{i}",
                endpoint="temp",
                status=results[agent_ids[0]].status
            ) for i, text in enumerate(texts)
        })
        
        # ê³µí†µ ìš”ì†Œë¥¼ í¬í•¨í•˜ëŠ” ë¬¸ì¥ë“¤ë¡œ í•©ì˜ ë‚´ìš© êµ¬ì„±
        consensus_sentences = []
        for text in texts:
            sentences = re.split(r'[.!?]+', text)
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 20:
                    # ê³µí†µ ìš”ì†Œë¥¼ ë§ì´ í¬í•¨í•˜ëŠ” ë¬¸ì¥
                    common_count = sum(1 for element in common_elements 
                                     if element.lower() in sentence.lower())
                    
                    if common_count >= len(common_elements) * 0.5:  # 50% ì´ìƒ
                        consensus_sentences.append(sentence)
        
        # ì¤‘ë³µ ì œê±°
        unique_sentences = []
        for sentence in consensus_sentences:
            is_duplicate = False
            for existing in unique_sentences:
                if self._calculate_text_similarity(sentence, existing) > 0.8:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_sentences.append(sentence)
        
        return '. '.join(unique_sentences[:3])  # ìµœëŒ€ 3ë¬¸ì¥