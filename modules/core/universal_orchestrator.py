"""
Universal Orchestrator - LLM ê¸°ë°˜ ë²”ìš© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

ê²€ì¦ëœ Universal Engine íŒ¨í„´ ê¸°ë°˜:
- MetaReasoningEngine: 4ë‹¨ê³„ ì¶”ë¡  (ì´ˆê¸° ê´€ì°° â†’ ë‹¤ê°ë„ ë¶„ì„ â†’ ìê°€ ê²€ì¦ â†’ ì ì‘ì  ì‘ë‹µ)
- A2AAgentDiscoverySystem: ë™ì  ì—ì´ì „íŠ¸ ì„ íƒ (í¬íŠ¸ 8306-8315)
- A2AWorkflowOrchestrator: ìˆœì°¨/ë³‘ë ¬ ì‹¤í–‰ íŒ¨í„´
- A2AResultIntegrator: ì¶©ëŒ í•´ê²° ë° í†µí•© ì¸ì‚¬ì´íŠ¸
- A2AErrorHandler: ì ì§„ì  ì¬ì‹œë„ ë° ì„œí‚· ë¸Œë ˆì´ì»¤ íŒ¨í„´
"""

import asyncio
import logging
from typing import Dict, List, Any, Optional, Tuple, AsyncGenerator
from datetime import datetime
import json
import uuid

from ..models import EnhancedTaskRequest, AgentProgressInfo, TaskState, StreamingResponse
from ..a2a.agent_client import A2AAgentClient
from .llm_recommendation_engine import LLMRecommendationEngine

# Universal Engine íŒ¨í„´ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
try:
    from core.universal_engine.meta_reasoning_engine import MetaReasoningEngine
    from core.universal_engine.a2a_integration.a2a_workflow_orchestrator import A2AWorkflowOrchestrator
    from core.universal_engine.a2a_integration.a2a_result_integrator import A2AResultIntegrator
    from core.universal_engine.a2a_integration.a2a_error_handler import A2AErrorHandler
    from core.universal_engine.a2a_integration.llm_based_agent_selector import LLMBasedAgentSelector
    from core.universal_engine.llm_factory import LLMFactory
    UNIVERSAL_ENGINE_AVAILABLE = True
except ImportError:
    UNIVERSAL_ENGINE_AVAILABLE = False

logger = logging.getLogger(__name__)


class UniversalOrchestrator:
    """
    ë²”ìš© ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ê²€ì¦ëœ Universal Engine íŒ¨í„´ ê¸°ë°˜
    LLM ì¶”ë¡  ëŠ¥ë ¥ì„ í™œìš©í•œ ë™ì  ì—ì´ì „íŠ¸ ì„ íƒ ë° ì›Œí¬í”Œë¡œìš° ì¡°ì •
    """
    
    # ê²€ì¦ëœ ì—ì´ì „íŠ¸ ì—­ëŸ‰ ë§¤í•‘ (Universal Engineì—ì„œ ê²€ì¦ë¨)
    AGENT_CAPABILITIES = {
        8306: {
            "name": "ğŸ§¹ Data Cleaning Agent",
            "description": "LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ë°ì´í„° ì •ë¦¬, ë¹ˆ ë°ì´í„° ì²˜ë¦¬, 7ë‹¨ê³„ í‘œì¤€ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤",
            "capabilities": ["data_cleaning", "missing_values", "outlier_detection", "data_validation"],
            "expertise": ["data_quality", "preprocessing", "anomaly_detection"]
        },
        8307: {
            "name": "ğŸ“ Data Loader Agent", 
            "description": "í†µí•© ë°ì´í„° ë¡œë”©, UTF-8 ì¸ì½”ë”© ë¬¸ì œ í•´ê²°, ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›",
            "capabilities": ["file_loading", "format_conversion", "encoding_handling", "data_import"],
            "expertise": ["file_formats", "data_ingestion", "encoding_resolution"]
        },
        8308: {
            "name": "ğŸ“Š Data Visualization Agent",
            "description": "Interactive ì‹œê°í™”, Plotly ê¸°ë°˜ ì°¨íŠ¸ ìƒì„±",
            "capabilities": ["interactive_charts", "plotly_visualization", "dashboard_creation"],
            "expertise": ["data_visualization", "chart_design", "interactive_plots"]
        },
        8309: {
            "name": "ğŸ”§ Data Wrangling Agent",
            "description": "ë°ì´í„° ë³€í™˜, ì¡°ì‘, êµ¬ì¡° ë³€ê²½",
            "capabilities": ["data_transformation", "reshaping", "merging", "aggregation"],
            "expertise": ["data_manipulation", "data_restructuring", "complex_transformations"]
        },
        8310: {
            "name": "âš™ï¸ Feature Engineering Agent",
            "description": "í”¼ì²˜ ìƒì„±, ë³€í™˜, ì„ íƒ, ì°¨ì› ì¶•ì†Œ",
            "capabilities": ["feature_creation", "feature_selection", "dimensionality_reduction"],
            "expertise": ["feature_engineering", "ml_preprocessing", "feature_optimization"]
        },
        8311: {
            "name": "ğŸ—„ï¸ SQL Database Agent",
            "description": "SQL ì¿¼ë¦¬ ì‹¤í–‰, ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°",
            "capabilities": ["sql_queries", "database_operations", "data_extraction"],
            "expertise": ["database_management", "sql_optimization", "data_retrieval"]
        },
        8312: {
            "name": "ğŸ” EDA Tools Agent",
            "description": "íƒìƒ‰ì  ë°ì´í„° ë¶„ì„, í†µê³„ ê³„ì‚°, íŒ¨í„´ ë°œê²¬",
            "capabilities": ["exploratory_analysis", "statistical_analysis", "pattern_discovery"],
            "expertise": ["data_exploration", "statistical_methods", "pattern_recognition"]
        },
        8313: {
            "name": "ğŸ¤– H2O ML Agent",
            "description": "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§, AutoML, ì˜ˆì¸¡ ë¶„ì„",
            "capabilities": ["machine_learning", "automl", "model_training", "predictions"],
            "expertise": ["ml_algorithms", "model_optimization", "predictive_analytics"]
        },
        8314: {
            "name": "ğŸ“ˆ MLflow Tools Agent",
            "description": "ëª¨ë¸ ê´€ë¦¬, ì‹¤í—˜ ì¶”ì , ë²„ì „ ê´€ë¦¬",
            "capabilities": ["model_management", "experiment_tracking", "version_control"],
            "expertise": ["ml_ops", "model_versioning", "experiment_management"]
        },
        8315: {
            "name": "ğŸ¼ Pandas Analyst Agent",
            "description": "íŒë‹¤ìŠ¤ ê¸°ë°˜ ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„",
            "capabilities": ["pandas_operations", "data_analysis", "statistical_computations"],
            "expertise": ["data_analysis", "pandas_expertise", "statistical_analysis"]
        }
    }
    
    def __init__(self):
        """Universal Orchestrator ì´ˆê¸°í™”"""
        
        # Universal Engine ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if UNIVERSAL_ENGINE_AVAILABLE:
            self.meta_reasoning_engine = MetaReasoningEngine()
            self.workflow_orchestrator = A2AWorkflowOrchestrator()
            self.result_integrator = A2AResultIntegrator()
            self.error_handler = A2AErrorHandler()
            self.agent_selector = LLMBasedAgentSelector()
            self.llm_client = LLMFactory.create_llm()
        else:
            self.meta_reasoning_engine = None
            self.workflow_orchestrator = None
            self.result_integrator = None
            self.error_handler = None
            self.agent_selector = None
            self.llm_client = None
        
        # A2A ì—ì´ì „íŠ¸ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.agent_clients = {
            port: A2AAgentClient(port) for port in self.AGENT_CAPABILITIES.keys()
        }
        
        # LLM ì¶”ì²œ ì—”ì§„
        self.recommendation_engine = LLMRecommendationEngine()
        
        # í™œì„± íƒœìŠ¤í¬ ì¶”ì 
        self.active_tasks: Dict[str, Dict] = {}
        
        logger.info("Universal Orchestrator initialized with proven patterns")
    
    async def orchestrate_analysis(self, 
                                 request: EnhancedTaskRequest,
                                 progress_callback: Optional[callable] = None) -> AsyncGenerator[StreamingResponse, None]:
        """
        ê²€ì¦ëœ 4ë‹¨ê³„ ë©”íƒ€ ì¶”ë¡ ì„ ì‚¬ìš©í•œ ë¶„ì„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜:
        1. ì´ˆê¸° ê´€ì°°: ë°ì´í„°ì™€ ì¿¼ë¦¬ ì˜ë„ íŒŒì•…
        2. ë‹¤ê°ë„ ë¶„ì„: ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì ‘ê·¼ë²• ê³ ë ¤  
        3. ìê°€ ê²€ì¦: ë¶„ì„ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„± í™•ì¸
        4. ì ì‘ì  ì‘ë‹µ: ìµœì  ì „ëµ ê²°ì •
        """
        task_id = request.id
        
        try:
            logger.info(f"Starting orchestrated analysis for task {task_id}")
            
            # íƒœìŠ¤í¬ ìƒíƒœ ì´ˆê¸°í™”
            self.active_tasks[task_id] = {
                'request': request,
                'start_time': datetime.now(),
                'status': 'started',
                'agents_working': [],
                'results': {}
            }
            
            # 1ë‹¨ê³„: ì´ˆê¸° ê´€ì°° (ë©”íƒ€ ì¶”ë¡ )
            yield StreamingResponse(
                content="ğŸ” **1ë‹¨ê³„: ì´ˆê¸° ê´€ì°°**\në°ì´í„°ì™€ ìš”ì²­ ì˜ë„ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                is_complete=False,
                chunk_index=0
            )
            
            meta_analysis = await self._perform_meta_reasoning(request)
            
            # 2ë‹¨ê³„: ë‹¤ê°ë„ ë¶„ì„ (ì—ì´ì „íŠ¸ ì„ íƒ)
            yield StreamingResponse(
                content="ğŸ¯ **2ë‹¨ê³„: ë‹¤ê°ë„ ë¶„ì„**\nìµœì ì˜ ì—ì´ì „íŠ¸ ì¡°í•©ì„ ì„ íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                is_complete=False,
                chunk_index=1
            )
            
            selected_agents = await self._select_optimal_agents(meta_analysis, request)
            
            # 3ë‹¨ê³„: ìê°€ ê²€ì¦ (ì›Œí¬í”Œë¡œìš° ê²€ì¦)
            yield StreamingResponse(
                content="âœ… **3ë‹¨ê³„: ìê°€ ê²€ì¦**\në¶„ì„ ê³„íšì˜ ë…¼ë¦¬ì  ì¼ê´€ì„±ì„ í™•ì¸í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                is_complete=False,
                chunk_index=2
            )
            
            validated_workflow = await self._validate_workflow(selected_agents, meta_analysis)
            
            # 4ë‹¨ê³„: ì ì‘ì  ì‘ë‹µ (ì‹¤í–‰)
            yield StreamingResponse(
                content="ğŸš€ **4ë‹¨ê³„: ì ì‘ì  ì‘ë‹µ**\nì—ì´ì „íŠ¸ë“¤ì´ í˜‘ì—…í•˜ì—¬ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤...",
                is_complete=False,
                chunk_index=3
            )
            
            # ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            async for result_chunk in self._execute_agent_workflow(
                validated_workflow, request, progress_callback
            ):
                yield result_chunk
            
            # ìµœì¢… ê²°ê³¼ í†µí•©
            final_results = await self._integrate_results(task_id)
            
            yield StreamingResponse(
                content=f"âœ¨ **ë¶„ì„ ì™„ë£Œ!**\n\n{final_results.get('summary', 'ë¶„ì„ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.')}",
                is_complete=True,
                chunk_index=999
            )
            
        except Exception as e:
            logger.error(f"Orchestration error for task {task_id}: {str(e)}")
            
            yield StreamingResponse(
                content=f"âŒ **ì˜¤ë¥˜ ë°œìƒ**: {str(e)}\n\nì£„ì†¡í•©ë‹ˆë‹¤. ë¶„ì„ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                is_complete=True,
                chunk_index=999
            )
        
        finally:
            # íƒœìŠ¤í¬ ì •ë¦¬
            if task_id in self.active_tasks:
                self.active_tasks[task_id]['status'] = 'completed'
                self.active_tasks[task_id]['end_time'] = datetime.now()
    
    async def _perform_meta_reasoning(self, request: EnhancedTaskRequest) -> Dict[str, Any]:
        """
        ê²€ì¦ëœ 4ë‹¨ê³„ ë©”íƒ€ ì¶”ë¡  ìˆ˜í–‰:
        - ì´ˆê¸° ê´€ì°°: ë°ì´í„°ì™€ ì¿¼ë¦¬ ì˜ë„ íŒŒì•…
        - ë‹¤ê°ë„ ë¶„ì„: ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì ‘ê·¼ë²• ê³ ë ¤
        - ìê°€ ê²€ì¦: ë¶„ì„ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„± í™•ì¸
        - ì ì‘ì  ì‘ë‹µ: ìµœì  ì „ëµ ê²°ì •
        """
        try:
            if UNIVERSAL_ENGINE_AVAILABLE and self.meta_reasoning_engine:
                # Universal Engine MetaReasoningEngine ì‚¬ìš©
                meta_analysis = await self.meta_reasoning_engine.perform_meta_reasoning(
                    query=request.user_message,
                    data=request.selected_datasets,
                    user_context=request.ui_context,
                    conversation_history=[]
                )
                
                return meta_analysis
            else:
                # ê¸°ë³¸ ë©”íƒ€ ì¶”ë¡ 
                return await self._basic_meta_reasoning(request)
                
        except Exception as e:
            logger.error(f"Meta reasoning error: {str(e)}")
            return await self._basic_meta_reasoning(request)
    
    async def _basic_meta_reasoning(self, request: EnhancedTaskRequest) -> Dict[str, Any]:
        """ê¸°ë³¸ ë©”íƒ€ ì¶”ë¡  (Universal Engineì´ ì—†ì„ ë•Œ)"""
        
        # ì‚¬ìš©ì ë©”ì‹œì§€ ë¶„ì„
        message_lower = request.user_message.lower()
        
        # ì˜ë„ ë¶„ë¥˜
        intent = 'general_analysis'
        if any(word in message_lower for word in ['visualize', 'plot', 'chart', 'graph']):
            intent = 'visualization'
        elif any(word in message_lower for word in ['clean', 'quality', 'missing', 'null']):
            intent = 'data_cleaning'
        elif any(word in message_lower for word in ['model', 'predict', 'ml', 'machine learning']):
            intent = 'machine_learning'
        elif any(word in message_lower for word in ['statistics', 'stats', 'summary', 'describe']):
            intent = 'statistical_analysis'
        
        # ë³µì¡ë„ ì¶”ì •
        complexity = 'intermediate'
        if len(request.selected_datasets) > 3:
            complexity = 'advanced'
        elif any(keyword in message_lower for keyword in ['simple', 'basic', 'quick']):
            complexity = 'beginner'
        
        return {
            'user_intent': intent,
            'complexity_level': complexity,
            'data_context': {
                'dataset_count': len(request.selected_datasets),
                'requires_integration': len(request.selected_datasets) > 1
            },
            'recommended_approach': 'sequential_with_integration',
            'priority_agents': self._get_priority_agents_for_intent(intent)
        }
    
    def _get_priority_agents_for_intent(self, intent: str) -> List[int]:
        """ì˜ë„ì— ë”°ë¥¸ ìš°ì„ ìˆœìœ„ ì—ì´ì „íŠ¸ ëª©ë¡"""
        intent_mapping = {
            'visualization': [8308, 8315, 8312],  # Visualization, Pandas, EDA
            'data_cleaning': [8306, 8315, 8312],  # Cleaning, Pandas, EDA
            'machine_learning': [8313, 8310, 8315, 8314],  # H2O ML, Feature Eng, Pandas, MLflow
            'statistical_analysis': [8312, 8315, 8308],  # EDA, Pandas, Visualization
            'general_analysis': [8315, 8312, 8308]  # Pandas, EDA, Visualization
        }
        
        return intent_mapping.get(intent, [8315, 8312, 8308])
    
    async def _select_optimal_agents(self, 
                                   meta_analysis: Dict[str, Any], 
                                   request: EnhancedTaskRequest) -> List[Dict[str, Any]]:
        """
        ê²€ì¦ëœ LLM ê¸°ë°˜ ì—ì´ì „íŠ¸ ì„ íƒ:
        - í•˜ë“œì½”ë”©ëœ ê·œì¹™ ì—†ì´ ìˆœìˆ˜ LLM ê¸°ë°˜ ì„ íƒ
        - ì‚¬ìš©ì ìš”ì²­ì˜ ë³¸ì§ˆì„ íŒŒì•…í•˜ì—¬ ì—ì´ì „íŠ¸ ì¡°í•© ê²°ì •
        - ìµœì ì˜ ì‹¤í–‰ ìˆœì„œ ë° ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ì„± ì‹ë³„
        """
        try:
            if UNIVERSAL_ENGINE_AVAILABLE and self.agent_selector:
                # Universal Engine LLMBasedAgentSelector ì‚¬ìš©
                selected_agents = await self.agent_selector.select_agents(
                    query=request.user_message,
                    available_agents=self.AGENT_CAPABILITIES,
                    meta_analysis=meta_analysis
                )
                
                return selected_agents
            else:
                # ê¸°ë³¸ ì—ì´ì „íŠ¸ ì„ íƒ
                return self._basic_agent_selection(meta_analysis, request)
                
        except Exception as e:
            logger.error(f"Agent selection error: {str(e)}")
            return self._basic_agent_selection(meta_analysis, request)
    
    def _basic_agent_selection(self, 
                             meta_analysis: Dict[str, Any], 
                             request: EnhancedTaskRequest) -> List[Dict[str, Any]]:
        """ê¸°ë³¸ ì—ì´ì „íŠ¸ ì„ íƒ ë¡œì§"""
        
        selected_agents = []
        priority_agents = meta_analysis.get('priority_agents', [8315, 8312, 8308])
        
        for port in priority_agents[:3]:  # ìµœëŒ€ 3ê°œ ì—ì´ì „íŠ¸
            if port in self.AGENT_CAPABILITIES:
                agent_info = self.AGENT_CAPABILITIES[port].copy()
                agent_info['port'] = port
                agent_info['execution_order'] = len(selected_agents) + 1
                selected_agents.append(agent_info)
        
        return selected_agents
    
    async def _validate_workflow(self, 
                                selected_agents: List[Dict[str, Any]], 
                                meta_analysis: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì›Œí¬í”Œë¡œìš° ê²€ì¦ ë° ìµœì í™”"""
        
        # ì˜ì¡´ì„± ë¶„ì„
        validated_workflow = []
        
        for agent in selected_agents:
            agent_config = agent.copy()
            
            # ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ì„± í™•ì¸
            agent_config['can_run_parallel'] = self._can_run_parallel(agent, validated_workflow)
            
            # ì…ë ¥ ì˜ì¡´ì„± ì„¤ì •
            agent_config['dependencies'] = self._get_agent_dependencies(agent, validated_workflow)
            
            validated_workflow.append(agent_config)
        
        return validated_workflow
    
    def _can_run_parallel(self, agent: Dict[str, Any], existing_agents: List[Dict[str, Any]]) -> bool:
        """ì—ì´ì „íŠ¸ê°€ ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œì§€ í™•ì¸"""
        
        # ë°ì´í„° ë³€ê²½í•˜ëŠ” ì—ì´ì „íŠ¸ë“¤ì€ ìˆœì°¨ ì‹¤í–‰
        data_modifying_agents = [8306, 8309, 8310]  # Cleaning, Wrangling, Feature Engineering
        
        if agent['port'] in data_modifying_agents:
            return False
        
        # ê°™ì€ ìœ í˜•ì˜ ì—ì´ì „íŠ¸ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì´ë©´ ìˆœì°¨
        for existing in existing_agents:
            if existing.get('capabilities', []) and agent.get('capabilities', []):
                if set(existing['capabilities']) & set(agent['capabilities']):
                    return False
        
        return True
    
    def _get_agent_dependencies(self, 
                               agent: Dict[str, Any], 
                               existing_agents: List[Dict[str, Any]]) -> List[int]:
        """ì—ì´ì „íŠ¸ ì˜ì¡´ì„± ë¶„ì„"""
        
        dependencies = []
        agent_port = agent['port']
        
        # ì¼ë°˜ì ì¸ ì˜ì¡´ì„± ê·œì¹™
        if agent_port == 8308:  # Visualizationì€ ë°ì´í„° ì²˜ë¦¬ í›„
            dependencies.extend([a['port'] for a in existing_agents 
                               if a['port'] in [8306, 8309, 8315]])
        
        elif agent_port == 8313:  # MLì€ Feature Engineering í›„
            dependencies.extend([a['port'] for a in existing_agents 
                               if a['port'] in [8310, 8306]])
        
        elif agent_port == 8314:  # MLflowëŠ” ML í›„
            dependencies.extend([a['port'] for a in existing_agents 
                               if a['port'] == 8313])
        
        return dependencies
    
    async def _execute_agent_workflow(self, 
                                    workflow: List[Dict[str, Any]], 
                                    request: EnhancedTaskRequest,
                                    progress_callback: Optional[callable] = None) -> AsyncGenerator[StreamingResponse, None]:
        """
        ê²€ì¦ëœ ìˆœì°¨/ë³‘ë ¬ ì‹¤í–‰ íŒ¨í„´ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        """
        task_id = request.id
        
        try:
            if UNIVERSAL_ENGINE_AVAILABLE and self.workflow_orchestrator:
                # Universal Engine A2AWorkflowOrchestrator ì‚¬ìš©
                async for result in self.workflow_orchestrator.execute_workflow(workflow, request):
                    yield StreamingResponse(
                        content=result.get('content', ''),
                        is_complete=result.get('is_complete', False),
                        chunk_index=result.get('chunk_index', 0),
                        progress_info=result.get('progress_info')
                    )
            else:
                # ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                async for result in self._basic_workflow_execution(workflow, request, progress_callback):
                    yield result
                    
        except Exception as e:
            logger.error(f"Workflow execution error: {str(e)}")
            yield StreamingResponse(
                content=f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                is_complete=True,
                chunk_index=999
            )
    
    async def _basic_workflow_execution(self, 
                                      workflow: List[Dict[str, Any]], 
                                      request: EnhancedTaskRequest,
                                      progress_callback: Optional[callable] = None) -> AsyncGenerator[StreamingResponse, None]:
        """ê¸°ë³¸ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰"""
        
        task_id = request.id
        completed_agents = set()
        agent_results = {}
        
        # ì—ì´ì „íŠ¸ë³„ ì§„í–‰ ìƒí™© ì¶”ì 
        agent_progress = {}
        for agent in workflow:
            port = agent['port']
            agent_progress[port] = AgentProgressInfo(
                port=port,
                name=agent['name'],
                status=TaskState.PENDING,
                execution_time=0.0,
                artifacts_generated=[],
                current_task="ëŒ€ê¸° ì¤‘"
            )
        
        total_agents = len(workflow)
        
        for i, agent in enumerate(workflow):
            port = agent['port']
            agent_name = agent['name']
            
            try:
                # ì˜ì¡´ì„± í™•ì¸
                dependencies = agent.get('dependencies', [])
                if not all(dep in completed_agents for dep in dependencies):
                    yield StreamingResponse(
                        content=f"â³ {agent_name}: ì˜ì¡´ì„± ëŒ€ê¸° ì¤‘...",
                        is_complete=False,
                        chunk_index=10 + i
                    )
                    continue
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰ ì‹œì‘
                agent_progress[port].status = TaskState.WORKING
                agent_progress[port].current_task = "ë¶„ì„ ì‹¤í–‰ ì¤‘"
                
                yield StreamingResponse(
                    content=f"ğŸš€ **{agent_name}** ì‹œì‘\në¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ìˆìŠµë‹ˆë‹¤...",
                    is_complete=False,
                    chunk_index=20 + i,
                    progress_info=self._create_progress_info(agent_progress)
                )
                
                # A2A ì—ì´ì „íŠ¸ í˜¸ì¶œ
                agent_client = self.agent_clients[port]
                
                start_time = datetime.now()
                
                # ì—ì´ì „íŠ¸ ìš”ì²­ ë°ì´í„° ì¤€ë¹„
                agent_request = {
                    "query": request.user_message,
                    "datasets": request.selected_datasets,
                    "context": {
                        "task_id": task_id,
                        "previous_results": agent_results,
                        "user_context": request.ui_context
                    }
                }
                
                # ì—ì´ì „íŠ¸ ì‹¤í–‰
                result = await agent_client.execute_task(agent_request)
                
                execution_time = (datetime.now() - start_time).total_seconds()
                
                # ê²°ê³¼ ì €ì¥
                agent_results[port] = result
                completed_agents.add(port)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                agent_progress[port].status = TaskState.COMPLETED
                agent_progress[port].execution_time = execution_time
                agent_progress[port].current_task = "ì™„ë£Œ"
                
                # ìƒì„±ëœ ì•„í‹°íŒ©íŠ¸ í™•ì¸
                artifacts = result.get('artifacts', [])
                agent_progress[port].artifacts_generated = [
                    art.get('type', 'unknown') for art in artifacts
                ]
                
                yield StreamingResponse(
                    content=f"âœ… **{agent_name}** ì™„ë£Œ ({execution_time:.1f}ì´ˆ)\n{len(artifacts)}ê°œì˜ ê²°ê³¼ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.",
                    is_complete=False,
                    chunk_index=30 + i,
                    progress_info=self._create_progress_info(agent_progress)
                )
                
                # ì§„í–‰ë¥  ì½œë°± í˜¸ì¶œ
                if progress_callback:
                    progress_callback(f"{agent_name} ì™„ë£Œ", (i + 1) / total_agents)
                
            except Exception as e:
                logger.error(f"Agent {port} execution error: {str(e)}")
                
                agent_progress[port].status = TaskState.FAILED
                agent_progress[port].current_task = f"ì˜¤ë¥˜: {str(e)}"
                
                yield StreamingResponse(
                    content=f"âŒ **{agent_name}** ì‹¤íŒ¨: {str(e)}",
                    is_complete=False,
                    chunk_index=40 + i,
                    progress_info=self._create_progress_info(agent_progress)
                )
        
        # ì›Œí¬í”Œë¡œìš° ì™„ë£Œ
        self.active_tasks[task_id]['results'] = agent_results
        
        yield StreamingResponse(
            content=f"ğŸ‰ **ì›Œí¬í”Œë¡œìš° ì™„ë£Œ!**\n{len(completed_agents)}/{total_agents}ê°œ ì—ì´ì „íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.",
            is_complete=False,
            chunk_index=90
        )
    
    def _create_progress_info(self, agent_progress: Dict[int, AgentProgressInfo]):
        """ì§„í–‰ ìƒí™© ì •ë³´ ìƒì„±"""
        from ..models import ProgressInfo
        
        agents_working = list(agent_progress.values())
        
        # ì§„í–‰ë¥  ê³„ì‚°
        total_agents = len(agents_working)
        completed_agents = sum(1 for agent in agents_working if agent.status == TaskState.COMPLETED)
        completion_percentage = (completed_agents / total_agents * 100) if total_agents > 0 else 0
        
        return ProgressInfo(
            agents_working=agents_working,
            current_step=f"{completed_agents}/{total_agents} ì—ì´ì „íŠ¸ ì™„ë£Œ",
            total_steps=total_agents,
            completion_percentage=completion_percentage
        )
    
    async def _integrate_results(self, task_id: str) -> Dict[str, Any]:
        """
        ê²€ì¦ëœ A2AResultIntegratorë¥¼ ì‚¬ìš©í•œ ê²°ê³¼ í†µí•©:
        - ì¶©ëŒ í•´ê²° ë° ì¼ê´€ì„± ê²€ì¦
        - í†µí•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        - ë©”íƒ€ë°ì´í„° ë³‘í•©
        """
        try:
            if task_id not in self.active_tasks:
                return {"summary": "ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
            
            agent_results = self.active_tasks[task_id]['results']
            
            if UNIVERSAL_ENGINE_AVAILABLE and self.result_integrator:
                # Universal Engine A2AResultIntegrator ì‚¬ìš©
                integrated_results = await self.result_integrator.integrate_results(agent_results)
                return integrated_results
            else:
                # ê¸°ë³¸ ê²°ê³¼ í†µí•©
                return self._basic_result_integration(agent_results)
                
        except Exception as e:
            logger.error(f"Result integration error: {str(e)}")
            return {"summary": f"ê²°ê³¼ í†µí•© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
    
    def _basic_result_integration(self, agent_results: Dict[int, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ê²°ê³¼ í†µí•©"""
        
        summary_parts = []
        total_artifacts = 0
        
        for port, result in agent_results.items():
            agent_name = self.AGENT_CAPABILITIES[port]['name']
            artifacts = result.get('artifacts', [])
            
            summary_parts.append(f"â€¢ **{agent_name}**: {len(artifacts)}ê°œ ê²°ê³¼ ìƒì„±")
            total_artifacts += len(artifacts)
        
        summary = f"ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\n\n" + "\n".join(summary_parts)
        summary += f"\n\nğŸ“Š **ì´ {total_artifacts}ê°œì˜ ë¶„ì„ ê²°ê³¼**ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤."
        
        return {
            "summary": summary,
            "total_artifacts": total_artifacts,
            "agent_count": len(agent_results),
            "execution_summary": summary_parts
        }
    
    async def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """íƒœìŠ¤í¬ ìƒíƒœ ì¡°íšŒ"""
        return self.active_tasks.get(task_id)
    
    async def cancel_task(self, task_id: str) -> bool:
        """íƒœìŠ¤í¬ ì·¨ì†Œ"""
        if task_id in self.active_tasks:
            self.active_tasks[task_id]['status'] = 'cancelled'
            return True
        return False
    
    def get_active_tasks(self) -> List[str]:
        """í™œì„± íƒœìŠ¤í¬ ëª©ë¡ ë°˜í™˜"""
        return [task_id for task_id, task_info in self.active_tasks.items() 
                if task_info['status'] not in ['completed', 'cancelled', 'failed']]
    
    async def health_check_agents(self) -> Dict[int, bool]:
        """ëª¨ë“  ì—ì´ì „íŠ¸ í—¬ìŠ¤ ì²´í¬"""
        health_status = {}
        
        for port, client in self.agent_clients.items():
            try:
                health_status[port] = await client.health_check()
            except Exception as e:
                logger.warning(f"Health check failed for agent {port}: {str(e)}")
                health_status[port] = False
        
        return health_status
    
    def get_agent_capabilities_summary(self) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ ì—­ëŸ‰ ìš”ì•½ ë°˜í™˜"""
        return {
            "total_agents": len(self.AGENT_CAPABILITIES),
            "agent_ports": list(self.AGENT_CAPABILITIES.keys()),
            "capabilities_by_agent": {
                port: info["capabilities"] 
                for port, info in self.AGENT_CAPABILITIES.items()
            },
            "expertise_areas": {
                port: info["expertise"]
                for port, info in self.AGENT_CAPABILITIES.items()
            }
        }