"""
LLM-Enhanced Security and Validation System

ê²€ì¦ëœ Universal Engine íŒ¨í„´:
- IntelligentThreatDetection: LLM ê¸°ë°˜ ìœ„í˜‘ íƒì§€
- SmartInputValidation: ì§€ëŠ¥í˜• ì…ë ¥ ê²€ì¦
- DataSanitization: ìë™ ë°ì´í„° ì‚´ê· 
- SecurityPolicyEnforcement: ë³´ì•ˆ ì •ì±… ìë™ ì ìš©
- AnomalyDetection: ì´ìƒ í–‰ë™ íƒì§€
"""

import re
import json
import logging
import hashlib
import secrets
from typing import Dict, List, Any, Optional, Tuple, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
import pandas as pd
from pathlib import Path
import mimetypes
import asyncio
import html

# Optional imports with fallbacks
try:
    import magic
    MAGIC_AVAILABLE = True
except ImportError:
    MAGIC_AVAILABLE = False

try:
    import bleach
    BLEACH_AVAILABLE = True
except ImportError:
    BLEACH_AVAILABLE = False

# Universal Engine íŒ¨í„´ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
try:
    from core.universal_engine.intelligent_threat_detection import IntelligentThreatDetection
    from core.universal_engine.smart_input_validation import SmartInputValidation
    from core.universal_engine.data_sanitization import DataSanitization
    from core.universal_engine.security_policy_enforcement import SecurityPolicyEnforcement
    from core.universal_engine.anomaly_detection import AnomalyDetection
    from core.universal_engine.llm_factory import LLMFactory
    UNIVERSAL_ENGINE_AVAILABLE = True
except ImportError:
    UNIVERSAL_ENGINE_AVAILABLE = False

logger = logging.getLogger(__name__)


class ThreatLevel(Enum):
    """ìœ„í˜‘ ìˆ˜ì¤€"""
    SAFE = "safe"
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ValidationResult(Enum):
    """ê²€ì¦ ê²°ê³¼"""
    VALID = "valid"
    SUSPICIOUS = "suspicious"
    MALICIOUS = "malicious"
    BLOCKED = "blocked"


@dataclass
class SecurityContext:
    """ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸"""
    user_id: str
    session_id: str
    ip_address: str
    user_agent: str
    timestamp: datetime
    request_count: int
    risk_score: float = 0.0
    previous_violations: List[str] = field(default_factory=list)


@dataclass
class ValidationReport:
    """ê²€ì¦ ë¦¬í¬íŠ¸"""
    validation_id: str
    timestamp: datetime
    input_type: str
    validation_result: ValidationResult
    threat_level: ThreatLevel
    issues_found: List[str]
    sanitized_data: Optional[Any] = None
    recommendations: List[str] = field(default_factory=list)
    llm_analysis: Optional[str] = None


@dataclass
class SecurityPolicy:
    """ë³´ì•ˆ ì •ì±…"""
    policy_id: str
    name: str
    description: str
    rules: List[Dict[str, Any]]
    enforcement_level: str  # 'strict', 'moderate', 'lenient'
    exceptions: List[str] = field(default_factory=list)


class LLMSecurityValidationSystem:
    """
    LLM ê¸°ë°˜ ë³´ì•ˆ ë° ê²€ì¦ ì‹œìŠ¤í…œ
    ì•…ì˜ì ì¸ ì…ë ¥ íƒì§€, ë°ì´í„° ê²€ì¦, ë³´ì•ˆ ì •ì±… ì ìš©
    """
    
    def __init__(self):
        """Security Validation System ì´ˆê¸°í™”"""
        
        # Universal Engine ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if UNIVERSAL_ENGINE_AVAILABLE:
            self.threat_detector = IntelligentThreatDetection()
            self.input_validator = SmartInputValidation()
            self.data_sanitizer = DataSanitization()
            self.policy_enforcer = SecurityPolicyEnforcement()
            self.anomaly_detector = AnomalyDetection()
            self.llm_client = LLMFactory.create_llm()
        else:
            self.threat_detector = None
            self.input_validator = None
            self.data_sanitizer = None
            self.policy_enforcer = None
            self.anomaly_detector = None
            self.llm_client = None
        
        # ë³´ì•ˆ ì„¤ì •
        self.security_config = self._initialize_security_config()
        
        # íŒŒì¼ ì—…ë¡œë“œ ì œí•œ
        self.file_upload_limits = {
            'max_size_mb': 100,
            'allowed_extensions': ['.csv', '.xlsx', '.xls', '.json', '.txt', '.tsv', '.parquet'],
            'allowed_mime_types': [
                'text/csv', 'text/plain',
                'application/vnd.ms-excel',
                'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
                'application/json',
                'application/octet-stream'  # for parquet
            ]
        }
        
        # SQL ì¸ì ì…˜ íŒ¨í„´
        self.sql_injection_patterns = [
            r"(\b(SELECT|INSERT|UPDATE|DELETE|DROP|UNION|CREATE|ALTER)\b)",
            r"(--|\#|\/\*|\*\/)",
            r"(\bOR\b\s*\d+\s*=\s*\d+)",
            r"(\bAND\b\s*\d+\s*=\s*\d+)",
            r"(\'|\"|;|\\)"
        ]
        
        # XSS íŒ¨í„´
        self.xss_patterns = [
            r"<script[^>]*>.*?</script>",
            r"javascript:",
            r"on\w+\s*=",
            r"<iframe[^>]*>",
            r"<object[^>]*>",
            r"<embed[^>]*>"
        ]
        
        # ì•…ì˜ì ì¸ íŒŒì¼ ì‹œê·¸ë‹ˆì²˜
        self.malicious_signatures = {
            'eicar': '44d88612fea8a8f36de82e1278abb02f',  # EICAR test file
            'exe_header': '4d5a',  # MZ header
            'elf_header': '7f454c46',  # ELF header
            'script_shebang': ['#!/bin/sh', '#!/bin/bash', '#!/usr/bin/env']
        }
        
        # ì„¸ì…˜ë³„ ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸
        self.security_contexts: Dict[str, SecurityContext] = {}
        
        # ì°¨ë‹¨ ëª©ë¡
        self.blocklist = {
            'ip_addresses': set(),
            'user_agents': set(),
            'file_hashes': set()
        }
        
        # ì†ë„ ì œí•œ ì„¤ì •
        self.rate_limits = {
            'requests_per_minute': 60,
            'uploads_per_hour': 20,
            'max_file_size_per_day_mb': 1000
        }
        
        logger.info("LLM Security Validation System initialized")
    
    def _initialize_security_config(self) -> Dict[str, Any]:
        """ë³´ì•ˆ ì„¤ì • ì´ˆê¸°í™”"""
        return {
            'enable_llm_threat_detection': True,
            'enable_content_filtering': True,
            'enable_rate_limiting': True,
            'enable_file_scanning': True,
            'log_security_events': True,
            'block_on_high_threat': True,
            'sanitize_inputs': True,
            'enforce_policies': True
        }
    
    async def validate_file_upload(self, 
                                  file_path: str,
                                  file_name: str,
                                  file_size: int,
                                  security_context: SecurityContext) -> ValidationReport:
        """
        íŒŒì¼ ì—…ë¡œë“œ ê²€ì¦
        - íŒŒì¼ í¬ê¸°, í˜•ì‹, ë‚´ìš© ê²€ì‚¬
        - ì•…ì„± ì½”ë“œ íƒì§€
        - LLM ê¸°ë°˜ ìœ„í˜‘ ë¶„ì„
        """
        try:
            validation_id = f"file_val_{int(datetime.now().timestamp())}_{hash(file_name) % 10000}"
            issues_found = []
            threat_level = ThreatLevel.SAFE
            
            # 1. íŒŒì¼ í¬ê¸° ê²€ì¦
            if file_size > self.file_upload_limits['max_size_mb'] * 1024 * 1024:
                issues_found.append(f"íŒŒì¼ í¬ê¸° ì´ˆê³¼: {file_size / 1024 / 1024:.1f}MB (ìµœëŒ€: {self.file_upload_limits['max_size_mb']}MB)")
                threat_level = ThreatLevel.MEDIUM
            
            # 2. íŒŒì¼ í™•ì¥ì ê²€ì¦
            file_extension = Path(file_name).suffix.lower()
            if file_extension not in self.file_upload_limits['allowed_extensions']:
                issues_found.append(f"í—ˆìš©ë˜ì§€ ì•Šì€ íŒŒì¼ í˜•ì‹: {file_extension}")
                threat_level = ThreatLevel.HIGH
            
            # 3. MIME íƒ€ì… ê²€ì¦
            try:
                if MAGIC_AVAILABLE:
                    mime_type = magic.from_file(file_path, mime=True)
                else:
                    # Fallback to mimetypes module
                    mime_type, _ = mimetypes.guess_type(file_path)
                    if mime_type is None:
                        mime_type = "application/octet-stream"
                
                if mime_type not in self.file_upload_limits['allowed_mime_types']:
                    issues_found.append(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ MIME íƒ€ì…: {mime_type}")
                    threat_level = max(threat_level, ThreatLevel.MEDIUM)
            except Exception as e:
                logger.warning(f"MIME type detection failed: {str(e)}")
            
            # 4. íŒŒì¼ ë‚´ìš© ê²€ì‚¬
            content_issues = await self._scan_file_content(file_path, file_name)
            if content_issues:
                issues_found.extend(content_issues)
                threat_level = ThreatLevel.HIGH
            
            # 5. íŒŒì¼ í•´ì‹œ í™•ì¸ (ë¸”ë™ë¦¬ìŠ¤íŠ¸)
            file_hash = self._calculate_file_hash(file_path)
            if file_hash in self.blocklist['file_hashes']:
                issues_found.append("ì°¨ë‹¨ëœ íŒŒì¼ (ë¸”ë™ë¦¬ìŠ¤íŠ¸)")
                threat_level = ThreatLevel.CRITICAL
            
            # 6. LLM ê¸°ë°˜ ìœ„í˜‘ ë¶„ì„ (Universal Engine ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
            llm_analysis = None
            if UNIVERSAL_ENGINE_AVAILABLE and self.threat_detector and threat_level.value in ['medium', 'high', 'critical']:
                threat_analysis = await self.threat_detector.analyze_file_threat(
                    file_name=file_name,
                    file_size=file_size,
                    file_type=file_extension,
                    content_preview=self._get_file_preview(file_path)
                )
                if threat_analysis.get('is_threat', False):
                    issues_found.append(f"LLM ìœ„í˜‘ íƒì§€: {threat_analysis.get('reason', 'Unknown')}")
                    threat_level = ThreatLevel.CRITICAL
                llm_analysis = threat_analysis.get('analysis', '')
            
            # ê²€ì¦ ê²°ê³¼ ê²°ì •
            if threat_level == ThreatLevel.CRITICAL:
                validation_result = ValidationResult.BLOCKED
            elif threat_level == ThreatLevel.HIGH:
                validation_result = ValidationResult.MALICIOUS
            elif threat_level == ThreatLevel.MEDIUM:
                validation_result = ValidationResult.SUSPICIOUS
            else:
                validation_result = ValidationResult.VALID
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = ValidationReport(
                validation_id=validation_id,
                timestamp=datetime.now(),
                input_type='file_upload',
                validation_result=validation_result,
                threat_level=threat_level,
                issues_found=issues_found,
                recommendations=self._generate_security_recommendations(issues_found, threat_level),
                llm_analysis=llm_analysis
            )
            
            # ë³´ì•ˆ ì´ë²¤íŠ¸ ë¡œê¹…
            if self.security_config['log_security_events'] and issues_found:
                self._log_security_event(security_context, report)
            
            return report
            
        except Exception as e:
            logger.error(f"Error in file upload validation: {str(e)}")
            return ValidationReport(
                validation_id=f"error_{datetime.now().timestamp()}",
                timestamp=datetime.now(),
                input_type='file_upload',
                validation_result=ValidationResult.BLOCKED,
                threat_level=ThreatLevel.CRITICAL,
                issues_found=[f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                recommendations=["ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”"]
            )
    
    async def _scan_file_content(self, file_path: str, file_name: str) -> List[str]:
        """íŒŒì¼ ë‚´ìš© ìŠ¤ìº”"""
        issues = []
        
        try:
            # í…ìŠ¤íŠ¸ íŒŒì¼ì¸ ê²½ìš° ë‚´ìš© ê²€ì‚¬
            if file_name.endswith(('.csv', '.txt', '.json', '.tsv')):
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read(1024 * 10)  # ì²˜ìŒ 10KBë§Œ ê²€ì‚¬
                
                # SQL ì¸ì ì…˜ íŒ¨í„´ ê²€ì‚¬
                for pattern in self.sql_injection_patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        issues.append(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ SQL íŒ¨í„´ ë°œê²¬: {pattern[:20]}...")
                        break
                
                # XSS íŒ¨í„´ ê²€ì‚¬
                for pattern in self.xss_patterns:
                    if re.search(pattern, content, re.IGNORECASE):
                        issues.append(f"ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ìŠ¤í¬ë¦½íŠ¸ íŒ¨í„´ ë°œê²¬")
                        break
                
                # ì‹¤í–‰ ê°€ëŠ¥í•œ ì½”ë“œ ê²€ì‚¬
                for shebang in self.malicious_signatures['script_shebang']:
                    if content.startswith(shebang):
                        issues.append("ì‹¤í–‰ ê°€ëŠ¥í•œ ìŠ¤í¬ë¦½íŠ¸ íŒŒì¼")
                        break
            
            # Excel íŒŒì¼ì˜ ê²½ìš° ë§¤í¬ë¡œ ê²€ì‚¬
            elif file_name.endswith(('.xlsx', '.xls')):
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” openpyxl ë“±ì„ ì‚¬ìš©í•˜ì—¬ ë§¤í¬ë¡œ ê²€ì‚¬
                pass
            
        except Exception as e:
            logger.error(f"Error scanning file content: {str(e)}")
        
        return issues
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        try:
            sha256_hash = hashlib.sha256()
            with open(file_path, "rb") as f:
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            return sha256_hash.hexdigest()
        except Exception as e:
            logger.error(f"Error calculating file hash: {str(e)}")
            return ""
    
    def _get_file_preview(self, file_path: str, max_size: int = 1024) -> str:
        """íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ì¶”ì¶œ"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read(max_size)
        except Exception:
            return ""
    
    async def validate_user_input(self, 
                                 input_text: str,
                                 input_type: str,
                                 security_context: SecurityContext) -> ValidationReport:
        """
        ì‚¬ìš©ì ì…ë ¥ ê²€ì¦
        - SQL ì¸ì ì…˜, XSS ë°©ì§€
        - ì•…ì˜ì ì¸ íŒ¨í„´ íƒì§€
        - LLM ê¸°ë°˜ ì˜ë„ ë¶„ì„
        """
        try:
            validation_id = f"input_val_{int(datetime.now().timestamp())}_{hash(input_text) % 10000}"
            issues_found = []
            threat_level = ThreatLevel.SAFE
            sanitized_input = input_text
            
            # 1. ì…ë ¥ ê¸¸ì´ ê²€ì¦
            if len(input_text) > 10000:
                issues_found.append("ê³¼ë„í•˜ê²Œ ê¸´ ì…ë ¥")
                threat_level = ThreatLevel.MEDIUM
                sanitized_input = input_text[:10000]
            
            # 2. SQL ì¸ì ì…˜ íŒ¨í„´ ê²€ì‚¬
            for pattern in self.sql_injection_patterns:
                if re.search(pattern, input_text, re.IGNORECASE):
                    issues_found.append("SQL ì¸ì ì…˜ íŒ¨í„´ ê°ì§€")
                    threat_level = max(threat_level, ThreatLevel.HIGH)
                    # ìœ„í—˜í•œ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
                    sanitized_input = re.sub(pattern, '', sanitized_input, flags=re.IGNORECASE)
            
            # 3. XSS íŒ¨í„´ ê²€ì‚¬
            for pattern in self.xss_patterns:
                if re.search(pattern, input_text, re.IGNORECASE):
                    issues_found.append("XSS íŒ¨í„´ ê°ì§€")
                    threat_level = max(threat_level, ThreatLevel.HIGH)
                    
            # ê³ ê¸‰ XSS ì‚´ê·  (bleach ì‚¬ìš© ê°€ëŠ¥ì‹œ)
            if BLEACH_AVAILABLE:
                sanitized_input = bleach.clean(
                    sanitized_input,
                    tags=[],  # ëª¨ë“  HTML íƒœê·¸ ì œê±°
                    attributes={},
                    strip=True,
                    protocols=['http', 'https']
                )
            else:
                # Fallback: ë” ê°•ë ¥í•œ HTML ì‚´ê· 
                sanitized_input = html.escape(sanitized_input)
                # ì¶”ê°€ XSS íŒ¨í„´ ì œê±°
                sanitized_input = re.sub(r'<[^>]*>', '', sanitized_input)
                sanitized_input = re.sub(r'javascript:', '', sanitized_input, flags=re.IGNORECASE)
                sanitized_input = re.sub(r'on\w+\s*=', '', sanitized_input, flags=re.IGNORECASE)
            
            # 4. íŠ¹ìˆ˜ ë¬¸ì ê²€ì‚¬
            special_chars = re.findall(r'[^\w\s\-.,!?@#$%&*()+=\[\]{}:;"\'<>/\\|~`]', input_text)
            if len(special_chars) > 10:
                issues_found.append(f"ê³¼ë„í•œ íŠ¹ìˆ˜ ë¬¸ì ì‚¬ìš©: {len(special_chars)}ê°œ")
                threat_level = max(threat_level, ThreatLevel.MEDIUM)
            
            # 5. LLM ê¸°ë°˜ ì˜ë„ ë¶„ì„
            llm_analysis = None
            if UNIVERSAL_ENGINE_AVAILABLE and self.input_validator and input_type == 'user_query':
                intent_analysis = await self.input_validator.analyze_user_intent(
                    input_text=input_text,
                    context=security_context
                )
                if intent_analysis.get('is_malicious', False):
                    issues_found.append(f"ì•…ì˜ì ì¸ ì˜ë„ ê°ì§€: {intent_analysis.get('intent_type', 'Unknown')}")
                    threat_level = ThreatLevel.CRITICAL
                llm_analysis = intent_analysis.get('analysis', '')
            
            # ê²€ì¦ ê²°ê³¼ ê²°ì •
            if threat_level == ThreatLevel.CRITICAL:
                validation_result = ValidationResult.BLOCKED
            elif threat_level == ThreatLevel.HIGH:
                validation_result = ValidationResult.MALICIOUS
            elif issues_found:
                validation_result = ValidationResult.SUSPICIOUS
            else:
                validation_result = ValidationResult.VALID
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = ValidationReport(
                validation_id=validation_id,
                timestamp=datetime.now(),
                input_type=input_type,
                validation_result=validation_result,
                threat_level=threat_level,
                issues_found=issues_found,
                sanitized_data=sanitized_input if self.security_config['sanitize_inputs'] else None,
                recommendations=self._generate_security_recommendations(issues_found, threat_level),
                llm_analysis=llm_analysis
            )
            
            return report
            
        except Exception as e:
            logger.error(f"Error in user input validation: {str(e)}")
            return ValidationReport(
                validation_id=f"error_{datetime.now().timestamp()}",
                timestamp=datetime.now(),
                input_type=input_type,
                validation_result=ValidationResult.BLOCKED,
                threat_level=ThreatLevel.CRITICAL,
                issues_found=[f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                recommendations=["ì…ë ¥ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”"]
            )
    
    async def validate_data_access(self, 
                                 resource_type: str,
                                 resource_id: str,
                                 action: str,
                                 security_context: SecurityContext) -> ValidationReport:
        """
        ë°ì´í„° ì ‘ê·¼ ê¶Œí•œ ê²€ì¦
        - ì‚¬ìš©ì ê¶Œí•œ í™•ì¸
        - ì ‘ê·¼ íŒ¨í„´ ë¶„ì„
        - ì´ìƒ í–‰ë™ íƒì§€
        """
        try:
            validation_id = f"access_val_{int(datetime.now().timestamp())}"
            issues_found = []
            threat_level = ThreatLevel.SAFE
            
            # 1. ì„¸ì…˜ ìœ íš¨ì„± ê²€ì¦
            if security_context.session_id not in self.security_contexts:
                issues_found.append("ìœ íš¨í•˜ì§€ ì•Šì€ ì„¸ì…˜")
                threat_level = ThreatLevel.HIGH
            
            # 2. ì†ë„ ì œí•œ í™•ì¸
            if self._is_rate_limited(security_context):
                issues_found.append("ì†ë„ ì œí•œ ì´ˆê³¼")
                threat_level = ThreatLevel.MEDIUM
            
            # 3. ì´ìƒ ì ‘ê·¼ íŒ¨í„´ íƒì§€
            if UNIVERSAL_ENGINE_AVAILABLE and self.anomaly_detector:
                anomaly_result = await self.anomaly_detector.detect_access_anomaly(
                    user_id=security_context.user_id,
                    resource_type=resource_type,
                    action=action,
                    timestamp=security_context.timestamp
                )
                if anomaly_result.get('is_anomaly', False):
                    issues_found.append(f"ì´ìƒ ì ‘ê·¼ íŒ¨í„´: {anomaly_result.get('anomaly_type', 'Unknown')}")
                    threat_level = ThreatLevel.HIGH
            
            # 4. ê¶Œí•œ ì •ì±… í™•ì¸
            if not self._check_access_policy(security_context, resource_type, action):
                issues_found.append("ì ‘ê·¼ ê¶Œí•œ ì—†ìŒ")
                threat_level = ThreatLevel.CRITICAL
            
            # ê²€ì¦ ê²°ê³¼ ê²°ì •
            if threat_level in [ThreatLevel.HIGH, ThreatLevel.CRITICAL]:
                validation_result = ValidationResult.BLOCKED
            elif issues_found:
                validation_result = ValidationResult.SUSPICIOUS
            else:
                validation_result = ValidationResult.VALID
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report = ValidationReport(
                validation_id=validation_id,
                timestamp=datetime.now(),
                input_type='data_access',
                validation_result=validation_result,
                threat_level=threat_level,
                issues_found=issues_found,
                recommendations=self._generate_security_recommendations(issues_found, threat_level)
            )
            
            return report
            
        except Exception as e:
            logger.error(f"Error in data access validation: {str(e)}")
            return ValidationReport(
                validation_id=f"error_{datetime.now().timestamp()}",
                timestamp=datetime.now(),
                input_type='data_access',
                validation_result=ValidationResult.BLOCKED,
                threat_level=ThreatLevel.CRITICAL,
                issues_found=[f"ê²€ì¦ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"],
                recommendations=["ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”"]
            )
    
    def _is_rate_limited(self, security_context: SecurityContext) -> bool:
        """ì†ë„ ì œí•œ í™•ì¸"""
        # ê°„ë‹¨í•œ ì†ë„ ì œí•œ ë¡œì§ (ì‹¤ì œë¡œëŠ” Redis ë“± ì‚¬ìš©)
        return security_context.request_count > self.rate_limits['requests_per_minute']
    
    def _check_access_policy(self, 
                           security_context: SecurityContext,
                           resource_type: str,
                           action: str) -> bool:
        """ì ‘ê·¼ ì •ì±… í™•ì¸"""
        # ê¸°ë³¸ ì •ì±…: ëª¨ë“  ì½ê¸° í—ˆìš©, ì“°ê¸°ëŠ” ì œí•œ
        if action == 'read':
            return True
        elif action == 'write' and resource_type in ['user_data', 'analysis_results']:
            return True
        return False
    
    def _generate_security_recommendations(self, 
                                         issues: List[str], 
                                         threat_level: ThreatLevel) -> List[str]:
        """ë³´ì•ˆ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if threat_level == ThreatLevel.CRITICAL:
            recommendations.append("ğŸš¨ ì¦‰ì‹œ ë³´ì•ˆíŒ€ì— ë³´ê³ í•˜ì„¸ìš”")
            recommendations.append("ğŸ”’ í•´ë‹¹ ì„¸ì…˜ì„ ì¢…ë£Œí•˜ê³  ì¬ì¸ì¦ì„ ìš”êµ¬í•˜ì„¸ìš”")
        
        elif threat_level == ThreatLevel.HIGH:
            recommendations.append("âš ï¸ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ í™œë™ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤")
            recommendations.append("ğŸ” ì…ë ¥ ë‚´ìš©ì„ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”")
        
        elif threat_level == ThreatLevel.MEDIUM:
            recommendations.append("ğŸ“‹ ë³´ì•ˆ ê°€ì´ë“œë¼ì¸ì„ ì°¸ê³ í•˜ì„¸ìš”")
            recommendations.append("âœ… ì•ˆì „í•œ ì…ë ¥ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”")
        
        # íŠ¹ì • ì´ìŠˆë³„ ê¶Œì¥ì‚¬í•­
        if "SQL ì¸ì ì…˜" in str(issues):
            recommendations.append("ğŸ’¡ íŠ¹ìˆ˜ ë¬¸ì ì‚¬ìš©ì„ í”¼í•˜ê³  ì¼ë°˜ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”")
        
        if "íŒŒì¼ í¬ê¸°" in str(issues):
            recommendations.append("ğŸ“¦ íŒŒì¼ì„ ì••ì¶•í•˜ê±°ë‚˜ ë¶„í• í•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”")
        
        if "ì†ë„ ì œí•œ" in str(issues):
            recommendations.append("â±ï¸ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”")
        
        return recommendations
    
    def _log_security_event(self, 
                          security_context: SecurityContext,
                          validation_report: ValidationReport):
        """ë³´ì•ˆ ì´ë²¤íŠ¸ ë¡œê¹…"""
        event = {
            'timestamp': validation_report.timestamp.isoformat(),
            'user_id': security_context.user_id,
            'session_id': security_context.session_id,
            'ip_address': security_context.ip_address,
            'validation_result': validation_report.validation_result.value,
            'threat_level': validation_report.threat_level.value,
            'issues': validation_report.issues_found
        }
        
        logger.warning(f"Security event: {json.dumps(event)}")
        
        # ë†’ì€ ìœ„í˜‘ ìˆ˜ì¤€ì˜ ê²½ìš° ì¶”ê°€ ì¡°ì¹˜
        if validation_report.threat_level in [ThreatLevel.HIGH, ThreatLevel.CRITICAL]:
            self._handle_high_threat(security_context, validation_report)
    
    def _handle_high_threat(self, 
                          security_context: SecurityContext,
                          validation_report: ValidationReport):
        """ë†’ì€ ìœ„í˜‘ ì²˜ë¦¬"""
        # IP ì°¨ë‹¨ ê³ ë ¤
        if len([v for v in security_context.previous_violations if 'HIGH' in v or 'CRITICAL' in v]) > 3:
            self.blocklist['ip_addresses'].add(security_context.ip_address)
            logger.error(f"IP blocked due to repeated violations: {security_context.ip_address}")
    
    def create_security_context(self, 
                              user_id: str,
                              session_id: str,
                              ip_address: str,
                              user_agent: str) -> SecurityContext:
        """ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        context = SecurityContext(
            user_id=user_id,
            session_id=session_id,
            ip_address=ip_address,
            user_agent=user_agent,
            timestamp=datetime.now(),
            request_count=0
        )
        
        self.security_contexts[session_id] = context
        return context
    
    def update_security_context(self, session_id: str, **kwargs):
        """ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸"""
        if session_id in self.security_contexts:
            context = self.security_contexts[session_id]
            for key, value in kwargs.items():
                if hasattr(context, key):
                    setattr(context, key, value)
    
    async def sanitize_dataframe(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
        """DataFrame ì‚´ê· """
        issues = []
        sanitized_df = df.copy()
        
        try:
            # ì»¬ëŸ¼ëª… ì‚´ê· 
            for col in df.columns:
                if any(re.search(pattern, str(col), re.IGNORECASE) for pattern in self.sql_injection_patterns):
                    new_col = re.sub(r'[^\w\s]', '_', str(col))
                    sanitized_df.rename(columns={col: new_col}, inplace=True)
                    issues.append(f"ìœ„í—˜í•œ ì»¬ëŸ¼ëª… ë³€ê²½: {col} -> {new_col}")
            
            # ë¬¸ìì—´ ë°ì´í„° ì‚´ê· 
            for col in sanitized_df.select_dtypes(include=['object']).columns:
                sanitized_df[col] = sanitized_df[col].apply(
                    lambda x: self._sanitize_string(x) if isinstance(x, str) else x
                )
            
            return sanitized_df, issues
            
        except Exception as e:
            logger.error(f"Error sanitizing dataframe: {str(e)}")
            return df, [f"ì‚´ê·  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"]
    
    def _sanitize_string(self, text: str) -> str:
        """ë¬¸ìì—´ ì‚´ê·  - ê°•í™”ëœ XSS ë°©ì§€"""
        if not text:
            return text
        
        # bleach ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        if BLEACH_AVAILABLE:
            text = bleach.clean(
                text,
                tags=[],  # ëª¨ë“  HTML íƒœê·¸ ì œê±°
                attributes={},
                strip=True
            )
        else:
            # Fallback ë°©ë²•: ë‹¤ì¸µ ì‚´ê· 
            text = html.escape(text)  # HTML ì—”í‹°í‹° ì¸ì½”ë”©
            text = re.sub(r'<[^>]*>', '', text)  # HTML íƒœê·¸ ì œê±°
            text = re.sub(r'javascript:', '', text, flags=re.IGNORECASE)  # JavaScript URL ì œê±°
            text = re.sub(r'on\w+\s*=', '', text, flags=re.IGNORECASE)  # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì œê±°
        
        # ìœ„í—˜í•œ ë¬¸ì ì´ìŠ¤ì¼€ì´í”„
        text = text.replace("'", "''")
        text = text.replace('"', '""')
        text = text.replace(";", "")
        text = text.replace("--", "")
        
        return text
    
    def generate_session_token(self) -> str:
        """ì•ˆì „í•œ ì„¸ì…˜ í† í° ìƒì„±"""
        return secrets.token_urlsafe(32)
    
    def get_security_status(self) -> Dict[str, Any]:
        """ë³´ì•ˆ ì‹œìŠ¤í…œ ìƒíƒœ"""
        return {
            'universal_engine_available': UNIVERSAL_ENGINE_AVAILABLE,
            'active_sessions': len(self.security_contexts),
            'blocked_ips': len(self.blocklist['ip_addresses']),
            'blocked_files': len(self.blocklist['file_hashes']),
            'security_config': self.security_config,
            'rate_limits': self.rate_limits,
            'file_upload_limits': self.file_upload_limits
        }
    
    def clear_expired_sessions(self, expire_hours: int = 24):
        """ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬"""
        cutoff_time = datetime.now() - timedelta(hours=expire_hours)
        
        expired_sessions = [
            session_id for session_id, context in self.security_contexts.items()
            if context.timestamp < cutoff_time
        ]
        
        for session_id in expired_sessions:
            del self.security_contexts[session_id]
        
        logger.info(f"Cleared {len(expired_sessions)} expired sessions")
        
        return len(expired_sessions)