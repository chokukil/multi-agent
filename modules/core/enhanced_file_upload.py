"""
Enhanced File Upload System with Chunked Processing

ê²€ì¦ëœ Universal Engine íŒ¨í„´:
- ChunkedFileUpload: ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
- ProgressiveProcessing: ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
- ErrorRecovery: ì—…ë¡œë“œ ì‹¤íŒ¨ ë³µêµ¬
- SecurityIntegration: ë³´ì•ˆ ê²€ì¦ í†µí•©
"""

import asyncio
import logging
import hashlib
import time
from typing import Dict, List, Any, Optional, Callable, AsyncGenerator
from dataclasses import dataclass, field
from pathlib import Path
import mimetypes
import streamlit as st
from io import BytesIO
import pandas as pd
import tempfile
import os

logger = logging.getLogger(__name__)


@dataclass
class UploadProgress:
    """ì—…ë¡œë“œ ì§„í–‰ ìƒíƒœ"""
    file_name: str
    total_size: int
    uploaded_size: int = 0
    chunk_count: int = 0
    current_chunk: int = 0
    start_time: float = field(default_factory=time.time)
    estimated_time_remaining: float = 0.0
    upload_speed_mbps: float = 0.0
    
    @property
    def progress_percent(self) -> float:
        if self.total_size == 0:
            return 0.0
        return (self.uploaded_size / self.total_size) * 100
    
    @property
    def elapsed_time(self) -> float:
        return time.time() - self.start_time


@dataclass
class ChunkUploadConfig:
    """ì²­í¬ ì—…ë¡œë“œ ì„¤ì •"""
    chunk_size_mb: float = 2.0          # ì²­í¬ í¬ê¸° (MB)
    max_parallel_chunks: int = 3        # ë³‘ë ¬ ì²˜ë¦¬ ì²­í¬ ìˆ˜
    retry_attempts: int = 3             # ì¬ì‹œë„ íšŸìˆ˜
    retry_delay: float = 1.0            # ì¬ì‹œë„ ì§€ì—° ì‹œê°„
    progress_callback_interval: float = 0.5  # ì§„í–‰ë¥  ì½œë°± ê°„ê²©
    enable_compression: bool = False     # ì••ì¶• í™œì„±í™”
    security_scan_chunks: bool = True    # ì²­í¬ë³„ ë³´ì•ˆ ìŠ¤ìº”


class EnhancedFileUploadSystem:
    """
    í–¥ìƒëœ íŒŒì¼ ì—…ë¡œë“œ ì‹œìŠ¤í…œ
    - ì²­í¬ ê¸°ë°˜ ì—…ë¡œë“œë¡œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬
    - ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
    - ì—ëŸ¬ ë³µêµ¬ ë° ì¬ì‹œë„
    - ë³´ì•ˆ í†µí•©
    """
    
    def __init__(self, config: Optional[ChunkUploadConfig] = None):
        """Enhanced File Upload System ì´ˆê¸°í™”"""
        self.config = config or ChunkUploadConfig()
        self.active_uploads: Dict[str, UploadProgress] = {}
        self.upload_callbacks: Dict[str, List[Callable]] = {}
        self.security_validator = None
        
        # Streamlit session state ì´ˆê¸°í™”
        if 'upload_progress' not in st.session_state:
            st.session_state.upload_progress = {}
        
        logger.info("Enhanced File Upload System initialized")
    
    def set_security_validator(self, validator):
        """ë³´ì•ˆ ê²€ì¦ê¸° ì„¤ì •"""
        self.security_validator = validator
        logger.info("Security validator configured")
    
    def register_progress_callback(self, upload_id: str, callback: Callable):
        """ì§„í–‰ë¥  ì½œë°± ë“±ë¡"""
        if upload_id not in self.upload_callbacks:
            self.upload_callbacks[upload_id] = []
        self.upload_callbacks[upload_id].append(callback)
    
    async def upload_file_chunked(self, 
                                 file_data: BytesIO, 
                                 file_name: str,
                                 upload_id: Optional[str] = None) -> Dict[str, Any]:
        """
        ì²­í¬ ê¸°ë°˜ íŒŒì¼ ì—…ë¡œë“œ
        ëŒ€ìš©ëŸ‰ íŒŒì¼ì„ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
        """
        if upload_id is None:
            upload_id = f"upload_{int(time.time())}_{hash(file_name) % 10000}"
        
        try:
            # íŒŒì¼ ì •ë³´ ë¶„ì„
            file_data.seek(0, 2)  # íŒŒì¼ ëìœ¼ë¡œ ì´ë™
            total_size = file_data.tell()
            file_data.seek(0)  # íŒŒì¼ ì‹œì‘ìœ¼ë¡œ ë³µê·€
            
            # ì²­í¬ ì„¤ì • ê³„ì‚°
            chunk_size_bytes = int(self.config.chunk_size_mb * 1024 * 1024)
            chunk_count = (total_size + chunk_size_bytes - 1) // chunk_size_bytes
            
            # ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
            progress = UploadProgress(
                file_name=file_name,
                total_size=total_size,
                chunk_count=chunk_count
            )
            self.active_uploads[upload_id] = progress
            
            logger.info(f"Starting chunked upload: {file_name} ({total_size:,} bytes, {chunk_count} chunks)")
            
            # ì„ì‹œ íŒŒì¼ ìƒì„±
            temp_dir = tempfile.mkdtemp(prefix="cherry_upload_")
            temp_file_path = Path(temp_dir) / file_name
            
            # ì²­í¬ë³„ ì²˜ë¦¬
            with open(temp_file_path, 'wb') as temp_file:
                for chunk_index in range(chunk_count):
                    chunk_start = chunk_index * chunk_size_bytes
                    chunk_size = min(chunk_size_bytes, total_size - chunk_start)
                    
                    # ì²­í¬ ë°ì´í„° ì½ê¸°
                    file_data.seek(chunk_start)
                    chunk_data = file_data.read(chunk_size)
                    
                    # ì²­í¬ ì²˜ë¦¬ (ë³´ì•ˆ ìŠ¤ìº” í¬í•¨)
                    await self._process_chunk(upload_id, chunk_index, chunk_data, temp_file)
                    
                    # ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
                    progress.current_chunk = chunk_index + 1
                    progress.uploaded_size += len(chunk_data)
                    
                    await self._update_progress(upload_id, progress)
                    
                    # CPU ì–‘ë³´ (UI ë°˜ì‘ì„± í–¥ìƒ)
                    await asyncio.sleep(0.01)
            
            # íŒŒì¼ ì™„ì„± í›„ ì „ì²´ ê²€ì¦
            final_result = await self._finalize_upload(upload_id, temp_file_path)
            
            # ì •ë¦¬
            self._cleanup_upload(upload_id, temp_dir)
            
            logger.info(f"Chunked upload completed: {file_name}")
            return final_result
            
        except Exception as e:
            logger.error(f"Error in chunked upload {upload_id}: {str(e)}")
            self._cleanup_upload(upload_id)
            raise
    
    async def _process_chunk(self, 
                           upload_id: str, 
                           chunk_index: int, 
                           chunk_data: bytes,
                           temp_file) -> bool:
        """ê°œë³„ ì²­í¬ ì²˜ë¦¬"""
        retry_count = 0
        
        while retry_count < self.config.retry_attempts:
            try:
                # ì²­í¬ ë³´ì•ˆ ìŠ¤ìº” (ì„¤ì •ëœ ê²½ìš°)
                if self.config.security_scan_chunks and self.security_validator:
                    scan_result = await self._scan_chunk_security(chunk_data, chunk_index)
                    if not scan_result.get('safe', True):
                        raise ValueError(f"Security threat detected in chunk {chunk_index}")
                
                # ì²­í¬ ë°ì´í„° ì“°ê¸°
                temp_file.write(chunk_data)
                temp_file.flush()
                
                return True
                
            except Exception as e:
                retry_count += 1
                logger.warning(f"Chunk {chunk_index} processing failed (attempt {retry_count}): {str(e)}")
                
                if retry_count < self.config.retry_attempts:
                    await asyncio.sleep(self.config.retry_delay * retry_count)
                else:
                    raise
        
        return False
    
    async def _scan_chunk_security(self, chunk_data: bytes, chunk_index: int) -> Dict[str, Any]:
        """ì²­í¬ ë³´ì•ˆ ìŠ¤ìº”"""
        try:
            # ê°„ë‹¨í•œ ì‹œê·¸ë‹ˆì²˜ ê²€ì‚¬
            chunk_hex = chunk_data[:100].hex()
            
            # ì‹¤í–‰ íŒŒì¼ ì‹œê·¸ë‹ˆì²˜ ê²€ì‚¬
            malicious_signatures = [
                '4d5a',      # MZ header (PE)
                '7f454c46',  # ELF header
                '504b0304'   # ZIP header (ì ì¬ì  ìœ„í—˜)
            ]
            
            for signature in malicious_signatures:
                if chunk_hex.lower().startswith(signature.lower()):
                    logger.warning(f"Malicious signature detected in chunk {chunk_index}: {signature}")
                    return {"safe": False, "reason": f"Malicious signature: {signature}"}
            
            return {"safe": True}
            
        except Exception as e:
            logger.error(f"Error scanning chunk {chunk_index}: {str(e)}")
            return {"safe": True}  # ì—ëŸ¬ ì‹œ í†µê³¼ (ë³´ìˆ˜ì  ì ‘ê·¼)
    
    async def _update_progress(self, upload_id: str, progress: UploadProgress):
        """ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        # ì—…ë¡œë“œ ì†ë„ ê³„ì‚°
        if progress.elapsed_time > 0:
            progress.upload_speed_mbps = (progress.uploaded_size / (1024 * 1024)) / progress.elapsed_time
        
        # ë‚¨ì€ ì‹œê°„ ì¶”ì •
        if progress.upload_speed_mbps > 0:
            remaining_mb = (progress.total_size - progress.uploaded_size) / (1024 * 1024)
            progress.estimated_time_remaining = remaining_mb / progress.upload_speed_mbps
        
        # Streamlit session state ì—…ë°ì´íŠ¸
        st.session_state.upload_progress[upload_id] = {
            "file_name": progress.file_name,
            "progress_percent": progress.progress_percent,
            "upload_speed_mbps": progress.upload_speed_mbps,
            "estimated_time_remaining": progress.estimated_time_remaining,
            "current_chunk": progress.current_chunk,
            "total_chunks": progress.chunk_count
        }
        
        # ë“±ë¡ëœ ì½œë°± í˜¸ì¶œ
        if upload_id in self.upload_callbacks:
            for callback in self.upload_callbacks[upload_id]:
                try:
                    await callback(progress)
                except Exception as e:
                    logger.error(f"Progress callback error: {str(e)}")
    
    async def _finalize_upload(self, upload_id: str, temp_file_path: Path) -> Dict[str, Any]:
        """ì—…ë¡œë“œ ì™„ë£Œ ì²˜ë¦¬"""
        try:
            # íŒŒì¼ ë¬´ê²°ì„± ê²€ì¦
            file_hash = await self._calculate_file_hash(temp_file_path)
            
            # ìµœì¢… ë³´ì•ˆ ê²€ì¦
            security_result = None
            if self.security_validator:
                security_context = self._create_dummy_security_context()
                security_result = await self.security_validator.validate_file_upload(
                    str(temp_file_path),
                    temp_file_path.name,
                    temp_file_path.stat().st_size,
                    security_context
                )
            
            # íŒŒì¼ íƒ€ì… ê²€ì¦
            mime_type = mimetypes.guess_type(str(temp_file_path))[0]
            
            # ê²°ê³¼ ë°˜í™˜
            result = {
                "upload_id": upload_id,
                "file_path": str(temp_file_path),
                "file_name": temp_file_path.name,
                "file_size": temp_file_path.stat().st_size,
                "file_hash": file_hash,
                "mime_type": mime_type,
                "upload_success": True,
                "security_scan_passed": security_result.validation_result.value != "blocked" if security_result else True,
                "processing_time": self.active_uploads[upload_id].elapsed_time
            }
            
            return result
            
        except Exception as e:
            logger.error(f"Error finalizing upload {upload_id}: {str(e)}")
            raise
    
    async def _calculate_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        sha256_hash = hashlib.sha256()
        
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        
        return sha256_hash.hexdigest()
    
    def _create_dummy_security_context(self):
        """ë”ë¯¸ ë³´ì•ˆ ì»¨í…ìŠ¤íŠ¸ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
        try:
            from modules.core.security_validation_system import SecurityContext
            from datetime import datetime
            
            return SecurityContext(
                user_id="upload_user",
                session_id="upload_session",
                ip_address="127.0.0.1",
                user_agent="streamlit_uploader",
                timestamp=datetime.now(),
                request_count=1
            )
        except ImportError:
            return None
    
    def _cleanup_upload(self, upload_id: str, temp_dir: Optional[str] = None):
        """ì—…ë¡œë“œ ì •ë¦¬"""
        # ì§„í–‰ ìƒíƒœ ì œê±°
        if upload_id in self.active_uploads:
            del self.active_uploads[upload_id]
        
        # ì½œë°± ì œê±°
        if upload_id in self.upload_callbacks:
            del self.upload_callbacks[upload_id]
        
        # Streamlit session state ì •ë¦¬
        if upload_id in st.session_state.upload_progress:
            del st.session_state.upload_progress[upload_id]
        
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
        if temp_dir and os.path.exists(temp_dir):
            try:
                import shutil
                shutil.rmtree(temp_dir)
                logger.debug(f"Cleaned up temp directory: {temp_dir}")
            except Exception as e:
                logger.error(f"Error cleaning up temp directory {temp_dir}: {str(e)}")
    
    def get_upload_status(self, upload_id: str) -> Optional[Dict[str, Any]]:
        """ì—…ë¡œë“œ ìƒíƒœ ì¡°íšŒ"""
        if upload_id in self.active_uploads:
            progress = self.active_uploads[upload_id]
            return {
                "upload_id": upload_id,
                "file_name": progress.file_name,
                "progress_percent": progress.progress_percent,
                "uploaded_size": progress.uploaded_size,
                "total_size": progress.total_size,
                "current_chunk": progress.current_chunk,
                "total_chunks": progress.chunk_count,
                "upload_speed_mbps": progress.upload_speed_mbps,
                "estimated_time_remaining": progress.estimated_time_remaining,
                "elapsed_time": progress.elapsed_time
            }
        return None
    
    def get_all_active_uploads(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  í™œì„± ì—…ë¡œë“œ ìƒíƒœ ì¡°íšŒ"""
        return [self.get_upload_status(upload_id) 
                for upload_id in self.active_uploads.keys()]
    
    def cancel_upload(self, upload_id: str) -> bool:
        """ì—…ë¡œë“œ ì·¨ì†Œ"""
        if upload_id in self.active_uploads:
            logger.info(f"Cancelling upload {upload_id}")
            self._cleanup_upload(upload_id)
            return True
        return False


def create_streamlit_upload_ui(upload_system: EnhancedFileUploadSystem) -> Optional[str]:
    """Streamlit ì—…ë¡œë“œ UI ìƒì„±"""
    st.subheader("ğŸ“ Enhanced File Upload")
    
    # íŒŒì¼ ì—…ë¡œë”
    uploaded_file = st.file_uploader(
        "Choose a file",
        type=['csv', 'xlsx', 'xls', 'json', 'txt', 'tsv', 'parquet'],
        help="Upload data files for analysis. Large files will be processed in chunks."
    )
    
    if uploaded_file is not None:
        # ì—…ë¡œë“œ ì •ë³´ í‘œì‹œ
        st.write(f"**File:** {uploaded_file.name}")
        st.write(f"**Size:** {uploaded_file.size:,} bytes ({uploaded_file.size / (1024*1024):.1f} MB)")
        
        # ì—…ë¡œë“œ ë²„íŠ¼
        if st.button("ğŸš€ Start Enhanced Upload", type="primary"):
            upload_id = f"streamlit_{int(time.time())}"
            
            # ì§„í–‰ë¥  í‘œì‹œ ì˜ì—­
            progress_container = st.container()
            
            with progress_container:
                progress_bar = st.progress(0)
                status_text = st.empty()
                metrics_col1, metrics_col2, metrics_col3 = st.columns(3)
                
                async def progress_callback(progress: UploadProgress):
                    progress_bar.progress(progress.progress_percent / 100)
                    status_text.text(
                        f"Uploading chunk {progress.current_chunk}/{progress.chunk_count} "
                        f"({progress.progress_percent:.1f}%) - "
                        f"{progress.upload_speed_mbps:.1f} MB/s"
                    )
                    
                    with metrics_col1:
                        st.metric("Speed", f"{progress.upload_speed_mbps:.1f} MB/s")
                    with metrics_col2:
                        st.metric("Progress", f"{progress.progress_percent:.1f}%")
                    with metrics_col3:
                        st.metric("ETA", f"{progress.estimated_time_remaining:.1f}s")
                
                # ì½œë°± ë“±ë¡
                upload_system.register_progress_callback(upload_id, progress_callback)
                
                try:
                    # ë¹„ë™ê¸° ì—…ë¡œë“œ ì‹¤í–‰
                    result = asyncio.run(upload_system.upload_file_chunked(
                        uploaded_file, uploaded_file.name, upload_id
                    ))
                    
                    # ì„±ê³µ ë©”ì‹œì§€
                    st.success(f"âœ… Upload completed successfully!")
                    st.json(result)
                    
                    return upload_id
                    
                except Exception as e:
                    st.error(f"âŒ Upload failed: {str(e)}")
                    logger.error(f"Streamlit upload failed: {str(e)}")
    
    return None


# ì „ì—­ ì—…ë¡œë“œ ì‹œìŠ¤í…œ ì¸ìŠ¤í„´ìŠ¤
_global_upload_system: Optional[EnhancedFileUploadSystem] = None


def get_upload_system() -> EnhancedFileUploadSystem:
    """ì „ì—­ ì—…ë¡œë“œ ì‹œìŠ¤í…œ ë°˜í™˜"""
    global _global_upload_system
    if _global_upload_system is None:
        _global_upload_system = EnhancedFileUploadSystem()
    return _global_upload_system