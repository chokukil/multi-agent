"""
LLM Recommendation Engine - LLM ê¸°ë°˜ ë¶„ì„ ì¶”ì²œ ì‹œìŠ¤í…œ

ê²€ì¦ëœ Universal Engine íŒ¨í„´ ê¸°ë°˜:
- DynamicContextDiscovery: í•˜ë“œì½”ë”© ì—†ëŠ” ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ìë™ ë°œê²¬
- AdaptiveUserUnderstanding: ì‚¬ìš©ì ì „ë¬¸ì„± ìˆ˜ì¤€ ì¶”ì •
- RealTimeLearningSystem: í”¼ë“œë°± ê¸°ë°˜ ì¶”ì²œ ê°œì„ 
- Progressive Disclosure: ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì¶”ì²œ ì ì‘
"""

import asyncio
import logging
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime
import json
import pandas as pd
import numpy as np
from dataclasses import asdict

from ..models import OneClickRecommendation, VisualDataCard, DataContext

# Universal Engine íŒ¨í„´ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
try:
    from core.universal_engine.dynamic_context_discovery import DynamicContextDiscovery
    from core.universal_engine.adaptive_user_understanding import AdaptiveUserUnderstanding
    from core.universal_engine.real_time_learning_system import RealTimeLearningSystem
    from core.universal_engine.llm_factory import LLMFactory
    UNIVERSAL_ENGINE_AVAILABLE = True
except ImportError:
    UNIVERSAL_ENGINE_AVAILABLE = False

logger = logging.getLogger(__name__)


class LLMRecommendationEngine:
    """
    LLM ê¸°ë°˜ ë¶„ì„ ì¶”ì²œ ì—”ì§„
    ê²€ì¦ëœ Universal Engine íŒ¨í„´ì„ í™œìš©í•œ ì§€ëŠ¥ì  ì¶”ì²œ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        """LLM ì¶”ì²œ ì—”ì§„ ì´ˆê¸°í™”"""
        self.max_recommendations = 3
        
        # Universal Engine ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if UNIVERSAL_ENGINE_AVAILABLE:
            self.context_discovery = DynamicContextDiscovery()
            self.user_understanding = AdaptiveUserUnderstanding()
            self.learning_system = RealTimeLearningSystem()
            self.llm_client = LLMFactory.create_llm()
        else:
            self.context_discovery = None
            self.user_understanding = None
            self.learning_system = None
            self.llm_client = None
            
        # ìºì‹œ ë° í•™ìŠµ ë°ì´í„°
        self.context_cache: Dict[str, DataContext] = {}
        self.user_feedback_history: List[Dict] = []
        self.recommendation_templates = self._load_recommendation_templates()
        
        logger.info("LLM Recommendation Engine initialized")
    
    async def generate_contextual_recommendations(self, 
                                                data_cards: List[VisualDataCard],
                                                user_query: Optional[str] = None,
                                                user_context: Optional[Dict] = None,
                                                interaction_history: Optional[List] = None) -> List[OneClickRecommendation]:
        """
        ê²€ì¦ëœ Universal Engine íŒ¨í„´ì„ ì‚¬ìš©í•œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¶”ì²œ ìƒì„±
        
        1. DynamicContextDiscovery: ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ìë™ ë°œê²¬ (í•˜ë“œì½”ë”© ì—†ìŒ)
        2. AdaptiveUserUnderstanding: ì‚¬ìš©ì ì „ë¬¸ì„± ìˆ˜ì¤€ ì¶”ì •
        3. RealTimeLearningSystem: í”¼ë“œë°± ê¸°ë°˜ ì¶”ì²œ ê°œì„ 
        4. Progressive Disclosure: ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì¶”ì²œ ì ì‘
        """
        try:
            logger.info(f"Generating recommendations for {len(data_cards)} datasets")
            
            # 1. ë™ì  ì»¨í…ìŠ¤íŠ¸ ë°œê²¬
            data_context = await self._discover_data_context_dynamically(data_cards, user_query)
            
            # 2. ì‚¬ìš©ì ìˆ˜ì¤€ ì ì‘ì  ì´í•´
            user_profile = await self._analyze_user_adaptively(user_query, interaction_history, user_context)
            
            # 3. LLM ê¸°ë°˜ ì¶”ì²œ ìƒì„±
            recommendations = await self._generate_llm_recommendations(
                data_cards, data_context, user_profile, user_query
            )
            
            # 4. ì ì§„ì  ê³µê°œ íŒ¨í„´ ì ìš©
            adapted_recommendations = await self._adapt_to_user_level(recommendations, user_profile)
            
            # 5. ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸
            await self._update_learning_system(data_context, user_profile, recommendations)
            
            return adapted_recommendations[:self.max_recommendations]
            
        except Exception as e:
            logger.error(f"Error generating recommendations: {str(e)}")
            return self._generate_fallback_recommendations(data_cards)
    
    async def _discover_data_context_dynamically(self, 
                                               data_cards: List[VisualDataCard], 
                                               user_query: Optional[str] = None) -> DataContext:
        """
        ê²€ì¦ëœ zero-hardcoding ì»¨í…ìŠ¤íŠ¸ ë°œê²¬:
        - ì‚¬ì „ ì •ì˜ëœ ë„ë©”ì¸ ì¹´í…Œê³ ë¦¬ ì—†ìŒ
        - LLMì´ ë°ì´í„° íŠ¹ì„±ì„ ë³´ê³  ì§ì ‘ ì»¨í…ìŠ¤íŠ¸ ë°œê²¬
        - íŒ¨í„´ ë§¤ì¹­ì´ ì•„ë‹Œ ì‹¤ì œ ì´í•´ì™€ ì¶”ë¡ 
        """
        try:
            # ë°ì´í„° ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            combined_metadata = self._extract_combined_metadata(data_cards)
            
            if UNIVERSAL_ENGINE_AVAILABLE and self.context_discovery:
                # Universal Engine DynamicContextDiscovery ì‚¬ìš©
                discovered_context = await self.context_discovery.discover_context(
                    data=combined_metadata,
                    query=user_query
                )
                
                return self._convert_to_data_context(discovered_context, data_cards)
            else:
                # ê¸°ë³¸ LLM ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë°œê²¬
                return await self._llm_context_discovery_fallback(combined_metadata, user_query)
                
        except Exception as e:
            logger.error(f"Context discovery error: {str(e)}")
            return self._create_basic_data_context(data_cards)
    
    async def _analyze_user_adaptively(self, 
                                     user_query: Optional[str],
                                     interaction_history: Optional[List],
                                     user_context: Optional[Dict]) -> Dict[str, Any]:
        """
        ê²€ì¦ëœ ì ì‘ì  ì‚¬ìš©ì ì´í•´:
        - ì–¸ì–´ ì‚¬ìš©, ìš©ì–´, ì§ˆë¬¸ ë³µì¡ë„ ë¶„ì„
        - ì ì§„ì  ê³µê°œë¥¼ í†µí•œ ì´í•´ ìˆ˜ì¤€ íŒŒì•…
        - ëŒ€í™” ì¤‘ ë™ì  ì¡°ì •
        """
        try:
            if UNIVERSAL_ENGINE_AVAILABLE and self.user_understanding:
                # Universal Engine AdaptiveUserUnderstanding ì‚¬ìš©
                user_analysis = await self.user_understanding.analyze_user_expertise(
                    query=user_query or "",
                    interaction_history=interaction_history or []
                )
                
                return user_analysis
            else:
                # ê¸°ë³¸ ì‚¬ìš©ì ë¶„ì„
                return self._basic_user_analysis(user_query, interaction_history)
                
        except Exception as e:
            logger.error(f"User analysis error: {str(e)}")
            return {"expertise_level": "intermediate", "domain_familiarity": "moderate"}
    
    async def _generate_llm_recommendations(self, 
                                          data_cards: List[VisualDataCard],
                                          data_context: DataContext,
                                          user_profile: Dict[str, Any],
                                          user_query: Optional[str]) -> List[OneClickRecommendation]:
        """LLMì„ ì‚¬ìš©í•œ ì§€ëŠ¥ì  ì¶”ì²œ ìƒì„±"""
        try:
            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_recommendation_prompt(
                data_cards, data_context, user_profile, user_query
            )
            
            if UNIVERSAL_ENGINE_AVAILABLE and self.llm_client:
                # Universal Engine LLM ì‚¬ìš©
                response = await self.llm_client.ainvoke(prompt)
                recommendations_data = self._parse_llm_response(response.content)
            else:
                # ê¸°ë³¸ ì¶”ì²œ ìƒì„±
                recommendations_data = self._generate_basic_recommendations(data_cards, data_context)
            
            # OneClickRecommendation ê°ì²´ë¡œ ë³€í™˜
            recommendations = []
            for i, rec_data in enumerate(recommendations_data[:self.max_recommendations]):
                recommendation = OneClickRecommendation(
                    title=rec_data.get('title', f'Analysis {i+1}'),
                    description=rec_data.get('description', 'Perform data analysis'),
                    action_type=rec_data.get('action_type', 'general_analysis'),
                    parameters=rec_data.get('parameters', {}),
                    estimated_time=rec_data.get('estimated_time', 60),
                    confidence_score=rec_data.get('confidence_score', 0.8),
                    complexity_level=rec_data.get('complexity_level', 'intermediate'),
                    expected_result_preview=rec_data.get('expected_result_preview', 'Analysis results'),
                    icon=rec_data.get('icon', 'ğŸ“Š'),
                    color_theme=rec_data.get('color_theme', 'blue'),
                    execution_button_text=rec_data.get('execution_button_text', 'Execute Analysis')
                )
                recommendations.append(recommendation)
            
            return recommendations
            
        except Exception as e:
            logger.error(f"LLM recommendation generation error: {str(e)}")
            return self._generate_fallback_recommendations(data_cards)
    
    def _build_recommendation_prompt(self, 
                                   data_cards: List[VisualDataCard],
                                   data_context: DataContext,
                                   user_profile: Dict[str, Any],
                                   user_query: Optional[str]) -> str:
        """LLM ì¶”ì²œ ìƒì„±ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        
        # ë°ì´í„° ìš”ì•½ ìƒì„±
        data_summary = self._create_data_summary(data_cards)
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ìš”ì•½
        expertise_level = user_profile.get('expertise_level', 'intermediate')
        domain_familiarity = user_profile.get('domain_familiarity', 'moderate')
        
        prompt = f"""
ë‹¹ì‹ ì€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ì™€ ì‚¬ìš©ì í”„ë¡œí•„ì„ ë°”íƒ•ìœ¼ë¡œ ìµœëŒ€ 3ê°œì˜ ë¶„ì„ ì¶”ì²œì„ ìƒì„±í•´ì£¼ì„¸ìš”.

## ë°ì´í„° ì •ë³´:
{data_summary}

## ë°ì´í„° ì»¨í…ìŠ¤íŠ¸:
- ë„ë©”ì¸: {data_context.domain}
- ë°ì´í„° íƒ€ì…: {', '.join(data_context.data_types)}
- í’ˆì§ˆ ì ìˆ˜: {data_context.quality_assessment.quality_score:.1f}%

## ì‚¬ìš©ì í”„ë¡œí•„:
- ì „ë¬¸ì„± ìˆ˜ì¤€: {expertise_level}
- ë„ë©”ì¸ ì¹œìˆ™ë„: {domain_familiarity}
- ì‚¬ìš©ì ì§ˆë¬¸: {user_query or 'ì—†ìŒ'}

## ìš”êµ¬ì‚¬í•­:
1. ì‚¬ìš©ì ìˆ˜ì¤€ì— ë§ëŠ” ë¶„ì„ ì¶”ì²œ
2. ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì ì¸ ë¶„ì„
3. ëª…í™•í•œ ê°€ì¹˜ ì œì•ˆ

ê° ì¶”ì²œì— ëŒ€í•´ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

```json
[
  {{
    "title": "ë¶„ì„ ì œëª©",
    "description": "ë¶„ì„ì— ëŒ€í•œ ëª…í™•í•œ ì„¤ëª… (1-2ë¬¸ì¥)",
    "action_type": "statistical_analysis|visualization|ml_analysis|data_quality",
    "parameters": {{"dataset_ids": ["dataset_id"], "analysis_type": "specific_type"}},
    "estimated_time": 60,
    "confidence_score": 0.8,
    "complexity_level": "beginner|intermediate|advanced",
    "expected_result_preview": "ì˜ˆìƒ ê²°ê³¼ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…",
    "icon": "ğŸ“Š|ğŸ“ˆ|ğŸ¤–|ğŸ”",
    "color_theme": "blue|green|purple|orange",
    "execution_button_text": "ì‹¤í–‰ ë²„íŠ¼ í…ìŠ¤íŠ¸"
  }}
]
```
"""
        
        return prompt
    
    def _create_data_summary(self, data_cards: List[VisualDataCard]) -> str:
        """ë°ì´í„° ì¹´ë“œë“¤ì˜ ìš”ì•½ ìƒì„±"""
        summaries = []
        
        for card in data_cards:
            summary = f"- {card.name}: {card.rows:,}í–‰ Ã— {card.columns}ì—´ ({card.format})"
            if card.quality_indicators:
                summary += f", í’ˆì§ˆ: {card.quality_indicators.quality_score:.0f}%"
            summaries.append(summary)
        
        return "\n".join(summaries)
    
    def _parse_llm_response(self, response_content: str) -> List[Dict]:
        """LLM ì‘ë‹µì„ íŒŒì‹±í•˜ì—¬ ì¶”ì²œ ë°ì´í„° ì¶”ì¶œ"""
        try:
            # JSON ë¸”ë¡ ì°¾ê¸°
            import re
            json_match = re.search(r'```json\s*(.*?)\s*```', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                return json.loads(json_str)
            
            # ì§ì ‘ JSON íŒŒì‹± ì‹œë„
            return json.loads(response_content)
            
        except json.JSONDecodeError:
            logger.warning("Failed to parse LLM response as JSON")
            return []
    
    async def _adapt_to_user_level(self, 
                                 recommendations: List[OneClickRecommendation],
                                 user_profile: Dict[str, Any]) -> List[OneClickRecommendation]:
        """ê²€ì¦ëœ progressive disclosure íŒ¨í„´ìœ¼ë¡œ ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì¶”ì²œ ì ì‘"""
        
        expertise_level = user_profile.get('expertise_level', 'intermediate')
        
        adapted_recommendations = []
        for rec in recommendations:
            # ì‚¬ìš©ì ìˆ˜ì¤€ì— ë”°ë¥¸ ë³µì¡ë„ ì¡°ì •
            if expertise_level == 'beginner':
                if rec.complexity_level == 'advanced':
                    continue  # ê³ ê¸‰ ë¶„ì„ì€ ì œì™¸
                rec.description = f"ğŸ”° ì´ˆë³´ììš©: {rec.description}"
                rec.estimated_time = int(rec.estimated_time * 1.2)  # ì‹œê°„ ì—¬ìœ  ì¶”ê°€
                
            elif expertise_level == 'advanced':
                if rec.complexity_level == 'beginner':
                    # ê³ ê¸‰ ì‚¬ìš©ìì—ê²ŒëŠ” ë” ìƒì„¸í•œ ì˜µì…˜ ì œê³µ
                    rec.description = f"âš¡ ê³ ê¸‰: {rec.description} (ë§ì¶¤ ì„¤ì • ê°€ëŠ¥)"
                
            adapted_recommendations.append(rec)
        
        return adapted_recommendations
    
    async def _update_learning_system(self, 
                                    data_context: DataContext,
                                    user_profile: Dict[str, Any],
                                    recommendations: List[OneClickRecommendation]):
        """ì‹¤ì‹œê°„ í•™ìŠµ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸"""
        try:
            if UNIVERSAL_ENGINE_AVAILABLE and self.learning_system:
                # Universal Engine RealTimeLearningSystem ì‚¬ìš©
                learning_data = {
                    'context': asdict(data_context),
                    'user_profile': user_profile,
                    'recommendations': [asdict(rec) for rec in recommendations],
                    'timestamp': datetime.now().isoformat()
                }
                
                await self.learning_system.update_system(learning_data)
            
        except Exception as e:
            logger.error(f"Learning system update error: {str(e)}")
    
    def _extract_combined_metadata(self, data_cards: List[VisualDataCard]) -> Dict[str, Any]:
        """ë°ì´í„° ì¹´ë“œë“¤ë¡œë¶€í„° ê²°í•©ëœ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        combined_metadata = {
            'total_datasets': len(data_cards),
            'total_rows': sum(card.rows for card in data_cards),
            'total_columns': sum(card.columns for card in data_cards),
            'formats': list(set(card.format for card in data_cards)),
            'column_names': [],
            'data_types': [],
            'quality_scores': []
        }
        
        for card in data_cards:
            if 'column_names' in card.metadata:
                combined_metadata['column_names'].extend(card.metadata['column_names'])
            if 'column_types' in card.metadata:
                combined_metadata['data_types'].extend(list(card.metadata['column_types'].values()))
            if card.quality_indicators:
                combined_metadata['quality_scores'].append(card.quality_indicators.quality_score)
        
        return combined_metadata
    
    async def _llm_context_discovery_fallback(self, 
                                            metadata: Dict[str, Any], 
                                            user_query: Optional[str]) -> DataContext:
        """ê¸°ë³¸ LLM ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë°œê²¬ (Universal Engineì´ ì—†ì„ ë•Œ)"""
        
        # ê¸°ë³¸ ë„ë©”ì¸ ì¶”ë¡ 
        column_names = metadata.get('column_names', [])
        domain = self._infer_domain_from_columns(column_names)
        
        # ê¸°ë³¸ DataContext ìƒì„±
        from ..models import DataQualityInfo
        
        avg_quality = np.mean(metadata.get('quality_scores', [85.0]))
        
        quality_info = DataQualityInfo(
            missing_values_count=0,
            missing_percentage=0.0,
            data_types_summary={},
            quality_score=avg_quality,
            issues=[]
        )
        
        return DataContext(
            domain=domain,
            data_types=list(set(metadata.get('data_types', []))),
            relationships=[],
            quality_assessment=quality_info,
            suggested_analyses=[]
        )
    
    def _infer_domain_from_columns(self, column_names: List[str]) -> str:
        """ì»¬ëŸ¼ëª…ìœ¼ë¡œë¶€í„° ë„ë©”ì¸ ì¶”ë¡ """
        column_text = ' '.join(column_names).lower()
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë„ë©”ì¸ ì¶”ë¡ 
        if any(word in column_text for word in ['price', 'sales', 'revenue', 'profit', 'cost']):
            return 'business_finance'
        elif any(word in column_text for word in ['patient', 'medical', 'diagnosis', 'treatment']):
            return 'healthcare'
        elif any(word in column_text for word in ['student', 'grade', 'course', 'education']):
            return 'education'
        elif any(word in column_text for word in ['temperature', 'pressure', 'sensor', 'measurement']):
            return 'iot_sensor'
        else:
            return 'general'
    
    def _create_basic_data_context(self, data_cards: List[VisualDataCard]) -> DataContext:
        """ê¸°ë³¸ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        from ..models import DataQualityInfo
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_scores = [card.quality_indicators.quality_score for card in data_cards 
                         if card.quality_indicators]
        avg_quality = np.mean(quality_scores) if quality_scores else 85.0
        
        quality_info = DataQualityInfo(
            missing_values_count=0,
            missing_percentage=0.0,
            data_types_summary={},
            quality_score=avg_quality,
            issues=[]
        )
        
        return DataContext(
            domain='general',
            data_types=[card.format for card in data_cards],
            relationships=[],
            quality_assessment=quality_info,
            suggested_analyses=[]
        )
    
    def _basic_user_analysis(self, 
                           user_query: Optional[str], 
                           interaction_history: Optional[List]) -> Dict[str, Any]:
        """ê¸°ë³¸ ì‚¬ìš©ì ë¶„ì„"""
        
        expertise_level = 'intermediate'  # ê¸°ë³¸ê°’
        
        if user_query:
            query_lower = user_query.lower()
            
            # ê³ ê¸‰ í‚¤ì›Œë“œ í™•ì¸
            advanced_keywords = ['model', 'algorithm', 'regression', 'classification', 'clustering', 
                               'neural', 'deep learning', 'feature engineering']
            if any(keyword in query_lower for keyword in advanced_keywords):
                expertise_level = 'advanced'
            
            # ì´ˆë³´ì í‚¤ì›Œë“œ í™•ì¸
            beginner_keywords = ['help', 'how to', 'what is', 'explain', 'simple', 'basic']
            if any(keyword in query_lower for keyword in beginner_keywords):
                expertise_level = 'beginner'
        
        return {
            'expertise_level': expertise_level,
            'domain_familiarity': 'moderate',
            'communication_style': 'detailed',
            'analysis_preferences': ['visual', 'statistical']
        }
    
    def _generate_basic_recommendations(self, 
                                      data_cards: List[VisualDataCard],
                                      data_context: DataContext) -> List[Dict]:
        """ê¸°ë³¸ ì¶”ì²œ ìƒì„± (LLMì´ ì—†ì„ ë•Œ)"""
        
        recommendations = []
        
        # ê¸°ë³¸ í†µê³„ ë¶„ì„ ì¶”ì²œ
        recommendations.append({
            'title': 'ê¸°ë³¸ í†µê³„ ë¶„ì„',
            'description': 'ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„ëŸ‰ê³¼ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤',
            'action_type': 'statistical_analysis',
            'parameters': {'dataset_ids': [card.id for card in data_cards]},
            'estimated_time': 30,
            'confidence_score': 0.9,
            'complexity_level': 'beginner',
            'expected_result_preview': 'ìš”ì•½ í†µê³„, ë¶„í¬ ì°¨íŠ¸, ìƒê´€ê´€ê³„ ë¶„ì„',
            'icon': 'ğŸ“Š',
            'color_theme': 'blue',
            'execution_button_text': 'í†µê³„ ë¶„ì„ ì‹¤í–‰'
        })
        
        # ë°ì´í„° ì‹œê°í™” ì¶”ì²œ
        if any(card.columns > 1 for card in data_cards):
            recommendations.append({
                'title': 'ë°ì´í„° ì‹œê°í™”',
                'description': 'ì£¼ìš” ë³€ìˆ˜ë“¤ì˜ ì‹œê°ì  íŒ¨í„´ì„ íƒìƒ‰í•©ë‹ˆë‹¤',
                'action_type': 'visualization',
                'parameters': {'dataset_ids': [card.id for card in data_cards]},
                'estimated_time': 45,
                'confidence_score': 0.85,
                'complexity_level': 'intermediate',
                'expected_result_preview': 'íˆìŠ¤í† ê·¸ë¨, ì‚°ì ë„, ë°•ìŠ¤í”Œë¡¯ ë“± ë‹¤ì–‘í•œ ì°¨íŠ¸',
                'icon': 'ğŸ“ˆ',
                'color_theme': 'green',
                'execution_button_text': 'ì‹œê°í™” ìƒì„±'
            })
        
        # ë°ì´í„° í’ˆì§ˆ í™•ì¸ ì¶”ì²œ
        avg_quality = np.mean([card.quality_indicators.quality_score for card in data_cards 
                              if card.quality_indicators])
        if avg_quality < 90:
            recommendations.append({
                'title': 'ë°ì´í„° í’ˆì§ˆ ê°œì„ ',
                'description': 'ëˆ„ë½ê°’, ì´ìƒì¹˜, ë°ì´í„° ì¼ê´€ì„±ì„ ê²€ì‚¬í•˜ê³  ê°œì„ í•©ë‹ˆë‹¤',
                'action_type': 'data_quality',
                'parameters': {'dataset_ids': [card.id for card in data_cards]},
                'estimated_time': 60,
                'confidence_score': 0.8,
                'complexity_level': 'intermediate',
                'expected_result_preview': 'í’ˆì§ˆ ë¦¬í¬íŠ¸, ì •ì œ ì œì•ˆ, ê°œì„ ëœ ë°ì´í„°',
                'icon': 'ğŸ”',
                'color_theme': 'orange',
                'execution_button_text': 'í’ˆì§ˆ ê²€ì‚¬ ì‹¤í–‰'
            })
        
        return recommendations
    
    def _generate_fallback_recommendations(self, data_cards: List[VisualDataCard]) -> List[OneClickRecommendation]:
        """ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ ì¶”ì²œ ìƒì„±"""
        
        basic_rec = OneClickRecommendation(
            title='ê¸°ë³¸ ë°ì´í„° íƒìƒ‰',
            description='ì—…ë¡œë“œëœ ë°ì´í„°ì˜ ê¸°ë³¸ì ì¸ íŠ¹ì„±ì„ íƒìƒ‰í•©ë‹ˆë‹¤',
            action_type='basic_exploration',
            parameters={'dataset_ids': [card.id for card in data_cards]},
            estimated_time=30,
            confidence_score=0.7,
            complexity_level='beginner',
            expected_result_preview='ë°ì´í„° ìš”ì•½, ê¸°ë³¸ í†µê³„, ëˆ„ë½ê°’ ì •ë³´',
            icon='ğŸ“‹',
            color_theme='gray',
            execution_button_text='íƒìƒ‰ ì‹œì‘'
        )
        
        return [basic_rec]
    
    def _load_recommendation_templates(self) -> Dict[str, Dict]:
        """ì¶”ì²œ í…œí”Œë¦¿ ë¡œë“œ"""
        return {
            'statistical_analysis': {
                'icon': 'ğŸ“Š',
                'color_theme': 'blue',
                'complexity_mapping': {
                    'beginner': 30,
                    'intermediate': 45,
                    'advanced': 60
                }
            },
            'visualization': {
                'icon': 'ğŸ“ˆ',
                'color_theme': 'green',
                'complexity_mapping': {
                    'beginner': 20,
                    'intermediate': 35,
                    'advanced': 50
                }
            },
            'ml_analysis': {
                'icon': 'ğŸ¤–',
                'color_theme': 'purple',
                'complexity_mapping': {
                    'beginner': 120,
                    'intermediate': 180,
                    'advanced': 240
                }
            },
            'data_quality': {
                'icon': 'ğŸ”',
                'color_theme': 'orange',
                'complexity_mapping': {
                    'beginner': 40,
                    'intermediate': 60,
                    'advanced': 90
                }
            }
        }
    
    def _convert_to_data_context(self, discovered_context: Dict, data_cards: List[VisualDataCard]) -> DataContext:
        """Universal Engine ì»¨í…ìŠ¤íŠ¸ë¥¼ DataContextë¡œ ë³€í™˜"""
        from ..models import DataQualityInfo
        
        # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
        quality_scores = [card.quality_indicators.quality_score for card in data_cards 
                         if card.quality_indicators]
        avg_quality = np.mean(quality_scores) if quality_scores else 85.0
        
        quality_info = DataQualityInfo(
            missing_values_count=0,
            missing_percentage=0.0,
            data_types_summary={},
            quality_score=avg_quality,
            issues=[]
        )
        
        return DataContext(
            domain=discovered_context.get('domain', 'general'),
            data_types=discovered_context.get('data_types', []),
            relationships=[],
            quality_assessment=quality_info,
            suggested_analyses=discovered_context.get('suggested_analyses', [])
        )
    
    async def process_user_feedback(self, 
                                  recommendation_id: str, 
                                  feedback_type: str, 
                                  feedback_data: Dict[str, Any]):
        """ì‚¬ìš©ì í”¼ë“œë°± ì²˜ë¦¬ ë° í•™ìŠµ ì‹œìŠ¤í…œ ì—…ë°ì´íŠ¸"""
        try:
            feedback_entry = {
                'recommendation_id': recommendation_id,
                'feedback_type': feedback_type,  # 'positive', 'negative', 'executed', 'ignored'
                'feedback_data': feedback_data,
                'timestamp': datetime.now().isoformat()
            }
            
            self.user_feedback_history.append(feedback_entry)
            
            # RealTimeLearningSystem ì—…ë°ì´íŠ¸ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
            if UNIVERSAL_ENGINE_AVAILABLE and self.learning_system:
                await self.learning_system.process_feedback(feedback_entry)
            
            logger.info(f"Processed user feedback for recommendation {recommendation_id}")
            
        except Exception as e:
            logger.error(f"Error processing user feedback: {str(e)}")
    
    def get_recommendation_analytics(self) -> Dict[str, Any]:
        """ì¶”ì²œ ë¶„ì„ ë°ì´í„° ë°˜í™˜"""
        return {
            'total_recommendations_generated': len(self.user_feedback_history),
            'feedback_summary': self._analyze_feedback_patterns(),
            'most_popular_analysis_types': self._get_popular_analysis_types(),
            'average_user_satisfaction': self._calculate_satisfaction_score()
        }
    
    def _analyze_feedback_patterns(self) -> Dict[str, int]:
        """í”¼ë“œë°± íŒ¨í„´ ë¶„ì„"""
        patterns = {'positive': 0, 'negative': 0, 'executed': 0, 'ignored': 0}
        
        for feedback in self.user_feedback_history:
            feedback_type = feedback.get('feedback_type', 'unknown')
            if feedback_type in patterns:
                patterns[feedback_type] += 1
        
        return patterns
    
    def _get_popular_analysis_types(self) -> List[str]:
        """ì¸ê¸° ìˆëŠ” ë¶„ì„ ìœ í˜• ë°˜í™˜"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” í”¼ë“œë°± ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ë°˜í™˜
        return ['statistical_analysis', 'visualization', 'data_quality']
    
    def _calculate_satisfaction_score(self) -> float:
        """ì‚¬ìš©ì ë§Œì¡±ë„ ì ìˆ˜ ê³„ì‚°"""
        if not self.user_feedback_history:
            return 0.0
        
        positive_feedback = sum(1 for f in self.user_feedback_history 
                               if f.get('feedback_type') in ['positive', 'executed'])
        
        return positive_feedback / len(self.user_feedback_history) * 100