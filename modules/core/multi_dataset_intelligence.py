"""
Multi-Dataset Intelligence System - LLM ê¸°ë°˜ ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì§€ëŠ¥ ë¶„ì„

LLMì„ í™œìš©í•œ ë°ì´í„°ì…‹ ê´€ê³„ ì´í•´ ë° ì°½ì˜ì  ë°ì´í„° ì¡°í•©:
- LLM ê¸°ë°˜ ë°ì´í„°ì…‹ ê´€ê³„ ìë™ ë°œê²¬
- ìì—°ì–´ ì„¤ëª…ì„ í†µí•œ ë°ì´í„° ì—°ê²° ê¸°íšŒ ì œì‹œ
- ì „í†µì ì¸ ìŠ¤í‚¤ë§ˆ ë§¤ì¹­ì„ ë„˜ì–´ì„  ì°½ì˜ì  ì¡°í•© ì œì•ˆ
- ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ ë° LLM ë©”ëª¨ë¦¬ í™œìš©
- ë°ì´í„° ì¹´ë“œ ì„ íƒ ì²´í¬ë°•ìŠ¤ ë° ë©”íƒ€ë°ì´í„° ì¸ì‚¬ì´íŠ¸
"""

import asyncio
import logging
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Set
from datetime import datetime
import json
from dataclasses import asdict, dataclass
from itertools import combinations
import uuid

from ..models import VisualDataCard, DataContext, DataQualityInfo, OneClickRecommendation

# Universal Engine íŒ¨í„´ ê°€ì ¸ì˜¤ê¸° (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
try:
    from core.universal_engine.llm_factory import LLMFactory
    UNIVERSAL_ENGINE_AVAILABLE = True
except ImportError:
    UNIVERSAL_ENGINE_AVAILABLE = False

logger = logging.getLogger(__name__)


@dataclass
class DatasetRelationship:
    """ë°ì´í„°ì…‹ ê°„ ê´€ê³„ ì •ë³´"""
    id: str
    source_dataset_id: str
    target_dataset_id: str
    relationship_type: str  # 'join', 'merge', 'union', 'reference', 'temporal', 'semantic'
    confidence_score: float
    join_keys: List[Tuple[str, str]]  # (source_column, target_column) pairs
    relationship_description: str
    suggested_integration_method: str
    creative_opportunities: List[str]
    llm_insights: str


@dataclass
class IntegratedDatasetSpec:
    """í†µí•© ë°ì´í„°ì…‹ ëª…ì„¸"""
    id: str
    name: str
    description: str
    source_datasets: List[str]
    integration_strategy: str
    expected_columns: List[str]
    expected_rows: int
    quality_improvement_expected: float
    analysis_opportunities: List[str]


class MultiDatasetIntelligence:
    """
    LLM ê¸°ë°˜ ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì§€ëŠ¥ ë¶„ì„ ì‹œìŠ¤í…œ
    ê²€ì¦ëœ Universal Engine íŒ¨í„´ì„ í™œìš©í•œ ìë™ ê´€ê³„ ë°œê²¬ ë° í†µí•© ë¶„ì„
    """
    
    def __init__(self):
        """Multi-Dataset Intelligence ì´ˆê¸°í™”"""
        
        # Universal Engine ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if UNIVERSAL_ENGINE_AVAILABLE:
            try:
                from core.universal_engine.data_integration.cross_dataset_relationship_discovery import CrossDatasetRelationshipDiscovery
                from core.universal_engine.data_integration.intelligent_data_integration import IntelligentDataIntegration
                from core.universal_engine.orchestration.multi_modal_analysis_orchestrator import MultiModalAnalysisOrchestrator
                from core.universal_engine.data_integration.semantic_data_mapping import SemanticDataMapping
                
                self.relationship_discovery = CrossDatasetRelationshipDiscovery()
                self.data_integration = IntelligentDataIntegration()
                self.analysis_orchestrator = MultiModalAnalysisOrchestrator()
                self.semantic_mapping = SemanticDataMapping()
                self.llm_client = LLMFactory.create_llm()
            except ImportError as e:
                logging.warning(f"Universal Engine components not available: {e}")
                self.relationship_discovery = None
                self.data_integration = None
                self.analysis_orchestrator = None
                self.semantic_mapping = None
                self.llm_client = LLMFactory.create_llm() if hasattr(LLMFactory, 'create_llm') else None
        else:
            self.relationship_discovery = None
            self.data_integration = None
            self.analysis_orchestrator = None
            self.semantic_mapping = None
            self.llm_client = None
        
        # ê´€ê³„ ë°œê²¬ ìºì‹œ
        self.relationship_cache: Dict[str, List[DatasetRelationship]] = {}
        self.integration_cache: Dict[str, IntegratedDatasetSpec] = {}
        
        # ì§€ì›ë˜ëŠ” ê´€ê³„ ìœ í˜•
        self.relationship_types = {
            'join': 'í…Œì´ë¸” ì¡°ì¸ (ê³µí†µ í‚¤ ê¸°ë°˜)',
            'merge': 'ë°ì´í„° ë³‘í•© (ìœ ì‚¬ ìŠ¤í‚¤ë§ˆ)',
            'union': 'ë°ì´í„° ì—°ê²° (ë™ì¼ ìŠ¤í‚¤ë§ˆ)',
            'reference': 'ì°¸ì¡° ê´€ê³„ (ì™¸ë˜í‚¤)',
            'temporal': 'ì‹œê°„ì  ì—°ê´€ì„±',
            'hierarchical': 'ê³„ì¸µì  ê´€ê³„',
            'complementary': 'ìƒí˜¸ ë³´ì™„ì  ë°ì´í„°'
        }
        
        logger.info("Multi-Dataset Intelligence System initialized")
    
    async def discover_dataset_relationships(self, 
                                           data_cards: List[VisualDataCard],
                                           user_context: Optional[Dict] = None) -> List[DatasetRelationship]:
        """
        ë°ì´í„°ì…‹ ê°„ ê´€ê³„ ìë™ ë°œê²¬
        - LLM ê¸°ë°˜ ìŠ¤í‚¤ë§ˆ ë¶„ì„
        - ì˜ë¯¸ì  ìœ ì‚¬ë„ ê³„ì‚°
        - ìë™ ì¡°ì¸ í‚¤ ì‹ë³„
        """
        try:
            logger.info(f"Discovering relationships between {len(data_cards)} datasets")
            
            if len(data_cards) < 2:
                return []
            
            # ìºì‹œ í‚¤ ìƒì„±
            cache_key = self._generate_cache_key(data_cards)
            if cache_key in self.relationship_cache:
                return self.relationship_cache[cache_key]
            
            relationships = []
            
            if UNIVERSAL_ENGINE_AVAILABLE and self.relationship_discovery:
                # Universal Engine CrossDatasetRelationshipDiscovery ì‚¬ìš©
                relationships = await self.relationship_discovery.discover_relationships(
                    datasets=data_cards,
                    user_context=user_context
                )
            else:
                # ê¸°ë³¸ ê´€ê³„ ë°œê²¬ ë¡œì§
                relationships = await self._basic_relationship_discovery(data_cards)
            
            # ìºì‹œì— ì €ì¥
            self.relationship_cache[cache_key] = relationships
            
            return relationships
            
        except Exception as e:
            logger.error(f"Error discovering dataset relationships: {str(e)}")
            return []
    
    async def _basic_relationship_discovery(self, data_cards: List[VisualDataCard]) -> List[DatasetRelationship]:
        """ê¸°ë³¸ ê´€ê³„ ë°œê²¬ ë¡œì§ (Universal Engineì´ ì—†ì„ ë•Œ)"""
        relationships = []
        
        # ëª¨ë“  ë°ì´í„°ì…‹ ìŒì— ëŒ€í•´ ê´€ê³„ ë¶„ì„
        for card1, card2 in combinations(data_cards, 2):
            try:
                # ì»¬ëŸ¼ëª… ê¸°ë°˜ ìœ ì‚¬ë„ ë¶„ì„
                relationship = await self._analyze_column_similarity(card1, card2)
                if relationship:
                    relationships.append(relationship)
                
                # ë°ì´í„° íƒ€ì… ê¸°ë°˜ í˜¸í™˜ì„± ë¶„ì„
                compatibility_rel = await self._analyze_data_compatibility(card1, card2)
                if compatibility_rel:
                    relationships.append(compatibility_rel)
                    
            except Exception as e:
                logger.error(f"Error analyzing relationship between {card1.name} and {card2.name}: {str(e)}")
        
        return relationships
    
    async def _analyze_column_similarity(self, 
                                       card1: VisualDataCard, 
                                       card2: VisualDataCard) -> Optional[DatasetRelationship]:
        """ì»¬ëŸ¼ëª… ê¸°ë°˜ ìœ ì‚¬ë„ ë¶„ì„"""
        try:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì»¬ëŸ¼ëª… ì¶”ì¶œ
            columns1 = card1.metadata.get('column_names', [])
            columns2 = card2.metadata.get('column_names', [])
            
            if not columns1 or not columns2:
                return None
            
            # ê³µí†µ ë˜ëŠ” ìœ ì‚¬í•œ ì»¬ëŸ¼ ì°¾ê¸°
            potential_joins = []
            for col1 in columns1:
                for col2 in columns2:
                    similarity = self._calculate_column_similarity(col1, col2)
                    if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬ë„
                        potential_joins.append((col1, col2))
            
            if potential_joins:
                # ì¡°ì¸ ê´€ê³„ë¡œ ë¶„ë¥˜
                return DatasetRelationship(
                    source_dataset_id=card1.id,
                    target_dataset_id=card2.id,
                    relationship_type='join',
                    confidence_score=0.8,
                    join_keys=potential_joins,
                    relationship_description=f"ê³µí†µ ì»¬ëŸ¼ ê¸°ë°˜ ì¡°ì¸ ê°€ëŠ¥: {len(potential_joins)}ê°œ í‚¤",
                    suggested_integration_method='inner_join'
                )
            
            return None
            
        except Exception as e:
            logger.error(f"Error in column similarity analysis: {str(e)}")
            return None
    
    def _calculate_column_similarity(self, col1: str, col2: str) -> float:
        """ì»¬ëŸ¼ëª… ìœ ì‚¬ë„ ê³„ì‚°"""
        # ê°„ë‹¨í•œ ë¬¸ìì—´ ìœ ì‚¬ë„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
        col1_clean = col1.lower().strip()
        col2_clean = col2.lower().strip()
        
        # ì™„ì „ ì¼ì¹˜
        if col1_clean == col2_clean:
            return 1.0
        
        # ê³µí†µ í‚¤ì›Œë“œ í™•ì¸
        common_keywords = ['id', 'key', 'name', 'date', 'time', 'user', 'customer', 'product']
        for keyword in common_keywords:
            if keyword in col1_clean and keyword in col2_clean:
                return 0.8
        
        # ë ˆë²¤ìŠˆíƒ€ì¸ ê±°ë¦¬ ê¸°ë°˜ ìœ ì‚¬ë„ (ê°„ë‹¨ ë²„ì „)
        max_len = max(len(col1_clean), len(col2_clean))
        if max_len == 0:
            return 1.0
        
        # ê°„ë‹¨í•œ ë¬¸ì ê¸°ë°˜ ìœ ì‚¬ë„
        common_chars = set(col1_clean) & set(col2_clean)
        return len(common_chars) / max_len
    
    async def _analyze_data_compatibility(self, 
                                        card1: VisualDataCard, 
                                        card2: VisualDataCard) -> Optional[DatasetRelationship]:
        """ë°ì´í„° í˜¸í™˜ì„± ë¶„ì„"""
        try:
            # ìŠ¤í‚¤ë§ˆ ìœ ì‚¬ì„± í™•ì¸
            columns1 = set(card1.metadata.get('column_names', []))
            columns2 = set(card2.metadata.get('column_names', []))
            
            # ìŠ¤í‚¤ë§ˆê°€ 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ë³‘í•© ê°€ëŠ¥
            if columns1 and columns2:
                overlap = len(columns1 & columns2)
                total_unique = len(columns1 | columns2)
                similarity = overlap / total_unique if total_unique > 0 else 0
                
                if similarity > 0.8:
                    return DatasetRelationship(
                        source_dataset_id=card1.id,
                        target_dataset_id=card2.id,
                        relationship_type='merge',
                        confidence_score=similarity,
                        join_keys=[],
                        relationship_description=f"ìŠ¤í‚¤ë§ˆ ìœ ì‚¬ë„ {similarity:.1%} - ë³‘í•© ê°€ëŠ¥",
                        suggested_integration_method='concat'
                    )
                elif similarity > 0.5:
                    return DatasetRelationship(
                        source_dataset_id=card1.id,
                        target_dataset_id=card2.id,
                        relationship_type='complementary',
                        confidence_score=similarity,
                        join_keys=[],
                        relationship_description=f"ìƒí˜¸ ë³´ì™„ì  ë°ì´í„° - í†µí•© ë¶„ì„ ê¶Œì¥",
                        suggested_integration_method='cross_analysis'
                    )
            
            return None
            
        except Exception as e:
            logger.error(f"Error in data compatibility analysis: {str(e)}")
            return None
    
    async def generate_integration_strategies(self, 
                                            data_cards: List[VisualDataCard],
                                            relationships: List[DatasetRelationship]) -> List[IntegratedDatasetSpec]:
        """
        í†µí•© ì „ëµ ìƒì„±
        - ìµœì  í†µí•© ë°©ë²• ì œì•ˆ
        - ì˜ˆìƒ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
        - ë¶„ì„ ê¸°íšŒ ì‹ë³„
        """
        try:
            logger.info(f"Generating integration strategies for {len(relationships)} relationships")
            
            integration_specs = []
            
            if UNIVERSAL_ENGINE_AVAILABLE and self.data_integration:
                # Universal Engine IntelligentDataIntegration ì‚¬ìš©
                integration_specs = await self.data_integration.generate_strategies(
                    datasets=data_cards,
                    relationships=relationships
                )
            else:
                # ê¸°ë³¸ í†µí•© ì „ëµ ìƒì„±
                integration_specs = await self._basic_integration_strategies(data_cards, relationships)
            
            return integration_specs
            
        except Exception as e:
            logger.error(f"Error generating integration strategies: {str(e)}")
            return []
    
    async def _basic_integration_strategies(self, 
                                          data_cards: List[VisualDataCard],
                                          relationships: List[DatasetRelationship]) -> List[IntegratedDatasetSpec]:
        """ê¸°ë³¸ í†µí•© ì „ëµ ìƒì„±"""
        strategies = []
        
        # ê´€ê³„ ìœ í˜•ë³„ ê·¸ë£¹í™”
        relationship_groups = {}
        for rel in relationships:
            rel_type = rel.relationship_type
            if rel_type not in relationship_groups:
                relationship_groups[rel_type] = []
            relationship_groups[rel_type].append(rel)
        
        # ê° ê´€ê³„ ìœ í˜•ë³„ í†µí•© ì „ëµ ìƒì„±
        for rel_type, rels in relationship_groups.items():
            try:
                if rel_type == 'join':
                    strategy = await self._create_join_strategy(data_cards, rels)
                elif rel_type == 'merge':
                    strategy = await self._create_merge_strategy(data_cards, rels)
                elif rel_type == 'complementary':
                    strategy = await self._create_complementary_strategy(data_cards, rels)
                else:
                    strategy = await self._create_generic_strategy(data_cards, rels, rel_type)
                
                if strategy:
                    strategies.append(strategy)
                    
            except Exception as e:
                logger.error(f"Error creating {rel_type} strategy: {str(e)}")
        
        return strategies
    
    async def _create_join_strategy(self, 
                                   data_cards: List[VisualDataCard],
                                   relationships: List[DatasetRelationship]) -> Optional[IntegratedDatasetSpec]:
        """ì¡°ì¸ ê¸°ë°˜ í†µí•© ì „ëµ"""
        try:
            # ê°€ì¥ ë†’ì€ confidenceë¥¼ ê°€ì§„ ê´€ê³„ ì„ íƒ
            best_rel = max(relationships, key=lambda r: r.confidence_score)
            
            source_card = next(c for c in data_cards if c.id == best_rel.source_dataset_id)
            target_card = next(c for c in data_cards if c.id == best_rel.target_dataset_id)
            
            # ì˜ˆìƒ ê²°ê³¼ ê³„ì‚°
            expected_rows = min(source_card.rows, target_card.rows)  # Inner join ê°€ì •
            expected_columns = source_card.columns + target_card.columns - len(best_rel.join_keys)
            
            return IntegratedDatasetSpec(
                id=f"join_{source_card.id}_{target_card.id}",
                name=f"í†µí•© ë°ì´í„°: {source_card.name} âŸ· {target_card.name}",
                description=f"ê³µí†µ í‚¤ë¥¼ í†µí•œ ë°ì´í„° ì¡°ì¸: {len(best_rel.join_keys)}ê°œ ì¡°ì¸ í‚¤",
                source_datasets=[source_card.id, target_card.id],
                integration_strategy='inner_join',
                expected_columns=source_card.metadata.get('column_names', []) + target_card.metadata.get('column_names', []),
                expected_rows=expected_rows,
                quality_improvement_expected=15.0,
                analysis_opportunities=[
                    "í†µí•©ëœ ê´€ì ì—ì„œì˜ ìƒê´€ê´€ê³„ ë¶„ì„",
                    "êµì°¨ í…Œì´ë¸” ë¶„ì„ ë° íŒ¨í„´ ë°œê²¬",
                    "í†µí•© ë°ì´í„° ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ë§"
                ]
            )
            
        except Exception as e:
            logger.error(f"Error creating join strategy: {str(e)}")
            return None
    
    async def _create_merge_strategy(self, 
                                    data_cards: List[VisualDataCard],
                                    relationships: List[DatasetRelationship]) -> Optional[IntegratedDatasetSpec]:
        """ë³‘í•© ê¸°ë°˜ í†µí•© ì „ëµ"""
        try:
            # ëª¨ë“  ê´€ë ¨ ë°ì´í„°ì…‹ ìˆ˜ì§‘
            dataset_ids = set()
            for rel in relationships:
                dataset_ids.add(rel.source_dataset_id)
                dataset_ids.add(rel.target_dataset_id)
            
            related_cards = [c for c in data_cards if c.id in dataset_ids]
            
            # ì˜ˆìƒ ê²°ê³¼ ê³„ì‚°
            total_rows = sum(card.rows for card in related_cards)
            max_columns = max(card.columns for card in related_cards)
            
            return IntegratedDatasetSpec(
                id=f"merge_{'_'.join(dataset_ids)}",
                name=f"í†µí•© ë°ì´í„°ì…‹ ({len(related_cards)}ê°œ ì†ŒìŠ¤)",
                description=f"ìœ ì‚¬í•œ ìŠ¤í‚¤ë§ˆë¥¼ ê°€ì§„ {len(related_cards)}ê°œ ë°ì´í„°ì…‹ ë³‘í•©",
                source_datasets=list(dataset_ids),
                integration_strategy='concatenate',
                expected_columns=[],  # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì»¬ëŸ¼ ë§¤í•‘ ìˆ˜í–‰
                expected_rows=total_rows,
                quality_improvement_expected=10.0,
                analysis_opportunities=[
                    "í™•ì¥ëœ ë°ì´í„°ë¡œ í†µê³„ì  íŒŒì›Œ ì¦ëŒ€",
                    "ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„ (ì‹œê°„ ìˆœì„œê°€ ìˆëŠ” ê²½ìš°)",
                    "ì„¸ê·¸ë¨¼íŠ¸ë³„ ë¹„êµ ë¶„ì„"
                ]
            )
            
        except Exception as e:
            logger.error(f"Error creating merge strategy: {str(e)}")
            return None
    
    async def _create_complementary_strategy(self, 
                                           data_cards: List[VisualDataCard],
                                           relationships: List[DatasetRelationship]) -> Optional[IntegratedDatasetSpec]:
        """ìƒí˜¸ ë³´ì™„ì  ë¶„ì„ ì „ëµ"""
        try:
            # ê´€ë ¨ ë°ì´í„°ì…‹ ìˆ˜ì§‘
            dataset_ids = set()
            for rel in relationships:
                dataset_ids.add(rel.source_dataset_id)
                dataset_ids.add(rel.target_dataset_id)
            
            related_cards = [c for c in data_cards if c.id in dataset_ids]
            
            return IntegratedDatasetSpec(
                id=f"complementary_{'_'.join(dataset_ids)}",
                name=f"ìƒí˜¸ ë³´ì™„ ë¶„ì„ ({len(related_cards)}ê°œ ë°ì´í„°ì…‹)",
                description=f"ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì˜ ë°ì´í„°ë¥¼ í™œìš©í•œ ì¢…í•© ë¶„ì„",
                source_datasets=list(dataset_ids),
                integration_strategy='cross_analysis',
                expected_columns=[],
                expected_rows=0,  # êµì°¨ ë¶„ì„ì´ë¯€ë¡œ ë‹¨ì¼ í…Œì´ë¸” ì•„ë‹˜
                quality_improvement_expected=25.0,
                analysis_opportunities=[
                    "ë‹¤ê°ë„ ë°ì´í„° ë¹„êµ ë¶„ì„",
                    "êµì°¨ ê²€ì¦ì„ í†µí•œ ì¸ì‚¬ì´íŠ¸ ê°•í™”",
                    "ì¢…í•©ì  ëŒ€ì‹œë³´ë“œ êµ¬ì„±"
                ]
            )
            
        except Exception as e:
            logger.error(f"Error creating complementary strategy: {str(e)}")
            return None
    
    async def _create_generic_strategy(self, 
                                      data_cards: List[VisualDataCard],
                                      relationships: List[DatasetRelationship],
                                      rel_type: str) -> Optional[IntegratedDatasetSpec]:
        """ì¼ë°˜ì ì¸ í†µí•© ì „ëµ"""
        try:
            dataset_ids = set()
            for rel in relationships:
                dataset_ids.add(rel.source_dataset_id)
                dataset_ids.add(rel.target_dataset_id)
            
            related_cards = [c for c in data_cards if c.id in dataset_ids]
            
            return IntegratedDatasetSpec(
                id=f"{rel_type}_{'_'.join(dataset_ids)}",
                name=f"{self.relationship_types.get(rel_type, rel_type)} í†µí•©",
                description=f"{len(related_cards)}ê°œ ë°ì´í„°ì…‹ì˜ {rel_type} ê´€ê³„ ê¸°ë°˜ ë¶„ì„",
                source_datasets=list(dataset_ids),
                integration_strategy=rel_type,
                expected_columns=[],
                expected_rows=0,
                quality_improvement_expected=20.0,
                analysis_opportunities=[
                    f"{rel_type} ê´€ê³„ ê¸°ë°˜ íŠ¹í™” ë¶„ì„",
                    "ë°ì´í„° ê´€ê³„ì„± ì‹œê°í™”",
                    "í†µí•© ì¸ì‚¬ì´íŠ¸ ë„ì¶œ"
                ]
            )
            
        except Exception as e:
            logger.error(f"Error creating generic {rel_type} strategy: {str(e)}")
            return None
    
    async def execute_integration_strategy(self, 
                                         strategy: IntegratedDatasetSpec,
                                         data_cards: List[VisualDataCard]) -> Optional[Dict[str, Any]]:
        """
        í†µí•© ì „ëµ ì‹¤í–‰
        - ì‹¤ì œ ë°ì´í„° í†µí•© ìˆ˜í–‰
        - í’ˆì§ˆ ê²€ì¦
        - ê²°ê³¼ ë©”íƒ€ë°ì´í„° ìƒì„±
        """
        try:
            logger.info(f"Executing integration strategy: {strategy.name}")
            
            if UNIVERSAL_ENGINE_AVAILABLE and self.data_integration:
                # Universal Engineë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ ë°ì´í„° í†µí•©
                result = await self.data_integration.execute_strategy(strategy, data_cards)
            else:
                # ê¸°ë³¸ í†µí•© ì‹¤í–‰
                result = await self._basic_integration_execution(strategy, data_cards)
            
            return result
            
        except Exception as e:
            logger.error(f"Error executing integration strategy: {str(e)}")
            return None
    
    async def _basic_integration_execution(self, 
                                         strategy: IntegratedDatasetSpec,
                                         data_cards: List[VisualDataCard]) -> Dict[str, Any]:
        """ê¸°ë³¸ í†µí•© ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            # ê´€ë ¨ ë°ì´í„° ì¹´ë“œ í•„í„°ë§
            source_cards = [card for card in data_cards if card.id in strategy.source_datasets]
            
            # í†µí•© ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
            integration_result = {
                'strategy_id': strategy.id,
                'strategy_name': strategy.name,
                'integration_method': strategy.integration_strategy,
                'source_datasets': len(source_cards),
                'execution_time': 2.5,
                'success': True,
                'quality_metrics': {
                    'completeness': 0.95,
                    'consistency': 0.88,
                    'accuracy': 0.92,
                    'overall_quality': 0.92
                },
                'integrated_data_info': {
                    'estimated_rows': strategy.expected_rows,
                    'estimated_columns': len(strategy.expected_columns),
                    'memory_usage_mb': sum(card.rows * card.columns for card in source_cards) * 0.008,
                    'processing_time': f"{len(source_cards) * 1.2:.1f} seconds"
                },
                'analysis_ready': True,
                'next_steps': strategy.analysis_opportunities
            }
            
            return integration_result
            
        except Exception as e:
            logger.error(f"Error in basic integration execution: {str(e)}")
            return {
                'strategy_id': strategy.id,
                'success': False,
                'error': str(e)
            }
    
    def _generate_cache_key(self, data_cards: List[VisualDataCard]) -> str:
        """ë°ì´í„° ì¹´ë“œë“¤ì˜ ìºì‹œ í‚¤ ìƒì„±"""
        card_ids = sorted([card.id for card in data_cards])
        return f"relationships_{'_'.join(card_ids)}"
    
    async def get_multi_dataset_insights(self, 
                                       data_cards: List[VisualDataCard],
                                       relationships: List[DatasetRelationship],
                                       integration_specs: List[IntegratedDatasetSpec]) -> Dict[str, Any]:
        """ë‹¤ì¤‘ ë°ì´í„°ì…‹ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            insights = {
                'dataset_overview': {
                    'total_datasets': len(data_cards),
                    'total_relationships': len(relationships),
                    'integration_opportunities': len(integration_specs),
                    'combined_data_volume': {
                        'total_rows': sum(card.rows for card in data_cards),
                        'total_columns': sum(card.columns for card in data_cards),
                        'estimated_memory_mb': sum(card.rows * card.columns for card in data_cards) * 0.008
                    }
                },
                'relationship_summary': {
                    'by_type': self._summarize_relationships_by_type(relationships),
                    'high_confidence': [r for r in relationships if r.confidence_score > 0.8],
                    'potential_joins': [r for r in relationships if r.relationship_type == 'join'],
                    'complementary_pairs': [r for r in relationships if r.relationship_type == 'complementary']
                },
                'integration_recommendations': {
                    'high_value_integrations': [spec for spec in integration_specs if spec.quality_improvement_expected > 20],
                    'quick_wins': [spec for spec in integration_specs if spec.integration_strategy in ['merge', 'concat']],
                    'advanced_analytics': [spec for spec in integration_specs if 'ML' in ' '.join(spec.analysis_opportunities)]
                },
                'analysis_opportunities': self._generate_analysis_opportunities(data_cards, relationships, integration_specs),
                'recommendations': self._generate_smart_recommendations(data_cards, relationships, integration_specs)
            }
            
            return insights
            
        except Exception as e:
            logger.error(f"Error generating multi-dataset insights: {str(e)}")
            return {}
    
    def _summarize_relationships_by_type(self, relationships: List[DatasetRelationship]) -> Dict[str, int]:
        """ê´€ê³„ ìœ í˜•ë³„ ìš”ì•½"""
        summary = {}
        for rel in relationships:
            rel_type = rel.relationship_type
            summary[rel_type] = summary.get(rel_type, 0) + 1
        return summary
    
    def _generate_analysis_opportunities(self, 
                                       data_cards: List[VisualDataCard],
                                       relationships: List[DatasetRelationship],
                                       integration_specs: List[IntegratedDatasetSpec]) -> List[str]:
        """ë¶„ì„ ê¸°íšŒ ìƒì„±"""
        opportunities = []
        
        # ê´€ê³„ ê¸°ë°˜ ê¸°íšŒ
        if any(r.relationship_type == 'join' for r in relationships):
            opportunities.append("ğŸ”— ì¡°ì¸ì„ í†µí•œ 360ë„ ê³ ê°/ì œí’ˆ ë·° êµ¬ì„±")
        
        if len(data_cards) > 2:
            opportunities.append("ğŸ“Š ë‹¤ì¤‘ ë°ì´í„° ì†ŒìŠ¤ ë¹„êµ ë¶„ì„")
        
        # í†µí•© ëª…ì„¸ ê¸°ë°˜ ê¸°íšŒ
        if integration_specs:
            opportunities.append("ğŸš€ í†µí•© ë°ì´í„°ì…‹ ê¸°ë°˜ ê³ ê¸‰ ë¶„ì„")
        
        # ë°ì´í„° ë³¼ë¥¨ ê¸°ë°˜ ê¸°íšŒ
        total_rows = sum(card.rows for card in data_cards)
        if total_rows > 10000:
            opportunities.append("ğŸ¤– ëŒ€ìš©ëŸ‰ ë°ì´í„° ê¸°ë°˜ ë¨¸ì‹ ëŸ¬ë‹")
        
        return opportunities
    
    def _generate_smart_recommendations(self, 
                                      data_cards: List[VisualDataCard],
                                      relationships: List[DatasetRelationship],
                                      integration_specs: List[IntegratedDatasetSpec]) -> List[str]:
        """ìŠ¤ë§ˆíŠ¸ ì¶”ì²œ ìƒì„±"""
        recommendations = []
        
        if len(relationships) > 0:
            recommendations.append(f"ğŸ¯ {len(relationships)}ê°œì˜ ë°ì´í„° ê´€ê³„ê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. í†µí•© ë¶„ì„ì„ ì‹œì‘í•´ë³´ì„¸ìš”!")
        
        high_conf_rels = [r for r in relationships if r.confidence_score > 0.8]
        if high_conf_rels:
            recommendations.append(f"â­ {len(high_conf_rels)}ê°œì˜ ê³ ì‹ ë¢°ë„ ê´€ê³„ëŠ” ì¦‰ì‹œ í™œìš© ê°€ëŠ¥í•©ë‹ˆë‹¤!")
        
        if integration_specs:
            recommendations.append(f"ğŸ”„ {len(integration_specs)}ê°€ì§€ í†µí•© ì „ëµì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ì›í´ë¦­ìœ¼ë¡œ ì‹¤í–‰í•˜ì„¸ìš”!")
        
        # ë°ì´í„° í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œ
        avg_quality = sum(card.quality_indicators.quality_score for card in data_cards if card.quality_indicators) / len(data_cards)
        if avg_quality < 85:
            recommendations.append("ğŸ§¹ í†µí•© ì „ ë°ì´í„° ì •ì œë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤ (í’ˆì§ˆ í–¥ìƒ ì˜ˆìƒ)")
        
        return recommendations
    
    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            'universal_engine_available': UNIVERSAL_ENGINE_AVAILABLE,
            'cache_status': {
                'relationships_cached': len(self.relationship_cache),
                'integrations_cached': len(self.integration_cache)
            },
            'supported_relationship_types': list(self.relationship_types.keys()),
            'component_status': {
                'relationship_discovery': self.relationship_discovery is not None,
                'data_integration': self.data_integration is not None,
                'analysis_orchestrator': self.analysis_orchestrator is not None,
                'semantic_mapping': self.semantic_mapping is not None
            }
        }