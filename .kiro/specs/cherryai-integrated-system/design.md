# CherryAI í†µí•© ì‹œìŠ¤í…œ ì„¤ê³„ ë¬¸ì„œ

## ğŸ“‹ ê°œìš”

ì´ ì„¤ê³„ ë¬¸ì„œëŠ” **ê²€ì¦ ì™„ë£Œëœ LLM-First Universal Engine ë°±ì—”ë“œ**ì™€ **ChatGPT ìŠ¤íƒ€ì¼ UI/UX**ë¥¼ í†µí•©í•œ CherryAI ì‹œìŠ¤í…œì˜ ì•„í‚¤í…ì²˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

### í†µí•© ì„¤ê³„ ì›ì¹™
- **ë°±ì—”ë“œ ë³´ì¡´**: ê²€ì¦ëœ Universal Engine êµ¬ì¡° 100% ìœ ì§€
- **UI ë ˆì´ì–´ ì¶”ê°€**: ê¸°ì¡´ ë°±ì—”ë“œ ìœ„ì— ìƒˆë¡œìš´ UI ë ˆì´ì–´ êµ¬ì¶•
- **ì ì§„ì  í†µí•©**: ê¸°ì¡´ ê¸°ëŠ¥ì„ ê¹¨ëœ¨ë¦¬ì§€ ì•ŠëŠ” ì ì§„ì  ê°œì„ 
- **ì„±ëŠ¥ ìœ ì§€**: ê²€ì¦ëœ 45ì´ˆ ì‘ë‹µ ì‹œê°„ ì„±ëŠ¥ ë³´ì¥

### í˜„ì¬ ê²€ì¦ëœ ë°±ì—”ë“œ êµ¬ì¡°
```
âœ… LLM-First Universal Engine (ê²€ì¦ ì™„ë£Œ)
â”œâ”€â”€ core/universal_engine/
â”‚   â”œâ”€â”€ universal_query_processor.py     # ë©”ì¸ ì¿¼ë¦¬ ì²˜ë¦¬ê¸°
â”‚   â”œâ”€â”€ meta_reasoning_engine.py         # 4ë‹¨ê³„ ë©”íƒ€ ì¶”ë¡ 
â”‚   â”œâ”€â”€ dynamic_context_discovery.py     # ë™ì  ì»¨í…ìŠ¤íŠ¸ ë°œê²¬
â”‚   â”œâ”€â”€ adaptive_user_understanding.py   # ì ì‘í˜• ì‚¬ìš©ì ì´í•´
â”‚   â”œâ”€â”€ universal_intent_detection.py    # ë²”ìš© ì˜ë„ ê°ì§€
â”‚   â””â”€â”€ a2a_integration/
â”‚       â”œâ”€â”€ a2a_agent_discovery.py       # A2A ì—ì´ì „íŠ¸ ë°œê²¬
â”‚       â””â”€â”€ a2a_workflow_orchestrator.py # A2A ì›Œí¬í”Œë¡œìš° ì¡°ìœ¨
â”œâ”€â”€ core/orchestrator/
â”‚   â”œâ”€â”€ planning_engine.py               # LLM ê¸°ë°˜ ê³„íš ìˆ˜ë¦½
â”‚   â””â”€â”€ execution_engine.py              # ì‹¤í–‰ ì—”ì§„
â””â”€â”€ a2a_orchestrator.py                  # A2A ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (í¬íŠ¸ 8100)
```

## ğŸ—ï¸ í†µí•© ì•„í‚¤í…ì²˜ ì„¤ê³„

### ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```mermaid
graph TB
    subgraph "ğŸ¨ New UI Layer (cherry_ai.py)"
        UI[Streamlit ChatGPT-style UI]
        Chat[Chat Interface]
        Stream[SSE Streaming Handler]
        Rec[Recommendation Engine]
        Upload[File Upload Handler]
    end
    
    subgraph "ğŸ”— Integration Layer"
        UIBridge[UI-Backend Bridge]
        StreamBridge[Streaming Bridge]
        LangfuseBridge[Langfuse Integration]
        SessionMgr[Session Manager]
    end
    
    subgraph "âœ… Verified Universal Engine Backend"
        UQP[UniversalQueryProcessor]
        MRE[MetaReasoningEngine]
        DCD[DynamicContextDiscovery]
        AUU[AdaptiveUserUnderstanding]
        UID[UniversalIntentDetection]
        
        subgraph "A2A Integration"
            AADS[A2AAgentDiscoverySystem]
            AWO[A2AWorkflowOrchestrator]
        end
    end
    
    subgraph "ğŸ¤– A2A Agents (Verified)"
        A1[Data Cleaning - 8306]
        A2[Data Loader - 8307]
        A3[Visualization - 8308]
        A4[Wrangling - 8309]
        A5[Feature Eng - 8310]
        A6[SQL DB - 8311]
        A7[EDA Tools - 8312]
        A8[H2O ML - 8313]
        A9[MLflow - 8314]
        A10[Pandas Agent - 8210]
        A11[Report Gen - 8316]
        Orch[Orchestrator - 8100]
    end
    
    subgraph "ğŸ“Š Enhanced Services"
        Langfuse[Langfuse v2 Tracing]
        SSE[SSE Streaming]
        E2E[E2E Testing]
        Perf[Performance Monitor]
    end
    
    UI --> UIBridge
    Chat --> StreamBridge
    Stream --> SSE
    Upload --> UIBridge
    
    UIBridge --> UQP
    StreamBridge --> MRE
    LangfuseBridge --> Langfuse
    SessionMgr --> Langfuse
    
    UQP --> MRE
    UQP --> DCD
    UQP --> AUU
    UQP --> UID
    
    MRE --> AADS
    MRE --> AWO
    
    AADS --> Orch
    AWO --> A1
    AWO --> A2
    AWO --> A3
    AWO --> A4
    AWO --> A5
    AWO --> A6
    AWO --> A7
    AWO --> A8
    AWO --> A9
    AWO --> A10
    AWO --> A11
    
    SSE --> StreamBridge
    E2E --> UI
    Perf --> UIBridge
```

### í†µí•© íŒŒì¼ êµ¬ì¡°

```
cherry_ai.py (ìƒˆë¡œìš´ í†µí•© ë©”ì¸ íŒŒì¼)
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ chatgpt_interface.py             # ChatGPT ìŠ¤íƒ€ì¼ ë©”ì¸ ì¸í„°í˜ì´ìŠ¤
â”‚   â”œâ”€â”€ streaming_handler.py             # SSE ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
â”‚   â”œâ”€â”€ recommendation_panel.py          # ì§€ëŠ¥í˜• ì¶”ì²œ íŒ¨ë„
â”‚   â”œâ”€â”€ agent_status_display.py          # A2A ì—ì´ì „íŠ¸ ìƒíƒœ í‘œì‹œ
â”‚   â”œâ”€â”€ file_upload_handler.py           # íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
â”‚   â””â”€â”€ result_renderer.py               # ê²°ê³¼ ë Œë”ë§ (ì½”ë“œ, ì°¨íŠ¸, í…Œì´ë¸”)
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ universal_engine_bridge.py       # Universal Engine ì—°ê²° ë¸Œë¦¬ì§€
â”‚   â”œâ”€â”€ a2a_agent_connector.py           # A2A ì—ì´ì „íŠ¸ ì—°ê²° ê´€ë¦¬
â”‚   â”œâ”€â”€ langfuse_session_tracer.py       # Langfuse v2 ì„¸ì…˜ ì¶”ì 
â”‚   â”œâ”€â”€ streaming_task_updater.py        # ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ì—…ë°ì´í„°
â”‚   â””â”€â”€ expert_scenario_handler.py       # ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ recommendation_service.py        # ë¶„ì„ ì¶”ì²œ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ session_service.py               # ì„¸ì…˜ ê´€ë¦¬ ì„œë¹„ìŠ¤
â”‚   â”œâ”€â”€ performance_service.py           # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì„œë¹„ìŠ¤
â”‚   â””â”€â”€ error_handling_service.py        # ì—ëŸ¬ ì²˜ë¦¬ ì„œë¹„ìŠ¤
â””â”€â”€ config/
    â”œâ”€â”€ ui_config.py                     # UI ì„¤ì •
    â”œâ”€â”€ streaming_config.py              # ìŠ¤íŠ¸ë¦¬ë° ì„¤ì •
    â””â”€â”€ langfuse_config.py               # Langfuse ì„¤ì •

âœ… core/ (ê¸°ì¡´ ê²€ì¦ëœ ë°±ì—”ë“œ - ìˆ˜ì • ì—†ìŒ)
âœ… a2a_ds_servers/ (ê¸°ì¡´ A2A ì—ì´ì „íŠ¸ë“¤ - ìˆ˜ì • ì—†ìŒ)
âœ… a2a_orchestrator.py (ê¸°ì¡´ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ìˆ˜ì • ì—†ìŒ)
```

## ğŸ”— í•µì‹¬ í†µí•© ì»´í¬ë„ŒíŠ¸

### 1. Universal Engine Bridge

```python
class UniversalEngineBridge:
    """ê²€ì¦ëœ Universal Engineê³¼ ìƒˆ UI ê°„ì˜ ë¸Œë¦¬ì§€"""
    
    def __init__(self):
        # ê²€ì¦ëœ ë°±ì—”ë“œ ì»´í¬ë„ŒíŠ¸ë“¤ ì´ˆê¸°í™” (ìˆ˜ì • ì—†ìŒ)
        self.universal_processor = UniversalQueryProcessor()
        self.meta_reasoning = MetaReasoningEngine()
        self.context_discovery = DynamicContextDiscovery()
        self.user_understanding = AdaptiveUserUnderstanding()
        self.intent_detection = UniversalIntentDetection()
        
        # A2A í†µí•© (ê²€ì¦ëœ êµ¬ì¡° ìœ ì§€)
        self.agent_discovery = A2AAgentDiscoverySystem()
        self.workflow_orchestrator = A2AWorkflowOrchestrator()
        
        # ìƒˆë¡œìš´ í†µí•© ë ˆì´ì–´
        self.streaming_updater = StreamingTaskUpdater()
        self.langfuse_tracer = LangfuseSessionTracer()
    
    async def process_user_query(self, query: str, session_id: str, user_id: str = "2055186") -> AsyncGenerator[str, None]:
        """
        ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ Universal Engineìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
        ê¸°ì¡´ ê²€ì¦ëœ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜ ìŠ¤íŠ¸ë¦¬ë° ë ˆì´ì–´ë§Œ ì¶”ê°€
        """
        # Langfuse ì„¸ì…˜ ì‹œì‘
        trace_id = await self.langfuse_tracer.start_session(
            session_id=f"user_query_{int(time.time())}_{user_id}",
            user_id=user_id,
            query=query
        )
        
        try:
            # 1. ê²€ì¦ëœ Universal Engine ì²˜ë¦¬ (ìˆ˜ì • ì—†ìŒ)
            yield "ğŸ’ Universal Engineì´ ìš”ì²­ì„ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."
            
            # ê¸°ì¡´ ê²€ì¦ëœ ë©”ì„œë“œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            processing_result = await self.universal_processor.process_query(query)
            
            yield f"ğŸ§  ë©”íƒ€ ì¶”ë¡  ì—”ì§„ì´ 4ë‹¨ê³„ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤..."
            
            # ê¸°ì¡´ ê²€ì¦ëœ ë©”íƒ€ ì¶”ë¡  (ìˆ˜ì • ì—†ìŒ)
            meta_result = await self.meta_reasoning.perform_meta_reasoning(
                query, processing_result.context
            )
            
            yield f"ğŸ” {len(meta_result.get('selected_agents', []))}ê°œ ì—ì´ì „íŠ¸ê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤..."
            
            # ê¸°ì¡´ ê²€ì¦ëœ A2A ì›Œí¬í”Œë¡œìš° (ìˆ˜ì • ì—†ìŒ)
            workflow_result = await self.workflow_orchestrator.execute_agent_workflow(
                meta_result.get('workflow_config', {})
            )
            
            # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ê²°ê³¼ ì „ì†¡
            async for chunk in self._stream_workflow_results(workflow_result):
                yield chunk
                
            # Langfuse ì„¸ì…˜ ì™„ë£Œ
            await self.langfuse_tracer.end_session(trace_id, workflow_result)
            
        except Exception as e:
            await self.langfuse_tracer.log_error(trace_id, str(e))
            yield f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
    async def _stream_workflow_results(self, workflow_result: Dict) -> AsyncGenerator[str, None]:
        """ì›Œí¬í”Œë¡œìš° ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ì†¡"""
        for step in workflow_result.get('execution_steps', []):
            yield f"âš¡ {step['agent_name']}: {step['description']}"
            await asyncio.sleep(0.001)  # 0.001ì´ˆ ì§€ì—°
            
            if step.get('result'):
                yield f"âœ… {step['agent_name']} ì™„ë£Œ: {step['result'][:100]}..."
                await asyncio.sleep(0.001)
```

### 2. ChatGPT Style Interface

```python
class ChatGPTInterface:
    """ChatGPT ìŠ¤íƒ€ì¼ ë©”ì¸ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self):
        self.engine_bridge = UniversalEngineBridge()
        self.recommendation_service = RecommendationService()
        self.session_service = SessionService()
        
    def render_main_interface(self):
        """ë©”ì¸ ChatGPT ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤ ë Œë”ë§"""
        st.set_page_config(
            page_title="ğŸ’ CherryAI - Universal Data Analysis Platform",
            page_icon="ğŸ’",
            layout="wide",
            initial_sidebar_state="expanded"
        )
        
        # ChatGPT ìŠ¤íƒ€ì¼ í—¤ë”
        st.markdown("""
        <div style="text-align: center; padding: 2rem 0;">
            <h1>ğŸ’ CherryAI</h1>
            <p style="font-size: 1.2rem; color: #666;">
                Universal Data Analysis Platform powered by LLM-First Architecture
            </p>
        </div>
        """, unsafe_allow_html=True)
        
        # ì‚¬ì´ë“œë°”: ì—ì´ì „íŠ¸ ìƒíƒœ ë° ì„¤ì •
        self.render_sidebar()
        
        # ë©”ì¸ ì±„íŒ… ì˜ì—­
        self.render_chat_area()
        
        # íŒŒì¼ ì—…ë¡œë“œ ì˜ì—­
        self.render_file_upload_area()
        
        # ì¶”ì²œ ë¶„ì„ íŒ¨ë„
        self.render_recommendation_panel()
    
    def render_chat_area(self):
        """ChatGPT ìŠ¤íƒ€ì¼ ì±„íŒ… ì˜ì—­"""
        # ì±„íŒ… íˆìŠ¤í† ë¦¬ í‘œì‹œ
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                if message["role"] == "assistant" and "streaming" in message:
                    # ìŠ¤íŠ¸ë¦¬ë° ë©”ì‹œì§€ í‘œì‹œ
                    self.render_streaming_message(message)
                else:
                    st.markdown(message["content"])
        
        # ì‚¬ìš©ì ì…ë ¥
        if prompt := st.chat_input("ë°ì´í„° ë¶„ì„ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”..."):
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            st.session_state.messages.append({"role": "user", "content": prompt})
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ í‘œì‹œ
            with st.chat_message("user"):
                st.markdown(prompt)
            
            # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ìƒì„± ë° ìŠ¤íŠ¸ë¦¬ë°
            with st.chat_message("assistant"):
                self.handle_user_query(prompt)
    
    async def handle_user_query(self, query: str):
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ë° ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ"""
        session_id = st.session_state.get("session_id", str(uuid.uuid4()))
        
        # ìŠ¤íŠ¸ë¦¬ë° ì»¨í…Œì´ë„ˆ ìƒì„±
        response_container = st.empty()
        full_response = ""
        
        # Universal Engineì„ í†µí•œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        async for chunk in self.engine_bridge.process_user_query(query, session_id):
            full_response += chunk + "\n"
            response_container.markdown(full_response)
            await asyncio.sleep(0.001)  # 0.001ì´ˆ ì§€ì—°
        
        # ìµœì¢… ì‘ë‹µ ì €ì¥
        st.session_state.messages.append({
            "role": "assistant", 
            "content": full_response,
            "streaming": True
        })
        
        # í›„ì† ì¶”ì²œ ìƒì„±
        recommendations = await self.recommendation_service.generate_followup_recommendations(
            query, full_response
        )
        self.display_recommendations(recommendations)
    
    def render_sidebar(self):
        """ì‚¬ì´ë“œë°”: ì—ì´ì „íŠ¸ ìƒíƒœ ë° ì‹œìŠ¤í…œ ì •ë³´"""
        with st.sidebar:
            st.markdown("## ğŸ¤– A2A ì—ì´ì „íŠ¸ ìƒíƒœ")
            
            # ê²€ì¦ëœ A2A ì—ì´ì „íŠ¸ë“¤ ìƒíƒœ í‘œì‹œ
            agent_status = self.get_agent_status()
            for agent_id, status in agent_status.items():
                status_icon = "ğŸŸ¢" if status["online"] else "ğŸ”´"
                st.markdown(f"{status_icon} **{status['name']}** (:{status['port']})")
                if status["online"]:
                    st.caption(f"ì‘ë‹µì‹œê°„: {status.get('response_time', 'N/A')}ms")
            
            st.markdown("---")
            st.markdown("## âš™ï¸ ì‹œìŠ¤í…œ ì„¤ì •")
            
            # Universal Engine ìƒíƒœ
            st.markdown("### ğŸ§  Universal Engine")
            st.success("âœ… LLM-First Architecture Active")
            st.info(f"ğŸš€ Model: qwen3-4b-fast")
            st.info(f"â±ï¸ Avg Response: 45s")
            
            # Langfuse ì¶”ì  ìƒíƒœ
            st.markdown("### ğŸ“Š Langfuse Tracing")
            if st.session_state.get("langfuse_enabled", True):
                st.success("âœ… Session Tracking Active")
                st.caption(f"User ID: {os.getenv('EMP_NO', '2055186')}")
            else:
                st.warning("âš ï¸ Tracing Disabled")
```

### 3. Langfuse Session Tracer

```python
class LangfuseSessionTracer:
    """Langfuse v2 ì„¸ì…˜ ê¸°ë°˜ ì¶”ì  ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.langfuse = None
        self.current_trace = None
        self.session_spans = {}
        
        # Langfuse v2 ì´ˆê¸°í™”
        try:
            from langfuse import Langfuse
            self.langfuse = Langfuse(
                public_key=os.getenv("LANGFUSE_PUBLIC_KEY"),
                secret_key=os.getenv("LANGFUSE_SECRET_KEY"),
                host=os.getenv("LANGFUSE_HOST", "http://localhost:3000")
            )
            self.enabled = True
        except Exception as e:
            logger.warning(f"Langfuse ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.enabled = False
    
    async def start_session(self, session_id: str, user_id: str, query: str) -> str:
        """ìƒˆ ì„¸ì…˜ ì‹œì‘ ë° ì¶”ì  ì´ˆê¸°í™”"""
        if not self.enabled:
            return "disabled"
        
        try:
            # ë©”ì¸ ì„¸ì…˜ trace ìƒì„±
            self.current_trace = self.langfuse.trace(
                name="CherryAI_Analysis_Session",
                session_id=session_id,
                user_id=user_id,
                metadata={
                    "query": query,
                    "timestamp": datetime.now().isoformat(),
                    "system": "CherryAI_Universal_Engine",
                    "version": "v2.0"
                }
            )
            
            # ì´ˆê¸° span ìƒì„±
            self.session_spans["query_processing"] = self.current_trace.span(
                name="Universal_Query_Processing",
                input={"user_query": query}
            )
            
            return self.current_trace.id
            
        except Exception as e:
            logger.error(f"Langfuse ì„¸ì…˜ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return "error"
    
    async def log_agent_execution(self, agent_name: str, input_data: Dict, output_data: Dict, execution_time: float):
        """ê°œë³„ ì—ì´ì „íŠ¸ ì‹¤í–‰ ë¡œê¹…"""
        if not self.enabled or not self.current_trace:
            return
        
        try:
            agent_span = self.current_trace.span(
                name=f"A2A_Agent_{agent_name}",
                input=input_data,
                output=output_data,
                metadata={
                    "agent_type": agent_name,
                    "execution_time": execution_time,
                    "timestamp": datetime.now().isoformat()
                }
            )
            
            # ì—ì´ì „íŠ¸ë³„ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡
            agent_span.score(
                name="execution_time",
                value=execution_time,
                comment=f"{agent_name} execution performance"
            )
            
        except Exception as e:
            logger.error(f"ì—ì´ì „íŠ¸ ì‹¤í–‰ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    async def log_universal_engine_step(self, step_name: str, input_data: Dict, output_data: Dict):
        """Universal Engine ë‹¨ê³„ë³„ ë¡œê¹…"""
        if not self.enabled or not self.current_trace:
            return
        
        try:
            step_span = self.current_trace.span(
                name=f"Universal_Engine_{step_name}",
                input=input_data,
                output=output_data,
                metadata={
                    "engine_component": step_name,
                    "timestamp": datetime.now().isoformat()
                }
            )
            
        except Exception as e:
            logger.error(f"Universal Engine ë‹¨ê³„ ë¡œê¹… ì‹¤íŒ¨: {e}")
    
    async def end_session(self, trace_id: str, final_result: Dict):
        """ì„¸ì…˜ ì¢…ë£Œ ë° ìµœì¢… ê²°ê³¼ ê¸°ë¡"""
        if not self.enabled or not self.current_trace:
            return
        
        try:
            # ìµœì¢… ê²°ê³¼ ê¸°ë¡
            self.current_trace.update(
                output=final_result,
                metadata={
                    "session_completed": datetime.now().isoformat(),
                    "total_agents_used": len(final_result.get("agent_contributions", {})),
                    "analysis_quality": final_result.get("quality_score", 0.0)
                }
            )
            
            # ì„¸ì…˜ í’ˆì§ˆ ì ìˆ˜ ê¸°ë¡
            if "quality_score" in final_result:
                self.current_trace.score(
                    name="analysis_quality",
                    value=final_result["quality_score"],
                    comment="Overall analysis quality assessment"
                )
            
            # ì„¸ì…˜ ì •ë¦¬
            self.current_trace = None
            self.session_spans.clear()
            
        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ë¡œê¹… ì‹¤íŒ¨: {e}")
```

### 4. Expert Scenario Handler

```python
class ExpertScenarioHandler:
    """ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ (ì´ì˜¨ì£¼ì… + query.txt) íŠ¹í™” ì²˜ë¦¬"""
    
    def __init__(self, universal_engine_bridge: UniversalEngineBridge):
        self.engine_bridge = universal_engine_bridge
        self.ion_implant_dataset_path = "ion_implant_3lot_dataset.csv"
        self.query_file_path = "query.txt"
    
    async def detect_expert_scenario(self, uploaded_file, user_query: str) -> bool:
        """ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ ê°ì§€"""
        # íŒŒì¼ëª… ê¸°ë°˜ ê°ì§€
        if uploaded_file and "ion_implant" in uploaded_file.name.lower():
            return True
        
        # ì¿¼ë¦¬ ë‚´ìš© ê¸°ë°˜ ê°ì§€ (Universal Engine í™œìš©)
        domain_analysis = await self.engine_bridge.context_discovery.detect_domain(
            data=None, query=user_query
        )
        
        detected_domains = domain_analysis.get("detected_domains", [])
        return any("semiconductor" in domain.lower() or "ion" in domain.lower() 
                  for domain in detected_domains)
    
    async def handle_expert_scenario(self, uploaded_file, user_query: str) -> AsyncGenerator[str, None]:
        """ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ ì „ìš© ì²˜ë¦¬"""
        yield "ğŸ”¬ ë°˜ë„ì²´ ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤..."
        
        # 1. ë°ì´í„° ë¡œë“œ ë° ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        if uploaded_file:
            data = pd.read_csv(uploaded_file)
            yield f"ğŸ“Š ì´ì˜¨ì£¼ì… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {data.shape[0]}ê°œ LOT, {data.shape[1]}ê°œ ì»¬ëŸ¼"
        else:
            # ê¸°ë³¸ ë°ì´í„°ì…‹ ì‚¬ìš©
            data = pd.read_csv(self.ion_implant_dataset_path)
            yield f"ğŸ“Š ê¸°ë³¸ ì´ì˜¨ì£¼ì… ë°ì´í„°ì…‹ ì‚¬ìš©: {data.shape[0]}ê°œ LOT"
        
        # 2. query.txt ë‚´ìš© í†µí•© (1íšŒì„±)
        if os.path.exists(self.query_file_path):
            with open(self.query_file_path, 'r', encoding='utf-8') as f:
                domain_knowledge = f.read()
            
            # ë„ë©”ì¸ ì§€ì‹ê³¼ ì‚¬ìš©ì ì¿¼ë¦¬ ê²°í•©
            enhanced_query = f"""
            {domain_knowledge}
            
            ì‚¬ìš©ì ìš”ì²­: {user_query}
            
            ìœ„ì˜ ë„ë©”ì¸ ì§€ì‹ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.
            """
            yield "ğŸ“š ë°˜ë„ì²´ ë„ë©”ì¸ ì§€ì‹ì´ í†µí•©ë˜ì—ˆìŠµë‹ˆë‹¤..."
        else:
            enhanced_query = user_query
            yield "âš ï¸ query.txt íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ ë¶„ì„ì„ ì§„í–‰í•©ë‹ˆë‹¤..."
        
        # 3. Universal Engineì„ í†µí•œ ì „ë¬¸ê°€ ìˆ˜ì¤€ ë¶„ì„
        yield "ğŸ§  Universal Engineì´ ì „ë¬¸ê°€ ìˆ˜ì¤€ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤..."
        
        # ê¸°ì¡´ ê²€ì¦ëœ Universal Engine ë¡œì§ í™œìš©
        async for chunk in self.engine_bridge.process_user_query(
            enhanced_query, 
            session_id=f"expert_scenario_{int(time.time())}",
            user_id=os.getenv('EMP_NO', '2055186')
        ):
            yield chunk
        
        yield "âœ… ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
```

## ğŸ”§ êµ¬í˜„ ì „ëµ

### Phase 1: ê¸°ë³¸ í†µí•© (3-5ì¼)
1. **Universal Engine Bridge êµ¬í˜„**: ê²€ì¦ëœ ë°±ì—”ë“œì™€ ìƒˆ UI ì—°ê²°
2. **ChatGPT Interface ê¸°ë³¸ êµ¬ì¡°**: Streamlit ê¸°ë°˜ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
3. **ê¸°ë³¸ ìŠ¤íŠ¸ë¦¬ë°**: SSE ê¸°ë°˜ ì‹¤ì‹œê°„ ì‘ë‹µ í‘œì‹œ

### Phase 2: ê³ ê¸‰ ê¸°ëŠ¥ í†µí•© (3-4ì¼)
1. **Langfuse v2 í†µí•©**: ì„¸ì…˜ ê¸°ë°˜ ì¶”ì  ì‹œìŠ¤í…œ
2. **ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤**: ì´ì˜¨ì£¼ì… ë°ì´í„° + query.txt ì²˜ë¦¬
3. **ì¶”ì²œ ì‹œìŠ¤í…œ**: ì§€ëŠ¥í˜• ë¶„ì„ ì¶”ì²œ ì—”ì§„

### Phase 3: E2E í…ŒìŠ¤íŠ¸ ë° ìµœì í™” (2-3ì¼)
1. **Playwright MCP í†µí•©**: ìë™í™”ëœ E2E í…ŒìŠ¤íŠ¸
2. **ì„±ëŠ¥ ìµœì í™”**: 45ì´ˆ ì‘ë‹µ ì‹œê°„ ìœ ì§€
3. **ì‹œìŠ¤í…œ ê´€ë¦¬**: start.sh/stop.sh ìŠ¤í¬ë¦½íŠ¸ ê°œì„ 

### Phase 4: í’ˆì§ˆ ë³´ì¦ (1-2ì¼)
1. **í†µí•© í…ŒìŠ¤íŠ¸**: UIì™€ ë°±ì—”ë“œ ì™„ì „ í†µí•© ê²€ì¦
2. **íšŒê·€ í…ŒìŠ¤íŠ¸**: ê¸°ì¡´ ê¸°ëŠ¥ ë¬´ê²°ì„± í™•ì¸
3. **ì‚¬ìš©ì í…ŒìŠ¤íŠ¸**: ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ê²€ì¦

## ğŸ¯ ì„±ê³µ ê¸°ì¤€

### ê¸°ìˆ ì  ì„±ê³µ ê¸°ì¤€
- **ë°±ì—”ë“œ ë³´ì¡´**: ê²€ì¦ëœ Universal Engine 100% ë¬´ê²°ì„± ìœ ì§€
- **UI/UX í’ˆì§ˆ**: ChatGPT ìˆ˜ì¤€ì˜ ì‚¬ìš©ì ê²½í—˜ ì œê³µ
- **ì„±ëŠ¥ ìœ ì§€**: í‰ê·  45ì´ˆ ì‘ë‹µ ì‹œê°„ ë‹¬ì„±
- **ê¸°ëŠ¥ ì™„ì„±**: Langfuse v2, SSE ìŠ¤íŠ¸ë¦¬ë°, E2E í…ŒìŠ¤íŠ¸ 100% êµ¬í˜„

### ì‚¬ìš©ì ê²½í—˜ ê¸°ì¤€
- **ì§ê´€ì  ì¸í„°í˜ì´ìŠ¤**: í•™ìŠµ ì—†ì´ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥
- **ì‹¤ì‹œê°„ í”¼ë“œë°±**: 0.001ì´ˆ ì§€ì—°ì˜ ë¶€ë“œëŸ¬ìš´ ìŠ¤íŠ¸ë¦¬ë°
- **ì „ë¬¸ê°€ ì§€ì›**: ë„ë©”ì¸ íŠ¹í™” ë¶„ì„ ì™„ë²½ ì§€ì›
- **ì‹ ë¢°ì„±**: 99.9% ì‹œìŠ¤í…œ ê°€ìš©ì„±

ì´ í†µí•© ì„¤ê³„ë¥¼ í†µí•´ ê²€ì¦ëœ ë°±ì—”ë“œì˜ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë©´ì„œ ìµœê³  ìˆ˜ì¤€ì˜ ì‚¬ìš©ì ê²½í—˜ì„ ì œê³µí•˜ëŠ” ì™„ì „í•œ CherryAI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.