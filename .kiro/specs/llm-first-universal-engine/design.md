# ğŸ§  LLM First ë²”ìš© ë„ë©”ì¸ ë¶„ì„ ì—”ì§„ ì„¤ê³„ ë¬¸ì„œ

## ğŸ“‹ ê°œìš”

ì´ ì„¤ê³„ ë¬¸ì„œëŠ” ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§„ì •í•œ LLM First ë²”ìš© ë„ë©”ì¸ ë¶„ì„ ì—”ì§„ì˜ ìƒì„¸ ì•„í‚¤í…ì²˜ì™€ êµ¬í˜„ ë°©ì•ˆì„ ì •ì˜í•©ë‹ˆë‹¤. 

### í•µì‹¬ ì„¤ê³„ ì›ì¹™
- **Zero Hardcoding**: ëª¨ë“  ë¡œì§ì„ LLM ê¸°ë°˜ ë™ì  ì¶”ë¡ ìœ¼ë¡œ êµ¬í˜„
- **Meta-Architecture**: ì‹œìŠ¤í…œì´ ìŠ¤ìŠ¤ë¡œ êµ¬ì¡°ì™€ ë™ì‘ì„ ê²°ì •
- **Universal Adaptability**: ë‹¨ì¼ ì—”ì§„ìœ¼ë¡œ ëª¨ë“  ë„ë©”ì¸ê³¼ ì‚¬ìš©ì ìˆ˜ì¤€ ëŒ€ì‘
- **Self-Evolving**: ìƒí˜¸ì‘ìš©ì„ í†µí•œ ì§€ì†ì  ìê°€ ê°œì„ 

## ğŸ—ï¸ ì „ì²´ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

### 0. LLM Factory í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ì œê³µì ì„ íƒ ì‹œìŠ¤í…œ

```python
class LLMFactory:
    """
    í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ë™ì  LLM ì œê³µì ì„ íƒ ì‹œìŠ¤í…œ
    - LLM_PROVIDER=OLLAMA ì‹œ ìš°ì„ ì ìœ¼ë¡œ Ollama ì‚¬ìš©
    - OLLAMA_MODEL í™˜ê²½ ë³€ìˆ˜ë¡œ ëª¨ë¸ ë™ì  ì„ íƒ
    - í´ë°± ë©”ì»¤ë‹ˆì¦˜: Ollama ì‹¤íŒ¨ ì‹œ OpenAI ìë™ ì „í™˜
    """
    
    # ê¸°ë³¸ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜)
    DEFAULT_CONFIGS = {
        "openai": {
            "model": "gpt-4o-mini",
            "temperature": 0.7,
            "max_tokens": 4000
        },
        "ollama": {
            "model": "okamototk/gemma3-tools:4b",  # ë„êµ¬ í˜¸ì¶œ ì§€ì› ëª¨ë¸
            "temperature": 0.7,
            "base_url": "http://localhost:11434"
        },
        "anthropic": {
            "model": "claude-3-haiku-20240307",
            "temperature": 0.7,
            "max_tokens": 4000
        }
    }
    
    @staticmethod
    def create_llm_client(provider=None, model=None, config=None, **kwargs):
        """
        í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        
        ë™ì‘ ë°©ì‹:
        1. LLM_PROVIDER=OLLAMAì¸ ê²½ìš° â†’ ì²˜ìŒë¶€í„° Ollama ì‚¬ìš©
        2. ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° â†’ OpenAI ì‚¬ìš©í•˜ë‹¤ê°€ ì‹¤íŒ¨ ì‹œ Ollama í´ë°±
        3. OLLAMA_MODEL í™˜ê²½ ë³€ìˆ˜ë¡œ ëª¨ë¸ ë™ì  ì„ íƒ
        """
        # í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ì œê³µì ê²°ì •
        env_provider = os.getenv("LLM_PROVIDER", "").upper()
        if env_provider == "OLLAMA":
            provider = provider or "ollama"
        else:
            provider = provider or os.getenv("LLM_PROVIDER", "openai").lower()
        
        # í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ëª¨ë¸ ì„ íƒ
        if provider == "ollama":
            env_model = os.getenv("OLLAMA_MODEL", "okamototk/gemma3-tools:4b")
            model = model or env_model
        
        return cls._create_provider_client(provider, model, config, **kwargs)
    
    @staticmethod
    def get_system_recommendations():
        """
        ì‹œìŠ¤í…œ í™˜ê²½ì— ë§ëŠ” LLM ì¶”ì²œ
        - LLM_PROVIDER=OLLAMA ì„¤ì • ì‹œ Ollama ìš°ì„  ì¶”ì²œ
        - Ollama ì„œë²„ ìƒíƒœ í™•ì¸ í›„ í´ë°± ì „ëµ ì œì‹œ
        """
        env_provider = os.getenv("LLM_PROVIDER", "").upper()
        env_model = os.getenv("OLLAMA_MODEL", "okamototk/gemma3-tools:4b")
        
        if env_provider == "OLLAMA":
            if ollama_server_available():
                return {
                    "primary": {"provider": "ollama", "model": env_model},
                    "reason": "LLM_PROVIDER=OLLAMA ì„¤ì • - êµ¬ì„±ëœ Ollama ëª¨ë¸ ì‚¬ìš©"
                }
            else:
                return {
                    "primary": {"provider": "openai", "model": "gpt-4o-mini"},
                    "reason": "Ollama ì„œë²„ ì ‘ê·¼ ë¶ˆê°€ - OpenAI í´ë°± ì‚¬ìš©",
                    "instructions": [
                        "Ollama ì‚¬ìš©ì„ ìœ„í•´: 'ollama serve' ì‹¤í–‰ í›„ ëª¨ë¸ ì„¤ì¹˜",
                        f"ëª¨ë¸ ì„¤ì¹˜: 'ollama pull {env_model}'"
                    ]
                }
```

### 1. ì‹œìŠ¤í…œ êµ¬ì¡° ë‹¤ì´ì–´ê·¸ë¨

```mermaid
graph TD
    A[User Query + Data] --> B[Universal Query Processor]
    
    B --> C[Meta-Reasoning Engine]
    C --> C1[Query Intent Analyzer]
    C --> C2[Domain Context Detector]
    C --> C3[User Expertise Estimator]
    C --> C4[Response Strategy Selector]
    
    C --> D[Dynamic Knowledge Orchestrator]
    D --> D1[Real-time Knowledge Retrieval]
    D --> D2[Context-Aware Reasoning]
    D --> D3[A2A Agent Collaboration]
    D --> D4[Self-Reflection & Refinement]
    
    D --> N[A2A Agent Discovery & Selection]
    N --> N1[Data Cleaning Server 8306]
    N --> N2[Data Loader Server 8307]
    N --> N3[EDA Tools Server 8312]
    N --> N4[Feature Engineering Server 8310]
    N --> N5[H2O ML Server 8313]
    N --> N6[Visualization Server 8308]
    N --> N7[SQL Database Server 8311]
    N --> N8[MLflow Tools Server 8314]
    N --> N9[Pandas Collaboration Hub 8315]
    N --> N10[Data Wrangling Server 8309]
    
    N1 --> O[A2A Workflow Orchestrator]
    N2 --> O
    N3 --> O
    N4 --> O
    N5 --> O
    N6 --> O
    N7 --> O
    N8 --> O
    N9 --> O
    N10 --> O
    
    O --> P[A2A Result Integration]
    P --> E[Adaptive Response Generator]
    
    E --> E1[Expertise-Aware Explanation]
    E --> E2[Progressive Disclosure]
    E --> E3[Interactive Clarification]
    E --> E4[Follow-up Recommendation]
    
    E --> F[User Response]
    F --> G{User Satisfied?}
    G -->|No| H[Adaptation Loop]
    H --> C
    G -->|Yes| I[Session Learning]
    I --> J[Knowledge Update]
    
    K[Real-time Learning System] --> C
    K --> D
    K --> E
    
    L[Cherry AI Integration Layer] --> B
    M[Performance Monitoring] --> B
    M --> C
    M --> D
    M --> E
    M --> N
    M --> O
    M --> P
```

### 2. í•µì‹¬ ì»´í¬ë„ŒíŠ¸ ì„¤ê³„

#### 2.1 Universal Query Processor
```python
class UniversalQueryProcessor:
    """
    ì™„ì „ ë²”ìš© ì¿¼ë¦¬ ì²˜ë¦¬ê¸°
    - ì–´ë–¤ ë„ë©”ì¸ ê°€ì •ë„ í•˜ì§€ ì•ŠìŒ
    - ìˆœìˆ˜ LLM ê¸°ë°˜ ë™ì  ë¶„ì„
    - ëª¨ë“  ì²˜ë¦¬ ê³¼ì •ì„ ë©”íƒ€ ì¶”ë¡ ìœ¼ë¡œ ê²°ì •
    """
    
    def __init__(self):
        self.llm_client = self._initialize_llm()
        self.meta_reasoning_engine = MetaReasoningEngine()
        self.knowledge_orchestrator = DynamicKnowledgeOrchestrator()
        self.response_generator = AdaptiveResponseGenerator()
        self.learning_system = RealTimeLearningSystem()
        
    async def process_query(self, query: str, data: Any, context: Dict = None) -> Dict:
        """
        ì™„ì „ ë™ì  ì¿¼ë¦¬ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
        1. ë©”íƒ€ ì¶”ë¡ ìœ¼ë¡œ ì²˜ë¦¬ ì „ëµ ê²°ì •
        2. ë™ì  ì§€ì‹ í†µí•© ë° ë¶„ì„
        3. ì ì‘í˜• ì‘ë‹µ ìƒì„±
        4. ì‹¤ì‹œê°„ í•™ìŠµ ë° ê°œì„ 
        """
        # ë©”íƒ€ ì¶”ë¡ ìœ¼ë¡œ ì „ì²´ ì²˜ë¦¬ ì „ëµ ê²°ì •
        meta_analysis = await self.meta_reasoning_engine.analyze_request(
            query=query, 
            data=data, 
            context=context
        )
        
        # ë™ì  ì§€ì‹ í†µí•© ë° ì¶”ë¡ 
        knowledge_result = await self.knowledge_orchestrator.process_with_context(
            meta_analysis=meta_analysis,
            query=query,
            data=data
        )
        
        # ì ì‘í˜• ì‘ë‹µ ìƒì„±
        response = await self.response_generator.generate_adaptive_response(
            knowledge_result=knowledge_result,
            user_profile=meta_analysis.get('user_profile', {}),
            interaction_context=context
        )
        
        # ì‹¤ì‹œê°„ í•™ìŠµ
        await self.learning_system.learn_from_interaction({
            'query': query,
            'data_characteristics': meta_analysis.get('data_characteristics'),
            'response': response,
            'user_profile': meta_analysis.get('user_profile'),
            'timestamp': datetime.now()
        })
        
        return response
```

#### 2.2 Meta-Reasoning Engine (2024-2025 ìµœì‹  ì—°êµ¬ ê¸°ë°˜)
```python
class MetaReasoningEngine:
    """
    ë©”íƒ€ ì¶”ë¡  ì—”ì§„ - ìƒê°ì— ëŒ€í•´ ìƒê°í•˜ê¸°
    DeepSeek-R1 ì˜ê°ì„ ë°›ì€ ìê°€ ë°˜ì„± ì¶”ë¡  ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.llm_client = None
        self.reasoning_patterns = {
            'self_reflection': self._load_self_reflection_pattern(),
            'meta_rewarding': self._load_meta_rewarding_pattern(),
            'chain_of_thought': self._load_chain_of_thought_pattern(),
            'zero_shot_adaptive': self._load_zero_shot_pattern()
        }
        
    async def analyze_request(self, query: str, data: Any, context: Dict) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 2ì— ë”°ë¥¸ ë©”íƒ€ ì¶”ë¡  ë¶„ì„
        - DeepSeek-R1 ì˜ê° 4ë‹¨ê³„ ì¶”ë¡ 
        - ìê°€ í‰ê°€ ë° ê°œì„ 
        - ë™ì  ì „ëµ ì„ íƒ
        """
        # 1ë‹¨ê³„: ì´ˆê¸° ê´€ì°° ë° ë¶„ì„
        initial_analysis = await self._perform_initial_observation(query, data)
        
        # 2ë‹¨ê³„: ë‹¤ê°ë„ ë¶„ì„
        multi_perspective_analysis = await self._perform_multi_perspective_analysis(
            initial_analysis, query, data
        )
        
        # 3ë‹¨ê³„: ìê°€ ê²€ì¦
        self_verification = await self._perform_self_verification(
            multi_perspective_analysis
        )
        
        # 4ë‹¨ê³„: ì ì‘ì  ì‘ë‹µ ì „ëµ ê²°ì •
        response_strategy = await self._determine_adaptive_strategy(
            self_verification, context
        )
        
        # ë©”íƒ€ ë³´ìƒ íŒ¨í„´ìœ¼ë¡œ ì „ì²´ ë¶„ì„ í’ˆì§ˆ í‰ê°€
        quality_assessment = await self._assess_analysis_quality(response_strategy)
        
        return {
            'initial_analysis': initial_analysis,
            'multi_perspective': multi_perspective_analysis,
            'self_verification': self_verification,
            'response_strategy': response_strategy,
            'quality_assessment': quality_assessment,
            'confidence_level': quality_assessment.get('confidence', 0.0),
            'user_profile': response_strategy.get('estimated_user_profile', {}),
            'domain_context': initial_analysis.get('domain_context', {}),
            'data_characteristics': initial_analysis.get('data_characteristics', {})
        }
    
    async def _perform_initial_observation(self, query: str, data: Any) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 2ì˜ ìê°€ ë°˜ì„± ì¶”ë¡  íŒ¨í„´ - ë‹¨ê³„ 1: ì´ˆê¸° ê´€ì°°
        """
        observation_prompt = f"""
        # ìê°€ ë°˜ì„± ì¶”ë¡  íŒ¨í„´ - ë‹¨ê³„ 1: ì´ˆê¸° ê´€ì°°
        ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ì¿¼ë¦¬ì™€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        
        ì¿¼ë¦¬: {query}
        ë°ì´í„° íŠ¹ì„±: {self._analyze_data_characteristics(data)}
        
        ë‹¨ê³„ 1: ì´ˆê¸° ê´€ì°°
        - ë°ì´í„°ë¥¼ ë³´ê³  ë¬´ì—‡ì„ ë°œê²¬í•˜ëŠ”ê°€?
        - ì‚¬ìš©ì ì¿¼ë¦¬ì˜ ì§„ì •í•œ ì˜ë„ëŠ”?
        - ë‚´ê°€ ë†“ì¹˜ê³  ìˆëŠ” ê²ƒì€ ì—†ëŠ”ê°€?
        
        ì‚¬ì „ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ë‚˜ íŒ¨í„´ì— ì˜ì¡´í•˜ì§€ ë§ê³ ,
        ìˆœìˆ˜í•˜ê²Œ ê´€ì°°ëœ ê²ƒë§Œì„ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.
        """
        
        return await self.llm_client.analyze(observation_prompt)
    
    async def _perform_multi_perspective_analysis(self, initial_analysis: Dict, query: str, data: Any) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 2ì˜ ìê°€ ë°˜ì„± ì¶”ë¡  íŒ¨í„´ - ë‹¨ê³„ 2: ë‹¤ê°ë„ ë¶„ì„
        """
        multi_perspective_prompt = f"""
        # ìê°€ ë°˜ì„± ì¶”ë¡  íŒ¨í„´ - ë‹¨ê³„ 2: ë‹¤ê°ë„ ë¶„ì„
        
        ì´ˆê¸° ê´€ì°° ê²°ê³¼: {initial_analysis}
        
        ë‹¨ê³„ 2: ë‹¤ê°ë„ ë¶„ì„
        - ì´ ë¬¸ì œë¥¼ ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•œë‹¤ë©´?
        - ì‚¬ìš©ìê°€ ì „ë¬¸ê°€ë¼ë©´ ì–´ë–¤ ë‹µì„ ì›í• ê¹Œ?
        - ì‚¬ìš©ìê°€ ì´ˆë³´ìë¼ë©´ ì–´ë–¤ ë„ì›€ì´ í•„ìš”í• ê¹Œ?
        
        ê° ê´€ì ì—ì„œì˜ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³ ,
        ì‚¬ìš©ì ìˆ˜ì¤€ ì¶”ì •ê³¼ ìµœì  ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ì„¸ìš”.
        """
        
        return await self.llm_client.analyze(multi_perspective_prompt)
    
    async def _perform_self_verification(self, multi_perspective_analysis: Dict) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 2ì˜ ìê°€ ë°˜ì„± ì¶”ë¡  íŒ¨í„´ - ë‹¨ê³„ 3: ìê°€ ê²€ì¦
        """
        verification_prompt = f"""
        # ìê°€ ë°˜ì„± ì¶”ë¡  íŒ¨í„´ - ë‹¨ê³„ 3: ìê°€ ê²€ì¦
        
        ë‹¤ê°ë„ ë¶„ì„ ê²°ê³¼: {multi_perspective_analysis}
        
        ë‹¨ê³„ 3: ìê°€ ê²€ì¦
        - ë‚´ ë¶„ì„ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ì¼ê´€ì„±ì´ ìˆëŠ”ê°€?
        - ì‚¬ìš©ìì—ê²Œ ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ”ê°€?
        - í™•ì‹ ì´ ì—†ëŠ” ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?
        
        ë¶„ì„ì˜ ê°•ì ê³¼ ì•½ì ì„ ì†”ì§í•˜ê²Œ í‰ê°€í•˜ê³ ,
        ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…í™•íˆ ì‹ë³„í•˜ì„¸ìš”.
        """
        
        return await self.llm_client.analyze(verification_prompt)
    
    async def _determine_adaptive_strategy(self, self_verification: Dict, context: Dict) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 2ì˜ ìê°€ ë°˜ì„± ì¶”ë¡  íŒ¨í„´ - ë‹¨ê³„ 4: ì ì‘ì  ì‘ë‹µ
        """
        strategy_prompt = f"""
        # ìê°€ ë°˜ì„± ì¶”ë¡  íŒ¨í„´ - ë‹¨ê³„ 4: ì ì‘ì  ì‘ë‹µ
        
        ìê°€ ê²€ì¦ ê²°ê³¼: {self_verification}
        ìƒí˜¸ì‘ìš© ì»¨í…ìŠ¤íŠ¸: {context}
        
        ë‹¨ê³„ 4: ì ì‘ì  ì‘ë‹µ
        - í™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…í™•íˆ ì œì‹œ
        - ë¶ˆí™•ì‹¤í•œ ë¶€ë¶„ì€ ëª…í™•í™” ì§ˆë¬¸
        - ì‚¬ìš©ì ìˆ˜ì¤€ì— ë§ëŠ” ì„¤ëª… ê¹Šì´ ì¡°ì ˆ
        
        ìµœì ì˜ ì‘ë‹µ ì „ëµì„ ê²°ì •í•˜ê³ ,
        ì‚¬ìš©ì í”„ë¡œí•„ê³¼ ìƒí˜¸ì‘ìš© ë°©ì‹ì„ ì œì•ˆí•˜ì„¸ìš”.
        """
        
        return await self.llm_client.analyze(strategy_prompt)
    
    async def _assess_analysis_quality(self, response_strategy: Dict) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 2ì˜ ë©”íƒ€ ë³´ìƒ íŒ¨í„´ìœ¼ë¡œ ë¶„ì„ í’ˆì§ˆ í‰ê°€
        """
        quality_prompt = f"""
        # ìê°€ í‰ê°€ ë° ê°œì„  íŒ¨í„´
        ë‚´ ë¶„ì„ì„ ìŠ¤ìŠ¤ë¡œ í‰ê°€í•´ë³´ê² ìŠµë‹ˆë‹¤:
        
        ë¶„ì„ ë‚´ìš©: {response_strategy}
        
        í‰ê°€ ê¸°ì¤€:
        1. ì •í™•ì„±: ë¶„ì„ì´ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ í•´ì„í–ˆëŠ”ê°€?
        2. ì™„ì „ì„±: ì¤‘ìš”í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë†“ì¹˜ì§€ ì•Šì•˜ëŠ”ê°€?
        3. ì ì ˆì„±: ì‚¬ìš©ì ìˆ˜ì¤€ê³¼ ìš”êµ¬ì— ë§ëŠ”ê°€?
        4. ëª…í™•ì„±: ì„¤ëª…ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ê°€?
        5. ì‹¤ìš©ì„±: ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ì¡°ì¹˜ë¥¼ ì œì•ˆí–ˆëŠ”ê°€?
        
        ê°œì„ ì :
        - ë¶€ì¡±í•œ ë¶€ë¶„ì€ ë¬´ì—‡ì¸ê°€?
        - ì–´ë–»ê²Œ ë” ë‚˜ì€ ë¶„ì„ì„ í•  ìˆ˜ ìˆëŠ”ê°€?
        - ì‚¬ìš©ìì—ê²Œ ì¶”ê°€ë¡œ í•„ìš”í•œ ì •ë³´ëŠ”?
        
        ì´ í‰ê°€ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ê°œì„ í•˜ê² ìŠµë‹ˆë‹¤.
        """
        
        return await self.llm_client.analyze(quality_prompt)
```

## ğŸ¯ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ì²˜ë¦¬ ì„¤ê³„

### ì‹œë‚˜ë¦¬ì˜¤ 1: ì™„ì „ ì´ˆë³´ ì‚¬ìš©ì ì²˜ë¦¬
```python
class BeginnerScenarioHandler:
    """
    ìš”êµ¬ì‚¬í•­ 15ì˜ ì´ˆë³´ì ì‹œë‚˜ë¦¬ì˜¤ ì •í™•í•œ êµ¬í˜„
    """
    
    async def handle_beginner_query(self, query: str, data: Any) -> Dict:
        """
        "ì´ ë°ì´í„° íŒŒì¼ì´ ë­˜ ë§í•˜ëŠ”ì§€ ì „í˜€ ëª¨ë¥´ê² ì–´ìš”. ë„ì›€ ì£¼ì„¸ìš”." ì²˜ë¦¬
        """
        # ì‚¬ìš©ì ìˆ˜ì¤€ ê°ì§€
        user_analysis = await self._analyze_user_level(query)
        
        if user_analysis.get('level') == 'complete_beginner':
            response_template = """
            ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š 
            
            ì´ ë°ì´í„°ë¥¼ ë³´ë‹ˆ ë­”ê°€ ê³µì¥ì—ì„œ ì œí’ˆì„ ë§Œë“œëŠ” ê³¼ì •ì„ ê¸°ë¡í•œ ê²ƒ ê°™ë„¤ìš”. 
            ë§ˆì¹˜ ìš”ë¦¬ ë ˆì‹œí”¼ì˜ ì¬ë£Œ ë¶„ëŸ‰ì„ ì¸¡ì •í•œ ê¸°ë¡ì²˜ëŸ¼ ë³´ì—¬ìš”.
            
            ì¼ë‹¨ ëª‡ ê°€ì§€ í¥ë¯¸ë¡œìš´ íŒ¨í„´ì´ ë³´ì´ëŠ”ë°ìš”:
            1. ìˆ«ìë“¤ì´ ì¼ì •í•œ ë²”ìœ„ ì•ˆì—ì„œ ì›€ì§ì´ê³  ìˆì–´ìš”
            2. ì‹œê°„ì— ë”°ë¼ ë³€í™”í•˜ëŠ” ëª¨ìŠµì´ ìˆì–´ìš”
            3. ëª‡ ê°œì˜ ì£¼ìš” ì¸¡ì •ê°’ë“¤ì´ ìˆëŠ” ê²ƒ ê°™ì•„ìš”
            
            ì–´ë–¤ ë¶€ë¶„ì´ ê°€ì¥ ê¶ê¸ˆí•˜ì„¸ìš”? 
            - ì´ ìˆ«ìë“¤ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€?
            - ì¢‹ì€ ê±´ì§€ ë‚˜ìœ ê±´ì§€?
            - ë­”ê°€ ë¬¸ì œê°€ ìˆëŠ” ê±´ì§€?
            
            í•˜ë‚˜ì”© ì²œì²œíˆ ì•Œì•„ê°€ë´ìš”! ğŸ”
            """
            
            return await self._generate_beginner_response(response_template, data)
```

### ì‹œë‚˜ë¦¬ì˜¤ 2: ì „ë¬¸ê°€ ì‚¬ìš©ì ì²˜ë¦¬
```python
class ExpertScenarioHandler:
    """
    ìš”êµ¬ì‚¬í•­ 15ì˜ ì „ë¬¸ê°€ ì‹œë‚˜ë¦¬ì˜¤ ì •í™•í•œ êµ¬í˜„
    """
    
    async def handle_expert_query(self, query: str, data: Any) -> Dict:
        """
        "ê³µì • ëŠ¥ë ¥ ì§€ìˆ˜ê°€ 1.2ì¸ë° íƒ€ê²Ÿì„ 1.33ìœ¼ë¡œ ì˜¬ë¦¬ë ¤ë©´..." ì²˜ë¦¬
        """
        if self._detect_expert_terminology(query):
            # ì „ë¬¸ê°€ ìˆ˜ì¤€ ë¶„ì„ ìˆ˜í–‰
            technical_analysis = await self._perform_technical_analysis(query, data)
            
            response_template = """
            í˜„ì¬ Cpk 1.2ì—ì„œ 1.33ìœ¼ë¡œ ê°œì„ í•˜ë ¤ë©´ ë³€ë™ì„±ì„ ì•½ 8.3% ê°ì†Œì‹œì¼œì•¼ í•©ë‹ˆë‹¤.
            
            ## ë„ì¦ˆ ê· ì¼ì„± ë¶„ì„ ê²°ê³¼
            
            **í˜„ì¬ ìƒíƒœ:**
            - ë„ì¦ˆ ê· ì¼ì„±: Â±1.8% (3Ïƒ)
            - ì£¼ìš” ë³€ë™ ìš”ì¸: ì›¨ì´í¼ ì¤‘ì‹¬ë¶€ ê³¼ë„ì¦ˆ (1.2% í¸ì°¨)
            - ì—ì§€ ì˜ì—­ ë„ì¦ˆ ë¶€ì¡±: í‰ê·  ëŒ€ë¹„ -2.1%
            
            **Cpk 1.33 ë‹¬ì„±ì„ ìœ„í•œ í•µì‹¬ íŒŒë¼ë¯¸í„° ì¡°ì •:**
            
            1. **ë¹” ìŠ¤ìº” ìµœì í™”** (ì˜ˆìƒ ê°œì„ : 40%)
               - ìŠ¤ìº” ì†ë„: í˜„ì¬ ëŒ€ë¹„ 5-7% ê°ì†Œ
               - ìŠ¤ìº” íŒ¨í„´: Raster â†’ Serpentine ë³€ê²½ ê²€í† 
               - ì˜ˆìƒ ê· ì¼ì„± ê°œì„ : Â±1.8% â†’ Â±1.4%
            
            [ìƒì„¸ ê¸°ìˆ  ë¶„ì„ ê³„ì†...]
            """
            
            return await self._generate_expert_response(response_template, technical_analysis)
```

## ğŸ¤– A2A Agent í†µí•© ì„¤ê³„

### 2.3 A2A Agent Discovery & Selection System
```python
class A2AAgentDiscoverySystem:
    """
    ìš”êµ¬ì‚¬í•­ 21ì— ë”°ë¥¸ A2A Agent ìë™ ë°œê²¬ ë° ì„ íƒ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.agent_registry = {}
        self.agent_health_monitor = A2AHealthMonitor()
        self.llm_client = None
        
        # A2A Agent í¬íŠ¸ ë§¤í•‘
        self.AGENT_PORTS = {
            "data_cleaning": 8306,
            "data_loader": 8307,
            "data_visualization": 8308,
            "data_wrangling": 8309,
            "feature_engineering": 8310,
            "sql_database": 8311,
            "eda_tools": 8312,
            "h2o_ml": 8313,
            "mlflow_tools": 8314,
            "pandas_collaboration_hub": 8315
        }
    
    async def discover_available_agents(self) -> Dict[str, Any]:
        """
        ì‚¬ìš© ê°€ëŠ¥í•œ A2A Agent ìë™ ë°œê²¬
        """
        discovered_agents = {}
        
        for agent_id, port in self.AGENT_PORTS.items():
            try:
                # /.well-known/agent.json ì—”ë“œí¬ì¸íŠ¸ í™•ì¸
                agent_info = await self._validate_agent_endpoint(f"http://localhost:{port}")
                
                if agent_info:
                    discovered_agents[agent_id] = {
                        'id': agent_id,
                        'port': port,
                        'endpoint': f"http://localhost:{port}",
                        'capabilities': agent_info.get('capabilities', {}),
                        'skills': agent_info.get('skills', []),
                        'status': 'available',
                        'health_score': await self.agent_health_monitor.check_health(agent_id)
                    }
                    
            except Exception as e:
                logger.warning(f"Agent {agent_id} not available: {e}")
        
        self.agent_registry = discovered_agents
        return discovered_agents
    
    async def select_optimal_agents(self, meta_analysis: Dict, available_agents: Dict) -> List[Dict]:
        """
        ìš”êµ¬ì‚¬í•­ 20ì— ë”°ë¥¸ LLM ê¸°ë°˜ ë™ì  ì—ì´ì „íŠ¸ ì„ íƒ
        """
        agent_selection_prompt = f"""
        # LLM ê¸°ë°˜ ë™ì  A2A ì—ì´ì „íŠ¸ ì„ íƒ
        
        ì‚¬ìš©ì ìš”ì²­ ë¶„ì„: {meta_analysis}
        ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸: {list(available_agents.keys())}
        
        ê° A2A ì—ì´ì „íŠ¸ì˜ ì „ë¬¸ ê¸°ëŠ¥:
        - data_cleaning (8306): ğŸ§¹ LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ë°ì´í„° ì •ë¦¬, ë¹ˆ ë°ì´í„° ì²˜ë¦¬, 7ë‹¨ê³„ í‘œì¤€ ì •ë¦¬ í”„ë¡œì„¸ìŠ¤
        - data_loader (8307): ğŸ“ í†µí•© ë°ì´í„° ë¡œë”©, UTF-8 ì¸ì½”ë”© ë¬¸ì œ í•´ê²°, ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›
        - data_visualization (8308): ğŸ“Š Interactive ì‹œê°í™”, Plotly ê¸°ë°˜ ì°¨íŠ¸ ìƒì„±
        - data_wrangling (8309): ğŸ”§ ë°ì´í„° ë³€í™˜, ì¡°ì‘, êµ¬ì¡° ë³€ê²½
        - feature_engineering (8310): âš™ï¸ í”¼ì²˜ ìƒì„±, ë³€í™˜, ì„ íƒ, ì°¨ì› ì¶•ì†Œ
        - sql_database (8311): ğŸ—„ï¸ SQL ì¿¼ë¦¬ ì‹¤í–‰, ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        - eda_tools (8312): ğŸ” íƒìƒ‰ì  ë°ì´í„° ë¶„ì„, í†µê³„ ê³„ì‚°, íŒ¨í„´ ë°œê²¬
        - h2o_ml (8313): ğŸ¤– ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§, AutoML, ì˜ˆì¸¡ ë¶„ì„
        - mlflow_tools (8314): ğŸ“ˆ ëª¨ë¸ ê´€ë¦¬, ì‹¤í—˜ ì¶”ì , ë²„ì „ ê´€ë¦¬
        - pandas_collaboration_hub (8315): ğŸ¼ íŒë‹¤ìŠ¤ ê¸°ë°˜ ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„
        
        í•˜ë“œì½”ë”©ëœ ê·œì¹™ ì—†ì´, ì‚¬ìš©ì ìš”ì²­ì˜ ë³¸ì§ˆì„ íŒŒì•…í•˜ì—¬:
        1. í•„ìš”í•œ ì—ì´ì „íŠ¸ë“¤ì„ ì„ íƒí•˜ì„¸ìš”
        2. ìµœì ì˜ ì‹¤í–‰ ìˆœì„œë¥¼ ê²°ì •í•˜ì„¸ìš”
        3. ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë“¤ì„ ì‹ë³„í•˜ì„¸ìš”
        4. ê° ì—ì´ì „íŠ¸ì˜ ê¸°ëŒ€ ê¸°ì—¬ë„ë¥¼ í‰ê°€í•˜ì„¸ìš”
        
        JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
        {{
            "selected_agents": ["agent_id1", "agent_id2"],
            "execution_order": ["sequential_agents", "parallel_agents"],
            "reasoning": "ì„ íƒ ê·¼ê±°",
            "expected_workflow": "ì›Œí¬í”Œë¡œìš° ì„¤ëª…"
        }}
        """
        
        selection_result = await self.llm_client.analyze(agent_selection_prompt)
        return self._parse_agent_selection(selection_result, available_agents)
```

### 2.4 A2A Workflow Orchestrator
```python
class A2AWorkflowOrchestrator:
    """
    ìš”êµ¬ì‚¬í•­ 20ì— ë”°ë¥¸ A2A Agent ì›Œí¬í”Œë¡œìš° ë™ì  ì‹¤í–‰
    """
    
    def __init__(self):
        self.a2a_client = A2AClient()
        self.result_integrator = A2AResultIntegrator()
        self.error_handler = A2AErrorHandler()
        
    async def execute_agent_workflow(self, selected_agents: List, query: str, data: Any, meta_analysis: Dict) -> Dict:
        """
        A2A ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ë™ì  ì‹¤í–‰
        - ìˆœì°¨ ì‹¤í–‰: data_loader â†’ data_cleaning â†’ eda_tools â†’ feature_engineering â†’ h2o_ml
        - ë³‘ë ¬ ì‹¤í–‰: visualization + sql_database (ë…ë¦½ì  ë¶„ì„)
        - ê²°ê³¼ í†µí•©: pandas_collaboration_hubê°€ ìµœì¢… í†µí•©
        """
        workflow_results = {}
        execution_timeline = []
        
        try:
            # 1. ìˆœì°¨ ì‹¤í–‰ì´ í•„ìš”í•œ ì—ì´ì „íŠ¸ë“¤
            sequential_agents = self._identify_sequential_agents(selected_agents)
            
            for agent in sequential_agents:
                start_time = time.time()
                
                # í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ë¡œ A2A Agent í˜¸ì¶œ
                enhanced_request = self._create_enhanced_agent_request(
                    agent=agent,
                    query=query,
                    data=data,
                    meta_analysis=meta_analysis,
                    previous_results=workflow_results
                )
                
                agent_result = await self._execute_single_agent(agent, enhanced_request)
                workflow_results[agent['id']] = agent_result
                
                # ì‹¤í–‰ ì‹œê°„ ê¸°ë¡
                execution_time = time.time() - start_time
                execution_timeline.append({
                    'agent_id': agent['id'],
                    'execution_time': execution_time,
                    'status': 'completed'
                })
                
                # ì¤‘ê°„ ê²°ê³¼ë¥¼ ë‹¤ìŒ ì—ì´ì „íŠ¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©
                if agent_result.get('processed_data'):
                    data = agent_result['processed_data']
            
            # 2. ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸ë“¤
            parallel_agents = self._identify_parallel_agents(selected_agents)
            
            if parallel_agents:
                parallel_tasks = []
                for agent in parallel_agents:
                    enhanced_request = self._create_enhanced_agent_request(
                        agent=agent,
                        query=query,
                        data=data,
                        meta_analysis=meta_analysis,
                        previous_results=workflow_results
                    )
                    parallel_tasks.append(self._execute_single_agent(agent, enhanced_request))
                
                parallel_results = await asyncio.gather(*parallel_tasks, return_exceptions=True)
                
                for agent, result in zip(parallel_agents, parallel_results):
                    if isinstance(result, Exception):
                        # ì—ëŸ¬ ì²˜ë¦¬
                        workflow_results[agent['id']] = await self.error_handler.handle_agent_error(
                            agent, result, workflow_results
                        )
                    else:
                        workflow_results[agent['id']] = result
            
            # 3. ê²°ê³¼ í†µí•© ë° í’ˆì§ˆ ê²€ì¦
            integrated_results = await self.result_integrator.integrate_agent_results(
                workflow_results=workflow_results,
                meta_analysis=meta_analysis,
                execution_timeline=execution_timeline
            )
            
            return integrated_results
            
        except Exception as e:
            logger.error(f"A2A Workflow execution failed: {e}")
            return await self.error_handler.handle_workflow_error(e, workflow_results)
    
    def _create_enhanced_agent_request(self, agent: Dict, query: str, data: Any, 
                                     meta_analysis: Dict, previous_results: Dict) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 25ì— ë”°ë¥¸ í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ë¡œ A2A Agent ìš”ì²­ ìƒì„±
        """
        enhanced_request = {
            'query': query,
            'data': data,
            'context': {
                'user_expertise_level': meta_analysis.get('user_profile', {}).get('expertise', 'intermediate'),
                'domain_context': meta_analysis.get('domain_context', {}),
                'analysis_goals': meta_analysis.get('response_strategy', {}).get('goals', []),
                'previous_agent_results': previous_results,
                'quality_requirements': meta_analysis.get('quality_assessment', {}),
                'collaboration_mode': True,
                'universal_engine_enhanced': True
            }
        }
        
        return enhanced_request
    
    async def _execute_single_agent(self, agent: Dict, request: Dict) -> Dict:
        """
        ê°œë³„ A2A Agent ì‹¤í–‰ (A2A SDK 0.2.9 í‘œì¤€ ì¤€ìˆ˜)
        """
        try:
            # A2A í”„ë¡œí† ì½œë¡œ ì—ì´ì „íŠ¸ í˜¸ì¶œ
            response = await self.a2a_client.send_message(
                agent_url=agent['endpoint'],
                message=request,
                timeout=30.0
            )
            
            return {
                'agent_id': agent['id'],
                'status': 'success',
                'result': response,
                'execution_time': response.get('execution_time', 0),
                'artifacts': response.get('artifacts', []),
                'processed_data': response.get('processed_data'),
                'insights': response.get('insights', [])
            }
            
        except Exception as e:
            logger.error(f"Agent {agent['id']} execution failed: {e}")
            return {
                'agent_id': agent['id'],
                'status': 'failed',
                'error': str(e),
                'fallback_available': True
            }
```

### 2.5 A2A Result Integration System
```python
class A2AResultIntegrator:
    """
    ìš”êµ¬ì‚¬í•­ 22ì— ë”°ë¥¸ A2A Agent ê²°ê³¼ í†µí•© ë° í’ˆì§ˆ ë³´ì¦
    """
    
    def __init__(self):
        self.llm_client = None
        self.conflict_resolver = A2AConflictResolver()
        
    async def integrate_agent_results(self, workflow_results: Dict, meta_analysis: Dict, 
                                    execution_timeline: List) -> Dict:
        """
        ë‹¤ì¤‘ A2A Agent ê²°ê³¼ë¥¼ ì¼ê´€ëœ ì¸ì‚¬ì´íŠ¸ë¡œ í†µí•©
        """
        # 1. ê²°ê³¼ ì¼ê´€ì„± ê²€ì¦
        consistency_check = await self._validate_result_consistency(workflow_results)
        
        # 2. ì¶©ëŒ í•´ê²° (í•„ìš”ì‹œ)
        if consistency_check.get('conflicts'):
            resolved_results = await self.conflict_resolver.resolve_conflicts(
                workflow_results, consistency_check['conflicts']
            )
            workflow_results.update(resolved_results)
        
        # 3. í†µí•© ë¶„ì„ ìˆ˜í–‰
        synthesis_prompt = f"""
        # A2A ì—ì´ì „íŠ¸ ê²°ê³¼ í†µí•© ë¶„ì„
        
        A2A ì—ì´ì „íŠ¸ ì‹¤í–‰ ê²°ê³¼ë“¤ì„ í†µí•© ë¶„ì„í•˜ê² ìŠµë‹ˆë‹¤:
        
        {self._format_agent_results_for_synthesis(workflow_results)}
        
        ì‚¬ìš©ì í”„ë¡œí•„: {meta_analysis.get('user_profile', {})}
        ì›ë³¸ ì¿¼ë¦¬: {meta_analysis.get('original_query', '')}
        ë„ë©”ì¸ ì»¨í…ìŠ¤íŠ¸: {meta_analysis.get('domain_context', {})}
        
        ê° ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬:
        1. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ - ê°€ì¥ ì¤‘ìš”í•œ ë°œê²¬ì‚¬í•­ë“¤
        2. ì‚¬ìš©ì ìˆ˜ì¤€ì— ë§ëŠ” ì„¤ëª… ìƒì„± - ì „ë¬¸ì„±ì— ë§ëŠ” ì–¸ì–´ì™€ ê¹Šì´
        3. ì‹¤í–‰ ê°€ëŠ¥í•œ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ - êµ¬ì²´ì ì´ê³  ì‹¤ìš©ì ì¸ ì¡°ì¹˜
        4. ê²°ê³¼ ê°„ ì¼ê´€ì„± ê²€ì¦ - ëª¨ìˆœë˜ëŠ” ë¶€ë¶„ í•´ê²°
        5. ì—ì´ì „íŠ¸ë³„ ê¸°ì—¬ë„ ëª…ì‹œ - ê° ê²°ê³¼ì˜ ì¶œì²˜ì™€ ì‹ ë¢°ë„
        
        í†µí•©ëœ ë¶„ì„ ê²°ê³¼ë¥¼ ì œì‹œí•˜ì„¸ìš”.
        """
        
        integrated_analysis = await self.llm_client.analyze(synthesis_prompt)
        
        # 4. ìµœì¢… ê²°ê³¼ êµ¬ì¡°í™”
        final_result = {
            'status': 'success',
            'integrated_analysis': integrated_analysis,
            'agent_contributions': self._create_agent_attribution(workflow_results),
            'unified_artifacts': self._merge_agent_artifacts(workflow_results),
            'execution_summary': {
                'total_agents': len(workflow_results),
                'successful_agents': len([r for r in workflow_results.values() if r.get('status') == 'success']),
                'total_execution_time': sum([t['execution_time'] for t in execution_timeline]),
                'timeline': execution_timeline
            },
            'quality_metrics': consistency_check,
            'next_steps': integrated_analysis.get('recommended_actions', [])
        }
        
        return final_result
    
    async def _validate_result_consistency(self, workflow_results: Dict) -> Dict:
        """
        A2A Agent ê²°ê³¼ ê°„ ì¼ê´€ì„± ê²€ì¦
        """
        consistency_prompt = f"""
        # A2A ì—ì´ì „íŠ¸ ê²°ê³¼ ì¼ê´€ì„± ê²€ì¦
        
        ë‹¤ìŒ ì—ì´ì „íŠ¸ ê²°ê³¼ë“¤ì˜ ì¼ê´€ì„±ì„ ê²€ì¦í•´ì£¼ì„¸ìš”:
        
        {json.dumps(workflow_results, indent=2, default=str)}
        
        ê²€ì¦ í•­ëª©:
        1. ë°ì´í„° í•´ì„ì˜ ì¼ê´€ì„± - ê°™ì€ ë°ì´í„°ì— ëŒ€í•œ í•´ì„ì´ ì¼ì¹˜í•˜ëŠ”ê°€?
        2. ìˆ˜ì¹˜ ê²°ê³¼ì˜ ì •í•©ì„± - í†µê³„ê°’, ì¸¡ì •ê°’ë“¤ì´ ë…¼ë¦¬ì ìœ¼ë¡œ ë§ëŠ”ê°€?
        3. ê²°ë¡ ì˜ ë…¼ë¦¬ì  ì—°ê²° - ê° ì—ì´ì „íŠ¸ì˜ ê²°ë¡ ì´ ì„œë¡œ ëª¨ìˆœë˜ì§€ ì•ŠëŠ”ê°€?
        4. í’ˆì§ˆ í‰ê°€ì˜ ì¼ì¹˜ì„± - ë°ì´í„° í’ˆì§ˆì— ëŒ€í•œ í‰ê°€ê°€ ìœ ì‚¬í•œê°€?
        
        ì¶©ëŒì´ ë°œê²¬ë˜ë©´ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œí•˜ê³  í•´ê²° ë°©ì•ˆì„ ì œì‹œí•˜ì„¸ìš”.
        """
        
        return await self.llm_client.analyze(consistency_prompt)
```

### 2.6 A2A Error Handling and Resilience
```python
class A2AErrorHandler:
    """
    ìš”êµ¬ì‚¬í•­ 24ì— ë”°ë¥¸ A2A Agent ì˜¤ë¥˜ ì²˜ë¦¬ ë° ë³µì›ë ¥
    """
    
    def __init__(self):
        self.fallback_strategies = {
            'data_cleaning': self._data_cleaning_fallback,
            'data_loader': self._data_loader_fallback,
            'eda_tools': self._eda_tools_fallback,
            'visualization': self._visualization_fallback,
            'h2o_ml': self._ml_fallback
        }
        
    async def handle_agent_error(self, agent: Dict, error: Exception, workflow_results: Dict) -> Dict:
        """
        ê°œë³„ A2A Agent ì˜¤ë¥˜ ì²˜ë¦¬
        """
        agent_id = agent['id']
        
        # 1. ì˜¤ë¥˜ ë¶„ë¥˜
        error_type = self._classify_error(error)
        
        # 2. ì ì§„ì  ì¬ì‹œë„ (5s â†’ 15s â†’ 30s)
        if error_type in ['timeout', 'connection_error']:
            retry_result = await self._progressive_retry(agent, error)
            if retry_result.get('success'):
                return retry_result
        
        # 3. Fallback ì „ëµ ì‹¤í–‰
        if agent_id in self.fallback_strategies:
            fallback_result = await self.fallback_strategies[agent_id](workflow_results)
            return {
                'agent_id': agent_id,
                'status': 'fallback_success',
                'result': fallback_result,
                'original_error': str(error),
                'fallback_method': f'{agent_id}_fallback'
            }
        
        # 4. ìš°ì•„í•œ ì‹¤íŒ¨ ì²˜ë¦¬
        return {
            'agent_id': agent_id,
            'status': 'graceful_failure',
            'error': str(error),
            'impact_assessment': self._assess_failure_impact(agent_id, workflow_results),
            'alternative_suggestions': self._suggest_alternatives(agent_id)
        }
    
    async def _progressive_retry(self, agent: Dict, error: Exception) -> Dict:
        """
        ì ì§„ì  íƒ€ì„ì•„ì›ƒ ì¬ì‹œë„ ì „ëµ
        """
        timeouts = [5, 15, 30]  # 5ì´ˆ â†’ 15ì´ˆ â†’ 30ì´ˆ
        
        for timeout in timeouts:
            try:
                # íƒ€ì„ì•„ì›ƒì„ ëŠ˜ë ¤ì„œ ì¬ì‹œë„
                response = await self.a2a_client.send_message(
                    agent_url=agent['endpoint'],
                    message=agent['last_request'],
                    timeout=timeout
                )
                
                return {
                    'success': True,
                    'result': response,
                    'retry_timeout': timeout
                }
                
            except Exception as retry_error:
                logger.warning(f"Retry failed for {agent['id']} with timeout {timeout}s: {retry_error}")
                continue
        
        return {'success': False, 'final_error': str(error)}
```

## ğŸ”§ Cherry AI í†µí•© ì„¤ê³„

### 3.1 Cherry AI UI/UX Architecture Integration
```python
class CherryAIUniversalEngineUI:
    """
    ìš”êµ¬ì‚¬í•­ 10ì— ë”°ë¥¸ Cherry AI UI/UX + Universal Engine ì™„ì „ í†µí•©
    ê¸°ì¡´ ChatGPT ìŠ¤íƒ€ì¼ ì¸í„°í˜ì´ìŠ¤ ìœ ì§€í•˜ë©´ì„œ Universal Engine ê¸°ëŠ¥ ê°•í™”
    """
    
    def __init__(self):
        self.universal_engine = UniversalQueryProcessor()
        self.a2a_discovery = A2AAgentDiscoverySystem()
        self.a2a_orchestrator = A2AWorkflowOrchestrator()
        self.session_manager = CherryAISessionManager()
        self.ui_components = CherryAIUIComponents()
        
    def render_enhanced_header(self):
        """
        ìš”êµ¬ì‚¬í•­ 10.1: ê¸°ì¡´ Cherry AI í—¤ë” + Universal Engine ìƒíƒœ í‘œì‹œ
        """
        st.markdown("# ğŸ’ Cherry AI - LLM First Universal Engine")
        
        # Universal Engine ìƒíƒœ í‘œì‹œ
        col1, col2, col3, col4 = st.columns([3, 1, 1, 1])
        
        with col1:
            if hasattr(self, 'universal_engine') and self.universal_engine.is_initialized:
                st.success("ğŸ§  Universal Engine í™œì„±í™”")
            else:
                st.warning("ğŸ§  Universal Engine ì´ˆê¸°í™” ì¤‘...")
        
        with col2:
            agent_count = len(getattr(self, 'available_agents', []))
            if agent_count > 0:
                st.info(f"ğŸ¤– {agent_count}ê°œ A2A ì—ì´ì „íŠ¸")
            else:
                st.warning("ğŸ¤– ì—ì´ì „íŠ¸ ì—°ê²° ì¤‘...")
        
        with col3:
            # ë©”íƒ€ ì¶”ë¡  í† ê¸€
            show_reasoning = st.checkbox("ğŸ§  ì¶”ë¡  ê³¼ì •", value=True, key="show_reasoning")
            st.session_state.show_reasoning = show_reasoning
        
        with col4:
            if st.button("âš™ï¸ ì„¤ì •"):
                st.session_state.show_settings = True
    
    def render_enhanced_chat_interface(self):
        """
        ìš”êµ¬ì‚¬í•­ 10.1: ChatGPT ìŠ¤íƒ€ì¼ ì±„íŒ… + ë©”íƒ€ ì¶”ë¡  ë° A2A í˜‘ì—… í‘œì‹œ
        """
        # ê¸°ì¡´ ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ (ê°•í™”ëœ ë²„ì „)
        for message in st.session_state.get('messages', []):
            with st.chat_message(message["role"]):
                # ë©”ì¸ ë©”ì‹œì§€ ë‚´ìš©
                st.write(message["content"])
                
                # Universal Engine ë©”íƒ€ ì¶”ë¡  ê²°ê³¼ í‘œì‹œ
                if message.get("meta_reasoning") and st.session_state.get('show_reasoning', True):
                    with st.expander("ğŸ§  ë©”íƒ€ ì¶”ë¡  ê³¼ì •", expanded=False):
                        meta = message["meta_reasoning"]
                        
                        # 4ë‹¨ê³„ ì¶”ë¡  ê³¼ì • ì‹œê°í™”
                        tab1, tab2, tab3, tab4 = st.tabs([
                            "1ï¸âƒ£ ì´ˆê¸° ê´€ì°°", "2ï¸âƒ£ ë‹¤ê°ë„ ë¶„ì„", 
                            "3ï¸âƒ£ ìê°€ ê²€ì¦", "4ï¸âƒ£ ì ì‘ì  ì‘ë‹µ"
                        ])
                        
                        with tab1:
                            st.write("**ë°ì´í„° ê´€ì°°:**")
                            st.write(meta.get('initial_analysis', {}).get('observations', ''))
                            st.write("**ì¿¼ë¦¬ ì˜ë„:**")
                            st.write(meta.get('initial_analysis', {}).get('intent', ''))
                        
                        with tab2:
                            st.write("**ì „ë¬¸ê°€ ê´€ì :**")
                            st.write(meta.get('multi_perspective', {}).get('expert_view', ''))
                            st.write("**ì´ˆë³´ì ê´€ì :**")
                            st.write(meta.get('multi_perspective', {}).get('beginner_view', ''))
                        
                        with tab3:
                            st.write("**ë…¼ë¦¬ì  ì¼ê´€ì„±:**")
                            st.write(meta.get('self_verification', {}).get('consistency', ''))
                            st.write("**í™•ì‹  ì—†ëŠ” ë¶€ë¶„:**")
                            st.write(meta.get('self_verification', {}).get('uncertainties', ''))
                        
                        with tab4:
                            st.write("**ì„ íƒëœ ì „ëµ:**")
                            st.write(meta.get('response_strategy', {}).get('strategy', ''))
                            st.write("**ì‚¬ìš©ì ìˆ˜ì¤€:**")
                            st.write(meta.get('response_strategy', {}).get('user_level', ''))
                
                # A2A Agent í˜‘ì—… ê²°ê³¼ í‘œì‹œ
                if message.get("agent_contributions"):
                    with st.expander("ğŸ¤– A2A ì—ì´ì „íŠ¸ í˜‘ì—…", expanded=False):
                        contributions = message["agent_contributions"]
                        
                        # ì—ì´ì „íŠ¸ë³„ ê¸°ì—¬ë„ ì°¨íŠ¸
                        if len(contributions) > 1:
                            import plotly.express as px
                            import pandas as pd
                            
                            agent_data = []
                            for agent_id, contrib in contributions.items():
                                agent_data.append({
                                    'Agent': agent_id,
                                    'Contribution': contrib.get('contribution_score', 0),
                                    'Status': contrib.get('status', 'unknown')
                                })
                            
                            df = pd.DataFrame(agent_data)
                            fig = px.bar(df, x='Agent', y='Contribution', 
                                       color='Status', title="ì—ì´ì „íŠ¸ë³„ ê¸°ì—¬ë„")
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # ê°œë³„ ì—ì´ì „íŠ¸ ê²°ê³¼
                        for agent_id, contrib in contributions.items():
                            with st.container():
                                col1, col2, col3 = st.columns([2, 1, 1])
                                
                                with col1:
                                    st.write(f"**ğŸ¤– {agent_id}**")
                                    st.caption(contrib.get('summary', 'ì‘ì—… ì™„ë£Œ'))
                                
                                with col2:
                                    score = contrib.get('contribution_score', 0)
                                    st.metric("ê¸°ì—¬ë„", f"{score:.1f}%")
                                
                                with col3:
                                    status = contrib.get('status', 'unknown')
                                    status_icons = {
                                        'success': 'âœ…',
                                        'partial': 'âš ï¸', 
                                        'failed': 'âŒ'
                                    }
                                    st.write(f"{status_icons.get(status, 'â“')} {status}")
                                
                                # ìƒì„¸ ê²°ê³¼ ë³´ê¸° ë²„íŠ¼
                                if contrib.get('detailed_result'):
                                    if st.button(f"ìƒì„¸ ë³´ê¸°", key=f"detail_{agent_id}_{message.get('timestamp', '')}"):
                                        st.json(contrib['detailed_result'])
                
                # ì‚¬ìš©ì í”¼ë“œë°± ë° ë§Œì¡±ë„
                if message["role"] == "assistant" and not message.get("feedback_recorded"):
                    st.subheader("ğŸ“ ì´ ì‘ë‹µì´ ë„ì›€ì´ ë˜ì…¨ë‚˜ìš”?")
                    
                    col1, col2, col3, col4, col5 = st.columns(5)
                    feedback_options = [
                        ("ğŸ˜", "ë§¤ìš° ë¶ˆë§Œì¡±", 1),
                        ("ğŸ˜", "ë¶ˆë§Œì¡±", 2),
                        ("ğŸ˜Š", "ë³´í†µ", 3),
                        ("ğŸ˜„", "ë§Œì¡±", 4),
                        ("ğŸ¤©", "ë§¤ìš° ë§Œì¡±", 5)
                    ]
                    
                    for i, (emoji, label, score) in enumerate(feedback_options):
                        with [col1, col2, col3, col4, col5][i]:
                            if st.button(f"{emoji}", key=f"feedback_{score}_{message.get('timestamp', '')}"):
                                # í”¼ë“œë°± ê¸°ë¡
                                await self.record_user_feedback(message, score)
                                message["feedback_recorded"] = True
                                st.success(f"{label} í”¼ë“œë°±ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                st.rerun()
    
    def render_enhanced_file_upload(self):
        """
        ìš”êµ¬ì‚¬í•­ 10.1: íŒŒì¼ ì—…ë¡œë“œ + Universal Engine ìë™ ë„ë©”ì¸ ê°ì§€
        """
        st.subheader("ğŸ“ ë°ì´í„° íŒŒì¼ ì—…ë¡œë“œ")
        
        uploaded_file = st.file_uploader(
            "íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”", 
            type=['csv', 'xlsx', 'json', 'txt', 'parquet'],
            help="Universal Engineì´ ìë™ìœ¼ë¡œ ë°ì´í„° ìœ í˜•ê³¼ ë„ë©”ì¸ì„ ê°ì§€í•©ë‹ˆë‹¤"
        )
        
        if uploaded_file and uploaded_file != st.session_state.get('last_uploaded_file'):
            st.session_state.last_uploaded_file = uploaded_file
            
            # Universal Engineìœ¼ë¡œ ë°ì´í„° ì‚¬ì „ ë¶„ì„
            with st.spinner("ğŸ§  Universal Engineì´ ë°ì´í„° ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ì¤‘..."):
                try:
                    # ë°ì´í„° ë¡œë“œ
                    if uploaded_file.name.endswith('.csv'):
                        data = pd.read_csv(uploaded_file)
                    elif uploaded_file.name.endswith('.xlsx'):
                        data = pd.read_excel(uploaded_file)
                    elif uploaded_file.name.endswith('.json'):
                        data = pd.read_json(uploaded_file)
                    
                    st.session_state.current_data = data
                    
                    # Universal Engineìœ¼ë¡œ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
                    context_analysis = await self.universal_engine.analyze_data_context(data)
                    
                    # ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                    if context_analysis:
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            detected_domain = context_analysis.get('domain', 'ì•Œ ìˆ˜ ì—†ìŒ')
                            confidence = context_analysis.get('confidence', 0)
                            
                            if confidence > 0.7:
                                st.success(f"âœ… **{detected_domain}** ë„ë©”ì¸ìœ¼ë¡œ ê°ì§€ë¨ (ì‹ ë¢°ë„: {confidence:.1%})")
                            else:
                                st.info(f"ğŸ¤” **{detected_domain}** ë„ë©”ì¸ìœ¼ë¡œ ì¶”ì •ë¨ (ì‹ ë¢°ë„: {confidence:.1%})")
                        
                        with col2:
                            data_quality = context_analysis.get('data_quality', {})
                            quality_score = data_quality.get('overall_score', 0)
                            
                            if quality_score > 0.8:
                                st.success(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ: ìš°ìˆ˜ ({quality_score:.1%})")
                            elif quality_score > 0.6:
                                st.warning(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ: ë³´í†µ ({quality_score:.1%})")
                            else:
                                st.error(f"ğŸ“Š ë°ì´í„° í’ˆì§ˆ: ê°œì„  í•„ìš” ({quality_score:.1%})")
                        
                        # ì¶”ì²œ ë¶„ì„ í‘œì‹œ
                        recommended_analyses = context_analysis.get('recommended_analyses', [])
                        if recommended_analyses:
                            st.info("ğŸ’¡ **ì¶”ì²œ ë¶„ì„:**")
                            
                            cols = st.columns(min(3, len(recommended_analyses)))
                            for i, analysis in enumerate(recommended_analyses[:3]):
                                with cols[i]:
                                    if st.button(f"ğŸ¯ {analysis['title']}", key=f"rec_analysis_{i}"):
                                        # ì¶”ì²œ ë¶„ì„ì„ ìƒˆë¡œìš´ ë©”ì‹œì§€ë¡œ ì¶”ê°€
                                        st.session_state.messages.append({
                                            'role': 'user',
                                            'content': analysis['query'],
                                            'timestamp': datetime.now(),
                                            'source': 'recommended_analysis'
                                        })
                                        st.rerun()
                        
                        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
                        with st.expander("ğŸ‘€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                            st.dataframe(data.head(10))
                            
                            col1, col2, col3 = st.columns(3)
                            with col1:
                                st.metric("í–‰ ìˆ˜", len(data))
                            with col2:
                                st.metric("ì—´ ìˆ˜", len(data.columns))
                            with col3:
                                missing_pct = (data.isnull().sum().sum() / (len(data) * len(data.columns))) * 100
                                st.metric("ê²°ì¸¡ê°’", f"{missing_pct:.1f}%")
                
                except Exception as e:
                    st.error(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.info("ğŸ’¡ ë‹¤ë¥¸ í˜•ì‹ì˜ íŒŒì¼ì„ ì‹œë„í•˜ê±°ë‚˜ ë°ì´í„° í˜•ì‹ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
    
    def render_enhanced_sidebar(self):
        """
        ìš”êµ¬ì‚¬í•­ 10.1: ì‚¬ì´ë“œë°” + Universal Engine ì œì–´íŒ
        """
        with st.sidebar:
            st.header("ğŸ”§ Universal Engine ì œì–´")
            
            # 1. ë©”íƒ€ ì¶”ë¡  ì„¤ì •
            st.subheader("ğŸ§  ë©”íƒ€ ì¶”ë¡  ì„¤ì •")
            
            show_reasoning = st.checkbox(
                "ì¶”ë¡  ê³¼ì • í‘œì‹œ", 
                value=st.session_state.get('show_reasoning', True),
                help="DeepSeek-R1 ê¸°ë°˜ 4ë‹¨ê³„ ë©”íƒ€ ì¶”ë¡  ê³¼ì •ì„ í‘œì‹œí•©ë‹ˆë‹¤"
            )
            st.session_state.show_reasoning = show_reasoning
            
            reasoning_depth = st.selectbox(
                "ì¶”ë¡  ê¹Šì´", 
                ["ê¸°ë³¸", "ìƒì„¸", "ì „ë¬¸ê°€"],
                index=0,
                help="ë©”íƒ€ ì¶”ë¡ ì˜ ê¹Šì´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤"
            )
            st.session_state.reasoning_depth = reasoning_depth
            
            # 2. A2A Agent ìƒíƒœ ë° ì œì–´
            st.subheader("ğŸ¤– A2A ì—ì´ì „íŠ¸ ìƒíƒœ")
            
            if hasattr(self, 'available_agents'):
                for agent in self.available_agents:
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        status_icon = "ğŸŸ¢" if agent.get('status') == "active" else "ğŸ”´"
                        st.write(f"{status_icon} **{agent['name']}**")
                        st.caption(f"í¬íŠ¸: {agent['port']}")
                    
                    with col2:
                        if agent.get('status') != "active":
                            if st.button("ğŸ”„", key=f"restart_{agent['id']}"):
                                # ì—ì´ì „íŠ¸ ì¬ì‹œì‘ ì‹œë„
                                await self.restart_agent(agent)
                
                # ì—ì´ì „íŠ¸ ìë™ ë°œê²¬
                if st.button("ğŸ” ì—ì´ì „íŠ¸ ì¬ê²€ìƒ‰"):
                    with st.spinner("A2A ì—ì´ì „íŠ¸ ê²€ìƒ‰ ì¤‘..."):
                        self.available_agents = await self.a2a_discovery.discover_available_agents()
                    st.success("ì—ì´ì „íŠ¸ ê²€ìƒ‰ ì™„ë£Œ!")
                    st.rerun()
            else:
                st.info("ì—ì´ì „íŠ¸ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
            
            # 3. ì‚¬ìš©ì í”„ë¡œí•„ ì„¤ì •
            st.subheader("ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„")
            
            expertise_level = st.selectbox(
                "ì „ë¬¸ì„± ìˆ˜ì¤€", 
                ["ìë™ ê°ì§€", "ì´ˆë³´ì", "ì¤‘ê¸‰ì", "ì „ë¬¸ê°€"],
                index=0,
                help="Universal Engineì´ ìë™ìœ¼ë¡œ ê°ì§€í•˜ê±°ë‚˜ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤"
            )
            
            if expertise_level != "ìë™ ê°ì§€":
                level_mapping = {
                    "ì´ˆë³´ì": "beginner",
                    "ì¤‘ê¸‰ì": "intermediate",
                    "ì „ë¬¸ê°€": "expert"
                }
                st.session_state.user_expertise = level_mapping[expertise_level]
            
            explanation_style = st.selectbox(
                "ì„¤ëª… ìŠ¤íƒ€ì¼",
                ["ì ì‘í˜•", "ì¹œê·¼í•œ ì„¤ëª…", "ê¸°ìˆ ì  ì„¤ëª…", "ë‹¨ê³„ë³„ ê°€ì´ë“œ"],
                index=0
            )
            st.session_state.explanation_style = explanation_style
            
            # 4. Universal Engine í†µê³„ ë° ì„±ëŠ¥
            st.subheader("ğŸ“Š ì—”ì§„ í†µê³„")
            
            # ì„¸ì…˜ í†µê³„
            total_analyses = st.session_state.get('total_analyses', 0)
            avg_response_time = st.session_state.get('avg_response_time', 0)
            satisfaction_scores = st.session_state.get('satisfaction_scores', [])
            avg_satisfaction = sum(satisfaction_scores) / len(satisfaction_scores) if satisfaction_scores else 0
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ì´ ë¶„ì„ ìˆ˜í–‰", total_analyses)
                st.metric("í‰ê·  ì‘ë‹µ ì‹œê°„", f"{avg_response_time:.1f}ì´ˆ")
            
            with col2:
                st.metric("ì‚¬ìš©ì ë§Œì¡±ë„", f"{avg_satisfaction:.1f}/5.0")
                if satisfaction_scores:
                    recent_trend = "ğŸ“ˆ" if len(satisfaction_scores) > 1 and satisfaction_scores[-1] > satisfaction_scores[-2] else "ğŸ“‰"
                    st.caption(f"ìµœê·¼ ì¶”ì„¸: {recent_trend}")
            
            # 5. ê³ ê¸‰ ì„¤ì •
            with st.expander("âš™ï¸ ê³ ê¸‰ ì„¤ì •"):
                
                # ìºì‹œ ê´€ë¦¬
                st.write("**ìºì‹œ ê´€ë¦¬**")
                col1, col2 = st.columns(2)
                with col1:
                    if st.button("ğŸ—‘ï¸ ìºì‹œ ì§€ìš°ê¸°"):
                        st.cache_data.clear()
                        st.success("ìºì‹œê°€ ì§€ì›Œì¡ŒìŠµë‹ˆë‹¤!")
                
                with col2:
                    if st.button("ğŸ”„ ì„¸ì…˜ ì´ˆê¸°í™”"):
                        # ì¤‘ìš”í•œ ì„¤ì •ë§Œ ìœ ì§€í•˜ê³  ì„¸ì…˜ ì´ˆê¸°í™”
                        important_keys = ['user_expertise', 'show_reasoning', 'explanation_style']
                        preserved = {k: st.session_state.get(k) for k in important_keys}
                        
                        st.session_state.clear()
                        st.session_state.update(preserved)
                        st.success("ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()
                
                # ë””ë²„ê·¸ ëª¨ë“œ
                debug_mode = st.checkbox("ğŸ› ë””ë²„ê·¸ ëª¨ë“œ", value=False)
                if debug_mode:
                    st.json(dict(st.session_state))
                
                # ì‹¤í—˜ì  ê¸°ëŠ¥
                st.write("**ì‹¤í—˜ì  ê¸°ëŠ¥**")
                enable_advanced_reasoning = st.checkbox("ğŸ§  ê³ ê¸‰ ì¶”ë¡  ëª¨ë“œ", value=False)
                enable_multi_agent_parallel = st.checkbox("âš¡ ë³‘ë ¬ ì—ì´ì „íŠ¸ ì‹¤í–‰", value=True)
                
                st.session_state.enable_advanced_reasoning = enable_advanced_reasoning
                st.session_state.enable_multi_agent_parallel = enable_multi_agent_parallel
```

### 3.2 Cherry AI Integration Layer
```python
class CherryAIUniversalA2AIntegration:
    """
    ìš”êµ¬ì‚¬í•­ 10ì— ë”°ë¥¸ Cherry AI + Universal Engine + A2A Agent ì™„ì „ í†µí•©
    """
    
    def __init__(self):
        self.universal_engine = UniversalQueryProcessor()
        self.a2a_discovery = A2AAgentDiscoverySystem()
        self.a2a_orchestrator = A2AWorkflowOrchestrator()
        self.session_manager = CherryAISessionManager()
        self.ui_integration = CherryAIUniversalEngineUI()
        self.error_handler = CherryAIErrorHandler()
        
    async def replace_hardcoded_analysis(self, user_query: str, session_state: Dict) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 10.2: ê¸°ì¡´ cherry_ai.pyì˜ í•˜ë“œì½”ë”©ëœ ë¶„ì„ ë¡œì§ì„ Universal Engine + A2Aë¡œ ì™„ì „ ëŒ€ì²´
        
        # âŒ ì œê±°ë˜ëŠ” ê¸°ì¡´ í•˜ë“œì½”ë”© íŒ¨í„´:
        # if SEMICONDUCTOR_ENGINE_AVAILABLE:
        #     semiconductor_result = await analyze_semiconductor_data(...)
        #     if confidence > 0.7:
        #         return self._format_semiconductor_analysis(semiconductor_result)
        # return await self._general_agent_analysis(user_query)
        """
        
        try:
            # 1. í¬ê´„ì  ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            context = await self.session_manager.extract_comprehensive_context(session_state)
            
            # 2. A2A Agent ìë™ ë°œê²¬ ë° ìƒíƒœ í™•ì¸
            available_agents = await self.a2a_discovery.discover_available_agents()
            
            # 3. Universal Engine ë©”íƒ€ ì¶”ë¡  ìˆ˜í–‰
            meta_analysis = await self.universal_engine.perform_meta_reasoning(
                query=user_query,
                data=session_state.get('current_data'),
                user_context=context.get('user_profile', {}),
                conversation_history=context.get('conversation_history', [])
            )
            
            # 4. A2A Agent ë™ì  ì„ íƒ ë° í˜‘ì—…
            unified_result = await self.process_unified_query_with_progress(
                query=user_query,
                meta_analysis=meta_analysis,
                available_agents=available_agents,
                context=context
            )
            
            # 5. ì‚¬ìš©ì ìˆ˜ì¤€ë³„ ì ì‘ì  ì‘ë‹µ ìƒì„±
            adaptive_response = await self.universal_engine.generate_adaptive_response(
                analysis_result=unified_result,
                user_profile=context.get('user_profile', {}),
                meta_reasoning=meta_analysis
            )
            
            # 6. Cherry AI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            cherry_compatible_result = await self.convert_to_cherry_format(
                adaptive_response, meta_analysis, unified_result
            )
            
            # 7. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            await self.update_session_state(session_state, cherry_compatible_result)
            
            return cherry_compatible_result
            
        except Exception as e:
            # ìš”êµ¬ì‚¬í•­ 10.9: ì‚¬ìš©ì ì¹œí™”ì  ì˜¤ë¥˜ ì²˜ë¦¬
            return await self.error_handler.handle_analysis_error(e, user_query, session_state)
    
    async def process_unified_query_with_progress(self, query: str, meta_analysis: Dict, 
                                                available_agents: Dict, context: Dict) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 10.3: ì‹¤ì‹œê°„ ë¶„ì„ ì§„í–‰ ìƒí™© í‘œì‹œì™€ í•¨ê»˜ í†µí•© ì¿¼ë¦¬ ì²˜ë¦¬
        """
        
        # Streamlit ì§„í–‰ ìƒí™© í‘œì‹œ ì»¨í…Œì´ë„ˆ
        progress_container = st.container()
        
        with progress_container:
            # 1ë‹¨ê³„: ë©”íƒ€ ì¶”ë¡  (ì´ë¯¸ ì™„ë£Œë¨)
            st.success("âœ… ğŸ§  ë©”íƒ€ ì¶”ë¡  ì™„ë£Œ")
            if st.session_state.get('show_reasoning', True):
                with st.expander("ğŸ§  ë©”íƒ€ ì¶”ë¡  ê²°ê³¼", expanded=False):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write(f"**ê°ì§€ëœ ë„ë©”ì¸**: {meta_analysis.get('domain_context', {}).get('domain', 'ë¶„ì„ ì¤‘')}")
                        st.write(f"**ì‚¬ìš©ì ìˆ˜ì¤€**: {meta_analysis.get('user_profile', {}).get('expertise', 'ì¶”ì • ì¤‘')}")
                    with col2:
                        st.write(f"**ë¶„ì„ ì „ëµ**: {meta_analysis.get('response_strategy', {}).get('strategy', 'ìˆ˜ë¦½ ì¤‘')}")
                        st.write(f"**ì‹ ë¢°ë„**: {meta_analysis.get('confidence_level', 0):.1%}")
            
            # 2ë‹¨ê³„: A2A ì—ì´ì „íŠ¸ ì„ íƒ
            with st.spinner("ğŸ¤– ìµœì  A2A ì—ì´ì „íŠ¸ ì„ íƒ ì¤‘..."):
                selected_agents = await self.a2a_discovery.select_optimal_agents(
                    meta_analysis, available_agents
                )
            
            if selected_agents:
                st.success(f"âœ… {len(selected_agents)}ê°œ ì—ì´ì „íŠ¸ ì„ íƒë¨")
                
                # ì„ íƒëœ ì—ì´ì „íŠ¸ í‘œì‹œ
                cols = st.columns(min(len(selected_agents), 4))
                for i, agent in enumerate(selected_agents[:4]):
                    with cols[i]:
                        st.write(f"ğŸ¤– **{agent['name']}**")
                        st.caption(f"í¬íŠ¸: {agent['port']}")
                        st.caption(f"ì—­í• : {agent.get('role', 'ë¶„ì„')}")
                
                if len(selected_agents) > 4:
                    st.caption(f"... ë° {len(selected_agents) - 4}ê°œ ì¶”ê°€ ì—ì´ì „íŠ¸")
            
            # 3ë‹¨ê³„: ì—ì´ì „íŠ¸ í˜‘ì—… ì‹¤í–‰
            with st.spinner("âš¡ A2A ì—ì´ì „íŠ¸ í˜‘ì—… ì‹¤í–‰ ì¤‘..."):
                
                # ì‹¤ì‹œê°„ ì§„í–‰ë¥  í‘œì‹œ
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # ì—ì´ì „íŠ¸ ìƒíƒœ ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸
                if st.session_state.get('show_agent_details', True):
                    agent_status_container = st.container()
                
                # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                workflow_results = {}
                total_agents = len(selected_agents)
                
                for i, agent in enumerate(selected_agents):
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    progress = (i / total_agents) * 100
                    progress_bar.progress(int(progress))
                    status_text.text(f"ì§„í–‰ ì¤‘: {agent['name']} ì‹¤í–‰ ì¤‘...")
                    
                    # ì—ì´ì „íŠ¸ ìƒíƒœ í‘œì‹œ
                    if st.session_state.get('show_agent_details', True):
                        with agent_status_container:
                            self.render_agent_status_update(selected_agents, i)
                    
                    # ì—ì´ì „íŠ¸ ì‹¤í–‰
                    try:
                        agent_result = await self.a2a_orchestrator.execute_single_agent(
                            agent, query, context.get('current_data'), meta_analysis
                        )
                        workflow_results[agent['id']] = agent_result
                        
                    except Exception as e:
                        # ì—ì´ì „íŠ¸ ì˜¤ë¥˜ ì²˜ë¦¬
                        error_result = await self.error_handler.handle_agent_error(agent, e)
                        workflow_results[agent['id']] = error_result
                
                # ì™„ë£Œ
                progress_bar.progress(100)
                status_text.text("âœ… ëª¨ë“  ì—ì´ì „íŠ¸ ì‹¤í–‰ ì™„ë£Œ")
            
            # 4ë‹¨ê³„: ê²°ê³¼ í†µí•©
            with st.spinner("ğŸ”„ ê²°ê³¼ í†µí•© ë° ì‘ë‹µ ìƒì„± ì¤‘..."):
                integrated_result = await self.a2a_orchestrator.integrate_agent_results(
                    workflow_results, meta_analysis
                )
            
            st.success("âœ… ë¶„ì„ ì™„ë£Œ!")
            
            return integrated_result
    
    def render_agent_status_update(self, selected_agents: List, current_index: int):
        """
        ìš”êµ¬ì‚¬í•­ 10.4: A2A Agent í˜‘ì—… ìƒíƒœ ì‹¤ì‹œê°„ ì‹œê°í™”
        """
        with st.expander("ğŸ¤– A2A ì—ì´ì „íŠ¸ ì‹¤í–‰ ìƒíƒœ", expanded=True):
            for i, agent in enumerate(selected_agents):
                col1, col2, col3 = st.columns([2, 1, 1])
                
                with col1:
                    st.write(f"**{agent['name']}**")
                    st.caption(f"í¬íŠ¸: {agent['port']}")
                
                with col2:
                    if i < current_index:
                        st.write("âœ… ì™„ë£Œ")
                    elif i == current_index:
                        st.write("ğŸŸ¡ ì‹¤í–‰ì¤‘")
                    else:
                        st.write("â³ ëŒ€ê¸°ì¤‘")
                
                with col3:
                    if i < current_index:
                        st.write("100%")
                    elif i == current_index:
                        # ì‹¤ì œ ì§„í–‰ë¥ ì€ ì—ì´ì „íŠ¸ì—ì„œ ë°›ì•„ì™€ì•¼ í•¨
                        st.write("ì§„í–‰ì¤‘...")
                    else:
                        st.write("0%")
    
    async def convert_to_cherry_format(self, adaptive_response: Dict, 
                                     meta_analysis: Dict, unified_result: Dict) -> Dict:
        """
        ìš”êµ¬ì‚¬í•­ 10.7: Universal Engine ê²°ê³¼ë¥¼ Cherry AI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        """
        return {
            'status': 'success',
            'content': adaptive_response.get('main_response', ''),
            'meta_reasoning': {
                'domain_context': meta_analysis.get('domain_context', {}),
                'user_profile': meta_analysis.get('user_profile', {}),
                'confidence_level': meta_analysis.get('confidence_level', 0),
                'reasoning_steps': meta_analysis.get('reasoning_steps', []),
                'quality_assessment': meta_analysis.get('quality_assessment', {})
            },
            'agent_contributions': unified_result.get('agent_contributions', {}),
            'suggested_followups': adaptive_response.get('suggested_questions', []),
            'artifacts': unified_result.get('unified_artifacts', []),
            'performance_metrics': {
                'total_execution_time': unified_result.get('execution_summary', {}).get('total_execution_time', 0),
                'agents_used': unified_result.get('execution_summary', {}).get('total_agents', 0),
                'success_rate': unified_result.get('execution_summary', {}).get('success_rate', 1.0)
            },
            'timestamp': datetime.now(),
            'source': 'universal_engine_a2a'
        }
    
    async def update_session_state(self, session_state: Dict, result: Dict):
        """
        ìš”êµ¬ì‚¬í•­ 10.6: ì„¸ì…˜ ìƒíƒœ ë° ëŒ€í™” íë¦„ ìœ ì§€
        """
        # ë¶„ì„ í†µê³„ ì—…ë°ì´íŠ¸
        session_state['total_analyses'] = session_state.get('total_analyses', 0) + 1
        
        # ì‘ë‹µ ì‹œê°„ ê¸°ë¡
        response_times = session_state.get('response_times', [])
        response_times.append(result['performance_metrics']['total_execution_time'])
        session_state['response_times'] = response_times[-50:]  # ìµœê·¼ 50ê°œë§Œ ìœ ì§€
        session_state['avg_response_time'] = sum(response_times) / len(response_times)
        
        # ì‚¬ìš©ì í”„ë¡œí•„ ì—…ë°ì´íŠ¸
        if result.get('meta_reasoning', {}).get('user_profile'):
            detected_profile = result['meta_reasoning']['user_profile']
            current_profile = session_state.get('user_profile', {})
            
            # ì ì§„ì  í”„ë¡œí•„ ì—…ë°ì´íŠ¸ (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€)
            for key, value in detected_profile.items():
                if key in current_profile:
                    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ ì—…ë°ì´íŠ¸
                    if isinstance(value, (int, float)):
                        current_profile[key] = 0.7 * current_profile[key] + 0.3 * value
                    else:
                        current_profile[key] = value
                else:
                    current_profile[key] = value
            
            session_state['user_profile'] = current_profile
        
        # ì—ì´ì „íŠ¸ ì„±ëŠ¥ ê¸°ë¡
        if result.get('agent_contributions'):
            agent_performance = session_state.get('agent_performance', {})
            for agent_id, contribution in result['agent_contributions'].items():
                if agent_id not in agent_performance:
                    agent_performance[agent_id] = []
                
                agent_performance[agent_id].append({
                    'timestamp': datetime.now(),
                    'contribution_score': contribution.get('contribution_score', 0),
                    'execution_time': contribution.get('execution_time', 0),
                    'status': contribution.get('status', 'unknown')
                })
                
                # ìµœê·¼ 20ê°œ ê¸°ë¡ë§Œ ìœ ì§€
                agent_performance[agent_id] = agent_performance[agent_id][-20:]
            
            session_state['agent_performance'] = agent_performance
```

### 3.3 System Initialization and Health Monitoring
```python
class CherryAISystemInitializer:
    """
    ìš”êµ¬ì‚¬í•­ 10.10: ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ìƒíƒœ ëª¨ë‹ˆí„°ë§
    """
    
    async def initialize_complete_system(self):
        """
        Universal Engine + A2A + Cherry AI í†µí•© ì‹œìŠ¤í…œ ì™„ì „ ì´ˆê¸°í™”
        """
        initialization_steps = [
            ("ğŸ§  Universal Engine ì´ˆê¸°í™”", self._init_universal_engine, 20),
            ("ğŸ¤– A2A ì—ì´ì „íŠ¸ ë°œê²¬", self._discover_agents, 40),
            ("ğŸ”— í†µí•© ì‹œìŠ¤í…œ êµ¬ì„±", self._setup_integration, 60),
            ("âœ… ìƒíƒœ ê²€ì¦", self._validate_system_health, 80),
            ("ğŸ¨ UI ì»´í¬ë„ŒíŠ¸ ì¤€ë¹„", self._prepare_ui_components, 100)
        ]
        
        # ì´ˆê¸°í™” ì§„í–‰ ìƒíƒœ í‘œì‹œ
        progress_container = st.container()
        
        with progress_container:
            st.info("ğŸ’ Cherry AI Universal Engine ì´ˆê¸°í™” ì¤‘...")
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            initialization_results = {}
            
            for step_name, step_func, progress_target in initialization_steps:
                status_text.text(step_name)
                
                try:
                    step_result = await step_func()
                    initialization_results[step_name] = {
                        'status': 'success',
                        'result': step_result
                    }
                    progress_bar.progress(progress_target)
                    
                except Exception as e:
                    initialization_results[step_name] = {
                        'status': 'failed',
                        'error': str(e)
                    }
                    st.warning(f"âš ï¸ {step_name} ì‹¤íŒ¨: {str(e)}")
            
            # ì´ˆê¸°í™” ê²°ê³¼ í‰ê°€
            successful_steps = sum(1 for result in initialization_results.values() 
                                 if result['status'] == 'success')
            total_steps = len(initialization_steps)
            
            # ìµœì¢… ìƒíƒœ í‘œì‹œ
            status_text.empty()
            progress_bar.empty()
            
            if successful_steps == total_steps:
                st.success("âœ… Cherry AI Universal Engine ì´ˆê¸°í™” ì™„ë£Œ!")
                
                # ì‹œìŠ¤í…œ ìƒíƒœ ìš”ì•½
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("Universal Engine", "ğŸŸ¢ í™œì„±í™”")
                
                with col2:
                    agent_count = len(initialization_results.get("ğŸ¤– A2A ì—ì´ì „íŠ¸ ë°œê²¬", {}).get('result', []))
                    st.metric("A2A ì—ì´ì „íŠ¸", f"{agent_count}ê°œ ì—°ê²°")
                
                with col3:
                    st.metric("ì‹œìŠ¤í…œ ìƒíƒœ", "ğŸŸ¢ ì •ìƒ")
                
                # í™˜ì˜ ë©”ì‹œì§€ í‘œì‹œ
                if not st.session_state.get('welcome_shown', False):
                    self.show_welcome_message()
                    st.session_state.welcome_shown = True
                
                return True
                
            else:
                st.warning(f"âš ï¸ ë¶€ë¶„ ì´ˆê¸°í™” ì™„ë£Œ ({successful_steps}/{total_steps})")
                
                # ì‹¤íŒ¨í•œ ë‹¨ê³„ë“¤ í‘œì‹œ
                failed_steps = [name for name, result in initialization_results.items() 
                              if result['status'] == 'failed']
                
                if failed_steps:
                    with st.expander("âŒ ì‹¤íŒ¨í•œ ì´ˆê¸°í™” ë‹¨ê³„", expanded=True):
                        for step in failed_steps:
                            st.error(f"â€¢ {step}: {initialization_results[step]['error']}")
                
                # ë³µêµ¬ ì˜µì…˜ ì œê³µ
                self.show_recovery_options(initialization_results)
                
                return False
    
    def show_welcome_message(self):
        """
        ìš”êµ¬ì‚¬í•­ 10.10: ì‚¬ìš©ì í™˜ì˜ ë©”ì‹œì§€ ë° ì‹œì‘ ê°€ì´ë“œ
        """
        st.balloons()  # ì¶•í•˜ íš¨ê³¼
        
        st.success("ğŸ‰ Cherry AI Universal Engineì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!")
        
        with st.expander("ğŸš€ ì‹œì‘í•˜ê¸° ê°€ì´ë“œ", expanded=True):
            st.markdown("""
            ### ğŸ’ Cherry AI Universal Engine íŠ¹ì§•
            
            - **ğŸ§  LLM First ì ‘ê·¼**: í•˜ë“œì½”ë”© ì—†ëŠ” ì§„ì •í•œ ì§€ëŠ¥í˜• ë¶„ì„
            - **ğŸ¤– A2A Agent í†µí•©**: 10ê°œ ì „ë¬¸ ì—ì´ì „íŠ¸ì™€ ìë™ í˜‘ì—…  
            - **ğŸ¯ ì‚¬ìš©ì ì ì‘**: ì´ˆë³´ìë¶€í„° ì „ë¬¸ê°€ê¹Œì§€ ìë™ ìˆ˜ì¤€ ì¡°ì ˆ
            - **ğŸ” ë©”íƒ€ ì¶”ë¡ **: DeepSeek-R1 ê¸°ë°˜ 4ë‹¨ê³„ ê³ ê¸‰ ì¶”ë¡ 
            - **ğŸ“Š ë²”ìš© ë¶„ì„**: ëª¨ë“  ë„ë©”ì¸, ëª¨ë“  ë°ì´í„° ìœ í˜• ì§€ì›
            
            ### ğŸ“ ì‚¬ìš© ë°©ë²•
            
            1. **ğŸ“ ë°ì´í„° ì—…ë¡œë“œ**: ì‚¬ì´ë“œë°”ì—ì„œ íŒŒì¼ ì—…ë¡œë“œ â†’ ìë™ ë„ë©”ì¸ ê°ì§€
            2. **ğŸ’¬ ìì—°ì–´ ì§ˆë¬¸**: "ì´ ë°ì´í„°ê°€ ë­˜ ë§í•˜ëŠ”ì§€ ëª¨ë¥´ê² ì–´ìš”" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸
            3. **ğŸ” ì ì§„ì  íƒìƒ‰**: ì‹œìŠ¤í…œì´ ì œì•ˆí•˜ëŠ” í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ê¹Šì´ ìˆëŠ” ë¶„ì„
            4. **âš™ï¸ ê°œì¸í™”**: ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ë‹¹ì‹ ì˜ ìˆ˜ì¤€ì— ë§ì¶° ì„¤ëª… ì¡°ì ˆ
            
            ### ğŸ’¡ ì‹œì‘ ì˜ˆì‹œ
            """)
            
            # ì˜ˆì‹œ ì§ˆë¬¸ ë²„íŠ¼ë“¤
            example_questions = [
                "ì´ ë°ì´í„° íŒŒì¼ì´ ë­˜ ë§í•˜ëŠ”ì§€ ì „í˜€ ëª¨ë¥´ê² ì–´ìš”. ë„ì›€ ì£¼ì„¸ìš”.",
                "ë°ì´í„°ì—ì„œ ì´ìƒí•œ íŒ¨í„´ì´ë‚˜ ë¬¸ì œì ì„ ì°¾ì•„ì£¼ì„¸ìš”.",
                "ì´ ê²°ê³¼ë¥¼ ì–´ë–»ê²Œ í•´ì„í•˜ê³  ë‹¤ìŒì— ë­˜ í•´ì•¼ í•˜ë‚˜ìš”?",
                "ì „ë¬¸ê°€ ìˆ˜ì¤€ì˜ ìƒì„¸í•œ í†µê³„ ë¶„ì„ì„ ì›í•©ë‹ˆë‹¤."
            ]
            
            st.write("**ğŸ¯ ì˜ˆì‹œ ì§ˆë¬¸ (í´ë¦­í•˜ë©´ ë°”ë¡œ ì‹œì‘):**")
            
            cols = st.columns(2)
            for i, question in enumerate(example_questions):
                with cols[i % 2]:
                    if st.button(f"ğŸ’¡ {question}", key=f"example_{i}"):
                        st.session_state.messages.append({
                            'role': 'user',
                            'content': question,
                            'timestamp': datetime.now(),
                            'source': 'welcome_example'
                        })
                        st.rerun()
    
    def show_recovery_options(self, initialization_results: Dict):
        """
        ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ ë³µêµ¬ ì˜µì…˜ ì œê³µ
        """
        st.subheader("ğŸ”„ ë³µêµ¬ ì˜µì…˜")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ”„ ì „ì²´ ì¬ì‹œë„"):
                st.rerun()
        
        with col2:
            if st.button("ğŸ§  Universal Engineë§Œ ì‚¬ìš©"):
                # A2A ì—†ì´ Universal Engineë§Œ ì´ˆê¸°í™”
                st.session_state.fallback_mode = 'universal_only'
                st.info("âœ… Universal Engine ê¸°ë³¸ ëª¨ë“œë¡œ ì‹œì‘ë¨")
        
        with col3:
            if st.button("ğŸ“ ì§€ì› ìš”ì²­"):
                st.session_state.show_support_form = True
                
        # ì§€ì› ìš”ì²­ í¼
        if st.session_state.get('show_support_form'):
            with st.form("support_form"):
                st.subheader("ğŸ“ ê¸°ìˆ  ì§€ì› ìš”ì²­")
                
                issue_description = st.text_area(
                    "ë¬¸ì œ ìƒí™©ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”:",
                    placeholder="ì´ˆê¸°í™” ì¤‘ ë°œìƒí•œ ë¬¸ì œë‚˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë¥¼ ìì„¸íˆ ì ì–´ì£¼ì„¸ìš”."
                )
                
                user_email = st.text_input(
                    "ì—°ë½ì²˜ ì´ë©”ì¼ (ì„ íƒì‚¬í•­):",
                    placeholder="ë‹µë³€ì„ ë°›ì„ ì´ë©”ì¼ ì£¼ì†Œ"
                )
                
                if st.form_submit_button("ğŸ“§ ì§€ì› ìš”ì²­ ë³´ë‚´ê¸°"):
                    # ì§€ì› ìš”ì²­ ì²˜ë¦¬ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì´ë©”ì¼ ë°œì†¡ ë“±)
                    support_data = {
                        'timestamp': datetime.now(),
                        'issue_description': issue_description,
                        'user_email': user_email,
                        'initialization_results': initialization_results,
                        'system_info': self._collect_system_info()
                    }
                    
                    # ë¡œê·¸ íŒŒì¼ì— ì €ì¥ ë˜ëŠ” ì™¸ë¶€ ì‹œìŠ¤í…œìœ¼ë¡œ ì „ì†¡
                    await self._send_support_request(support_data)
                    
                    st.success("âœ… ì§€ì› ìš”ì²­ì´ ì „ì†¡ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.session_state.show_support_form = False
                    st.rerun()
```

## ğŸš€ êµ¬í˜„ ë¡œë“œë§µ

### Phase 1: Meta-Reasoning Core (4ì£¼)
- Universal Query Processor êµ¬í˜„
- Meta-Reasoning Engine ê°œë°œ
- Self-Reflection ë©”ì»¤ë‹ˆì¦˜ í†µí•©

### Phase 2: Semantic Intelligence (4ì£¼)
- Semantic Intent Recognition êµ¬í˜„
- Chain-of-Thought Reasoning ê°œë°œ
- Zero-Shot Adaptive Reasoning í†µí•©

### Phase 3: Dynamic Orchestration (4ì£¼)
- Dynamic Knowledge Orchestrator êµ¬í˜„
- Adaptive Response Generator ê°œë°œ
- Progressive Disclosure System í†µí•©

### Phase 4: Integration & Optimization (2ì£¼)
- Cherry AI í†µí•© ì™„ë£Œ
- Performance Monitoring êµ¬í˜„
- Security & Privacy ì‹œìŠ¤í…œ í†µí•©

ì´ ì„¤ê³„ëŠ” ìš”êµ¬ì‚¬í•­ ëª…ì„¸ì„œì˜ ëª¨ë“  í•µì‹¬ ìš”ì†Œë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” ìƒì„¸í•œ ì•„í‚¤í…ì²˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.