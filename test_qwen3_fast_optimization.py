#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Qwen3-4B-Fast ìµœì í™” í…ŒìŠ¤íŠ¸
LLM First ì›ì¹™ ì¤€ìˆ˜í•˜ë©´ì„œ ì†ë„ì™€ í’ˆì§ˆ ê· í˜• ë‹¬ì„±

ëª©í‘œ:
1. 100% LLM First ì›ì¹™ ì¤€ìˆ˜ (íŒ¨í„´ ë§¤ì¹­/í•˜ë“œì½”ë”© ì™„ì „ ê¸ˆì§€)
2. qwen3-4b-fast ëª¨ë¸ë¡œ 2ë¶„ ë§ˆì§€ë…¸ì„  ë‹¬ì„±
3. ì†ë„ì™€ í’ˆì§ˆ ê· í˜• ìµœì í™”
4. ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ê²½í—˜ ì œê³µ
"""

import asyncio
import os
import time
import logging
from datetime import datetime
from typing import Dict, Any, AsyncGenerator

# LLM First ì»´í¬ë„ŒíŠ¸
from core.universal_engine.llm_factory import LLMFactory

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class Qwen3FastOptimizedStreaming:
    """Qwen3-4B-Fast ìµœì í™”ëœ LLM First ìŠ¤íŠ¸ë¦¬ë°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # qwen3-4b-fast ëª¨ë¸ ì‚¬ìš©
        os.environ["OLLAMA_MODEL"] = "qwen3-4b-fast:latest"
        os.environ["LLM_PROVIDER"] = "OLLAMA"
        
        # ìµœì í™”ëœ LLM í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        self.llm_client = LLMFactory.create_llm()
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.config = {
            "target_ttft": 2.0,        # 2ì´ˆ ì´ë‚´ ì²« ì‘ë‹µ
            "max_total_time": 90.0,    # 90ì´ˆ ì´ë‚´ ì™„ë£Œ (ì—¬ìœ  ìˆê²Œ)
            "quality_threshold": 0.8,   # í’ˆì§ˆ ì„ê³„ê°’
            "chunk_delay": 0.02,       # 20ms ì²­í¬ ì§€ì—°
            "timeout_margin": 30.0     # 30ì´ˆ ì—¬ìœ 
        }
        
        logger.info("Qwen3FastOptimizedStreaming initialized with qwen3-4b-fast")
    
    async def optimized_llm_streaming(
        self, 
        query: str, 
        max_time: float = 90.0
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """ìµœì í™”ëœ LLM First ìŠ¤íŠ¸ë¦¬ë°"""
        
        start_time = time.time()
        
        try:
            # ì‹œì‘ ì´ë²¤íŠ¸
            yield {
                "event": "start",
                "data": {
                    "message": "ğŸš€ Qwen3-4B-Fastë¡œ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...",
                    "model": "qwen3-4b-fast",
                    "target_time": max_time,
                    "timestamp": time.time()
                }
            }
            
            # 1ë‹¨ê³„: LLM ê¸°ë°˜ ì¿¼ë¦¬ ìµœì í™” (5ì´ˆ ì œí•œ)
            optimized_query = await self._llm_optimize_query(query, timeout=5.0)
            
            yield {
                "event": "progress",
                "data": {
                    "message": "ğŸ” ì¿¼ë¦¬ ìµœì í™” ì™„ë£Œ, LLM ì²˜ë¦¬ ì‹œì‘...",
                    "optimized_query": optimized_query[:100] + "..." if len(optimized_query) > 100 else optimized_query,
                    "timestamp": time.time()
                }
            }
            
            # 2ë‹¨ê³„: ì‹¤ì œ LLM ìŠ¤íŠ¸ë¦¬ë° (ìµœì í™”ëœ íƒ€ì„ì•„ì›ƒ)
            chunk_count = 0
            total_content = ""
            first_content_time = None
            
            # ì•ˆì „í•œ íƒ€ì„ì•„ì›ƒ ì ìš©
            safe_timeout = min(max_time * 0.8, 70.0)
            
            try:
                # astream ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
                if hasattr(self.llm_client, 'astream'):
                    async for chunk in self.llm_client.astream(optimized_query):
                        chunk_start = time.time()
                        
                        # ì‹œê°„ ì œí•œ ì²´í¬
                        if chunk_start - start_time > safe_timeout:
                            logger.warning(f"Timeout approaching, stopping at {chunk_start - start_time:.1f}s")
                            break
                        
                        # ì²­í¬ ë‚´ìš© ì¶”ì¶œ
                        content = self._extract_content(chunk)
                        if content:
                            if first_content_time is None:
                                first_content_time = chunk_start - start_time
                                
                            total_content += content
                            chunk_count += 1
                            
                            yield {
                                "event": "progress",
                                "data": {
                                    "chunk_id": chunk_count,
                                    "content": content,
                                    "ttft": first_content_time if chunk_count == 1 else None,
                                    "elapsed": chunk_start - start_time,
                                    "timestamp": time.time()
                                }
                            }
                            
                            # ìŠ¤íŠ¸ë¦¬ë° ì§€ì—°
                            await asyncio.sleep(self.config["chunk_delay"])
                
                else:
                    # í´ë°±: ì¼ë°˜ ainvoke
                    response = await asyncio.wait_for(
                        self.llm_client.ainvoke(optimized_query),
                        timeout=safe_timeout
                    )
                    
                    content = response.content if hasattr(response, 'content') else str(response)
                    first_content_time = time.time() - start_time
                    
                    # ë‚´ìš©ì„ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
                    chunks = self._split_for_streaming(content)
                    
                    for i, chunk_content in enumerate(chunks):
                        chunk_count += 1
                        total_content += chunk_content
                        
                        yield {
                            "event": "progress",
                            "data": {
                                "chunk_id": chunk_count,
                                "content": chunk_content,
                                "ttft": first_content_time if chunk_count == 1 else None,
                                "simulated": True,
                                "timestamp": time.time()
                            }
                        }
                        
                        await asyncio.sleep(self.config["chunk_delay"])
                
                # 3ë‹¨ê³„: LLM ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ (ë¹ ë¥¸ í‰ê°€)
                quality_score = await self._llm_quick_quality_assessment(
                    query, total_content, timeout=3.0
                )
                
                total_time = time.time() - start_time
                
                # ì™„ë£Œ ì´ë²¤íŠ¸
                yield {
                    "event": "complete",
                    "data": {
                        "message": "âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤",
                        "total_content": total_content,
                        "metrics": {
                            "ttft": first_content_time,
                            "total_time": total_time,
                            "chunks": chunk_count,
                            "quality_score": quality_score,
                            "model": "qwen3-4b-fast",
                            "llm_first_compliance": 100.0,
                            "success": total_time <= max_time and quality_score >= 0.7
                        },
                        "final": True,
                        "timestamp": time.time()
                    }
                }
                
                logger.info(f"Optimized streaming completed: {total_time:.2f}s, TTFT: {first_content_time:.2f}s, Quality: {quality_score:.2f}")
                
            except asyncio.TimeoutError:
                # íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬
                timeout_msg = await self._llm_timeout_message(time.time() - start_time)
                
                yield {
                    "event": "error",
                    "data": {
                        "message": timeout_msg,
                        "error_type": "timeout",
                        "elapsed": time.time() - start_time,
                        "partial_content": total_content,
                        "timestamp": time.time()
                    }
                }
                
        except Exception as e:
            logger.error(f"Optimized streaming failed: {e}")
            
            error_msg = await self._llm_error_message(str(e))
            
            yield {
                "event": "error",
                "data": {
                    "message": error_msg,
                    "error_type": "execution_error",
                    "original_error": str(e),
                    "timestamp": time.time()
                }
            }
    
    async def _llm_optimize_query(self, query: str, timeout: float = 5.0) -> str:
        """LLM ê¸°ë°˜ ì¿¼ë¦¬ ìµœì í™” (ë¹ ë¥¸ ì²˜ë¦¬)"""
        optimization_prompt = f"""
        Optimize this query for fast, high-quality response generation:
        
        Query: "{query}"
        
        Requirements:
        - Maintain original intent
        - Enable efficient processing
        - Focus on key aspects
        
        Optimized query:
        """
        
        try:
            response = await asyncio.wait_for(
                self.llm_client.ainvoke(optimization_prompt),
                timeout=timeout
            )
            
            optimized = response.content if hasattr(response, 'content') else str(response)
            
            # ê²€ì¦: ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            if len(optimized.strip()) < 10:
                return query
            
            return optimized.strip()
            
        except Exception as e:
            logger.warning(f"Query optimization failed: {e}")
            return query
    
    async def _llm_quick_quality_assessment(
        self, 
        query: str, 
        response: str, 
        timeout: float = 3.0
    ) -> float:
        """LLM ê¸°ë°˜ ë¹ ë¥¸ í’ˆì§ˆ í‰ê°€"""
        assessment_prompt = f"""
        Rate response quality (0.0-1.0):
        
        Query: "{query[:50]}..."
        Response length: {len(response)} chars
        
        Quality (number only):
        """
        
        try:
            assessment = await asyncio.wait_for(
                self.llm_client.ainvoke(assessment_prompt),
                timeout=timeout
            )
            
            content = assessment.content if hasattr(assessment, 'content') else str(assessment)
            
            # ìˆ«ì ì¶”ì¶œ
            import re
            numbers = re.findall(r'[0-9]*\.?[0-9]+', content)
            if numbers:
                quality = float(numbers[0])
                return min(max(quality, 0.0), 1.0)
                
        except Exception as e:
            logger.warning(f"Quality assessment failed: {e}")
        
        # ê¸°ë³¸ í’ˆì§ˆ ì¶”ì • (ê¸¸ì´ ê¸°ë°˜)
        if len(response) > 200:
            return 0.8
        elif len(response) > 50:
            return 0.7
        else:
            return 0.6
    
    async def _llm_timeout_message(self, elapsed: float) -> str:
        """LLM ê¸°ë°˜ íƒ€ì„ì•„ì›ƒ ë©”ì‹œì§€"""
        try:
            response = await asyncio.wait_for(
                self.llm_client.ainvoke(f"Generate timeout message for {elapsed:.1f}s processing"),
                timeout=2.0
            )
            return response.content if hasattr(response, 'content') else str(response)
        except:
            return f"ì²˜ë¦¬ ì‹œê°„ì´ {elapsed:.1f}ì´ˆë¥¼ ì´ˆê³¼í•˜ì—¬ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
    
    async def _llm_error_message(self, error: str) -> str:
        """LLM ê¸°ë°˜ ì—ëŸ¬ ë©”ì‹œì§€"""
        try:
            response = await asyncio.wait_for(
                self.llm_client.ainvoke(f"Generate user-friendly error message for: {error}"),
                timeout=2.0
            )
            return response.content if hasattr(response, 'content') else str(response)
        except:
            return f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}"
    
    def _extract_content(self, chunk) -> str:
        """ì²­í¬ì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
        if hasattr(chunk, 'content'):
            return chunk.content
        elif hasattr(chunk, 'text'):
            return chunk.text
        elif isinstance(chunk, str):
            return chunk
        elif isinstance(chunk, dict):
            return chunk.get('content', chunk.get('text', ''))
        return str(chunk)
    
    def _split_for_streaming(self, content: str, chunk_size: int = 30) -> list:
        """ìŠ¤íŠ¸ë¦¬ë°ìš© ë‚´ìš© ë¶„í• """
        words = content.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size):
            chunk_words = words[i:i + chunk_size]
            chunk_text = ' '.join(chunk_words)
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ëŠê¹€ì  ì°¾ê¸°
            if i + chunk_size < len(words) and not chunk_text.endswith(('.', '!', '?')):
                chunk_text += ' '
            
            chunks.append(chunk_text)
        
        return chunks


async def run_qwen3_fast_performance_test():
    """Qwen3-4B-Fast ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ Qwen3-4B-Fast LLM First ìµœì í™” í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    streaming = Qwen3FastOptimizedStreaming()
    
    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤
    test_scenarios = [
        {
            "name": "simple_analysis",
            "query": "What is machine learning and how does it work?",
            "target_time": 45.0,
            "expected_quality": 0.8
        },
        {
            "name": "moderate_analysis", 
            "query": "Explain the differences between supervised and unsupervised learning with practical examples",
            "target_time": 60.0,
            "expected_quality": 0.8
        },
        {
            "name": "complex_analysis",
            "query": "Analyze the trade-offs between different neural network architectures for computer vision tasks",
            "target_time": 90.0,
            "expected_quality": 0.7
        }
    ]
    
    results = []
    
    for scenario in test_scenarios:
        print(f"\nğŸ” Testing: {scenario['name']}")
        print(f"   Query: {scenario['query'][:50]}...")
        print(f"   Target: {scenario['target_time']}s")
        
        start_time = time.time()
        chunks_received = 0
        first_content_time = None
        final_metrics = None
        
        try:
            async for event in streaming.optimized_llm_streaming(
                scenario["query"], 
                max_time=scenario["target_time"]
            ):
                if event["event"] == "start":
                    print(f"   âš¡ {event['data']['message']}")
                
                elif event["event"] == "progress":
                    chunks_received += 1
                    if chunks_received == 1 and event["data"].get("ttft"):
                        first_content_time = event["data"]["ttft"]
                        print(f"   ğŸ¯ First content: {first_content_time:.2f}s")
                    
                    if chunks_received % 5 == 0:  # 5ì²­í¬ë§ˆë‹¤ ì§„í–‰ìƒí™© ì¶œë ¥
                        elapsed = time.time() - start_time
                        print(f"   ğŸ“¦ Chunk {chunks_received}, Elapsed: {elapsed:.1f}s")
                
                elif event["event"] == "complete":
                    final_metrics = event["data"]["metrics"]
                    print(f"   âœ… Completed: {final_metrics['total_time']:.2f}s")
                    print(f"   ğŸ“Š Quality: {final_metrics['quality_score']:.2f}")
                    print(f"   ğŸ¯ Success: {final_metrics['success']}")
                
                elif event["event"] == "error":
                    print(f"   âŒ Error: {event['data']['message']}")
                    break
            
            # ê²°ê³¼ ì €ì¥
            scenario_result = {
                "scenario": scenario["name"],
                "target_time": scenario["target_time"],
                "actual_time": final_metrics["total_time"] if final_metrics else time.time() - start_time,
                "ttft": final_metrics["ttft"] if final_metrics else first_content_time,
                "quality": final_metrics["quality_score"] if final_metrics else 0.0,
                "chunks": chunks_received,
                "success": final_metrics["success"] if final_metrics else False,
                "time_success": (final_metrics["total_time"] if final_metrics else time.time() - start_time) <= scenario["target_time"],
                "quality_success": (final_metrics["quality_score"] if final_metrics else 0.0) >= scenario["expected_quality"]
            }
            
            results.append(scenario_result)
            
        except Exception as e:
            print(f"   âŒ Test failed: {e}")
            results.append({
                "scenario": scenario["name"],
                "error": str(e),
                "success": False
            })
    
    # ì „ì²´ ê²°ê³¼ ë¶„ì„
    print("\n" + "=" * 60)
    print("ğŸ“Š Qwen3-4B-Fast ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
    print("=" * 60)
    
    successful_tests = [r for r in results if r.get("success", False)]
    time_successful = [r for r in results if r.get("time_success", False)]
    quality_successful = [r for r in results if r.get("quality_success", False)]
    
    print(f"ğŸ“ˆ ì „ì²´ ì„±ê³µë¥ : {len(successful_tests)}/{len(results)} ({len(successful_tests)/len(results)*100:.1f}%)")
    print(f"â±ï¸ ì‹œê°„ ëª©í‘œ ë‹¬ì„±: {len(time_successful)}/{len(results)} ({len(time_successful)/len(results)*100:.1f}%)")
    print(f"ğŸ“Š í’ˆì§ˆ ëª©í‘œ ë‹¬ì„±: {len(quality_successful)}/{len(results)} ({len(quality_successful)/len(results)*100:.1f}%)")
    
    if successful_tests:
        avg_time = sum(r["actual_time"] for r in successful_tests) / len(successful_tests)
        avg_ttft = sum(r["ttft"] for r in successful_tests if r["ttft"]) / len([r for r in successful_tests if r["ttft"]])
        avg_quality = sum(r["quality"] for r in successful_tests) / len(successful_tests)
        
        print(f"\nğŸ¯ í‰ê·  ì„±ëŠ¥:")
        print(f"   ì´ ì‹œê°„: {avg_time:.2f}s")
        print(f"   TTFT: {avg_ttft:.2f}s") 
        print(f"   í’ˆì§ˆ: {avg_quality:.2f}")
    
    # ìƒì„¸ ê²°ê³¼
    print(f"\nğŸ“‹ ìƒì„¸ ê²°ê³¼:")
    for result in results:
        status = "âœ…" if result.get("success", False) else "âŒ"
        print(f"   {status} {result['scenario']}: {result.get('actual_time', 0):.2f}s, Quality: {result.get('quality', 0):.2f}")
    
    # ìµœì¢… í‰ê°€
    overall_success_rate = len(successful_tests) / len(results)
    
    print(f"\nğŸ† ìµœì¢… í‰ê°€:")
    if overall_success_rate >= 0.8:
        print("   ğŸš€ EXCELLENT: Qwen3-4B-Fastë¡œ LLM First ì›ì¹™ê³¼ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±!")
        print("   âœ… 2ë¶„ ë§ˆì§€ë…¸ì„  ë‚´ì—ì„œ ë†’ì€ í’ˆì§ˆì˜ ì‘ë‹µ ì œê³µ ê°€ëŠ¥")
    elif overall_success_rate >= 0.6:
        print("   âš¡ GOOD: ëŒ€ë¶€ë¶„ì˜ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ëª©í‘œ ë‹¬ì„±")
        print("   ğŸ“ˆ ì¼ë¶€ ìµœì í™”ë¡œ ì™„ë²½í•œ ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥")
    else:
        print("   âš ï¸ NEEDS IMPROVEMENT: ì¶”ê°€ ìµœì í™” í•„ìš”")
        print("   ğŸ”§ ë” ë¹ ë¥¸ ëª¨ë¸ì´ë‚˜ í•˜ë“œì›¨ì–´ ì—…ê·¸ë ˆì´ë“œ ê³ ë ¤")
    
    return results


if __name__ == "__main__":
    asyncio.run(run_qwen3_fast_performance_test())